Sure! Here's a reading list of 20 articles on the interpretability of NLP models, up to the year 2020:

1. **"Attention is not Explanation"** - Jain, S., & Wallace, B. C. (2019)
   - This paper questions the interpretability of attention mechanisms in neural models.

2. **"A Survey of Methods for Explaining Black Box Models"** - Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018)
   - A comprehensive survey of methods for explaining black-box models, including those used in NLP.

3. **"Interpreting and Explaining Deep Models in NLP: A Roadmap"** - Belinkov, Y., & Glass, J. (2019)
   - A roadmap for interpreting and explaining deep learning models in NLP.

4. **"LIME: Local Interpretable Model-agnostic Explanations"** - Ribeiro, M. T., Singh, S., & Guestrin, C. (2016)
   - Introduces LIME, a technique for explaining the predictions of any classifier.

5. **"Anchors: High-Precision Model-Agnostic Explanations"** - Ribeiro, M. T., Singh, S., & Guestrin, C. (2018)
   - Proposes Anchors, a method for generating high-precision explanations.

6. **"Rationalizing Neural Predictions"** - Lei, T., Barzilay, R., & Jaakkola, T. (2016)
   - Introduces a framework for rationalizing neural network predictions in NLP.

7. **"Learning to Explain: An Information-Theoretic Perspective on Model Interpretation"** - Chen, J., Song, L., Wainwright, M. J., & Jordan, M. I. (2018)
   - Discusses an information-theoretic approach to model interpretation.

8. **"Visualizing and Understanding Neural Models in NLP"** - Li, J., Chen, X., Hovy, E., & Jurafsky, D. (2016)
   - Techniques for visualizing and understanding neural models in NLP.

9. **"A Unified Approach to Interpreting Model Predictions"** - Lundberg, S. M., & Lee, S.-I. (2017)
   - Introduces SHAP (SHapley Additive exPlanations), a unified approach to interpreting model predictions.

10. **"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"** - Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., & Sayres, R. (2018)
    - Proposes TCAV for testing model interpretability beyond feature attribution.

11. **"Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsers"** - Belinkov, Y., Durrani, N., Dalvi, F., Sajjad, H., & Glass, J. (2017)
    - Evaluates the robustness and interpretability of neural network-based dependency parsers.

12. **"Interpretability of Neural Networks: A Survey"** - Montavon, G., Samek, W., & MÃ¼ller, K.-R. (2018)
    - A survey on the interpretability of neural networks, including applications in NLP.

13. **"Towards a Rigorous Science of Interpretable Machine Learning"** - Doshi-Velez, F., & Kim, B. (2017)
    - Discusses the need for a rigorous science of interpretable machine learning.

14. **"On the Importance of Single Directions for Generalization"** - Morcos, A. S., Barrett, D. G. T., Rabinowitz, N. C., & Botvinick, M. (2018)
    - Examines the role of single directions in neural network generalization and interpretability.

15. **"Contextual Decomposition for Neural Network Interpretability"** - Murdoch, W. J., & Szlam, A. (2017)
    - Proposes contextual decomposition for interpreting neural networks.

16. **"Interpretability and Explainability in Artificial Intelligence"** - Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., & Kagal, L. (2018)
    - Discusses various aspects of interpretability and explainability in AI, including NLP.

17. **"Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions"** - Koh, P. W., & Liang, P. (2017)
    - Introduces influence functions for explaining black-box predictions.

18. **"Deep Learning for Natural Language Processing: Advancements and Trends"** - Young, T., Hazarika, D., Poria, S., & Cambria, E. (2018)
    - Reviews advancements and trends in deep learning for NLP, including interpretability.

19. **"The Mythos of Model Interpretability"** - Lipton, Z. C. (2016)
    - Discusses the challenges and misconceptions surrounding model interpretability.

20. **"Towards Transparent and Explainable Attention Models"** - Wiegreffe, S., & Pinter, Y. (2019)
    - Investigates the transparency and explainability of attention models in NLP.

These articles cover a range of approaches and perspectives on the interpretability of NLP models, providing a solid foundation for understanding this important area of research.