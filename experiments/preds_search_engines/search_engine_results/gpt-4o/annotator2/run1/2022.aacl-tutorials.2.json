[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here's a list of 20 influential articles on pre-trained language models (PLMs) up to 2022. These papers cover a range of topics including model architectures, training techniques, and applications"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Jacob Devlin, Ming-Wei Chang"
    ],
    "location": [
      "Kenton Lee"
    ],
    "publisher": [
      "Kristina Toutanova"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [BERT](https://arxiv.org/abs/1810.04805"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**GPT-2: Language Models are Unsupervised Multitask Learners**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Alec Radford, Jeffrey Wu, Rewon Child"
    ],
    "location": [
      "David Luan"
    ],
    "publisher": [
      "Dario Amodei, Ilya Sutskever"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**GPT-3: Language Models are Few-Shot Learners**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Tom B.Brown"
      },
      {
        "family": "Mann",
        "given": "Benjamin"
      },
      {
        "family": "Ryder",
        "given": "Nick"
      },
      {
        "others": true
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [GPT-3](https://arxiv.org/abs/2005.14165"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**RoBERTa: A Robustly Optimized BERT Pretraining Approach**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Yinhan Liu, Myle Ott, Naman Goyal, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [RoBERTa](https://arxiv.org/abs/1907.11692"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**XLNet: Generalized Autoregressive Pretraining for Language Understanding**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Zhilin Yang, Zihang Dai, Yiming Yang, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [XLNet](https://arxiv.org/abs/1906.08237"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**ALBERT: A Lite BERT for Self-supervised Learning of Language Representations**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Zhenzhong Lan"
    ],
    "editor": [
      {
        "family": "Chen",
        "given": "Mingda"
      },
      {
        "family": "Goodman",
        "given": "Sebastian"
      },
      {
        "others": true
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [ALBERT](https://arxiv.org/abs/1909.11942"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Colin Raffel, Noam Shazeer, Adam Roberts, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [T5](https://arxiv.org/abs/1910.10683"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "given": "E.L.E.C.T.R.A."
      }
    ],
    "title": [
      "Pre-training Text Encoders as Discriminators Rather Than Generators**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Kevin Clark"
      },
      {
        "family": "Luong",
        "given": "Minh-Thang"
      },
      {
        "family": "Le",
        "given": "Quoc V."
      },
      {
        "family": "Manning",
        "given": "Christopher D."
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [ELECTRA](https://arxiv.org/abs/2003.10555"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "given": "E.R.N.I.E."
      }
    ],
    "title": [
      "Enhanced Language Representation with Informative Entities**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Yu Sun, Shuohuan Wang, Yukun Li, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [ERNIE](https://arxiv.org/abs/1905.07129"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**DistilBERT: A Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Victor Sanh, Lysandre Debut"
    ],
    "publisher": [
      "Julien Chaumond, Thomas Wolf"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [DistilBERT](https://arxiv.org/abs/1910.01108"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**SpanBERT: Improving Pre-training by Representing and Predicting Spans**"
    ],
    "type": null
  },
  {
    "note": [
      "- Authors: Mandar Joshi, Danqi Chen, Yinhan Liu, et al."
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [SpanBERT](https://arxiv.org/abs/1907.10529"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "given": "B.A.R.T."
      }
    ],
    "title": [
      "Denoising Sequence-to-Sequence Pre-training for Natural Language Generation"
    ],
    "translator": [
      {
        "given": "Comprehension"
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Mike Lewis"
      },
      {
        "family": "Liu",
        "given": "Yinhan"
      },
      {
        "family": "Goyal",
        "given": "Naman"
      },
      {
        "others": true
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [BART](https://arxiv.org/abs/1910.13461"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Mohammad Shoeybi, Mostofa Patwary, Raul Puri, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [Megatron-LM](https://arxiv.org/abs/1909.08053"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "note": [
      "**DeBERTa: Decoding-enhanced BERT with Disentangled Attention**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [DeBERTa](https://arxiv.org/abs/2006.03654"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**ERNIE 2.0: A Continual Pre-training Framework for Language Understanding**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Yu Sun, Shuohuan Wang, Yukun Li, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [ERNIE 2.0](https://arxiv.org/abs/1907.12412"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "location": [
      "Reformer"
    ],
    "publisher": [
      "The Efficient Transformer**"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [Reformer](https://arxiv.org/abs/2001.04451"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**Big Bird: Transformers for Longer Sequences**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Manzil Zaheer, Guru Guruganesh, Avinava Dubey, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [Big Bird](https://arxiv.org/abs/2007.14062"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity**"
    ],
    "type": null
  },
  {
    "citation-number": [
      "-"
    ],
    "location": [
      "Authors"
    ],
    "publisher": [
      "William Fedus, Barret Zoph, Noam Shazeer"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [Switch Transformers](https://arxiv.org/abs/2101.03961"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**Turing-NLG: A 17-billion-parameter Language Model by Microsoft**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Adam Eversole, Sameer Bansal, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [Turing-NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Noam Shazeer, Youlong Cheng, Niki Parmar, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [GShard](https://arxiv.org/abs/2006.16668"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the development and advancements in pre-trained language models up to"
    ],
    "date": [
      "2022"
    ],
    "type": null
  }
]
