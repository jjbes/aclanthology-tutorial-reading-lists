[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles on the interpretability of NLP models, up to the year 2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "literal": "**\"Attention is not Explanation\"** - Jain, S., & Wallace, B. C."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper questions the interpretability of attention mechanisms in neural models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"A Survey of Methods for Explaining Black Box Models\"**"
    ],
    "author": [
      {
        "family": "Guidotti",
        "given": "R."
      },
      {
        "family": "Monreale",
        "given": "A."
      },
      {
        "family": "Ruggieri",
        "given": "S."
      },
      {
        "family": "Turini",
        "given": "F."
      },
      {
        "family": "Giannotti",
        "given": "F."
      },
      {
        "family": "Pedreschi",
        "given": "D."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A comprehensive survey of methods for explaining black-box models, including those used in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Interpreting and Explaining Deep Models in NLP: A Roadmap\"**"
    ],
    "editor": [
      {
        "family": "Belinkov",
        "given": "Y."
      },
      {
        "family": "Glass",
        "given": "J."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A roadmap for interpreting and explaining deep learning models in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "given": "L.I.M.E."
      }
    ],
    "title": [
      "Local Interpretable Model-agnostic Explanations\"**"
    ],
    "editor": [
      {
        "family": "Ribeiro",
        "given": "M.T."
      },
      {
        "family": "Singh",
        "given": "S."
      },
      {
        "family": "Guestrin",
        "given": "C."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces LIME, a technique for explaining the predictions of any classifier"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "literal": "**\"Anchors: High-Precision Model-Agnostic Explanations\"** - Ribeiro, M. T., Singh, S., & Guestrin, C."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes Anchors, a method for generating high-precision explanations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "literal": "**\"Rationalizing Neural Predictions\"** - Lei, T., Barzilay, R., & Jaakkola, T."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a framework for rationalizing neural network predictions in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Learning to Explain: An Information-Theoretic Perspective on Model Interpretation\"**"
    ],
    "editor": [
      {
        "family": "Chen",
        "given": "J."
      },
      {
        "family": "Song",
        "given": "L."
      },
      {
        "family": "Wainwright",
        "given": "M.J."
      },
      {
        "family": "Jordan",
        "given": "M.I."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses an information-theoretic approach to model interpretation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "literal": "**\"Visualizing and Understanding Neural Models in NLP\"** - Li, J., Chen, X., Hovy, E., & Jurafsky, D."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Techniques for visualizing and understanding neural models in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"A Unified Approach to Interpreting Model Predictions\"**"
    ],
    "editor": [
      {
        "family": "Lundberg",
        "given": "S.M."
      }
    ],
    "location": [
      "Lee, S.-I"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces SHAP (SHapley Additive exPlanations), a unified approach to interpreting model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)\"**"
    ],
    "editor": [
      {
        "family": "Kim",
        "given": "B."
      },
      {
        "family": "Wattenberg",
        "given": "M."
      },
      {
        "family": "Gilmer",
        "given": "J."
      },
      {
        "family": "Cai",
        "given": "C."
      },
      {
        "family": "Wexler",
        "given": "J."
      },
      {
        "family": "Viegas",
        "given": "F."
      },
      {
        "family": "Sayres",
        "given": "R."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes TCAV for testing model interpretability beyond feature attribution"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsers\"**"
    ],
    "director": [
      {
        "family": "Belinkov",
        "given": "Y."
      },
      {
        "family": "Durrani",
        "given": "N."
      },
      {
        "family": "Dalvi",
        "given": "F."
      },
      {
        "family": "Sajjad",
        "given": "H."
      },
      {
        "family": "Glass",
        "given": "J."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Evaluates the robustness and interpretability of neural network-based dependency parsers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Interpretability of Neural Networks: A Survey\"**"
    ],
    "editor": [
      {
        "family": "Montavon",
        "given": "G."
      },
      {
        "family": "Samek",
        "given": "W."
      },
      {
        "family": "MÃ¼ller",
        "given": "K.-R."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A survey on the interpretability of neural networks, including applications in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Towards a Rigorous Science of Interpretable Machine Learning\"**"
    ],
    "editor": [
      {
        "family": "Doshi-Velez",
        "given": "F."
      },
      {
        "family": "Kim",
        "given": "B."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the need for a rigorous science of interpretable machine learning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"On the Importance of Single Directions for Generalization\"**"
    ],
    "editor": [
      {
        "family": "Morcos",
        "given": "A.S."
      },
      {
        "family": "Barrett",
        "given": "D.G.T."
      },
      {
        "family": "Rabinowitz",
        "given": "N.C."
      },
      {
        "family": "Botvinick",
        "given": "M."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines the role of single directions in neural network generalization and interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Contextual Decomposition for Neural Network Interpretability\"**"
    ],
    "editor": [
      {
        "family": "Murdoch",
        "given": "W.J."
      },
      {
        "family": "Szlam",
        "given": "A."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes contextual decomposition for interpreting neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "literal": "**\"Interpretability and Explainability in Artificial Intelligence\"** - Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., & Kagal, L."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses various aspects of interpretability and explainability in AI, including NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions\"**"
    ],
    "editor": [
      {
        "family": "Koh",
        "given": "P.W."
      },
      {
        "family": "Liang",
        "given": "P."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces influence functions for explaining black-box predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Deep Learning for Natural Language Processing: Advancements and Trends\"**"
    ],
    "editor": [
      {
        "family": "Young",
        "given": "T."
      },
      {
        "family": "Hazarika",
        "given": "D."
      },
      {
        "family": "Poria",
        "given": "S."
      },
      {
        "family": "Cambria",
        "given": "E."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Reviews advancements and trends in deep learning for NLP, including interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "literal": "**\"The Mythos of Model Interpretability\"** - Lipton, Z. C."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the challenges and misconceptions surrounding model interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Towards Transparent and Explainable Attention Models\"**"
    ],
    "editor": [
      {
        "family": "Wiegreffe",
        "given": "S."
      },
      {
        "family": "Pinter",
        "given": "Y."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the transparency and explainability of attention models in NLP"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of approaches and perspectives on the interpretability of NLP models, providing a solid foundation for understanding this important area of research"
    ],
    "type": null
  }
]
