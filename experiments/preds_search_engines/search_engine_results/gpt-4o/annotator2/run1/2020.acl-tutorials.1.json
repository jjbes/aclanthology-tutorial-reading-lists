[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a curated list of 20 articles up to 2020 that focus on the interpretability and analysis of neural network models in Natural Language Processing (NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "literal": "**\"Attention is All You Need\"** - Vaswani, A., et al."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces the Transformer model, which uses self-attention mechanisms and has significant implications for interpretability in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Visualizing and Understanding Neural Models in NLP\"**"
    ],
    "editor": [
      {
        "family": "Li",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses methods for visualizing and interpreting neural network models in NLP, including saliency maps and attention mechanisms"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "literal": "**\"Rationalizing Neural Predictions\"** - Lei, T., et al."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a method for generating rationales (subsets of input text) that justify the predictions made by neural models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "literal": "**\"Interpreting and Understanding Bert\"** - Clark, K., et al."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analyzes the BERT model to understand what it learns and how it makes decisions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\"**"
    ],
    "editor": [
      {
        "family": "Serban",
        "given": "I.V."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores interpretability in the context of dialogue generation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Learning to Explain: An Information-Theoretic Perspective on Model Interpretation\"**"
    ],
    "editor": [
      {
        "family": "Chen",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces an information-theoretic framework for model interpretation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "literal": "**\"Attention is not Explanation\"** - Jain, S., & Wallace, B. C."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Critically examines the use of attention mechanisms as explanations for model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Pathologies of Neural Models Make Interpretations Difficult\"**"
    ],
    "editor": [
      {
        "family": "Serrano",
        "given": "S."
      },
      {
        "given": "Smith"
      }
    ],
    "volume": [
      "N. A"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses challenges in interpreting neural models due to their complex and sometimes pathological behaviors"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Anchors: High-Precision Model-Agnostic Explanations\"**"
    ],
    "editor": [
      {
        "family": "Ribeiro",
        "given": "M.T."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents a method for generating high-precision, model-agnostic explanations for predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "given": "L.I.M.E."
      }
    ],
    "title": [
      "Local Interpretable Model-Agnostic Explanations\"**"
    ],
    "editor": [
      {
        "family": "Ribeiro",
        "given": "M.T."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces LIME, a technique for explaining the predictions of any classifier in an interpretable and faithful manner"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions\"**"
    ],
    "editor": [
      {
        "family": "Koh",
        "given": "P.W."
      },
      {
        "family": "Liang",
        "given": "P."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Uses influence functions to understand the impact of training data on model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Towards a Rigorous Science of Interpretable Machine Learning\"**"
    ],
    "editor": [
      {
        "family": "Doshi-Velez",
        "given": "F."
      },
      {
        "family": "Kim",
        "given": "B."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Provides a comprehensive review and framework for interpretability in machine learning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "family": "at?",
        "given": "What Does B.E.R.T.Look"
      }
    ],
    "title": [
      "An Analysis of BERT's Attention\"**"
    ],
    "editor": [
      {
        "family": "Clark",
        "given": "K."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analyzes the attention heads in BERT to understand what the model focuses on during different tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Evaluating and Understanding the Robustness of Dialogue Systems: A Case Study on a Negotiation Agent\"**"
    ],
    "editor": [
      {
        "family": "He",
        "given": "H."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the robustness and interpretability of dialogue systems"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Deep Learning for NLP and Speech Recognition\"**"
    ],
    "editor": [
      {
        "family": "Deng",
        "given": "L."
      },
      {
        "family": "Liu",
        "given": "Y."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A broad overview of deep learning techniques in NLP and speech recognition, with discussions on interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)\"**"
    ],
    "editor": [
      {
        "family": "Kim",
        "given": "B."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces TCAV, a method for testing the influence of high-level concepts on model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"On the Importance of Single Directions for Generalization\"**"
    ],
    "editor": [
      {
        "family": "Morcos",
        "given": "A.S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the role of individual neurons and directions in the generalization ability of neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"A Survey of Methods for Explaining Black Box Models\"**"
    ],
    "editor": [
      {
        "family": "Guidotti",
        "given": "R."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A comprehensive survey of methods for explaining black-box models, including those used in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"The Building Blocks of Interpretability\"**"
    ],
    "editor": [
      {
        "family": "Olah",
        "given": "C."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores the fundamental components that contribute to the interpretability of neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Analyzing Neural Language Models: Contextual Decomposition Reveals Default Reasoning in Sentence Processing\"**"
    ],
    "editor": [
      {
        "family": "Jumelet",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Uses contextual decomposition to analyze and interpret the reasoning processes of neural language models"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of techniques and perspectives on interpretability in neural network models for NLP, providing a solid foundation for understanding the current state of research in this area up to"
    ],
    "date": [
      "2020"
    ],
    "type": null
  }
]
