[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here is a curated list of 20 articles up to 2023 that focus on security concerns associated with NLP models, specifically addressing backdoor attacks, private data leakage, and imitation attacks"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Attacks",
        "given": "Backdoor"
      }
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Backdoor Attacks on Deep Learning Models\"**"
    ],
    "type": null
  },
  {
    "title": [
      "Authors: Eugene Bagdasaryan, Andreas Veit"
    ],
    "publisher": [
      "Yiqing Hua, Deborah Estrin, Vitaly Shmatikov"
    ],
    "type": "book"
  },
  {
    "container-title": [
      "Conference: NeurIPS 2020"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: This paper explores the vulnerabilities of deep learning models, including NLP models, to backdoor attacks and proposes methods to detect and mitigate such threats"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg"
    ],
    "type": "book"
  },
  {
    "note": [
      "Conference: arXiv preprint arXiv:1708.06733,"
    ],
    "date": [
      "2017"
    ],
    "arxiv": [
      "1708.06733"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Introduces the concept of BadNets, a type of backdoor attack, and discusses their implications for NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "container-title": [
      "**\"Trojaning Attack on Neural Networks\"**"
    ],
    "type": "chapter"
  },
  {
    "title": [
      "Authors: Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C"
    ],
    "publisher": [
      "Ranasinghe"
    ],
    "location": [
      "Surya Nepal"
    ],
    "type": "book"
  },
  {
    "container-title": [
      "Conference: NDSS 2020"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Discusses Trojan attacks on neural networks, including NLP models, and presents techniques for embedding and detecting backdoors"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "publisher": [
      "**\"Neural Trojans\"**"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Tom Goldstein"
      },
      {
        "family": "Duan",
        "given": "Shiyuan"
      },
      {
        "family": "Wang",
        "given": "Shiyu"
      },
      {
        "family": "Singh",
        "given": "Ajit"
      },
      {
        "family": "Gupta",
        "given": "Arjun"
      }
    ],
    "type": null
  },
  {
    "note": [
      "Conference: arXiv preprint arXiv:1906.01736,"
    ],
    "date": [
      "2019"
    ],
    "arxiv": [
      "1906.01736"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Investigates the insertion of backdoors into neural networks and the specific challenges posed by NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "publisher": [
      "**\"Can You Really Backdoor Federated Learning?\"**"
    ],
    "type": "book"
  },
  {
    "title": [
      "Authors: Xinyang Zhang, Lingjuan Lyu"
    ],
    "publisher": [
      "Bo Li, Qiang Yang"
    ],
    "type": "book"
  },
  {
    "note": [
      "Conference: arXiv preprint arXiv:1911.07963,"
    ],
    "date": [
      "2019"
    ],
    "arxiv": [
      "1911.07963"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Examines the feasibility of backdoor attacks in federated learning settings, which are increasingly used for NLP tasks"
    ],
    "type": null
  },
  {
    "container-title": [
      "### Private Data Leakage"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Membership Inference Attacks Against Machine Learning Models\"**"
    ],
    "type": null
  },
  {
    "title": [
      "Authors: Reza Shokri, Marco Stronati"
    ],
    "publisher": [
      "Congzheng Song, Vitaly Shmatikov"
    ],
    "type": "book"
  },
  {
    "container-title": [
      "Conference: IEEE Symposium on Security and Privacy (SP"
    ],
    "date": [
      "2017"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Discusses membership inference attacks, which can lead to private data leakage in NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Auditing Data Provenance in Text Generation Models\"**"
    ],
    "type": null
  },
  {
    "note": [
      "Authors: Eric Wallace, Mitchell Stern, Dawn Song"
    ],
    "type": null
  },
  {
    "container-title": [
      "Conference: EMNLP 2020"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Investigates how text generation models can inadvertently reveal training data, leading to privacy concerns"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Privacy Risks of Securing NLP Models via Adversarial Training\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Florian Tramèr, Dan Boneh"
    ],
    "type": "book"
  },
  {
    "container-title": [
      "Conference: ACL 2020"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Explores the privacy risks associated with adversarial training techniques used to secure NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Exploiting Unintended Feature Leakage in Collaborative Learning\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Milad Nasr, Reza Shokri, Amir Houmansadr"
    ],
    "type": "book"
  },
  {
    "container-title": [
      "Conference: IEEE Symposium on Security and Privacy (SP"
    ],
    "date": [
      "2019"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Discusses how collaborative learning frameworks for NLP can lead to unintended feature leakage"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "container-title": [
      "**\"Data Poisoning Attacks on Federated Machine Learning\"**"
    ],
    "type": "chapter"
  },
  {
    "title": [
      "Authors: Eugene Bagdasaryan, Andreas Veit"
    ],
    "publisher": [
      "Yiqing Hua, Deborah Estrin, Vitaly Shmatikov"
    ],
    "type": "book"
  },
  {
    "container-title": [
      "Conference: NeurIPS 2020"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Explores data poisoning attacks in federated learning, with implications for NLP models and data privacy"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Attacks",
        "given": "Imitation"
      }
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Stealing Machine Learning Models via Prediction APIs\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Florian Tramèr"
      },
      {
        "family": "Zhang",
        "given": "Fan"
      },
      {
        "family": "Juels",
        "given": "Ari"
      },
      {
        "family": "Reiter",
        "given": "Michael K."
      }
    ],
    "title": [
      "Thomas Ristenpart"
    ],
    "type": null
  },
  {
    "container-title": [
      "Conference: USENIX Security Symposium"
    ],
    "date": [
      "2016"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Discusses model extraction attacks, which can be used to steal NLP models via prediction APIs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Model Extraction and Active Learning\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Florian Tramèr, Dan Boneh"
    ],
    "type": "book"
  },
  {
    "note": [
      "Conference: arXiv preprint arXiv:2007.06776,"
    ],
    "date": [
      "2020"
    ],
    "arxiv": [
      "2007.06776"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Investigates the use of active learning techniques to perform model extraction attacks on NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Knockoff Nets: Stealing Functionality of Black-Box Models\"**"
    ],
    "type": null
  },
  {
    "note": [
      "Authors: Lingjuan Lyu, Huili Chen, Jinyuan Jia, Bo Li"
    ],
    "type": null
  },
  {
    "container-title": [
      "Conference: CVPR 2020"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Explores techniques for imitating the functionality of black-box models, including NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "container-title": [
      "**\"Model Extraction Attacks on NLP APIs\"**"
    ],
    "type": "chapter"
  },
  {
    "title": [
      "Authors: Robin Jia, Aditi Raghunathan"
    ],
    "publisher": [
      "Kerem Göksel, Percy Liang"
    ],
    "type": "book"
  },
  {
    "container-title": [
      "Conference: ACL 2019"
    ],
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "given": "Summary"
      }
    ],
    "title": [
      "Focuses on model extraction attacks specifically targeting NLP APIs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Thieves on Sesame Street! Model Extraction of BERT-based APIs\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Mathias Payer, Florian Tramèr, Dan Boneh"
    ],
    "type": "book"
  },
  {
    "note": [
      "Conference: arXiv preprint arXiv:2004.08947,"
    ],
    "date": [
      "2020"
    ],
    "arxiv": [
      "2004.08947"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Investigates the vulnerability of BERT-based NLP models to extraction attacks"
    ],
    "type": null
  },
  {
    "container-title": [
      "### General Security Concerns"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "16."
    ],
    "publisher": [
      "**\"Adversarial Examples Are Not Bugs, They Are Features\"**"
    ],
    "type": "book"
  },
  {
    "title": [
      "Authors: Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras"
    ],
    "publisher": [
      "Logan Engstrom, Brandon Tran, Aleksander Madry"
    ],
    "type": "book"
  },
  {
    "container-title": [
      "Conference: NeurIPS 2019"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Discusses adversarial examples in machine learning, with implications for NLP model security"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review\"**"
    ],
    "type": null
  },
  {
    "title": [
      "Authors: Jinfeng Yi, Tong Zhang, et al"
    ],
    "type": null
  },
  {
    "note": [
      "Conference: arXiv preprint arXiv:2003.00653,"
    ],
    "date": [
      "2020"
    ],
    "arxiv": [
      "2003.00653"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Provides a comprehensive review of adversarial attacks and defenses across different domains, including NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Mitigating Adversarial Effects Through Randomization\"**"
    ],
    "type": null
  },
  {
    "title": [
      "Authors: Anish Athalye"
    ],
    "editor": [
      {
        "family": "Engstrom",
        "given": "Logan"
      },
      {
        "family": "Ilyas",
        "given": "Andrew"
      },
      {
        "family": "Kwok",
        "given": "Kevin"
      }
    ],
    "type": null
  },
  {
    "container-title": [
      "Conference: ICLR 2018"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Explores randomization techniques to mitigate adversarial attacks, relevant for NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "publisher": [
      "**\"Adversarial Training for Free!\"**"
    ],
    "type": "book"
  },
  {
    "note": [
      "Authors: Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, Quanquan Gu"
    ],
    "type": null
  },
  {
    "container-title": [
      "Conference: NeurIPS 2019"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "Summary: Proposes efficient adversarial training methods to enhance the security of NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"A Survey of Adversarial Machine Learning in Text Classification\"**"
    ],
    "type": null
  },
  {
    "note": [
      "Authors: Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits"
    ],
    "type": null
  },
  {
    "note": [
      "Conference: arXiv preprint arXiv:1909.02249,"
    ],
    "date": [
      "2019"
    ],
    "arxiv": [
      "1909.02249"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Surveys adversarial machine learning techniques specifically in the context of text classification"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the current research and developments in the security of NLP models, addressing critical issues such as backdoor attacks, private data leakage, and imitation attacks"
    ],
    "type": null
  }
]
