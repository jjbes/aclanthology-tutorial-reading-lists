[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here is a list of 20 articles up to 2023 that focus on techniques for editing large language models (LLMs). These articles cover a range of topics including fine-tuning, transfer learning, prompt engineering, model distillation, and more"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Fine-Tuning Language Models from Human Preferences\"**"
    ],
    "publisher": [
      "OpenAI"
    ],
    "date": [
      "2022"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Transfer Learning in Natural Language Processing\"**"
    ],
    "author": [
      {
        "family": "Ruder",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm\"** - Liu"
    ],
    "date": [
      "2021"
    ],
    "type": "article-journal",
    "container-title": [
      "P. et al"
    ]
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\"**"
    ],
    "location": [
      "Sanh, V"
    ],
    "note": [
      "et al., 2019."
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "given": "LoRA"
      }
    ],
    "title": [
      "Low-Rank Adaptation of Large Language Models\"** - Hu"
    ],
    "date": [
      "2021"
    ],
    "type": "article-journal",
    "container-title": [
      "E. J. et al"
    ]
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Adapters: Efficient Transfer Learning for NLP\"**"
    ],
    "editor": [
      {
        "family": "Houlsby",
        "given": "N."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Knowledge Distillation: A Survey\"**"
    ],
    "editor": [
      {
        "family": "Gou",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Recycling Knowledge from Pre-trained Language Models for Neural Machine Translation\"**"
    ],
    "editor": [
      {
        "family": "Yang",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"**"
    ],
    "editor": [
      {
        "family": "Raffel",
        "given": "C."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Parameter-Efficient Transfer Learning for NLP\"**"
    ],
    "editor": [
      {
        "family": "Houlsby",
        "given": "N."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\"** - Frankle"
    ],
    "volume": [
      "M"
    ],
    "date": [
      "2019"
    ],
    "type": "article-journal",
    "container-title": [
      "J. and Carbin"
    ],
    "issue": [
      ""
    ]
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Efficient Fine-Tuning of BERT for Text Classification\"**"
    ],
    "editor": [
      {
        "family": "Sun",
        "given": "C."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Universal Language Model Fine-tuning for Text Classification\"**"
    ],
    "editor": [
      {
        "family": "Howard",
        "given": "J."
      },
      {
        "family": "Ruder",
        "given": "S."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** - Devlin"
    ],
    "date": [
      "2019"
    ],
    "type": "article-journal",
    "container-title": [
      "J. et al"
    ]
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"GPT-3: Language Models are Few-Shot Learners\"**"
    ],
    "editor": [
      {
        "family": "Brown",
        "given": "T.B."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Exploring the Role of Task Transfer Learning in Large Language Models\"**"
    ],
    "location": [
      "Radford"
    ],
    "publisher": [
      "A. et al"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Efficient Transfer Learning with Model Compression\"**"
    ],
    "editor": [
      {
        "family": "Hinton",
        "given": "G."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2015"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Improving Language Understanding by Generative Pre-Training\"**"
    ],
    "editor": [
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "literal": "**\"Knowledge Distillation for BERT Unsupervised Domain Adaptation\"** - Xu, H. et al."
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "given": "AdapterFusion"
      }
    ],
    "title": [
      "Non-destructive Task Composition for Transfer Learning\"** - Pfeiffer"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "J. et al"
    ]
  },
  {
    "title": [
      "These articles should give you a comprehensive overview of the various techniques and methodologies for editing and fine-tuning large language models"
    ],
    "type": null
  }
]
