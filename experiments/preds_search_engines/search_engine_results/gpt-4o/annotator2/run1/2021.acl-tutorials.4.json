[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here is a list of 20 articles on pre-training methods for neural machine"
    ],
    "note": [
      "translation (NMT) published before 2022:"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "literal": "**\"Attention Is All You Need\"** - Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I."
      }
    ],
    "date": [
      "2017"
    ],
    "container-title": [
      "*Advances in Neural Information Processing Systems (NeurIPS)*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      },
      {
        "family": "Devlin",
        "given": "J."
      },
      {
        "family": "Chang",
        "given": "M.W."
      },
      {
        "family": "Lee",
        "given": "K."
      },
      {
        "family": "Toutanova",
        "given": "K."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"**"
    ],
    "date": [
      "2019"
    ],
    "container-title": [
      "*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "literal": "**\"Unsupervised Machine Translation Using Monolingual Corpora Only\"** - Lample, G., Conneau, A., Denoyer, L., & Ranzato, M."
      }
    ],
    "date": [
      "2018"
    ],
    "container-title": [
      "*International Conference on Learning Representations (ICLR)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "4."
    ],
    "container-title": [
      "**\"Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges\"**",
      "*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*"
    ],
    "author": [
      {
        "family": "Aharoni",
        "given": "R."
      },
      {
        "family": "Johnson",
        "given": "M."
      },
      {
        "family": "Firat",
        "given": "O."
      }
    ],
    "date": [
      "2019"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Pre-trained Language Model Representations for Language Generation\"**",
      "*OpenAI Blog*"
    ],
    "author": [
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "family": "Wu",
        "given": "J."
      },
      {
        "family": "Child",
        "given": "R."
      },
      {
        "family": "Luan",
        "given": "D."
      },
      {
        "family": "Amodei",
        "given": "D."
      },
      {
        "family": "Sutskever",
        "given": "I."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "literal": "**\"Cross-lingual Language Model Pretraining\"** - Conneau, A., Lample, G., Ranzato, M., Denoyer, L., & Jégou, H."
      }
    ],
    "date": [
      "2019"
    ],
    "container-title": [
      "*Advances in Neural Information Processing Systems (NeurIPS)*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "literal": "**\"mBERT: Multilingual BERT\"** - Devlin, J., Chang, M. W., Lee, K., & Toutanova, K."
      }
    ],
    "date": [
      "2019"
    ],
    "note": [
      "*arXiv preprint arXiv:1810.04805*."
    ],
    "arxiv": [
      "1810.04805"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "literal": "**\"XLM-R: Robust Cross-lingual Representation Pretraining\"** - Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., ... & Stoyanov, V."
      }
    ],
    "date": [
      "2020"
    ],
    "container-title": [
      "*Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "literal": "**\"Unsupervised Cross-lingual Representation Learning at Scale\"** - Conneau, A., & Lample, G."
      }
    ],
    "date": [
      "2019"
    ],
    "container-title": [
      "*Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "literal": "**\"Pre-training Methods for Neural Machine Translation\"** - Zoph, B., Yuret, D., May, J., & Knight, K."
      }
    ],
    "date": [
      "2016"
    ],
    "container-title": [
      "*Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Improving Neural Machine Translation Models with Monolingual Data\"**"
    ],
    "author": [
      {
        "family": "Sennrich",
        "given": "R."
      },
      {
        "family": "Haddow",
        "given": "B."
      },
      {
        "family": "Birch",
        "given": "A."
      }
    ],
    "date": [
      "2016"
    ],
    "container-title": [
      "*Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "literal": "**\"Dual Learning for Machine Translation\"** - He, D., Xia, Y., Qin, T., Wang, L., Yu, N., Liu, T. Y., & Ma, W. Y."
      }
    ],
    "date": [
      "2016"
    ],
    "container-title": [
      "*Advances in Neural Information Processing Systems (NeurIPS)*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "literal": "**\"Unsupervised Neural Machine Translation\"** - Artetxe, M., Labaka, G., & Agirre, E."
      }
    ],
    "date": [
      "2018"
    ],
    "container-title": [
      "*Proceedings of the 6th International Conference on Learning Representations (ICLR)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "14."
    ],
    "author": [
      {
        "literal": "**\"Language Model Pre-training for Hierarchical Document Representations\"** - Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V."
      }
    ],
    "date": [
      "2019"
    ],
    "container-title": [
      "*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "literal": "**\"Pre-training with Whole Word Masking for Chinese BERT\"** - Cui, Y., Che, W., Liu, T., Qin, B., Yang, Z., & Hu, G."
      }
    ],
    "date": [
      "2019"
    ],
    "note": [
      "*arXiv preprint arXiv:1906.08101*."
    ],
    "arxiv": [
      "1906.08101"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "literal": "**\"Pre-trained Language Model Embeddings for Sentence-level Representations\"** - Reimers, N., & Gurevych, I."
      }
    ],
    "date": [
      "2019"
    ],
    "container-title": [
      "*Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "17."
    ],
    "container-title": [
      "**\"Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information\"**",
      "*Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*"
    ],
    "author": [
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "family": "Lin",
        "given": "H."
      }
    ],
    "date": [
      "2020"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Pre-training and Fine-tuning: A Comprehensive Study on BERT for Text Classification\"**"
    ],
    "author": [
      {
        "family": "Sun",
        "given": "C."
      },
      {
        "family": "Qiu",
        "given": "X."
      },
      {
        "family": "Xu",
        "given": "Y."
      },
      {
        "family": "Huang",
        "given": "X."
      }
    ],
    "date": [
      "2019"
    ],
    "container-title": [
      "*Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "literal": "**\"Pre-training Transformers as Energy-based Cloze Models\"** - Clark, K., Luong, M. T., Le, Q. V., & Manning, C. D."
      }
    ],
    "date": [
      "2020"
    ],
    "container-title": [
      "*Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "literal": "**\"Pre-training with Extracted Gap-sentences for Abstractive Summarization\"** - Zhang, J., Zhao, Y., Saleh, M., & Liu, P."
      }
    ],
    "date": [
      "2019"
    ],
    "container-title": [
      "*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*"
    ],
    "type": "paper-conference"
  },
  {
    "title": [
      "These articles cover a range of pre-training methods and their applications to neural machine translation, providing a comprehensive overview of the field up to"
    ],
    "date": [
      "2022"
    ],
    "type": null
  }
]
