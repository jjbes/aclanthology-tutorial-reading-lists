[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2021 that focus on fine-grained interpretation and causation analysis in deep neural network NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "literal": "**\"Attention is All You Need\"** - Vaswani, A., et al."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces the Transformer model, which has become foundational for many NLP tasks and interpretability studies"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"**"
    ],
    "editor": [
      {
        "family": "Devlin",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper presents BERT, a model that has been extensively analyzed for interpretability and causation in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Interpretable and Explainable Deep Learning: A Survey\"**"
    ],
    "editor": [
      {
        "family": "Zhang",
        "given": "Q."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A comprehensive survey on interpretability methods in deep learning, including NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"A Survey of Methods for Explaining Black Box Models\"**"
    ],
    "editor": [
      {
        "family": "Guidotti",
        "given": "R."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This survey covers various methods for explaining black-box models, relevant to NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "literal": "**\"Attention is not Explanation\"** - Jain, S., & Wallace, B. C."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper critically evaluates the use of attention mechanisms as explanations in NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "given": "L.I.M.E."
      }
    ],
    "title": [
      "Local Interpretable Model-agnostic Explanations\"**"
    ],
    "editor": [
      {
        "family": "Ribeiro",
        "given": "M.T."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces LIME, a popular method for explaining individual predictions of black-box models, applicable to NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Anchors: High-Precision Model-Agnostic Explanations\"**"
    ],
    "editor": [
      {
        "family": "Ribeiro",
        "given": "M.T."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Extends LIME to provide high-precision explanations, useful for NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Integrated Gradients: Axiomatic Attribution for Deep Networks\"**"
    ],
    "editor": [
      {
        "family": "Sundararajan",
        "given": "M."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes Integrated Gradients, a method for attributing the prediction of deep networks to their input features"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Visualizing and Understanding Neural Models in NLP\"**"
    ],
    "editor": [
      {
        "family": "Li",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses methods for visualizing and understanding neural models in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Interpreting and Understanding Deep Neural Networks for Text Classification\"**"
    ],
    "editor": [
      {
        "family": "Arras",
        "given": "L."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Focuses on interpretation techniques for text classification models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Explaining Predictions of Non-Linear Classifiers in NLP\"**"
    ],
    "editor": [
      {
        "family": "Arras",
        "given": "L."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores explanation methods for non-linear classifiers in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"What does BERT Look at? An Analysis of BERT's Attention\"**"
    ],
    "editor": [
      {
        "family": "Clark",
        "given": "K."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analyzes the attention mechanisms in BERT to understand what the model focuses on"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"A Unified Approach to Interpreting Model Predictions\"**"
    ],
    "editor": [
      {
        "family": "Lundberg",
        "given": "S.M."
      }
    ],
    "location": [
      "Lee, S.-I"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces SHAP values, a unified framework for interpreting model predictions, including NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Contextual Decomposition for Neural Network Interpretability\"**"
    ],
    "editor": [
      {
        "family": "Murdoch",
        "given": "W.J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a method for decomposing neural network predictions into interpretable components"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsers\"**"
    ],
    "editor": [
      {
        "family": "Belinkov",
        "given": "Y."
      },
      {
        "family": "Bisk",
        "given": "Y."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the robustness and interpretability of neural dependency parsers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Learning Important Features Through Propagating Activation Differences\"**"
    ],
    "editor": [
      {
        "family": "Shrikumar",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces DeepLIFT, a method for attributing the importance of input features in neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Towards a Rigorous Science of Interpretable Machine Learning\"**"
    ],
    "editor": [
      {
        "family": "Doshi-Velez",
        "given": "F."
      },
      {
        "family": "Kim",
        "given": "B."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the principles and challenges of interpretable machine learning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "literal": "**\"Rationalizing Neural Predictions\"** - Lei, T., et al."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a method for generating rationales for neural network predictions in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Attention-based Models for Text Classification with Interpretability\"**"
    ],
    "editor": [
      {
        "family": "Yang",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces attention-based models for text classification with a focus on interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions\"**"
    ],
    "editor": [
      {
        "family": "Koh",
        "given": "P.W."
      },
      {
        "family": "Liang",
        "given": "P."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Uses influence functions to explain model predictions and identify data artifacts"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of methods and perspectives on interpreting and analyzing causation in deep neural network NLP models. They provide a solid foundation for understanding the current state of research in this area"
    ],
    "type": null
  }
]
