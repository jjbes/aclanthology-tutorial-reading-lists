[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here is a reading list of 20 articles on LLM (Large Language Model)-driven instruction following up to 2023. These articles cover a range of topics including the development, application, and evaluation of LLMs in instruction following tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "family": "Brown",
        "given": "T.B."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Language Models are Few-Shot Learners"
    ],
    "note": [
      "*arXiv preprint arXiv:2005.14165.*"
    ],
    "arxiv": [
      "2005.14165"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Language Models are Unsupervised Multitask Learners"
    ],
    "container-title": [
      "*OpenAI Blog.*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "family": "Raffel",
        "given": "C."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
    ],
    "volume": [
      "21"
    ],
    "type": "article-journal",
    "container-title": [
      "*Journal of Machine Learning Research"
    ],
    "issue": [
      "140"
    ],
    "pages": [
      "1–67"
    ]
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "family": "Wei",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
    ],
    "note": [
      "*arXiv preprint arXiv:2201.11903.*"
    ],
    "arxiv": [
      "2201.11903"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "family": "Ouyang",
        "given": "L."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Training language models to follow instructions with human feedback"
    ],
    "note": [
      "*arXiv preprint arXiv:2203.02155.*"
    ],
    "arxiv": [
      "2203.02155"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "family": "Zhang",
        "given": "H."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "OPT: Open Pre-trained Transformer Language Models"
    ],
    "note": [
      "*arXiv preprint arXiv:2205.01068.*"
    ],
    "arxiv": [
      "2205.01068"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "family": "Kojima",
        "given": "T."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Large Language Models are Zero-Shot Reasoners"
    ],
    "note": [
      "*arXiv preprint arXiv:2205.11916.*"
    ],
    "arxiv": [
      "2205.11916"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "family": "Gao",
        "given": "L."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
    ],
    "note": [
      "*arXiv preprint arXiv:2101.00027.*"
    ],
    "arxiv": [
      "2101.00027"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "family": "Schick",
        "given": "T."
      },
      {
        "family": "Schütze",
        "given": "H."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"
    ],
    "note": [
      "*arXiv preprint arXiv:2001.07676.*"
    ],
    "arxiv": [
      "2001.07676"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "family": "Ziegler",
        "given": "D.M."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Fine-Tuning Language Models from Human Preferences"
    ],
    "note": [
      "*arXiv preprint arXiv:1909.08593.*"
    ],
    "arxiv": [
      "1909.08593"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "family": "Levine",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "The Unreasonable Effectiveness of Structured Randomness for Transfer Learning in Large Language Models"
    ],
    "note": [
      "*arXiv preprint arXiv:2204.02311.*"
    ],
    "arxiv": [
      "2204.02311"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "family": "Wang",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
    ],
    "note": [
      "*arXiv preprint arXiv:1905.00537.*"
    ],
    "arxiv": [
      "1905.00537"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "family": "Bommasani",
        "given": "R."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "On the Opportunities and Risks of Foundation Models"
    ],
    "note": [
      "*arXiv preprint arXiv:2108.07258.*"
    ],
    "arxiv": [
      "2108.07258"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "author": [
      {
        "family": "Gururangan",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks"
    ],
    "note": [
      "*arXiv preprint arXiv:2004.10964.*"
    ],
    "arxiv": [
      "2004.10964"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "P."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
    ],
    "note": [
      "*arXiv preprint arXiv:2107.13586.*"
    ],
    "arxiv": [
      "2107.13586"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "family": "Mishra",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"
    ],
    "note": [
      "*arXiv preprint arXiv:2104.08773.*"
    ],
    "arxiv": [
      "2104.08773"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "Zellers",
        "given": "R."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World"
    ],
    "note": [
      "*arXiv preprint arXiv:2106.00188.*"
    ],
    "arxiv": [
      "2106.00188"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "family": "Kim",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Few-shot Instruction Following with Recurrent Language Models"
    ],
    "note": [
      "*arXiv preprint arXiv:2107.02192.*"
    ],
    "arxiv": [
      "2107.02192"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "family": "Chen",
        "given": "M."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Evaluating Large Language Models Trained on Code"
    ],
    "note": [
      "*arXiv preprint arXiv:2107.03374.*"
    ],
    "arxiv": [
      "2107.03374"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "family": "Li",
        "given": "X.L."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
    ],
    "note": [
      "*arXiv preprint arXiv:2101.00190.*"
    ],
    "arxiv": [
      "2101.00190"
    ],
    "type": null
  },
  {
    "title": [
      "These articles should provide a comprehensive overview of the current state of research and development in LLM-driven instruction following"
    ],
    "type": null
  }
]
