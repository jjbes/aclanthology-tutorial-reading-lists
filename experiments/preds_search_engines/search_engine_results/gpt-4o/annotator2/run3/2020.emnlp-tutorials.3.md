Sure! Here's a reading list of 20 articles on the interpretability of NLP models, up to the year 2020:

1. **"Attention is not Explanation"** by Jain, Sarthak, and Byron C. Wallace (2019)
   - This paper questions the common assumption that attention weights can be used as explanations for model predictions.

2. **"Interpretable and Explainable Deep Learning for Medical Image Analysis: Survey, Applications, and Future Directions"** by Tjoa, Ernesta Meutia, and Cuntai Guan (2020)
   - Although focused on medical image analysis, this survey provides insights into interpretability techniques that are applicable to NLP.

3. **"Rationalizing Neural Predictions"** by Lei, Tao, Regina Barzilay, and Tommi Jaakkola (2016)
   - Introduces a framework for generating rationales, which are subsets of input text that justify the model's predictions.

4. **"LIME: Local Interpretable Model-Agnostic Explanations"** by Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin (2016)
   - Proposes a model-agnostic approach to explain individual predictions, applicable to NLP models.

5. **"Anchors: High-Precision Model-Agnostic Explanations"** by Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin (2018)
   - Extends LIME by providing high-precision explanations using anchors, which are conditions that almost always guarantee a particular prediction.

6. **"Why Should I Trust You?": Explaining the Predictions of Any Classifier"** by Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin (2016)
   - Discusses the importance of model interpretability and introduces techniques to explain classifier predictions.

7. **"A Unified Approach to Interpreting Model Predictions"** by Lundberg, Scott M., and Su-In Lee (2017)
   - Introduces SHAP (SHapley Additive exPlanations), a unified framework for interpreting predictions.

8. **"Visualizing and Understanding Neural Models in NLP"** by Karpathy, Andrej, Justin Johnson, and Li Fei-Fei (2015)
   - Uses visualization techniques to understand the inner workings of neural networks in NLP.

9. **"Evaluating the Interpretability of Generative Models by Interactive Reconstruction"** by Kim, Been, Rajiv Khanna, and Oluwasanmi Koyejo (2016)
   - Proposes methods to evaluate the interpretability of generative models through interactive reconstruction.

10. **"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"** by Kim, Been, et al. (2018)
    - Introduces TCAV, a method to test the influence of high-level concepts on model predictions.

11. **"Interpretability of Deep Learning Models: A Survey of Results"** by Gilpin, Leilani H., et al. (2018)
    - Surveys various interpretability methods for deep learning models, including those used in NLP.

12. **"Learning to Explain: An Information-Theoretic Perspective on Model Interpretation"** by Chen, Jianbo, et al. (2018)
    - Proposes an information-theoretic approach to model interpretation.

13. **"Contextual Decomposition for Neural Network Interpretability"** by Murdoch, W. James, and Bin Yu (2018)
    - Introduces contextual decomposition, a method to interpret individual predictions by decomposing the network's output.

14. **"The Mythos of Model Interpretability"** by Lipton, Zachary C. (2016)
    - Discusses the challenges and misconceptions surrounding model interpretability.

15. **"Attention is All You Need"** by Vaswani, Ashish, et al. (2017)
    - While primarily introducing the Transformer model, this paper has significant implications for interpretability in NLP.

16. **"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI"** by Arrieta, Alejandro Barredo, et al. (2020)
    - Provides a comprehensive overview of explainable AI, including techniques relevant to NLP.

17. **"A Survey of Methods for Explaining Black Box Models"** by Guidotti, Riccardo, et al. (2018)
    - Surveys various methods for explaining black-box models, including those used in NLP.

18. **"Interpretability for NLP: State of the Art and Challenges"** by Danilevsky, Marina, et al. (2020)
    - A survey specifically focused on interpretability techniques in NLP.

19. **"Towards a Rigorous Science of Interpretable Machine Learning"** by Doshi-Velez, Finale, and Been Kim (2017)
    - Discusses the need for rigorous scientific methods in the study of interpretable machine learning.

20. **"From Word Embeddings to Document Distances"** by Kusner, Matt J., et al. (2015)
    - Introduces the Word Mover's Distance, a method that can be used to interpret word embeddings in NLP models.

These articles cover a range of techniques and perspectives on the interpretability of NLP models, providing a solid foundation for understanding this important area of research.