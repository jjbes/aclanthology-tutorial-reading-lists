[
  {
    "title": [
      "Sure, here is a reading list of 20 articles up to 2023 that focus on security concerns associated with NLP models, particularly backdoor attacks, private data leakage, and imitation attacks"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Attacks",
        "given": "Backdoor"
      }
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Backdoor Attacks on Deep Learning Models\"** - Yiming Li, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain\"** - Tianyu Gu, et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "literal": "**\"Trojaning Attack on Neural Networks\"** - Yingqi Liu, et al."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"A Survey of Backdoor Attacks and Defenses in Machine Learning\"** - Yansong Gao, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Backdoor Attacks and Defenses in Federated Learning: Survey, Challenges and Future Research Directions\"** - Zhaohan Xiang, et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "container-title": [
      "### Private Data Leakage"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Membership Inference Attacks Against Machine Learning Models\"** - Reza Shokri, et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning\"** - Ling Liu, et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Auditing Data Provenance in Text Generation Models\"** - Eric Wallace, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Privacy-Preserving Machine Learning: Threats and Solutions\"** - Yang Liu, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Differentially Private Text Generation via Plug-and-Play Perturbation\"** - Zhijing Jin, et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Attacks",
        "given": "Imitation"
      }
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Adversarial Examples in the Physical World\"** - Alexey Kurakin, et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review\"** - Jinyin Chen, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Model Extraction Attacks Against Machine Learning Models\"** - Florian Tramer, et al"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Stealing Machine Learning Models via Prediction APIs\"** - Battista Biggio, et al"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Adversarial Attacks on Neural Networks for Graph Data\"** - Daniel ZÃ¼gner, et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "title": [
      "### General Security Concerns in NLP Models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Adversarial Attacks and Defenses in Deep Learning\"**"
    ],
    "editor": [
      {
        "family": "Goodfellow",
        "given": "Ian J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2015"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Universal Adversarial Triggers for Attacking and Analyzing NLP\"** - Eric Wallace, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods\"** - Nicholas Carlini, et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Mitigating Adversarial Effects Through Randomization\"** - Anish Athalye, et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Robustness and Generalization of Deep Learning Models: A Survey\"** - Xiaoyong Yuan, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of topics within the realm of security concerns for NLP models, providing a comprehensive overview of the current state of research up to"
    ],
    "date": [
      "2023"
    ],
    "type": null
  }
]
