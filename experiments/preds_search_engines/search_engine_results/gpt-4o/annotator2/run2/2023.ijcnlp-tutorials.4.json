[
  {
    "title": [
      "Sure, here's a curated list of 20 articles up to 2023 that cover various techniques for editing large language models (LLMs). These articles span a range of topics including fine-tuning, prompt engineering, model distillation, and other advanced methods for modifying and improving LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**Fine-Tuning Language Models from Human Preferences**"
    ],
    "publisher": [
      "OpenAI"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter** - Hugging Face"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "given": "LoRA"
      }
    ],
    "title": [
      "Low-Rank Adaptation of Large Language Models**"
    ],
    "publisher": [
      "Microsoft Research"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm**"
    ],
    "publisher": [
      "Stanford University"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**The Power of Scale for Parameter-Efficient Prompt Tuning**"
    ],
    "publisher": [
      "Google Research"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**AdapterHub: A Framework for Adapting Transformers**"
    ],
    "publisher": [
      "University of Hamburg"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**Recycling Knowledge from Pretrained Language Models for Efficient Text Generation**"
    ],
    "publisher": [
      "University of Washington"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**Knowledge Distillation: A Survey**"
    ],
    "publisher": [
      "University of Chinese Academy of Sciences"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**Parameter-Efficient Transfer Learning for NLP**"
    ],
    "publisher": [
      "Carnegie Mellon University"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**"
    ],
    "publisher": [
      "Google Research"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**Efficient Fine-Tuning of BERT for Text Classification**"
    ],
    "publisher": [
      "University of Waterloo"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**Meta-Learning for Few-Shot Natural Language Processing: A Survey**"
    ],
    "publisher": [
      "University of California"
    ],
    "location": [
      "Berkeley"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**Prefix-Tuning: Optimizing Continuous Prompts for Generation**"
    ],
    "publisher": [
      "University of Washington"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "14."
    ],
    "container-title": [
      "**Efficient Large-Scale Language Model Training on GPU Clusters** - NVIDIA"
    ],
    "date": [
      "2020"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**Zero-Shot Text Classification with Generative Language Models**"
    ],
    "publisher": [
      "OpenAI"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**Adapting Language Models for Zero-Shot Learning by Meta-Tuning on Dataset and Prompt Collections**"
    ],
    "publisher": [
      "University of Illinois"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**Parameter-Efficient Transfer Learning with Diff Pruning**"
    ],
    "publisher": [
      "University of Washington"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**Knowledge Injection into Pretrained Models: A Survey**"
    ],
    "publisher": [
      "Tsinghua University"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**Improving Language Understanding by Generative Pre-Training**"
    ],
    "publisher": [
      "OpenAI"
    ],
    "date": [
      "2018"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks**"
    ],
    "publisher": [
      "MIT"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the state-of-the-art techniques for editing and enhancing large language models. They cover both theoretical foundations and practical implementations, making them valuable resources for researchers and practitioners in the field"
    ],
    "type": null
  }
]
