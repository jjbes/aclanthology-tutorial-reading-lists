Sure, here is a reading list of 20 influential articles on embeddings in Natural Language Processing (NLP) up to the year 2020:

1. **Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). "Efficient Estimation of Word Representations in Vector Space."** arXiv preprint arXiv:1301.3781.
2. **Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). "Distributed Representations of Words and Phrases and their Compositionality."** Advances in Neural Information Processing Systems (NeurIPS).
3. **Pennington, J., Socher, R., & Manning, C. D. (2014). "GloVe: Global Vectors for Word Representation."** Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).
4. **Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). "Enriching Word Vectors with Subword Information."** Transactions of the Association for Computational Linguistics (TACL).
5. **Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). "Deep Contextualized Word Representations."** Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).
6. **Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding."** Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).
7. **Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). "Attention is All You Need."** Advances in Neural Information Processing Systems (NeurIPS).
8. **Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). "Improving Language Understanding by Generative Pre-Training."** OpenAI.
9. **Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). "Language Models are Unsupervised Multitask Learners."** OpenAI.
10. **Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019). "XLNet: Generalized Autoregressive Pretraining for Language Understanding."** Advances in Neural Information Processing Systems (NeurIPS).
11. **Lample, G., & Conneau, A. (2019). "Cross-lingual Language Model Pretraining."** Advances in Neural Information Processing Systems (NeurIPS).
12. **Peters, M. E., Ammar, W., Bhagavatula, C., & Power, R. (2017). "Semi-supervised Sequence Tagging with Bidirectional Language Models."** Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL).
13. **Johnson, R., & Zhang, T. (2015). "Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding."** Advances in Neural Information Processing Systems (NeurIPS).
14. **Le, Q., & Mikolov, T. (2014). "Distributed Representations of Sentences and Documents."** Proceedings of the 31st International Conference on Machine Learning (ICML).
15. **Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R., Urtasun, R., Torralba, A., & Fidler, S. (2015). "Skip-Thought Vectors."** Advances in Neural Information Processing Systems (NeurIPS).
16. **Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data."** Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).
17. **Cer, D., Yang, Y., Kong, S. Y., Hua, N., Limtiaco, N., John, R. S., ... & Kurzweil, R. (2018). "Universal Sentence Encoder."** arXiv preprint arXiv:1803.11175.
18. **Pang, B., & Lee, L. (2008). "Opinion Mining and Sentiment Analysis."** Foundations and Trends in Information Retrieval.
19. **Schuster, M., & Nakajima, K. (2012). "Japanese and Korean Voice Search."** 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
20. **Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). "RoBERTa: A Robustly Optimized BERT Pretraining Approach."** arXiv preprint arXiv:1907.11692.

These articles cover a range of topics within embeddings in NLP, from foundational word embeddings like Word2Vec and GloVe to more advanced contextual embeddings like BERT and its variants. They also include works on sentence embeddings and cross-lingual embeddings, providing a comprehensive overview of the field up to 2020.