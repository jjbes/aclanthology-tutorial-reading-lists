[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Document-level representation learning is a crucial area in natural language processing (NLP) that focuses on understanding and representing entire documents rather than just sentences or phrases. Here is a reading list of 20 influential articles up to 2021"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** by Jacob Devlin"
    ],
    "publisher": [
      "Ming-Wei Chang"
    ],
    "location": [
      "Kenton Lee, Kristina Toutanova"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\"** by Zhilin Yang, Zihang Dai"
    ],
    "editor": [
      {
        "family": "Yang",
        "given": "Yiming"
      },
      {
        "family": "Carbonell",
        "given": "Jaime"
      },
      {
        "family": "Salakhutdinov",
        "given": "Ruslan"
      },
      {
        "family": "Le",
        "given": "Quoc V."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"** by Yinhan Liu"
    ],
    "editor": [
      {
        "family": "Ott",
        "given": "Myle"
      },
      {
        "family": "Goyal",
        "given": "Naman"
      },
      {
        "family": "Du",
        "given": "Jingfei"
      },
      {
        "family": "Joshi",
        "given": "Mandar"
      },
      {
        "family": "Chen",
        "given": "Danqi"
      },
      {
        "family": "Levy",
        "given": "Omer"
      },
      {
        "family": "Lewis",
        "given": "Mike"
      },
      {
        "family": "Zettlemoyer",
        "given": "Luke"
      },
      {
        "family": "Stoyanov",
        "given": "Veselin"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "given": "Longformer"
      }
    ],
    "title": [
      "The Long-Document Transformer\"** by Iz Beltagy"
    ],
    "editor": [
      {
        "family": "Peters",
        "given": "Matthew E."
      },
      {
        "family": "Cohan",
        "given": "Arman"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "given": "E.R.N.I.E."
      }
    ],
    "title": [
      "Enhanced Representation through Knowledge Integration\"** by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng"
    ],
    "publisher": [
      "Hao Tian, Hua Wu, Haifeng Wang"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\"** by Zihang Dai"
    ],
    "editor": [
      {
        "family": "Yang",
        "given": "Zhilin"
      },
      {
        "family": "Yang",
        "given": "Yiming"
      },
      {
        "family": "Carbonell",
        "given": "Jaime"
      },
      {
        "family": "Le",
        "given": "Quoc V."
      },
      {
        "family": "Salakhutdinov",
        "given": "Ruslan"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"** by Colin Raffel, Noam Shazeer, Adam Roberts"
    ],
    "location": [
      "Katherine Lee, Sharan Narang"
    ],
    "note": [
      "Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu (2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "given": "P.E.G.A.S.U.S."
      }
    ],
    "title": [
      "Pre-training with Extracted Gap-sentences for Abstractive Summarization\"** by Jingqing Zhang"
    ],
    "editor": [
      {
        "family": "Zhao",
        "given": "Yao"
      },
      {
        "family": "Saleh",
        "given": "Mohammad"
      },
      {
        "family": "Liu",
        "given": "Peter J."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Big Bird: Transformers for Longer Sequences\"** by Manzil Zaheer, Guru Guruganesh"
    ],
    "note": [
      "Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed (2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Hierarchical Attention Networks for Document Classification\"** by Zichao Yang"
    ],
    "note": [
      "Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy (2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "given": "DocBERT"
      }
    ],
    "title": [
      "BERT for Document Classification\"** by Aditya P"
    ],
    "publisher": [
      "Das, Sagnik Ray Choudhury, Manish Gupta, Vasudeva Varma"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Discourse-Aware Neural Extractive Text Summarization\"** by Ramesh Nallapati"
    ],
    "editor": [
      {
        "family": "Zhai",
        "given": "Feifei"
      },
      {
        "family": "Zhou",
        "given": "Bowen"
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"A Hierarchical Neural Autoencoder for Paragraphs and Documents\"** by Jiwei Li"
    ],
    "publisher": [
      "Minh-Thang Luong, Dan Jurafsky"
    ],
    "date": [
      "2015"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Unified Language Model Pre-training for Natural Language Understanding and Generation\"** by Li Dong"
    ],
    "note": [
      "Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon (2019"
    ],
    "type": "article-journal",
    "container-title": [
      "Nan Yang"
    ]
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\"** by Noam Shazeer, Youlong Cheng"
    ],
    "editor": [
      {
        "family": "Parmar",
        "given": "Niki"
      },
      {
        "family": "Tran",
        "given": "Dustin"
      },
      {
        "family": "Vaswani",
        "given": "Ashish"
      },
      {
        "family": "Koanantakool",
        "given": "Penporn"
      },
      {
        "family": "Hawkins",
        "given": "Peter"
      },
      {
        "family": "Lee",
        "given": "HyoukJoong"
      },
      {
        "family": "Krikun",
        "given": "Maxim"
      },
      {
        "family": "Goyal",
        "given": "Naman"
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling\"** by Pengfei Liu"
    ],
    "publisher": [
      "Xipeng Qiu, Xuanjing Huang"
    ],
    "date": [
      "2016"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "Ashish Vaswani",
        "given": "Attention Is All You Need\"",
        "particle": "by"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Parmar",
        "given": "Niki"
      },
      {
        "family": "Uszkoreit",
        "given": "Jakob"
      },
      {
        "family": "Jones",
        "given": "Llion"
      },
      {
        "family": "Gomez",
        "given": "Aidan N."
      },
      {
        "family": "Kaiser",
        "given": "≈Åukasz"
      }
    ],
    "title": [
      "Illia Polosukhin"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "given": "B.A.R.T."
      }
    ],
    "title": [
      "Denoising Sequence-to-Sequence Pre-training for Natural Language Generation"
    ],
    "editor": [
      {
        "family": "Mike Lewis",
        "given": "Comprehension\"",
        "particle": "by"
      },
      {
        "family": "Liu",
        "given": "Yinhan"
      },
      {
        "family": "Goyal",
        "given": "Naman"
      },
      {
        "family": "Ghazvininejad",
        "given": "Marjan"
      },
      {
        "family": "Mohamed",
        "given": "Abdelrahman"
      },
      {
        "family": "Levy",
        "given": "Omer"
      },
      {
        "family": "Stoyanov",
        "given": "Veselin"
      },
      {
        "family": "Zettlemoyer",
        "given": "Luke"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"SpanBERT: Improving Pre-training by Representing and Predicting Spans\"** by Mandar Joshi"
    ],
    "editor": [
      {
        "family": "Chen",
        "given": "Danqi"
      },
      {
        "family": "Liu",
        "given": "Yinhan"
      },
      {
        "family": "Weld",
        "given": "Daniel S."
      },
      {
        "family": "Zettlemoyer",
        "given": "Luke"
      },
      {
        "family": "Levy",
        "given": "Omer"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"GPT-3: Language Models are Few-Shot Learners\"** by Tom B"
    ],
    "editor": [
      {
        "family": "Brown",
        "given": "Benjamin Mann"
      },
      {
        "family": "Ryder",
        "given": "Nick"
      },
      {
        "family": "Subbiah",
        "given": "Melanie"
      },
      {
        "family": "Kaplan",
        "given": "Jared"
      },
      {
        "family": "Dhariwal",
        "given": "Prafulla"
      },
      {
        "family": "Neelakantan",
        "given": "Arvind"
      }
    ],
    "container-title": [
      "Pranav Shyam, Girish Sastry, Amanda Askell, et al"
    ],
    "date": [
      "2020"
    ],
    "type": "chapter"
  },
  {
    "title": [
      "These articles cover a range of topics within document-level representation learning, including pre-training techniques, hierarchical models, attention mechanisms, and applications in classification and summarization. They provide a solid foundation for understanding the state-of-the-art methods and advancements in this field up to"
    ],
    "date": [
      "2021"
    ],
    "type": null
  }
]
