[
  {
    "title": [
      "Sure, here's a reading list of 20 articles on LLM-driven instruction following up to 2023. These articles cover various aspects of large language models (LLMs) and their ability to follow instructions, including architecture, training methods, applications, and evaluations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "family": "Brown",
        "given": "T.B."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Language Models are Few-Shot Learners.\"** NeurIPS"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "family": "Raffel",
        "given": "C."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.\"** JMLR"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Language Models are Unsupervised Multitask Learners.\"** OpenAI"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "family": "Wei",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Chain of Thought Prompting Elicits Reasoning in Large Language Models.\"** arXiv"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "family": "Ouyang",
        "given": "L."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Training language models to follow instructions with human feedback.\"** arXiv"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "family": "Ziegler",
        "given": "D.M."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Fine-Tuning Language Models from Human Preferences.\"** arXiv"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "family": "Gao",
        "given": "L."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "The Pile: An 800GB Dataset of Diverse Text for Language Modeling.\"** arXiv"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "family": "Schick",
        "given": "T."
      },
      {
        "family": "Sch√ºtze",
        "given": "H."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners.\"** NAACL"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "family": "Sanh",
        "given": "V."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Multitask Prompted Training Enables Zero-Shot Task Generalization.\"** ICLR"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "family": "Kojima",
        "given": "T."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Large Language Models are Zero-Shot Reasoners.\"** arXiv"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "family": "Mishra",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Cross-Task Generalization via Natural Language Crowdsourcing Instructions.\"** ACL"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "family": "Zhong",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Adapting Language Models for Zero-Shot Learning by Meta-Learning.\"** NeurIPS"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "family": "Reed",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "A Generalist Agent.\"** arXiv"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "author": [
      {
        "family": "Gururangan",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks.\"** ACL"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "family": "Wang",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems.\"** NeurIPS"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "family": "Zellers",
        "given": "R."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Defending Against Neural Fake News.\"** NeurIPS"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "Khashabi",
        "given": "D."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "UNIFIEDQA: Crossing Format Boundaries with a Single QA System.\"** EMNLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "P."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.\"** arXiv"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "family": "Gao",
        "given": "T."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Making Pre-trained Language Models Better Few-shot Learners.\"** ACL"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "family": "Zhou",
        "given": "W."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Towards Understanding the Effect of Pretraining Settings on Few-Shot Learners.\"** ACL"
    ],
    "type": null
  },
  {
    "title": [
      "These articles will give you a comprehensive understanding of the current state of research in LLM-driven instruction following, including foundational works, recent advancements, and various approaches to improving model performance"
    ],
    "type": null
  }
]
