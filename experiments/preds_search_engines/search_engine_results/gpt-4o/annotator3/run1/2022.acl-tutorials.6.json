[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here's a curated list of 20 articles up to 2022 that focus on zero-shot and few-shot learning using pretrained language models. These articles cover a range of topics, including methodologies, applications, and evaluations of zero-shot and few-shot learning in various contexts"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "family": "Brown",
        "given": "T.B."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Language Models are Few-Shot Learners"
    ],
    "container-title": [
      "*Advances in Neural Information Processing Systems (NeurIPS)*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Language Models are Unsupervised Multitask Learners"
    ],
    "publisher": [
      "*OpenAI Blog*"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "family": "Schick",
        "given": "T."
      },
      {
        "family": "Schütze",
        "given": "H."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"
    ],
    "publisher": [
      "*EACL*"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "family": "Gao",
        "given": "T."
      },
      {
        "family": "Fisch",
        "given": "A."
      },
      {
        "family": "Chen",
        "given": "D."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Making Pre-trained Language Models Better Few-shot Learners"
    ],
    "container-title": [
      "*Association for Computational Linguistics (ACL)*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "family": "Yin",
        "given": "W."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Universal Natural Language Processing with Limited Annotations: Try Few-shot Text Generation"
    ],
    "note": [
      "*arXiv preprint arXiv:2012.15445*."
    ],
    "arxiv": [
      "2012.15445"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "family": "Zhao",
        "given": "W."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Calibrate Before Use: Improving Few-Shot Performance of Language Models"
    ],
    "container-title": [
      "*International Conference on Machine Learning (ICML)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "family": "Perez",
        "given": "E."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "True Few-Shot Learning with Language Models"
    ],
    "note": [
      "*arXiv preprint arXiv:2105.11447*."
    ],
    "arxiv": [
      "2105.11447"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "P."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
    ],
    "note": [
      "*arXiv preprint arXiv:2107.13586*."
    ],
    "arxiv": [
      "2107.13586"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "family": "Reed",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Self-Supervised Pretraining of Visual Features in the Wild"
    ],
    "note": [
      "*arXiv preprint arXiv:2103.01988*."
    ],
    "arxiv": [
      "2103.01988"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "family": "Zhong",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Factual Probing Is [MASK]: Learning vs"
    ],
    "container-title": [
      "Learning to Recall.\" *North American Chapter of the Association for Computational Linguistics (NAACL)*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "family": "Petroni",
        "given": "F."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Language Models as Knowledge Bases?"
    ],
    "container-title": [
      "*Empirical Methods in Natural Language Processing (EMNLP)*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "family": "Schick",
        "given": "T."
      },
      {
        "family": "Schütze",
        "given": "H."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"
    ],
    "container-title": [
      "*North American Chapter of the Association for Computational Linguistics (NAACL)*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "family": "Lester",
        "given": "B."
      },
      {
        "family": "Al-Rfou",
        "given": "R."
      },
      {
        "family": "Constant",
        "given": "N."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "The Power of Scale for Parameter-Efficient Prompt Tuning"
    ],
    "publisher": [
      "*EMNLP*"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "14."
    ],
    "author": [
      {
        "family": "Zhang",
        "given": "T."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Revisiting Few-sample BERT Fine-tuning"
    ],
    "container-title": [
      "*International Conference on Learning Representations (ICLR)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "family": "Han",
        "given": "X."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "PTR: Prompt Tuning with Rules for Text Classification"
    ],
    "note": [
      "*arXiv preprint arXiv:2105.11259*."
    ],
    "arxiv": [
      "2105.11259"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "family": "Yin",
        "given": "W."
      },
      {
        "family": "Neubig",
        "given": "G."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "GPT-2 is Not All You Need: The Case for Compressible Models"
    ],
    "note": [
      "*arXiv preprint arXiv:2104.08886*."
    ],
    "arxiv": [
      "2104.08886"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "Wang",
        "given": "Y."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Entailment as Few-Shot Learner"
    ],
    "note": [
      "*arXiv preprint arXiv:2104.14690*."
    ],
    "arxiv": [
      "2104.14690"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "X."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "GPT Understands, Too"
    ],
    "note": [
      "*arXiv preprint arXiv:2103.10385*."
    ],
    "arxiv": [
      "2103.10385"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "family": "Li",
        "given": "X.L."
      },
      {
        "family": "Liang",
        "given": "P."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
    ],
    "note": [
      "*arXiv preprint arXiv:2101.00190*."
    ],
    "arxiv": [
      "2101.00190"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "family": "Zhong",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Adapting Language Models for Zero-Shot Learning by Meta-Tuning on Dataset and Prompt Collections"
    ],
    "note": [
      "*arXiv preprint arXiv:2104.04670*."
    ],
    "arxiv": [
      "2104.04670"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the advancements in zero-shot and few-shot learning using pretrained language models up to 2022"
    ],
    "note": [
      "They cover foundational theories, practical applications, and innovative techniques in the field."
    ],
    "type": null
  }
]
