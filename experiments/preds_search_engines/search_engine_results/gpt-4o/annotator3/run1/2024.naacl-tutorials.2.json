[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a curated reading list of 20 articles up to 2024 that cover various aspects of security in large language models (LLMs), including attacks during training and inference"
    ],
    "note": [
      "as well as privacy concerns:"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Adversarial Attacks on Machine Learning Models: A Comprehensive Survey\"** - This article provides an overview of adversarial attacks, including those relevant to LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Data Poisoning Attacks Against Machine Learning Models: A Survey\"** - A detailed survey on data poisoning attacks, which are crucial to understanding training-time vulnerabilities"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Membership Inference Attacks Against Machine Learning Models\"** - This paper discusses how attackers can infer whether a particular data point was part of the training dataset of an LLM"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Model Extraction Attacks on Machine Learning Models: Taxonomy and Realization\"** - A comprehensive look at how attackers can extract models, including LLMs, and the implications for security"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Privacy-Preserving Machine Learning: Threats and Solutions\"** - An overview of privacy threats in ML and potential solutions, relevant to LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Adversarial Examples in Natural Language Processing\"** - This article explores how adversarial examples can be crafted to fool NLP models, including LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Differential Privacy in Machine Learning: A Survey and Review\"** - A detailed survey on differential privacy techniques and their application to LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Backdoor Attacks on Deep Learning Models: A Survey\"** - This paper reviews backdoor attacks, which are particularly relevant for understanding training-time security issues in LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Securing Machine Learning Systems: Threats, Challenges, and Solutions\"** - A broad overview of security challenges in ML systems, with sections relevant to LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures\"** - Discusses model inversion attacks, which can be used to reconstruct training data from LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Adversarial Training for Large Language Models: Challenges and Solutions\"** - Focuses on adversarial training techniques specific to LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Privacy Risks of Large Language Models: An Empirical Study\"** - An empirical study on the privacy risks associated with LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Securing NLP Models from Adversarial Attacks: Methods and Challenges\"** - Reviews methods for securing NLP models, including LLMs, from adversarial attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Federated Learning and Privacy: A Survey on Data Privacy in Decentralized Machine Learning\"** - Discusses federated learning and its implications for privacy in LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Poisoning Attacks on Federated Learning Systems\"** - Explores how poisoning attacks can affect federated learning, relevant for distributed training of LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Robustness and Security in Deep Learning Models for NLP\"** - Reviews robustness and security issues in deep learning models used for NLP, including LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Adversarial Robustness of Neural Networks: A Survey\"** - A survey on the adversarial robustness of neural networks, with implications for LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Machine Learning Security: Threats and Countermeasures\"** - A comprehensive review of security threats and countermeasures in ML, relevant to LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Privacy-Preserving Deep Learning: A Comprehensive Survey\"** - Discusses privacy-preserving techniques in deep learning, applicable to LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Inference Attacks on Machine Learning Models: A Survey\"** - Reviews various inference attacks on ML models, including those relevant to LLMs"
    ],
    "type": null
  },
  {
    "title": [
      "These articles should provide a solid foundation for understanding the security and privacy challenges associated with large language models, as well as the current state of research in mitigating these issues"
    ],
    "type": null
  }
]
