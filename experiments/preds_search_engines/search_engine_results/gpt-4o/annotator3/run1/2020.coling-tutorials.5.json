[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here is a reading list of 20 articles up to 2020 that discuss benchmarking datasets for question answering (QA"
    ],
    "container-title": [
      "natural language inference (NLI), commonsense reasoning"
    ],
    "note": [
      "and the associated problems of these datasets:"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"SQuAD: 100,000+ Questions for Machine Comprehension of Text\"** by Rajpurkar et al"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the Stanford Question Answering Dataset (SQuAD) and its impact on QA benchmarks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\"** by Joshi et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces TriviaQA and explores its challenges for QA systems"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"The Stanford Question Answering Dataset (SQuAD) v2.0\"** by Rajpurkar et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents an updated version of SQuAD with unanswerable questions to test model robustness"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Natural Questions: A Benchmark for Question Answering Research\"** by Kwiatkowski et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Describes the Natural Questions dataset and its use in evaluating QA systems"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\"** by Wang et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the GLUE benchmark for evaluating NLU models across multiple tasks, including NLI"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\"** by Wang et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents SuperGLUE, an extension of GLUE with more challenging tasks, including NLI and commonsense reasoning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Adversarial Examples for Evaluating Reading Comprehension Systems\"** by Jia and Liang"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the creation of adversarial examples to test the robustness of QA systems"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\"** by Zellers et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the SWAG dataset for commonsense reasoning and discusses its challenges"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge\"** by Talmor et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents the CommonsenseQA dataset and its role in evaluating commonsense reasoning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "given": "MultiNLI"
      }
    ],
    "title": [
      "The Stanford Question Answering Dataset (SQuAD) v2.0\"** by Williams et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the Multi-Genre Natural Language Inference (MultiNLI) dataset and its use in NLI tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"QuAC: Question Answering in Context\"** by Choi et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the QuAC dataset for QA in conversational contexts and discusses its challenges"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\"** by Dua et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Describes the DROP dataset, which focuses on discrete reasoning in QA"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "given": "R.A.C.E."
      }
    ],
    "title": [
      "Large-scale ReAding Comprehension Dataset From Examinations\"** by Lai et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the RACE dataset, based on English exams, and discusses its implications for QA research"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\"** by Yang et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents the HotpotQA dataset for multi-hop QA and discusses its challenges"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "given": "BoolQ"
      }
    ],
    "title": [
      "Exploring the Surprising Difficulty of Natural Yes/No Questions\"** by Clark et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the BoolQ dataset and explores the challenges of yes/no QA"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Story Cloze Test: A New Benchmark for Story Understanding and Generation\"** by Mostafazadeh et al"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the Story Cloze Test for evaluating story understanding and commonsense reasoning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\"** by Gao et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces The Pile, a large-scale dataset for language modeling, and discusses its diverse applications"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Adversarial NLI: A New Benchmark for Natural Language Understanding\"** by Nie et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents the Adversarial NLI dataset and discusses its role in evaluating NLU models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\"** by Zellers et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the SWAG dataset for commonsense reasoning and discusses its challenges"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList\"** by Ribeiro et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the CheckList framework for behavioral testing of NLP models, highlighting issues in existing benchmarks"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of key datasets and the challenges associated with them in the fields of QA, NLI, and commonsense reasoning"
    ],
    "type": null
  }
]
