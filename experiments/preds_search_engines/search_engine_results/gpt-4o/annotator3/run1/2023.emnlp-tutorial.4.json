[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here is a reading list of 20 articles up to 2023 that discuss conditioning large language models (LLMs) using task instructions, including various methods of creating such instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Language Models are Few-Shot Learners\"**"
    ],
    "publisher": [
      "Brown et al"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces GPT-3 and its ability to perform tasks with few-shot, one-shot, and zero-shot learning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Raffel et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the T5 model and the text-to-text framework for task conditioning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Instruction-based Fine-Tuning for Zero-Shot Task Generalization\"** - Wei et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores fine-tuning LLMs with task instructions to improve zero-shot performance"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm\"**"
    ],
    "publisher": [
      "Reynolds and McDonell"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines the use of prompt programming to condition LLMs for various tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Multitask Prompted Training Enables Zero-Shot Task Generalization\"** - Sanh et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates multitask prompted training to enhance zero-shot task generalization"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Unsupervised Translation of Programming Languages\"** - Lachaux et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses unsupervised methods for translating programming languages using LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Learning to Summarize with Human Feedback\"** - Stiennon et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores using human feedback to improve task-specific performance in summarization"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Scaling Laws for Neural Language Models\"** - Kaplan et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analyzes scaling laws and their implications for conditioning LLMs on tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Zero-Shot Text Classification with Generative Language Models\"** - Yin et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses zero-shot text classification using generative language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Raffel et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Provides insights into the T5 model and its application to various NLP tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Language Models as Knowledge Bases?\"** - Petroni et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the potential of LLMs to serve as knowledge bases and respond to factual queries"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Improving Language Understanding by Generative Pre-Training\"** - Radford et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the concept of generative pre-training for improving language understanding"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** - Devlin et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses BERT and its impact on task-specific performance through pre-training"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"UnifiedQA: Crossing Format Boundaries with a Single QA System\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Khashabi et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores a unified approach to question answering across different formats"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Few-Shot Text Generation with Pattern-Exploiting Training\"**"
    ],
    "publisher": [
      "Schick and Sch√ºtze"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines few-shot text generation using pattern-exploiting training techniques"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Language Models as Few-Shot Learners: A Study on Task Instructions\"**"
    ],
    "date": [
      "2021"
    ],
    "type": "article-journal",
    "container-title": [
      "Mishra et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the effectiveness of task instructions in few-shot learning scenarios"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Prompting GPT-3 to be Reliable\"** - Perez et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses methods to improve the reliability of GPT-3 through prompting"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Meta-Learning for Few-Shot NLP: A Survey\"** - Hospedales et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Surveys meta-learning approaches for few-shot learning in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "container-title": [
      "**\"Cross-Task Generalization via Natural Language Crowdsourcing Instructions\"** - Mishra et al"
    ],
    "date": [
      "2021"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores the use of crowdsourced natural language instructions for cross-task generalization"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Task-Agnostic Meta-Learning for Few-Shot Learning\"** - Finn et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces task-agnostic meta-learning and its application to few-shot learning"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of topics related to conditioning LLMs with task instructions, including few-shot learning, prompt programming, multitask training, and the use of human feedback"
    ],
    "type": null
  }
]
