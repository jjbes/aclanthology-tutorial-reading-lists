Sure! Here's a reading list of 20 seminal articles on word embeddings in natural language processing (NLP), covering conventional embeddings, sense embeddings, graph-based embeddings, and contextual embeddings, up to the year 2020:

### Conventional Embeddings
1. **Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013).** Efficient Estimation of Word Representations in Vector Space. *arXiv preprint arXiv:1301.3781*.
2. **Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013).** Distributed Representations of Words and Phrases and their Compositionality. *Advances in Neural Information Processing Systems (NeurIPS)*.
3. **Pennington, J., Socher, R., & Manning, C. D. (2014).** GloVe: Global Vectors for Word Representation. *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
4. **Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017).** Enriching Word Vectors with Subword Information. *Transactions of the Association for Computational Linguistics (TACL)*.

### Sense Embeddings
5. **Huang, E. H., Socher, R., Manning, C. D., & Ng, A. Y. (2012).** Improving Word Representations via Global Context and Multiple Word Prototypes. *Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)*.
6. **Neelakantan, A., Shankar, J., Passos, A., & McCallum, A. (2014).** Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space. *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
7. **Iacobacci, I., Pilehvar, M. T., & Navigli, R. (2015).** Sensembed: Learning Sense Embeddings for Word and Relational Similarity. *Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)*.

### Graph-based Embeddings
8. **Levy, O., & Goldberg, Y. (2014).** Neural Word Embedding as Implicit Matrix Factorization. *Advances in Neural Information Processing Systems (NeurIPS)*.
9. **Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., & Mei, Q. (2015).** LINE: Large-scale Information Network Embedding. *Proceedings of the 24th International Conference on World Wide Web (WWW)*.
10. **Grover, A., & Leskovec, J. (2016).** node2vec: Scalable Feature Learning for Networks. *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)*.

### Contextual Embeddings
11. **Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018).** Deep Contextualized Word Representations. *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*.
12. **Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019).** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*.
13. **Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018).** Improving Language Understanding by Generative Pre-Training. *OpenAI Preprint*.
14. **Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).** Language Models are Unsupervised Multitask Learners. *OpenAI Preprint*.
15. **Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019).** RoBERTa: A Robustly Optimized BERT Pretraining Approach. *arXiv preprint arXiv:1907.11692*.
16. **Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019).** XLNet: Generalized Autoregressive Pretraining for Language Understanding. *Advances in Neural Information Processing Systems (NeurIPS)*.
17. **Clark, K., Luong, M. T., Le, Q. V., & Manning, C. D. (2020).** ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. *Proceedings of the 8th International Conference on Learning Representations (ICLR)*.

### Miscellaneous and Advanced Topics
18. **Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017).** Enriching Word Vectors with Subword Information. *Transactions of the Association for Computational Linguistics (TACL)*.
19. **Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018).** Deep Contextualized Word Representations. *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*.
20. **Ethayarajh, K. (2019).** How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.

This list should provide a comprehensive overview of the evolution and diversity of word embeddings in NLP up to 2020.