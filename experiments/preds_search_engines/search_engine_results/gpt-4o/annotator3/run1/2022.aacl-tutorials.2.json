[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a curated list of 20 articles up to 2022 that cover various aspects of pretrained language models, including their architectures, pretraining methods, and fine-tuning techniques"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Jacob Devlin, Ming-Wei Chang"
    ],
    "location": [
      "Kenton Lee"
    ],
    "publisher": [
      "Kristina Toutanova"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1810.04805](https://arxiv.org/abs/1810.04805"
    ],
    "arxiv": [
      "1810.04805"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**GPT-3: Language Models are Few-Shot Learners**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Tom B.Brown"
      },
      {
        "family": "Mann",
        "given": "Benjamin"
      },
      {
        "family": "Ryder",
        "given": "Nick"
      },
      {
        "others": true
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:2005.14165](https://arxiv.org/abs/2005.14165"
    ],
    "arxiv": [
      "2005.14165"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**RoBERTa: A Robustly Optimized BERT Pretraining Approach**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Yinhan Liu, Myle Ott, Naman Goyal, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1907.11692](https://arxiv.org/abs/1907.11692"
    ],
    "arxiv": [
      "1907.11692"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**XLNet: Generalized Autoregressive Pretraining for Language Understanding**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Zhilin Yang, Zihang Dai, Yiming Yang, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1906.08237](https://arxiv.org/abs/1906.08237"
    ],
    "arxiv": [
      "1906.08237"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**ALBERT: A Lite BERT for Self-supervised Learning of Language Representations**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Zhenzhong Lan"
    ],
    "editor": [
      {
        "family": "Chen",
        "given": "Mingda"
      },
      {
        "family": "Goodman",
        "given": "Sebastian"
      },
      {
        "others": true
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1909.11942](https://arxiv.org/abs/1909.11942"
    ],
    "arxiv": [
      "1909.11942"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Colin Raffel, Noam Shazeer, Adam Roberts, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1910.10683](https://arxiv.org/abs/1910.10683"
    ],
    "arxiv": [
      "1910.10683"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "given": "E.R.N.I.E."
      }
    ],
    "title": [
      "Enhanced Representation through Knowledge Integration**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Yu Sun, Shuohuan Wang, Yukun Li, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1904.09223](https://arxiv.org/abs/1904.09223"
    ],
    "arxiv": [
      "1904.09223"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "given": "E.L.E.C.T.R.A."
      }
    ],
    "title": [
      "Pre-training Text Encoders as Discriminators Rather Than Generators**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Kevin Clark"
      },
      {
        "family": "Luong",
        "given": "Minh-Thang"
      },
      {
        "family": "Le",
        "given": "Quoc V."
      },
      {
        "family": "Manning",
        "given": "Christopher D."
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:2003.10555](https://arxiv.org/abs/2003.10555"
    ],
    "arxiv": [
      "2003.10555"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Victor Sanh, Lysandre Debut"
    ],
    "publisher": [
      "Julien Chaumond, Thomas Wolf"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1910.01108](https://arxiv.org/abs/1910.01108"
    ],
    "arxiv": [
      "1910.01108"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**SpanBERT: Improving Pre-training by Representing and Predicting Spans**"
    ],
    "type": null
  },
  {
    "note": [
      "- Authors: Mandar Joshi, Danqi Chen, Yinhan Liu, et al."
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1907.10529](https://arxiv.org/abs/1907.10529"
    ],
    "arxiv": [
      "1907.10529"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "given": "B.A.R.T."
      }
    ],
    "title": [
      "Denoising Sequence-to-Sequence Pre-training for Natural Language Generation"
    ],
    "translator": [
      {
        "given": "Comprehension"
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Mike Lewis"
      },
      {
        "family": "Liu",
        "given": "Yinhan"
      },
      {
        "family": "Goyal",
        "given": "Naman"
      },
      {
        "others": true
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1910.13461](https://arxiv.org/abs/1910.13461"
    ],
    "arxiv": [
      "1910.13461"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "note": [
      "**DeBERTa: Decoding-enhanced BERT with Disentangled Attention**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:2006.03654](https://arxiv.org/abs/2006.03654"
    ],
    "arxiv": [
      "2006.03654"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**BigBird: Transformers for Longer Sequences**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Manzil Zaheer, Guru Guruganesh, Avinava Dubey, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:2007.14062](https://arxiv.org/abs/2007.14062"
    ],
    "arxiv": [
      "2007.14062"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "location": [
      "Reformer"
    ],
    "publisher": [
      "The Efficient Transformer**"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Nikita Kitaev, ≈Åukasz Kaiser, Anselm Levskaya"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:2001.04451](https://arxiv.org/abs/2001.04451"
    ],
    "arxiv": [
      "2001.04451"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity**"
    ],
    "type": null
  },
  {
    "citation-number": [
      "-"
    ],
    "location": [
      "Authors"
    ],
    "publisher": [
      "William Fedus, Barret Zoph, Noam Shazeer"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:2101.03961](https://arxiv.org/abs/2101.03961"
    ],
    "arxiv": [
      "2101.03961"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**Pegasus: Pre-training with Extracted Gap-sentences for Abstractive Summarization**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Jingqing Zhang"
    ],
    "editor": [
      {
        "family": "Zhao",
        "given": "Yao"
      },
      {
        "family": "Saleh",
        "given": "Mohammad"
      },
      {
        "family": "Liu",
        "given": "Peter J."
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1912.08777](https://arxiv.org/abs/1912.08777"
    ],
    "arxiv": [
      "1912.08777"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**MT-DNN: A Deep Learning Approach to Natural Language Understanding**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Xiaodong Liu, Pengcheng He, Weizhu Chen"
    ],
    "location": [
      "Jianfeng Gao"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1901.11504](https://arxiv.org/abs/1901.11504"
    ],
    "arxiv": [
      "1901.11504"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "location": [
      "Adapter-BERT"
    ],
    "publisher": [
      "Adapting Pre-trained Transformers with Adapters**"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Jonas Pfeiffer, Aishwarya Kamath, Andreas R√ºckl√©, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:2002.01886](https://arxiv.org/abs/2002.01886"
    ],
    "arxiv": [
      "2002.01886"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Colin Raffel, Tunstall, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:2006.05987](https://arxiv.org/abs/2006.05987"
    ],
    "arxiv": [
      "2006.05987"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**Parameter-Efficient Transfer Learning for NLP**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Authors: Neil Houlsby"
    ],
    "editor": [
      {
        "family": "Giurgiu",
        "given": "Andrei"
      },
      {
        "family": "Jastrzebski",
        "given": "Stanislaw"
      },
      {
        "others": true
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "- Link: [arXiv:1902.00751](https://arxiv.org/abs/1902.00751"
    ],
    "arxiv": [
      "1902.00751"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the advancements in pretrained language models, their architectures, and various fine-tuning techniques"
    ],
    "type": null
  }
]
