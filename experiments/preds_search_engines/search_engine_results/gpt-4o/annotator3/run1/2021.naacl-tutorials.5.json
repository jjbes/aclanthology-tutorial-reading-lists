[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2021 that focus on dealing with long documents in natural language processing (NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "literal": "**\"Attention is All You Need\"** - Vaswani et al."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the Transformer model, which forms the basis for many subsequent models handling long documents"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Longformer: The Long-Document Transformer\"** - Beltagy et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes the Longformer model designed to handle long documents efficiently using a combination of local and global attention mechanisms"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Reformer: The Efficient Transformer\"** - Kitaev et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the Reformer model, which uses locality-sensitive hashing and reversible layers to handle long sequences"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "given": "E.T.C."
      }
    ],
    "title": [
      "Encoding Long and Structured Inputs in Transformers\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Ainslie et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents the Extended Transformer Construction (ETC) for encoding long and structured inputs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Big Bird: Transformers for Longer Sequences\"** - Zaheer et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes Big Bird, a sparse attention mechanism to handle longer sequences efficiently"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Efficient Transformers: A Survey\"** - Tay et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Surveys various approaches to making Transformers more efficient, including handling long documents"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Hierarchical Attention Networks for Document Classification\"** - Yang et al"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a hierarchical attention network that processes documents at the word and sentence levels"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Dissecting Long Documents with Local Attentions\"** - Guo et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a method for handling long documents by breaking them into smaller chunks and applying local attention"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Extractive Summarization as Text Matching\"** - Liu et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses methods for summarizing long documents by treating summarization as a text matching problem"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Hierarchical Transformers for Long Document Classification\"** - Dai et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a hierarchical Transformer model for classifying long documents"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Sparse Transformer: Generating Text with Sparse Attention\"** - Child et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a sparse attention mechanism to handle long sequences in text generation tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Lepikhin et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses techniques for scaling large models, including handling long sequences through model sharding"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Long-Range Arena: A Benchmark for Efficient Transformers\"** - Tay et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a benchmark for evaluating the efficiency of Transformers on long-range tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\"**"
    ],
    "date": [
      "2019"
    ],
    "type": "article-journal",
    "container-title": [
      "Dai et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces Transformer-XL, which extends the context length of Transformers using segment-level recurrence"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Compressive Transformers for Long-Range Sequence Modelling\"** - Rae et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes Compressive Transformers, which compress past hidden states to handle long-range dependencies"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "literal": "**\"Structured Attention Networks\"** - Kim et al."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces structured attention mechanisms to handle structured data within long documents"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Hierarchical Transformers are More Efficient Language Models\"** - Ainslie et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses hierarchical Transformers for more efficient language modeling of long documents"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Efficient Attention: Attention with Linear Complexities\"** - Katharopoulos et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes an efficient attention mechanism with linear complexity for handling long sequences"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Long Document Classification from Local Word Glimpses via Recurrent Attention Learning\"**"
    ],
    "date": [
      "2019"
    ],
    "type": "article-journal",
    "container-title": [
      "Adhikari et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a recurrent attention mechanism to classify long documents by focusing on local word glimpses"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "literal": "**\"Efficient and Scalable Multi-Document Summarization\"** - Liu et al."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses methods for summarizing multiple long documents efficiently and scalably"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of techniques and models developed to handle the challenges posed by long documents in NLP, from attention mechanisms and hierarchical models to efficient training and summarization methods"
    ],
    "type": null
  }
]
