[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a list of 20 articles up to 2023 that cover various security challenges in Natural Language Processing (NLP) models, including different types of attacks and defenses"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Attacks on Neural Network Policies\"** - Huang, S., Papernot, N., Goodfellow, I., Duan, Y., & Abbeel, P."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses adversarial attacks on NLP models and their implications"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"HotFlip: White-Box Adversarial Examples for Text Classification\"**"
    ],
    "editor": [
      {
        "family": "Ebrahimi",
        "given": "J."
      },
      {
        "family": "Rao",
        "given": "A."
      },
      {
        "family": "Lowd",
        "given": "D."
      },
      {
        "family": "Dou",
        "given": "D."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a method for generating white-box adversarial examples in text classification"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"TextBugger: Generating Adversarial Text Against Real-world Applications\"**"
    ],
    "director": [
      {
        "family": "Li",
        "given": "J."
      },
      {
        "family": "Ji",
        "given": "S."
      },
      {
        "family": "Du",
        "given": "T."
      },
      {
        "family": "Li",
        "given": "B."
      },
      {
        "family": "Wang",
        "given": "T."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores adversarial text generation techniques and their impact on real-world NLP applications"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "literal": "**\"Universal Adversarial Triggers for Attacking and Analyzing NLP\"** - Wallace, E., Feng, S., Kandpal, N., Gardner, M., & Singh, S."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates universal adversarial triggers that can fool NLP models across various tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Robustness and Generalization of Structured Prediction Models\"**"
    ],
    "editor": [
      {
        "family": "Belinkov",
        "given": "Y."
      },
      {
        "family": "Bisk",
        "given": "Y."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines the robustness of structured prediction models in NLP against adversarial attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"On the Robustness of Language Encoders against Grammatical Errors\"**"
    ],
    "editor": [
      {
        "family": "Pruthi",
        "given": "D."
      },
      {
        "family": "Dhingra",
        "given": "B."
      },
      {
        "family": "Lipton",
        "given": "Z.C."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Studies the impact of grammatical errors on the robustness of language encoders"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Mitigating Adversarial Effects Through Randomized Substitution and Voting\"**"
    ],
    "editor": [
      {
        "family": "Dong",
        "given": "Y."
      },
      {
        "family": "Liu",
        "given": "J."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes defense mechanisms against adversarial attacks in NLP through randomized substitution and voting"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "literal": "**\"Certified Robustness to Adversarial Word Substitutions\"** - Jia, R., & Liang, P."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces methods for certifying the robustness of NLP models to adversarial word substitutions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Adversarial Training for Large Neural Language Models\"**"
    ],
    "editor": [
      {
        "family": "Zhu",
        "given": "C."
      },
      {
        "family": "Xu",
        "given": "Y."
      },
      {
        "family": "Yu",
        "given": "B."
      },
      {
        "family": "Wang",
        "given": "L."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses adversarial training techniques to enhance the robustness of large neural language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "literal": "**\"Detecting Adversarial Samples from Artifacts\"** - Mosca, A., & Maggi, F."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores methods for detecting adversarial samples in NLP by identifying artifacts introduced during adversarial attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Improving the Robustness of NLP Models to Adversarial Attacks with Data Augmentation\"**"
    ],
    "editor": [
      {
        "family": "Wei",
        "given": "J."
      },
      {
        "family": "Zou",
        "given": "K."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the use of data augmentation to improve the robustness of NLP models against adversarial attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Adversarial Examples for Evaluating Reading Comprehension Systems\"**"
    ],
    "editor": [
      {
        "family": "Jia",
        "given": "R."
      },
      {
        "family": "Liang",
        "given": "P."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Evaluates the vulnerability of reading comprehension systems to adversarial examples"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Towards Robust Toxic Content Classification\"**"
    ],
    "editor": [
      {
        "family": "Mozafari",
        "given": "M."
      },
      {
        "family": "Farahbakhsh",
        "given": "R."
      },
      {
        "family": "Crespi",
        "given": "N."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses techniques to improve the robustness of toxic content classification systems against adversarial attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review\"**"
    ],
    "editor": [
      {
        "family": "Sun",
        "given": "X."
      },
      {
        "family": "Wang",
        "given": "Y."
      },
      {
        "family": "Tang",
        "given": "J."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Provides a comprehensive review of adversarial attacks and defenses across images, graphs, and text"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Attacks on Text Classifiers via Sentence Rearrangement\"** - Iyyer, M., Wieting, J., Gimpel, K., & Zettlemoyer, L."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores adversarial attacks on text classifiers through sentence rearrangement"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Adversarial Examples for Text: A Review\"**"
    ],
    "editor": [
      {
        "family": "Zhang",
        "given": "J."
      },
      {
        "family": "Yang",
        "given": "J."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Reviews various methods for generating adversarial examples in text and their implications for NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "literal": "**\"Robustness of Neural Networks to Adversarial and Noisy Word Substitutions\"** - Alzantot, M., Sharma, Y., Elgohary, A., Ho, B., Srivastava, M. B., & Chang, K. W."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Studies the robustness of neural networks to adversarial and noisy word substitutions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Adversarial Training for Text Classification with Virtual Adversarial Training\"**"
    ],
    "editor": [
      {
        "family": "Miyato",
        "given": "T."
      },
      {
        "family": "Dai",
        "given": "A.M."
      },
      {
        "family": "Goodfellow",
        "given": "I."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the use of virtual adversarial training to improve the robustness of text classification models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Exploring the Vulnerability of Deep Neural Networks: A Study of Parameter Corruption\"**"
    ],
    "editor": [
      {
        "family": "Wang",
        "given": "Z."
      },
      {
        "given": "Gong"
      }
    ],
    "volume": [
      "N. Z"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the vulnerability of deep neural networks in NLP to parameter corruption attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey\"**"
    ],
    "editor": [
      {
        "family": "Zhang",
        "given": "X."
      },
      {
        "family": "Zhou",
        "given": "Y."
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Provides a survey of adversarial attacks on deep learning models in NLP and discusses various defense strategies"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of topics related to security challenges in NLP, including different types of adversarial attacks and various defense mechanisms"
    ],
    "type": null
  }
]
