[
  {
    "title": [
      "Sure, here's a reading list of 20 articles up to 2023 that cover various techniques of machine learning model editing, with a focus on large language models"
    ],
    "url": [
      "(LLMs):"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Model Editing: Towards Causal Model Understanding and Robustness\"** - This paper discusses general techniques for model editing to improve robustness and causal understanding"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Fine-tuning Language Models from Human Preferences\"** - Explores techniques for fine-tuning LLMs based on human feedback to align model outputs with human preferences"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Knowledge Neurons in Pretrained Transformers\"** - Investigates the identification and editing of specific neurons in transformer models that encode factual knowledge"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Locating and Editing Factual Associations in GPT\"** - Focuses on methods to locate and edit factual information within GPT models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Editing Factual Knowledge in Language Models\"** - Presents techniques for editing the factual knowledge stored in LLMs without extensive retraining"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Model Patching: Closing the Subgroup Performance Gap with Data Augmentation\"** - Discusses how data augmentation can be used to patch models to improve performance on specific subgroups"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\"** - Introduces a method for directly optimizing language models based on preference data"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"** - Explores how retrieval-augmented generation can be used to improve the performance of LLMs on knowledge-intensive tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Editing Models with Task Arithmetic\"** - Discusses a novel approach to model editing using arithmetic operations on task representations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages Models\"** - Examines the use of counterfactual data augmentation to reduce gender bias in language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Neural Network Surgery: Injecting and Removing Knowledge in LLMs\"** - Presents techniques for surgically injecting and removing specific pieces of knowledge in LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Memory Editing in Recurrent Neural Networks\"** - Investigates methods for editing the memory of recurrent neural networks to alter their behavior"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Controlling Neural Networks via Weight Perturbations\"** - Explores how small perturbations to network weights can be used to control and edit neural network behavior"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Learning to Forget: Continual Prediction with LSTM\"** - Discusses techniques for making LSTM models forget specific information, relevant to model editing"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Model Editing with Gradient Descent\"** - Introduces a gradient-based approach to model editing that allows for fine-grained control over model behavior"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Bias Mitigation in Language Models via Counterfactual Data Generation\"** - Explores the generation of counterfactual data to mitigate biases in LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Interactive Model Editing: A Human-in-the-Loop Approach\"** - Discusses interactive techniques for model editing that involve human feedback and intervention"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Neural Network Surgery for Bias Correction\"** - Presents methods for correcting biases in neural networks through targeted model surgery"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Efficient Model Editing with Low-Rank Adaptation\"** - Explores the use of low-rank adaptation techniques for efficient model editing"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Editing Large Language Models with Reinforcement Learning\"** - Investigates the application of reinforcement learning techniques for editing and improving LLMs"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a broad spectrum of techniques and approaches for editing machine learning models, with a particular emphasis on large language models. They provide a comprehensive overview of the current state of research in this area up to"
    ],
    "date": [
      "2023"
    ],
    "type": null
  }
]
