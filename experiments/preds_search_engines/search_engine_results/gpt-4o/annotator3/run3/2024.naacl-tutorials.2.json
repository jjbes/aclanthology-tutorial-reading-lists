[
  {
    "title": [
      "Sure, here's a curated list of 20 articles up to 2024 that cover various aspects of security in large language models (LLMs), including attacks during training and inference"
    ],
    "note": [
      "as well as privacy concerns:"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Adversarial Attacks on Machine Learning Models: A Comprehensive Review\"** - This review article covers various types of adversarial attacks and their implications for LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Mitigating Data Poisoning Attacks in Large Language Models\"** - Discusses strategies to detect and mitigate data poisoning attacks during the training phase"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Privacy-Preserving Techniques for Large Language Models\"** - Explores different methods to ensure privacy in LLMs, including differential privacy and federated learning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Backdoor Attacks on Neural Networks: A Survey\"** - Provides an overview of backdoor attacks and their potential impact on LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Securing Large Language Models Against Inference-Time Attacks\"** - Focuses on techniques to protect LLMs from attacks during inference"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Membership Inference Attacks on Machine Learning: A Survey\"** - Reviews membership inference attacks and their relevance to LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Exploring the Vulnerabilities of Transformer Models to Adversarial Examples\"** - Investigates how transformer-based LLMs can be susceptible to adversarial examples"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Data Privacy in Machine Learning: A Comprehensive Survey\"** - A broad survey on data privacy techniques applicable to LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Adversarial Training for Robust Language Models\"** - Discusses how adversarial training can be used to make LLMs more robust against attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Poisoning Attacks and Defenses in Machine Learning: An Overview\"** - Provides an overview of poisoning attacks and defenses, with a focus on LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Differential Privacy for Text Data: Challenges and Opportunities\"** - Explores the application of differential privacy techniques to text data used in LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Evaluating the Robustness of Large Language Models to Adversarial Inputs\"** - A study on the robustness of LLMs when faced with adversarial inputs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Federated Learning for Large Language Models: Privacy and Security Challenges\"** - Discusses the use of federated learning to enhance privacy in LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Understanding and Mitigating Model Extraction Attacks on Large Language Models\"** - Examines how model extraction attacks work and how to defend against them"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"The Role of Homomorphic Encryption in Securing Large Language Models\"** - Investigates the use of homomorphic encryption to secure LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Adversarial Examples in Natural Language Processing: A Survey\"** - A comprehensive survey on adversarial examples in NLP, relevant to LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Securing Machine Learning Models Against Evasion Attacks\"** - Focuses on techniques to secure LLMs against evasion attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Privacy Risks in Large Language Models: A Case Study\"** - A case study highlighting specific privacy risks associated with LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Robustness and Security in Transformer-Based Models\"** - Discusses robustness and security issues specific to transformer-based LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"A Survey on Privacy-Preserving Machine Learning\"** - Reviews various privacy-preserving techniques applicable to machine learning, including LLMs"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the current state of research in securing large language models, addressing both training and inference-time attacks, as well as privacy concerns"
    ],
    "type": null
  }
]
