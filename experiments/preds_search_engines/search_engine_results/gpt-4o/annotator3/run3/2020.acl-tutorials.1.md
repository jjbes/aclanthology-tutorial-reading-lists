Certainly! Here's a curated list of 20 articles up to 2020 that focus on explaining or interpreting natural language processing (NLP) neural models. These articles cover a range of techniques and perspectives, from visualization and attention mechanisms to probing and interpretability frameworks.

1. **Attention is All You Need** - Vaswani et al., 2017
   - Introduces the Transformer model and the attention mechanism, which has become a cornerstone for interpreting NLP models.
   
2. **Visualizing and Understanding Neural Models in NLP** - Li et al., 2016
   - Discusses methods for visualizing and understanding the inner workings of neural models in NLP.

3. **A Structural Probe for Finding Syntax in Word Representations** - Hewitt and Manning, 2019
   - Proposes a method to probe word embeddings for syntactic structure, offering insights into what neural models learn about syntax.

4. **Interpreting and Understanding Bert** - Clark et al., 2019
   - Analyzes the BERT model to understand how it processes and represents language.

5. **What Does BERT Look At? An Analysis of BERT's Attention** - Clark et al., 2019
   - Investigates the attention heads in BERT to understand what aspects of the input they focus on.

6. **Deep Learning for NLP and Speech Recognition** - Deng and Liu, 2018
   - Provides a comprehensive overview of deep learning techniques in NLP and speech recognition, including interpretability aspects.

7. **Learning to Explain: An Information-Theoretic Perspective on Model Interpretation** - Chen et al., 2018
   - Introduces an information-theoretic approach to model interpretation, focusing on the trade-off between explanation fidelity and complexity.

8. **Evaluating the Interpretability of Generative Models by Interactive Reconstruction** - Kim et al., 2016
   - Proposes a method for evaluating the interpretability of generative models through interactive reconstruction tasks.

9. **Attention is Not Explanation** - Jain and Wallace, 2019
   - Critically examines the use of attention mechanisms as explanations for model predictions.

10. **Probing Neural Network Comprehension of Natural Language Arguments** - Williams et al., 2018
    - Uses probing tasks to evaluate how well neural networks understand natural language arguments.

11. **A Survey of Methods for Explaining Black Box Models** - Guidotti et al., 2018
    - Surveys various methods for explaining black-box models, including those used in NLP.

12. **Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)** - Kim et al., 2018
    - Introduces TCAV, a method for testing the interpretability of models by quantifying the influence of high-level concepts.

13. **LIME: Local Interpretable Model-Agnostic Explanations** - Ribeiro et al., 2016
    - Proposes LIME, a technique for explaining individual predictions of any classifier in an interpretable and faithful manner.

14. **Anchors: High-Precision Model-Agnostic Explanations** - Ribeiro et al., 2018
    - Extends LIME by introducing anchors, which provide high-precision explanations for model predictions.

15. **Explaining Neural Networks by Decoding Layer Activations** - Alvarez-Melis and Jaakkola, 2018
    - Proposes a method for explaining neural networks by decoding the activations of their layers.

16. **Contextual Decomposition for Neural Network Interpretability** - Murdoch et al., 2018
    - Introduces contextual decomposition, a technique for interpreting the contributions of individual input features in neural networks.

17. **On the Importance of Single Directions for Generalization** - Morcos et al., 2018
    - Investigates the role of individual directions in the latent space of neural networks and their impact on generalization.

18. **Dissecting Contextual Word Embeddings: Architecture and Representation** - Peters et al., 2018
    - Analyzes the architecture and representation of contextual word embeddings to understand their properties.

19. **The Building Blocks of Interpretability** - Olah et al., 2018
    - Explores the fundamental components of interpretability in neural networks, including feature visualization and attribution.

20. **Towards a Rigorous Science of Interpretable Machine Learning** - Doshi-Velez and Kim, 2017
    - Discusses the need for a rigorous approach to interpretability in machine learning and proposes a framework for evaluating interpretability methods.

These articles provide a comprehensive overview of various approaches and methodologies for explaining and interpreting NLP neural models.