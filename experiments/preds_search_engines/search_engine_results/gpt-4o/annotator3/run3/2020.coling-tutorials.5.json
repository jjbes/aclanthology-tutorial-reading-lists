[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2020 that discuss benchmarking datasets for question answering (QA"
    ],
    "container-title": [
      "natural language inference (NLI), commonsense reasoning"
    ],
    "note": [
      "and the associated problems with these datasets:"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"SQuAD: 100,000+ Questions for Machine Comprehension of Text\"** by Rajpurkar et al"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the Stanford Question Answering Dataset (SQuAD) and its impact on QA benchmarks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"The Stanford Question Answering Dataset (SQuAD)\"** by Rajpurkar et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "An updated version of the original SQuAD paper, addressing improvements and challenges"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\"** by Wang et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the GLUE benchmark, which includes tasks for NLI and discusses the limitations and challenges"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Adversarial Examples for Evaluating Reading Comprehension Systems\"** by Jia and Liang"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines how adversarial examples can expose weaknesses in QA systems"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\"** by Zellers et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the SWAG dataset for commonsense reasoning and discusses its challenges"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge\"** by Talmor et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents the CommonsenseQA dataset and highlights the difficulties in commonsense reasoning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"MultiNLI: The Stanford Natural Language Inference Corpus\"** by Williams et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Describes the MultiNLI dataset and discusses the challenges in creating a diverse NLI dataset"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"QuAC: Question Answering in Context\"** by Choi et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the QuAC dataset for QA in conversational contexts and discusses its unique challenges"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\"** by Dua et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the DROP dataset, which requires discrete reasoning, and the associated challenges"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\"** by Yang et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the HotpotQA dataset and discusses the challenges of multi-hop QA"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "given": "R.A.C.E."
      }
    ],
    "title": [
      "Large-scale ReAding Comprehension Dataset From Examinations\"** by Lai et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents the RACE dataset and discusses its use in evaluating reading comprehension"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "given": "BoolQ"
      }
    ],
    "title": [
      "Exploring the Surprising Difficulty of Natural Yes/No Questions\"** by Clark et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the BoolQ dataset and discusses the challenges of yes/no QA"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Natural Questions: A Benchmark for Question Answering Research\"** by Kwiatkowski et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the Natural Questions dataset and the challenges it presents for QA research"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\"** by Gao et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces The Pile dataset and discusses its potential use and challenges in various NLP tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Adversarial NLI: A New Benchmark for Natural Language Understanding\"** by Nie et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the Adversarial NLI dataset and discusses the challenges of creating robust NLI models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"NarrativeQA: Reading Comprehension Challenge with Humans in the Loop\"** by Kočiský et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents the NarrativeQA dataset and discusses the challenges of narrative comprehension"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"SciTail: A Textual Entailment Dataset from Science Question Answering\"** by Khot et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the SciTail dataset and discusses its use in evaluating entailment in scientific texts"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "container-title": [
      "**\"OpenBookQA: Open Book Question Answering\"** by Mihaylov et al"
    ],
    "date": [
      "2018"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the OpenBookQA dataset and the challenges of open-book question answering"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"CoQA: A Conversational Question Answering Challenge\"** by Reddy et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the CoQA dataset and discusses the challenges of conversational QA"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\"** by Wang et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents the SuperGLUE benchmark, an extension of GLUE, and discusses the challenges it addresses"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the various datasets used in QA, NLI, and commonsense reasoning, as well as the inherent challenges and problems associated with these datasets"
    ],
    "type": null
  }
]
