Sure, here is a reading list of 20 articles up to 2020 that cover the current issues in natural language generation (NLG), specifically focusing on the lack of diversity, the non-factuality of generated text, and evaluation metrics.

### Lack of Diversity
1. **"A Diversity-Promoting Objective Function for Neural Conversation Models"** - Li et al., 2016
   - This paper introduces a diversity-promoting objective function to address the issue of repetitive and generic responses in dialogue systems.

2. **"Maximum Mutual Information Model for Neural Sequence Generation"** - Li et al., 2016
   - The authors propose using Maximum Mutual Information (MMI) as an objective to improve the diversity of generated sequences.

3. **"Generating Diverse and Consistent QA Pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs"** - Duan et al., 2019
   - This paper presents a hierarchical conditional variational autoencoder (VAE) to generate diverse and consistent question-answer pairs.

4. **"Improving Diversity in Dialogue Generation by Injecting Data-Specific Variations"** - Zhao et al., 2017
   - The authors propose a method to inject data-specific variations to improve the diversity of dialogue responses.

5. **"Learning Diverse Text Generation Policies via Submodular Optimization"** - Ippolito et al., 2019
   - This paper explores the use of submodular optimization to learn diverse text generation policies.

### Non-Factuality of Generated Text
6. **"Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence"** - Zhou et al., 2019
   - The authors propose a method for robust fact verification to ensure the factual accuracy of generated text.

7. **"Fact-Checking in Community Forums"** - Mihaylova et al., 2018
   - This paper discusses methods for fact-checking in community forums, which is relevant for ensuring the factuality of user-generated content.

8. **"Learning to Fact-Check: Regularized Adversarial Learning for Generative Fact-Checking Models"** - Shu et al., 2019
   - The authors present a generative fact-checking model that uses regularized adversarial learning to improve factual accuracy.

9. **"Neural Text Generation: A Practical Guide"** - Gatt and Krahmer, 2018
   - This comprehensive guide covers various aspects of neural text generation, including challenges related to factuality.

10. **"Improving Factual Consistency in Abstractive Summarization with Knowledge Graph"** - Huang et al., 2020
    - The paper proposes using knowledge graphs to improve the factual consistency of abstractive summaries.

### Evaluation Metrics
11. **"BLEU: a Method for Automatic Evaluation of Machine Translation"** - Papineni et al., 2002
    - A foundational paper introducing BLEU, a widely used metric for evaluating the quality of machine-generated text.

12. **"ROUGE: A Package for Automatic Evaluation of Summaries"** - Lin, 2004
    - This paper introduces ROUGE, another key metric used for evaluating the quality of text summaries.

13. **"BERTScore: Evaluating Text Generation with BERT"** - Zhang et al., 2020
    - The authors propose BERTScore, a new evaluation metric that leverages BERT embeddings to assess the quality of generated text.

14. **"Unsupervised Quality Estimation for Neural NLG"** - Chaganty et al., 2018
    - This paper presents methods for unsupervised quality estimation of neural NLG systems.

15. **"Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer"** - Yamshchikov et al., 2020
    - The authors evaluate various metrics for style transfer, providing insights into the effectiveness of different evaluation approaches.

16. **"Towards a Better Metric for Evaluating Question Generation Systems"** - Chen et al., 2019
    - This paper discusses the limitations of existing metrics and proposes a new metric for evaluating question generation systems.

17. **"Re-evaluating Evaluation in Text Generation"** - Hashimoto et al., 2019
    - The authors critique current evaluation practices and propose new methods for more robust evaluation of text generation systems.

18. **"Perplexity is Not a Good Measure of Text Generation Quality"** - Theis et al., 2016
    - This paper argues against the use of perplexity as a sole measure of text generation quality and suggests alternative approaches.

19. **"Towards Better Storylines with Sentence-Level Language Models"** - Fan et al., 2018
    - The authors propose improvements in storyline generation and discuss evaluation metrics for narrative coherence.

20. **"Evaluating the Factual Consistency of Abstractive Text Summarization"** - Kryściński et al., 2019
    - This paper introduces methods to evaluate the factual consistency of abstractive text summarization systems.

These articles should provide a comprehensive overview of the current issues in NLG related to diversity, factuality, and evaluation metrics.