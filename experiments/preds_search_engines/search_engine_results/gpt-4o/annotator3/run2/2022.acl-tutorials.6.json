[
  {
    "title": [
      "Sure, here is a list of 20 articles up to 2022 that discuss zero-shot and few-shot learning using pretrained language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "family": "Brown",
        "given": "T.B."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Language Models are Few-Shot Learners.\"**"
    ],
    "container-title": [
      "Advances in Neural Information Processing Systems (NeurIPS"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Language Models are Unsupervised Multitask Learners.\"** OpenAI"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "family": "Schick",
        "given": "T."
      },
      {
        "family": "Schütze",
        "given": "H."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference.\"**"
    ],
    "container-title": [
      "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "family": "Gao",
        "given": "T."
      },
      {
        "family": "Fisch",
        "given": "A."
      },
      {
        "family": "Chen",
        "given": "D."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Making Pre-trained Language Models Better Few-shot Learners.\"**"
    ],
    "container-title": [
      "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "family": "Yin",
        "given": "W."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Universal Natural Language Processing with Limited Annotations"
    ],
    "container-title": [
      "Try Few-shot Text Generation.\"** In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "family": "Zhao",
        "given": "W."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Calibrate Before Use"
    ],
    "container-title": [
      "Improving Few-shot Performance of Language Models.\"** In Proceedings of the 38th International Conference on Machine Learning (ICML"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "family": "Schick",
        "given": "T."
      },
      {
        "family": "Schütze",
        "given": "H."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners.\"**"
    ],
    "container-title": [
      "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "P."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "GPT Understands"
    ],
    "note": [
      "Too.\"** arXiv preprint arXiv:2103.10385."
    ],
    "arxiv": [
      "2103.10385"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "family": "Petroni",
        "given": "F."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Language Models as Knowledge Bases?\"**"
    ],
    "container-title": [
      "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "family": "Kumar",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Few-shot Learning with Pre-trained Language Models.\"**"
    ],
    "note": [
      "arXiv preprint arXiv:2106.13884."
    ],
    "arxiv": [
      "2106.13884"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "family": "Raffel",
        "given": "C."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.\"**"
    ],
    "type": "article-journal",
    "container-title": [
      "Journal of Machine Learning Research"
    ]
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "family": "Sanh",
        "given": "V."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Multitask Prompted Training Enables Zero-Shot Task Generalization.\"**"
    ],
    "note": [
      "arXiv preprint arXiv:2110.08207."
    ],
    "arxiv": [
      "2110.08207"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "family": "Wei",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Finetuned Language Models Are Zero-Shot Learners.\"**"
    ],
    "note": [
      "arXiv preprint arXiv:2109.01652."
    ],
    "arxiv": [
      "2109.01652"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "author": [
      {
        "family": "Lester",
        "given": "B."
      },
      {
        "family": "Al-Rfou",
        "given": "R."
      },
      {
        "family": "Constant",
        "given": "N."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "The Power of Scale for Parameter-Efficient Prompt Tuning.\"**"
    ],
    "note": [
      "arXiv preprint arXiv:2104.08691."
    ],
    "arxiv": [
      "2104.08691"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "family": "Li",
        "given": "X.L."
      },
      {
        "family": "Liang",
        "given": "P."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Prefix-Tuning"
    ],
    "container-title": [
      "Optimizing Continuous Prompts for Generation.\"** In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "family": "Zhong",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Factual Probing Is [MASK]: Learning vs"
    ],
    "container-title": [
      "Learning to Recall.\"** In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "Jiang",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "How Can We Know What Language Models Know?\"** Transactions of the Association for Computational Linguistics"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "family": "Wang",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "note": [
      "\"Entailment as Few-Shot Learner.\"** arXiv preprint arXiv:2104.14690."
    ],
    "arxiv": [
      "2104.14690"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "X."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "GPT Understands"
    ],
    "note": [
      "Too.\"** arXiv preprint arXiv:2103.10385."
    ],
    "arxiv": [
      "2103.10385"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "family": "Reynolds",
        "given": "L."
      },
      {
        "family": "McDonell",
        "given": "K."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm.\"**"
    ],
    "note": [
      "arXiv preprint arXiv:2102.07350."
    ],
    "arxiv": [
      "2102.07350"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of topics related to zero-shot and few-shot learning with pretrained language models, providing insights into various approaches, methodologies, and applications"
    ],
    "type": null
  }
]
