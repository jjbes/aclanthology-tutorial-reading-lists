[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2023 that cover various security challenges in natural language processing (NLP) models, including different types of attacks and defenses"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Adversarial Examples in NLP: A Survey\"**"
    ],
    "editor": [
      {
        "family": "Zhang",
        "given": "T."
      },
      {
        "family": "Yang",
        "given": "Q."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Overview of adversarial attacks and defenses in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "literal": "**\"Universal Adversarial Triggers for Attacking and Analyzing NLP\"** - Wallace, E., Feng, S., Kandpal, N., Singh, S., & Gardner, M."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduction to universal adversarial triggers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"TextFooler: A Text Generation-Based Adversarial Attack for Text Classification\"**"
    ],
    "editor": [
      {
        "family": "Jin",
        "given": "D."
      },
      {
        "family": "Jin",
        "given": "Z."
      },
      {
        "family": "Zhou",
        "given": "J.T."
      },
      {
        "family": "Szolovits",
        "given": "P."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Detailed methodology for generating adversarial text examples"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"On the Robustness of Language Encoders against Grammatical Errors\"**"
    ],
    "editor": [
      {
        "family": "Pruthi",
        "given": "D."
      },
      {
        "family": "Dhingra",
        "given": "B."
      },
      {
        "family": "Lipton",
        "given": "Z.C."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analysis of language model robustness to grammatical errors"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "literal": "**\"Generating Natural Language Adversarial Examples\"** - Alzantot, M., Sharma, Y., Elgohary, A., Ho, B., Srivastava, M. B., & Chang, K. W."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Techniques for generating adversarial text examples"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Attacks on Neural Network Policies\"** - Lin, Y. C., Hong, Z. W., Liao, Y. H., Shih, M. L., Liu, M. Y., & Sun, M."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Study on adversarial attacks on neural network policies"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Robustness and Reliability of NLP Models\"**"
    ],
    "editor": [
      {
        "family": "Belinkov",
        "given": "Y."
      },
      {
        "family": "Bisk",
        "given": "Y."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Survey on robustness and reliability in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "literal": "**\"Mitigating Adversarial Effects Through Randomization\"** - Xie, C., Wang, J., Zhang, Z., Ren, Z., Yuille, A. L., & Lim, S. N."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Defense strategies using randomization techniques"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Training for Free!\"** - Shafahi, A., Najibi, M., Ghiasi, A., Xu, Z., Dickerson, J., Studer, C., Davis, L. S., Taylor, G., & Goldstein, T."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Efficient adversarial training methods"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Exploring the Vulnerability of Deep Neural Networks: A Study of Parameter Corruption\"**"
    ],
    "editor": [
      {
        "family": "Wang",
        "given": "B."
      },
      {
        "family": "Yao",
        "given": "Y."
      },
      {
        "family": "Shan",
        "given": "S."
      },
      {
        "family": "Viswanath",
        "given": "B."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Study on parameter corruption in deep neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review\"**"
    ],
    "editor": [
      {
        "family": "Sun",
        "given": "L."
      },
      {
        "family": "Wu",
        "given": "J."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Comprehensive review of adversarial attacks and defenses across different domains"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "literal": "**\"Certified Robustness to Adversarial Word Substitutions\"** - Jia, R., Raghunathan, A., Goel, K., & Liang, P."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Techniques for certifying robustness against word substitution attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Adversarial Examples for Evaluating Reading Comprehension Systems\"**"
    ],
    "editor": [
      {
        "family": "Jia",
        "given": "R."
      },
      {
        "family": "Liang",
        "given": "P."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Generating adversarial examples to test reading comprehension systems"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"On the Limitations of Unsupervised Bilingual Dictionary Induction\"**"
    ],
    "editor": [
      {
        "family": "Artetxe",
        "given": "M."
      },
      {
        "family": "Labaka",
        "given": "G."
      },
      {
        "family": "Agirre",
        "given": "E."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Challenges in unsupervised bilingual dictionary induction"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Robustness of Deep Language Models to Input Perturbations\"**"
    ],
    "editor": [
      {
        "family": "Ebrahimi",
        "given": "J."
      },
      {
        "family": "Rao",
        "given": "A."
      },
      {
        "family": "Lowd",
        "given": "D."
      },
      {
        "family": "Dou",
        "given": "D."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analysis of deep language model robustness to input perturbations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Towards Robust Toxic Content Classification\"**"
    ],
    "editor": [
      {
        "family": "Mozafari",
        "given": "M."
      },
      {
        "family": "Farahbakhsh",
        "given": "R."
      },
      {
        "family": "Crespi",
        "given": "N."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Strategies for robust classification of toxic content"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Adversarial Training for Large Neural Language Models\"**"
    ],
    "editor": [
      {
        "family": "Zügner",
        "given": "D."
      },
      {
        "family": "Akbarnejad",
        "given": "A."
      },
      {
        "family": "Günnemann",
        "given": "S."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Adversarial training techniques for large NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Improving the Robustness of NLP Models to Adversarial Attacks with Data Augmentation\"**"
    ],
    "editor": [
      {
        "family": "Wei",
        "given": "J."
      },
      {
        "family": "Zou",
        "given": "K."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Data augmentation methods to improve model robustness"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "literal": "**\"Detecting Adversarial Samples from Artifacts\"** - Carlini, N., & Wagner, D."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Techniques for detecting adversarial samples based on artifacts"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Adversarial Attacks on Text Classifiers: A Survey\"**"
    ],
    "editor": [
      {
        "family": "Zhang",
        "given": "Y."
      },
      {
        "family": "Zhou",
        "given": "J."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Survey on adversarial attacks specifically targeting text classifiers"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the current state of research on security challenges in NLP, including various attack methods and defense mechanisms"
    ],
    "type": null
  }
]
