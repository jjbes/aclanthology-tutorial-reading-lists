Sure, here's a reading list of 20 articles on instruction following using large language models (LLMs) up to 2023:

1. **"Language Models are Few-Shot Learners"** - Brown et al., 2020
2. **"GPT-3: Language Models are Few-Shot Learners"** - OpenAI, 2020
3. **"T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** - Raffel et al., 2020
4. **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** - Devlin et al., 2019
5. **"Instruction Tuning: Improving Zero-Shot Learning by Explicitly Tuning to Instructions"** - Wei et al., 2021
6. **"FLAN: Few-Shot Learning with Instruction-Finetuned Language Models"** - Wei et al., 2021
7. **"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"** - Reynolds and McDonell, 2021
8. **"InstructGPT: Training Language Models to Follow Instructions with Human Feedback"** - Ouyang et al., 2022
9. **"Aligning Language Models to Follow Instructions"** - Bai et al., 2022
10. **"The Power of Scale for Parameter-Efficient Prompt Tuning"** - Lester et al., 2021
11. **"Learning to Summarize with Human Feedback"** - Stiennon et al., 2020
12. **"Self-Supervised Learning of Language Models with Instruction-based Data Augmentation"** - Wang et al., 2021
13. **"Improving Language Models by Retraining on Their Own Feedback"** - Saunders et al., 2022
14. **"Prompt-Based Learning for Natural Language Processing: A Survey"** - Liu et al., 2021
15. **"Multitask Prompted Training Enables Zero-Shot Task Generalization"** - Sanh et al., 2021
16. **"Scaling Instruction-Finetuned Language Models"** - Mishra et al., 2022
17. **"Language Models as Knowledge Bases?"** - Petroni et al., 2019
18. **"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"** - Ahn et al., 2022
19. **"Chain of Thought Prompting Elicits Reasoning in Large Language Models"** - Wei et al., 2022
20. **"Evaluating Large Language Models Trained on Code"** - Chen et al., 2021

These articles cover a range of topics related to instruction following, prompt engineering, and the capabilities of large language models in various contexts.