[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a curated list of 20 articles up to 2024 that focus on the evaluation of large language models"
    ],
    "note": [
      "LLMs), particularly on benchmarks and frameworks:"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models\"**"
    ],
    "type": null
  },
  {
    "note": [
      "Authors: Ethan Perez, Douwe Kiela, Kyunghyun Cho"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: This paper proposes new benchmarks to evaluate the capabilities of language models beyond traditional metrics"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Alex Wang"
      },
      {
        "family": "Singh",
        "given": "Amanpreet"
      },
      {
        "family": "Michael",
        "given": "Julian"
      },
      {
        "family": "Hill",
        "given": "Felix"
      },
      {
        "family": "Levy",
        "given": "Omer"
      },
      {
        "family": "Bowman",
        "given": "Samuel R."
      }
    ],
    "type": null
  },
  {
    "note": [
      "Year: 2018"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Introduces the GLUE benchmark, a comprehensive suite for evaluating the performance of LLMs on a variety of language understanding tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Alex Wang"
      },
      {
        "family": "Pruksachatkun",
        "given": "Yada"
      },
      {
        "family": "Nangia",
        "given": "Nikita"
      },
      {
        "family": "Singh",
        "given": "Amanpreet"
      },
      {
        "family": "Michael",
        "given": "Julian"
      },
      {
        "family": "Hill",
        "given": "Felix"
      },
      {
        "family": "Levy",
        "given": "Omer"
      },
      {
        "family": "Bowman",
        "given": "Samuel R."
      }
    ],
    "type": null
  },
  {
    "note": [
      "Year: 2019"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: An extension of GLUE, SuperGLUE is designed to be more challenging and to better evaluate the capabilities of advanced LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Sebastian Gehrmann"
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori B."
      },
      {
        "others": true
      }
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: GEM provides a benchmark for natural language generation tasks and introduces new metrics for evaluation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Alexis Conneau"
      },
      {
        "family": "Khandelwal",
        "given": "Kartikay"
      },
      {
        "family": "Goyal",
        "given": "Naman"
      },
      {
        "family": "Chaudhary",
        "given": "Vishrav"
      },
      {
        "family": "Wenzek",
        "given": "Guillaume"
      },
      {
        "family": "Guzm√°n",
        "given": "Francisco"
      },
      {
        "family": "Grave",
        "given": "Edouard"
      }
    ],
    "title": [
      "Myle Ott"
    ],
    "publisher": [
      "Luke Zettlemoyer, Veselin Stoyanov"
    ],
    "type": "book"
  },
  {
    "note": [
      "Year: 2020"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: XTREME is a benchmark for evaluating the cross-lingual generalization capabilities of LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"The EleutherAI Language Model Evaluation Harness\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "EleutherAI Team"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: A comprehensive evaluation framework developed by EleutherAI for assessing the performance of large language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Evaluation of Transformer-based Models on the General Language Understanding Evaluation (GLUE) Benchmark\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Various"
    ],
    "type": "book"
  },
  {
    "note": [
      "Year: 2020"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: A detailed evaluation of transformer-based models on the GLUE benchmark, highlighting strengths and weaknesses"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"The BIG-bench: A Benchmark for Large-Scale Language Models\"**"
    ],
    "type": null
  },
  {
    "note": [
      "Authors: Various (Google Research"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: BIG-bench is a collaborative benchmark designed to evaluate the performance of large-scale language models on a wide range of tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList\"**"
    ],
    "type": null
  },
  {
    "title": [
      "Authors: Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh"
    ],
    "type": null
  },
  {
    "note": [
      "Year: 2020"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Introduces CheckList, a framework for behavioral testing of NLP models to ensure robustness and fairness"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Evaluating Large Language Models Trained on Code\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Authors",
        "given": "Mark Chen"
      },
      {
        "family": "Tworek",
        "given": "Jerry"
      },
      {
        "family": "Jun",
        "given": "Heewoo"
      },
      {
        "family": "Yuan",
        "given": "Qiming"
      },
      {
        "family": "Oliveira Pinto",
        "given": "Henrique Ponde",
        "particle": "de"
      },
      {
        "family": "Kaplan",
        "given": "Jared"
      },
      {
        "family": "Edwards",
        "given": "Harri"
      },
      {
        "family": "Burda",
        "given": "Yuri"
      },
      {
        "family": "Joseph",
        "given": "Nicholas"
      },
      {
        "family": "Brockman",
        "given": "Greg"
      },
      {
        "family": "Ray",
        "given": "Alex"
      },
      {
        "family": "Puri",
        "given": "Raul"
      },
      {
        "family": "Krueger",
        "given": "Gretchen"
      },
      {
        "family": "Petrov",
        "given": "Michael"
      },
      {
        "family": "Khlaaf",
        "given": "Heidy"
      },
      {
        "family": "Sastry",
        "given": "Girish"
      },
      {
        "family": "Mishkin",
        "given": "Pamela"
      },
      {
        "family": "Chan",
        "given": "Brooke"
      },
      {
        "family": "Gray",
        "given": "Scott"
      },
      {
        "family": "Ryder",
        "given": "Nick"
      },
      {
        "family": "Pavlov",
        "given": "Mikhail"
      },
      {
        "family": "Power",
        "given": "Alethea"
      },
      {
        "family": "Kaiser",
        "given": "Lukasz"
      },
      {
        "family": "Bavarian",
        "given": "Mohammad"
      },
      {
        "family": "Winter",
        "given": "Clemens"
      },
      {
        "family": "Tillet",
        "given": "Philippe"
      },
      {
        "family": "Such",
        "given": "Felipe Petroski"
      },
      {
        "family": "Cummings",
        "given": "Dave"
      },
      {
        "family": "Plappert",
        "given": "Matthias"
      },
      {
        "family": "Chantzis",
        "given": "Fotios"
      },
      {
        "family": "Barnes",
        "given": "Elizabeth"
      },
      {
        "family": "Herbert-Voss",
        "given": "Ariel"
      },
      {
        "family": "Guss",
        "given": "William Hebgen"
      },
      {
        "family": "Nichol",
        "given": "Alex"
      },
      {
        "family": "Paino",
        "given": "Alex"
      },
      {
        "family": "Tezak",
        "given": "Nikolas"
      }
    ],
    "note": [
      "Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Evaluates the performance of LLMs specifically trained on code, with a focus on coding tasks and benchmarks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "publisher": [
      "**\"The State of AI Ethics Report\"**"
    ],
    "type": "book"
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Various (Montreal AI Ethics Institute"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Discusses ethical considerations and evaluation metrics for AI models, including LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Evaluating the Robustness of Language Models to Input Perturbations\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Various"
    ],
    "type": "book"
  },
  {
    "note": [
      "Year: 2022"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Examines how robust LLMs are to various types of input perturbations and proposes new evaluation metrics"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\"**"
    ],
    "type": null
  },
  {
    "editor": [
      {
        "family": "Authors",
        "given": "Leo Gao"
      },
      {
        "family": "Biderman",
        "given": "Stella"
      },
      {
        "family": "Black",
        "given": "Sid"
      },
      {
        "family": "Golding",
        "given": "Laurence"
      },
      {
        "family": "Hoppe",
        "given": "Travis"
      },
      {
        "family": "Foster",
        "given": "Charles"
      },
      {
        "family": "Phang",
        "given": "Jason"
      },
      {
        "family": "He",
        "given": "Horace"
      },
      {
        "family": "Thite",
        "given": "Anish"
      },
      {
        "family": "Nabeshima",
        "given": "Noa"
      },
      {
        "family": "Presser",
        "given": "Shawn"
      },
      {
        "family": "Leahy",
        "given": "Connor"
      }
    ],
    "type": null
  },
  {
    "note": [
      "Year: 2020"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Introduces The Pile, a large and diverse dataset for training and evaluating LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Evaluation of LLMs on Commonsense Reasoning Benchmarks\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Various"
    ],
    "type": "book"
  },
  {
    "note": [
      "Year: 2022"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Summary"
      }
    ],
    "title": [
      "Focuses on the performance of LLMs on commonsense reasoning tasks and benchmarks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"The HELM Benchmark: Holistic Evaluation of Language Models\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Various"
    ],
    "type": "book"
  },
  {
    "note": [
      "Year: 2023"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: HELM is a comprehensive benchmark designed to evaluate LLMs across a wide range of tasks and metrics"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Evaluating Language Models with a Suite of Long-Form Question Answering Benchmarks\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Various"
    ],
    "type": "book"
  },
  {
    "note": [
      "Year: 2023"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Introduces a suite of benchmarks specifically for evaluating the performance of LLMs on long-form question answering tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"The Dynabench Platform: Dynamic Benchmarking for NLP\"**"
    ],
    "type": null
  },
  {
    "title": [
      "Authors: Douwe Kiela, Max Bartolo, et al"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Dynabench is a dynamic benchmarking platform that allows for continuous evaluation and improvement of NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"The LAMBADA Benchmark: Evaluating the Ability of Language Models to Understand Text\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Various"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "given": "Year"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: LAMBADA is a benchmark designed to evaluate the ability of LLMs to understand and generate coherent text"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "container-title": [
      "**\"The MMLU Benchmark: Measuring Massive Multitask Language Understanding\"**"
    ],
    "type": "chapter"
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Various"
    ],
    "type": "book"
  },
  {
    "note": [
      "Year: 2022"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: MMLU evaluates the performance of LLMs on a wide range of multitask language understanding tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Evaluating the Ethical Implications of Language Models\"**"
    ],
    "type": null
  },
  {
    "location": [
      "Authors"
    ],
    "publisher": [
      "Various"
    ],
    "type": "book"
  },
  {
    "note": [
      "Year: 2023"
    ],
    "type": null
  },
  {
    "title": [
      "Summary: Discusses the ethical implications of LLMs and proposes new evaluation frameworks to address these concerns"
    ],
    "type": null
  },
  {
    "title": [
      "These articles and benchmarks provide a comprehensive overview of the current state of LLM evaluation, focusing on various aspects such as robustness, ethical considerations, and specific task performance"
    ],
    "type": null
  }
]
