[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2023 that cover various security challenges in natural language processing (NLP), with a focus on black-box models, data leakage, backdoors, and imitation attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Adversarial Examples in NLP: A Survey\"**"
    ],
    "editor": [
      {
        "family": "Zhang",
        "given": "H."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Overview of adversarial attacks and defenses in NLP, including black-box models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Data Leakage in Machine Learning: A Survey\"**"
    ],
    "editor": [
      {
        "family": "Kandpal",
        "given": "N."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Comprehensive survey on data leakage issues in machine learning, with applications to NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Backdoor Attacks on Deep Learning Models: A Survey\"**"
    ],
    "editor": [
      {
        "family": "Li",
        "given": "Y."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Detailed survey on backdoor attacks, including those targeting NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "literal": "**\"Black-box Adversarial Attacks on Sequence-to-Sequence Models\"** - Cheng, H., et al."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Study on black-box adversarial attacks specifically on sequence-to-sequence models in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Mitigating Data Leakage in Machine Learning Models\"**"
    ],
    "editor": [
      {
        "family": "Shokri",
        "given": "R."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Techniques and strategies to mitigate data leakage, with examples in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Imitation Attacks and Defenses in Machine Learning\"**"
    ],
    "editor": [
      {
        "family": "Orekondy",
        "given": "T."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Exploration of imitation attacks and defenses, applicable to NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Backdoor Attacks on NLP Models with Data Poisoning\"**"
    ],
    "editor": [
      {
        "family": "Kurita",
        "given": "K."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examination of backdoor attacks using data poisoning in NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Attacks on Black-box NLP Systems\"** - Wallace, E., et al."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analysis of adversarial attacks on black-box NLP systems and potential defenses"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Understanding Data Leakage in NLP Models\"**"
    ],
    "editor": [
      {
        "family": "Song",
        "given": "C."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "In-depth study on how data leakage occurs in NLP models and its implications"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Backdoor Attacks in Transfer Learning\"**"
    ],
    "editor": [
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigation of backdoor attacks in the context of transfer learning, relevant to NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Black-box Adversarial Attacks on Text Classification Models\"**"
    ],
    "editor": [
      {
        "family": "Ebrahimi",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Research on black-box adversarial attacks targeting text classification models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Data Leakage in Natural Language Processing: Causes and Solutions\"**"
    ],
    "editor": [
      {
        "family": "Zhang",
        "given": "X."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analysis of causes and solutions for data leakage in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Backdoor Attacks on Pre-trained Language Models\"**"
    ],
    "editor": [
      {
        "family": "Dai",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Study on the vulnerability of pre-trained language models to backdoor attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Imitation Attacks on NLP Models: Threats and Countermeasures\"**"
    ],
    "editor": [
      {
        "family": "Chen",
        "given": "X."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examination of imitation attacks on NLP models and proposed countermeasures"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "container-title": [
      "**\"Adversarial Attacks on Black-box Machine Translation Systems\"**"
    ],
    "editor": [
      {
        "family": "Belinkov",
        "given": "Y."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analysis of adversarial attacks on black-box machine translation systems"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Data Leakage in Text Data: Challenges and Solutions\"**"
    ],
    "editor": [
      {
        "family": "Wang",
        "given": "Y."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discussion on challenges and solutions for data leakage in text data"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Backdoor Attacks on NLP Models via Poisoned Data\"**"
    ],
    "editor": [
      {
        "family": "Chen",
        "given": "T."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Research on backdoor attacks implemented through poisoned data in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Black-box Adversarial Attacks on Sentiment Analysis Models\"**"
    ],
    "editor": [
      {
        "family": "Alzantot",
        "given": "M."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Study on black-box adversarial attacks specifically targeting sentiment analysis models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Data Leakage in Deep Learning Models: A Case Study in NLP\"**"
    ],
    "editor": [
      {
        "family": "Kim",
        "given": "J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Case study on data leakage in deep learning models with a focus on NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Backdoor Attacks on Text Generation Models\"**"
    ],
    "editor": [
      {
        "family": "Yang",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examination of backdoor attacks on text generation models in NLP"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the security challenges in NLP, focusing on black-box models, data leakage, backdoors, and imitation attacks"
    ],
    "type": null
  }
]
