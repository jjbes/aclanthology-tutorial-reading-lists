Sure! Here's a curated list of 20 influential articles on embeddings and word embeddings up to 2020:

1. **Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013).** "Efficient Estimation of Word Representations in Vector Space." *arXiv preprint arXiv:1301.3781.*

2. **Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013).** "Distributed Representations of Words and Phrases and their Compositionality." *Advances in Neural Information Processing Systems (NeurIPS).*

3. **Pennington, J., Socher, R., & Manning, C. D. (2014).** "GloVe: Global Vectors for Word Representation." *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).*

4. **Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017).** "Enriching Word Vectors with Subword Information." *Transactions of the Association for Computational Linguistics (TACL).*

5. **Le, Q., & Mikolov, T. (2014).** "Distributed Representations of Sentences and Documents." *Proceedings of the 31st International Conference on Machine Learning (ICML).*

6. **Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018).** "Deep Contextualized Word Representations." *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).*

7. **Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019).** "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).*

8. **Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017).** "Attention is All You Need." *Advances in Neural Information Processing Systems (NeurIPS).*

9. **Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018).** "Improving Language Understanding by Generative Pre-Training." *OpenAI.*

10. **Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).** "Language Models are Unsupervised Multitask Learners." *OpenAI.*

11. **Mikolov, T., Yih, W. T., & Zweig, G. (2013).** "Linguistic Regularities in Continuous Space Word Representations." *Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).*

12. **Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R., Urtasun, R., Torralba, A., & Fidler, S. (2015).** "Skip-Thought Vectors." *Advances in Neural Information Processing Systems (NeurIPS).*

13. **Mnih, A., & Kavukcuoglu, K. (2013).** "Learning Word Embeddings Efficiently with Noise-Contrastive Estimation." *Advances in Neural Information Processing Systems (NeurIPS).*

14. **Weston, J., Chopra, S., & Bordes, A. (2014).** "Memory Networks." *arXiv preprint arXiv:1410.3916.*

15. **Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019).** "XLNet: Generalized Autoregressive Pretraining for Language Understanding." *Advances in Neural Information Processing Systems (NeurIPS).*

16. **Lample, G., & Conneau, A. (2019).** "Cross-lingual Language Model Pretraining." *Advances in Neural Information Processing Systems (NeurIPS).*

17. **Grave, E., Bojanowski, P., Gupta, P., Joulin, A., & Mikolov, T. (2018).** "Learning Word Vectors for 157 Languages." *Proceedings of the International Conference on Language Resources and Evaluation (LREC).*

18. **Peters, M. E., Ammar, W., Bhagavatula, C., & Power, R. (2017).** "Semi-supervised Sequence Tagging with Bidirectional Language Models." *Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL).*

19. **Howard, J., & Ruder, S. (2018).** "Universal Language Model Fine-tuning for Text Classification." *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL).*

20. **McCann, B., Bradbury, J., Xiong, C., & Socher, R. (2017).** "Learned in Translation: Contextualized Word Vectors." *Advances in Neural Information Processing Systems (NeurIPS).*

These articles cover a range of foundational works and significant advancements in the field of word embeddings and contextual embeddings.