[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Understanding and interpreting the predictions of neural networks is a crucial area of research, especially given the complexity and often opaque nature of these models. Here is a list of 20 articles up to 2020 that focus on interpreting neural network predictions and understanding their decisions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Explaining Explanations: An Overview of Interpretability of Machine Learning\"** by Finale Doshi-Velez and Been Kim"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Provides a comprehensive overview of interpretability in machine learning, including neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "family": "You?\"",
        "given": "Why Should I.Trust"
      }
    ],
    "title": [
      "Explaining the Predictions of Any Classifier\"** by Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces LIME (Local Interpretable Model-agnostic Explanations), a technique for explaining the predictions of any classifier"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Axiomatic Attribution for Deep Networks\"** by Mukund Sundararajan, Ankur Taly, and Qiqi Yan"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes Integrated Gradients, a method for attributing the prediction of a deep network to its input features"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "given": "DeepLIFT"
      }
    ],
    "title": [
      "Learning Important Features Through Propagating Activation Differences\"** by Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces DeepLIFT, a method for decomposing the output prediction by backpropagating the contributions of each input feature"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"** by Ramprasaath R"
    ],
    "date": [
      "2017"
    ],
    "type": "article-journal",
    "container-title": [
      "Selvaraju et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes Grad-CAM, a technique for producing visual explanations for decisions from a variety of CNN-based models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Learning Important Features Through Propagating Activation Differences\"** by Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the DeepLIFT method for understanding neural network predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"The Building Blocks of Interpretability\"** by Chris Olah, Arvind Satyanarayan"
    ],
    "editor": [
      {
        "family": "Johnson",
        "given": "Ian"
      },
      {
        "family": "Carter",
        "given": "Shan"
      },
      {
        "family": "Schubert",
        "given": "Ludwig"
      },
      {
        "family": "Ye",
        "given": "Katherine"
      },
      {
        "family": "Mordvintsev",
        "given": "Alexander"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores various techniques and tools for interpreting neural networks, focusing on visualizations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "given": "Interpretable"
      },
      {
        "family": "Been Kim",
        "given": "Pedagogical Examples\"",
        "particle": "by"
      },
      {
        "family": "Khanna",
        "given": "Rajiv"
      },
      {
        "family": "Koyejo",
        "given": "Oluwasanmi"
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the concept of interpretability through pedagogical examples, providing insights into model behavior"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Anchors: High-Precision Model-Agnostic Explanations\"** by Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces Anchors, a method for providing high-precision explanations for model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Attention is All You Need\"** by Ashish Vaswani et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "While primarily about the Transformer model, it introduces attention mechanisms, which have interpretability benefits"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Feature Importance Estimation for Explainable AI: A Survey\"** by Erik Å trumbelj and Igor Kononenko"
    ],
    "date": [
      "2014"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Surveys methods for estimating feature importance, a key aspect of interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "given": "Visualizing"
      },
      {
        "family": "Matthew D. Zeiler",
        "given": "Understanding Convolutional Networks\"",
        "particle": "by"
      },
      {
        "family": "Fergus",
        "given": "Rob"
      }
    ],
    "date": [
      "2014"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses techniques for visualizing the features learned by convolutional neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Understanding Neural Networks Through Deep Visualization\"** by Jason Yosinski"
    ],
    "editor": [
      {
        "family": "Clune",
        "given": "Jeff"
      },
      {
        "family": "Nguyen",
        "given": "Anh"
      },
      {
        "family": "Fuchs",
        "given": "Thomas"
      },
      {
        "family": "Lipson",
        "given": "Hod"
      }
    ],
    "date": [
      "2015"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores deep visualization techniques to understand neural network behavior"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Interpretable Machine Learning: A Guide for Making Black Box Models Explainable\"** by Christoph Molnar"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A comprehensive guide on various interpretability techniques and their applications"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"A Unified Approach to Interpreting Model Predictions\"** by Scott M"
    ],
    "publisher": [
      "Lundberg and Su-In Lee"
    ],
    "date": [
      "2017"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces SHAP (SHapley Additive exPlanations), a unified framework for interpreting model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"PatternNet and PatternAttribution: Improving the Interpretability of Neural Networks\"** by Alexander J"
    ],
    "date": [
      "2018"
    ],
    "type": "article-journal",
    "container-title": [
      "Kindermans et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes methods for improving the interpretability of neural networks by focusing on patterns"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\"** by Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman"
    ],
    "date": [
      "2014"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses saliency maps as a way to visualize and understand image classification models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"The Tree of Thoughts: Understanding Neural Networks Through Hierarchical Visualizations\"** by Chris Olah et al"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores hierarchical visualizations to understand the decisions of neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Interpreting and Understanding Deep Neural Networks by Information Bottleneck\"** by Ravid Schwartz-Ziv and Naftali Tishby"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Applies the information bottleneck theory to interpret and understand deep neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Towards a Rigorous Science of Interpretable Machine Learning\"** by Finale Doshi-Velez and Been Kim"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the need for a rigorous scientific approach to interpretability in machine learning"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of techniques and approaches for interpreting neural networks, from visualization and feature importance to model-agnostic methods and theoretical frameworks"
    ],
    "type": null
  }
]
