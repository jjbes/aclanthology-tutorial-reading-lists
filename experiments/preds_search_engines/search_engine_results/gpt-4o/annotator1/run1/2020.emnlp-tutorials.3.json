[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here is a list of 20 articles published before 2021 that focus on interpreting the predictions of neural networks and understanding their decisions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "given": "Visualizing"
      },
      {
        "family": "Matthew D. Zeiler",
        "given": "Understanding Convolutional Networks\"",
        "particle": "by"
      },
      {
        "family": "Fergus",
        "given": "Rob"
      }
    ],
    "date": [
      "2014"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces deconvolutional networks to visualize the features learned by convolutional networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Explaining Explanations: Axiomatic Feature Interactions for Deep Networks\"** by Mukund Sundararajan, Ankur Taly, and Qiqi Yan"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces Integrated Gradients, a method for attributing the prediction of a deep network to its input features"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Axiomatic Attribution for Deep Networks\"** by Mukund Sundararajan, Ankur Taly, and Qiqi Yan"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper provides a theoretical foundation for feature attribution methods in deep networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "given": "DeepLIFT"
      }
    ],
    "title": [
      "Learning Important Features Through Propagating Activation Differences\"** by Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces DeepLIFT, a method for decomposing the output prediction of a neural network"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"** by Ramprasaath R"
    ],
    "date": [
      "2017"
    ],
    "type": "article-journal",
    "container-title": [
      "Selvaraju et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper presents Grad-CAM, a technique for producing visual explanations for decisions from a convolutional network"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "given": "L.I.M.E."
      }
    ],
    "title": [
      "Local Interpretable Model-agnostic Explanations\"** by Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces LIME, a method for explaining the predictions of any classifier in an interpretable and faithful manner"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"The Building Blocks of Interpretability\"** by Chris Olah, Arvind Satyanarayan"
    ],
    "editor": [
      {
        "family": "Johnson",
        "given": "Ian"
      },
      {
        "family": "Carter",
        "given": "Shan"
      },
      {
        "family": "Schubert",
        "given": "Ludwig"
      },
      {
        "family": "Ye",
        "given": "Katherine"
      },
      {
        "family": "Mordvintsev",
        "given": "Alexander"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This article explores various techniques and tools for interpreting neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Understanding Neural Networks Through Deep Visualization\"** by Jason Yosinski"
    ],
    "editor": [
      {
        "family": "Clune",
        "given": "Jeff"
      },
      {
        "family": "Nguyen",
        "given": "Anh"
      },
      {
        "family": "Fuchs",
        "given": "Thomas"
      },
      {
        "family": "Lipson",
        "given": "Hod"
      }
    ],
    "date": [
      "2015"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper discusses techniques for visualizing and understanding the features learned by neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "given": "Interpretable"
      },
      {
        "family": "Been Kim",
        "given": "Pedagogical Examples\"",
        "particle": "by"
      },
      {
        "family": "Khanna",
        "given": "Rajiv"
      },
      {
        "family": "Koyejo",
        "given": "Oluwasanmi"
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces the concept of interpretability in machine learning models and discusses pedagogical examples"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Anchors: High-Precision Model-Agnostic Explanations\"** by Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper presents Anchors, a method for providing high-precision explanations for model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "family": "Chris Olah",
        "given": "Feature Visualization\"",
        "particle": "by"
      },
      {
        "family": "Mordvintsev",
        "given": "Alexander"
      },
      {
        "family": "Schubert",
        "given": "Ludwig"
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This article explores techniques for visualizing the features learned by neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Towards a Rigorous Science of Interpretable Machine Learning\"** by Finale Doshi-Velez and Been Kim"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper discusses the need for a rigorous science of interpretable machine learning and proposes a framework for evaluation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Learning Important Features Through Propagating Activation Differences\"** by Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces DeepLIFT, a method for attributing the prediction of a neural network to its input features"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Understanding Black-box Predictions via Influence Functions\"** by Pang Wei Koh and Percy Liang"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces influence functions, a method for understanding the impact of training data on model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"A Unified Approach to Interpreting Model Predictions\"** by Scott M"
    ],
    "publisher": [
      "Lundberg and Su-In Lee"
    ],
    "date": [
      "2017"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces SHAP (SHapley Additive exPlanations), a unified framework for interpreting model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"The Tree of Thoughts: Understanding Neural Networks Through Hierarchical Visualizations\"** by Chris Olah, Arvind Satyanarayan"
    ],
    "editor": [
      {
        "family": "Johnson",
        "given": "Ian"
      },
      {
        "family": "Carter",
        "given": "Shan"
      },
      {
        "family": "Schubert",
        "given": "Ludwig"
      },
      {
        "family": "Ye",
        "given": "Katherine"
      },
      {
        "family": "Mordvintsev",
        "given": "Alexander"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This article explores hierarchical visualizations for understanding neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\"** by Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman"
    ],
    "date": [
      "2014"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces techniques for visualizing the inner workings of convolutional networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Interpretable Machine Learning: A Guide for Making Black Box Models Explainable\"** by Christoph Molnar"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This book provides a comprehensive guide to various methods for making machine learning models interpretable"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Saliency Maps for Explaining Adversarial Attacks\"** by Nicholas Carlini and David Wagner"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper discusses the use of saliency maps for understanding adversarial attacks on neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Towards Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks\"** by Qiang Zhang and Yu Zhang"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This survey paper provides an overview of various methods for interpreting the inner structures of deep neural networks"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of techniques and approaches for interpreting neural networks, from visualization methods to theoretical frameworks and practical tools"
    ],
    "type": null
  }
]
