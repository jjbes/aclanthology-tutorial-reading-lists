[
  {
    "title": [
      "Sure, here is a reading list of 20 articles on the interpretability of transformer models, covering research up to 2024"
    ],
    "note": [
      "These articles include foundational works as well as recent advancements in the field:"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "literal": "**Attention is All You Need** - Vaswani et al."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "The seminal paper introducing the Transformer model"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding** - Devlin et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduction of BERT and its impact on NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**Visualizing and Understanding Neural Models in NLP** - Karpathy et al"
    ],
    "date": [
      "2015"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Early work on understanding neural models, relevant for transformers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**A Structural Probe for Finding Syntax in Word Representations**"
    ],
    "publisher": [
      "Hewitt and Manning"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Techniques for probing syntactic information in transformer models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "family": "At?",
        "given": "What Does B.E.R.T.Look"
      }
    ],
    "title": [
      "An Analysis of BERT's Attention**"
    ],
    "publisher": [
      "Clark et al"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analysis of attention mechanisms in BERT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**Dissecting Contextual Word Embeddings: Architecture and Representation** - Liu et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examination of contextual embeddings in transformer models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**Analyzing the Structure of Attention in a Transformer Language Model**"
    ],
    "publisher": [
      "Vig and Belinkov"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Detailed analysis of attention patterns in transformers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "literal": "**Interpretable Multi-Head Self-Attention via Visualizations** - Lin et al."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Visualization techniques for understanding multi-head self-attention"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "literal": "**Are Sixteen Heads Really Better than One?** - Michel et al."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Study on the necessity of multiple attention heads"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**Transformer Interpretability Beyond Attention Visualization** - Chefer et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Methods for interpreting transformers beyond simple attention maps"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**Explaining Transformers as Bayesian Inference Engines** - Ravfogel et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Interpreting transformers using Bayesian inference principles"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**Towards Robust and Interpretable Neural Networks** - Zhang et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Approaches to enhance robustness and interpretability in neural networks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**Understanding Attention in Transformers: A Visual Analytics Approach**"
    ],
    "publisher": [
      "Hoover et al"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Visual analytics techniques for understanding transformer attention"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**Probing Classifiers: Promises, Shortcomings, and Advances**"
    ],
    "location": [
      "Belinkov"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discussion on probing methods for interpreting classifiers, including transformers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**Attention Interpretability Across NLP Tasks**"
    ],
    "publisher": [
      "Jain and Wallace"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Evaluation of attention interpretability across various NLP tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**The Lottery Ticket Hypothesis for Pre-trained BERT Networks** - Chen et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigating the lottery ticket hypothesis in the context of BERT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "literal": "**Explaining BERT with SHAP** - Lundberg et al."
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Using SHAP (SHapley Additive exPlanations) to interpret BERT models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**Self-Explaining Structures Improve NLP Models** - Rajani et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Incorporating self-explaining structures into NLP models for better interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**Transformers as Meta-Learners: A Case Study on Extractive Summarization**"
    ],
    "date": [
      "2021"
    ],
    "type": "article-journal",
    "container-title": [
      "Dou et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Case study on using transformers for meta-learning and interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**Explaining Transformers with Contextual Decomposition** - Murdoch et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Applying contextual decomposition to explain transformer decisions"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of topics from foundational principles to advanced interpretability techniques, providing a comprehensive overview of the current state of research in transformer interpretability"
    ],
    "type": null
  }
]
