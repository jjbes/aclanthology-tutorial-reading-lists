[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here's a curated reading list of 20 articles up to 2023 that focus on security challenges in natural language processing"
    ],
    "note": [
      "NLP), particularly on black-box models, data leakage, backdoors, and imitation attacks:"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Adversarial Examples for Evaluating Reading Comprehension Systems\"**"
    ],
    "editor": [
      {
        "family": "Jia",
        "given": "Robin"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Universal Adversarial Triggers for Attacking and Analyzing NLP\"** - Wallace, Eric, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"On the Robustness of Language Encoders against Grammatical Errors\"** - Pruthi, Danish, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Mitigating Adversarial Effects in NLP through Randomized Substitution and Voting\"** - Wang, Alex, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Backdoor Attacks on Deep Learning Models\"** - Gu, Tianyu, et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "literal": "**\"Trojaning Attack on Neural Networks\"** - Liu, Yingqi, et al."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Data Poisoning Attacks on Neural Networks\"**"
    ],
    "editor": [
      {
        "family": "Biggio",
        "given": "Battista"
      },
      {
        "others": true
      }
    ],
    "date": [
      "2012"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Attacks on Neural Network Policies\"** - Huang, Sandy, et al."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Exploring the Vulnerability of Deep Neural Networks: A Study of Parameter Corruption\"** - Chen, Pin-Yu, et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Defending Against Neural Fake News\"**"
    ],
    "editor": [
      {
        "family": "Zellers",
        "given": "Rowan"
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review\"** - Sun, Lichao, et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Adversarial Attacks on Text Classifiers via Sentence Transformation\"** - Li, Jinfeng, et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"TextFooler: A Text Generation-Based Adversarial Attack for Text Classification\"** - Jin, Di, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Towards Robust Toxic Content Classification\"** - Mozafari, Mohammadhossein, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Training for Free!\"** - Shafahi, Ali, et al."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Adversarial Examples Are Not Bugs, They Are Features\"** - Ilyas, Andrew, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"The Limitations of Stylometry for Detecting Machine-Generated Fake News\"** - Uchendu, Adedotun, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Adversarial Attacks on Neural Networks for Graph Data\"** - Dai, Hanjun, et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Adversarial Training for Large Neural Language Models\"** - Zhu, Chengxu, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"On the Security and Robustness of Training-Free Indexing for Deep Neural Networks\"** - Chen, Pin-Yu, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of topics related to the security challenges in NLP, including adversarial attacks, backdoor attacks, data leakage, and the robustness of black-box models. They provide a comprehensive overview of the current state of research in this field up to"
    ],
    "date": [
      "2023"
    ],
    "type": null
  }
]
