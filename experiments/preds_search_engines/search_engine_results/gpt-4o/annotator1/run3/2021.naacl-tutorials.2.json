[
  {
    "title": [
      "Sure, here's a curated reading list of 20 articles up to 2021 that focus on fine-grained interpretation and causation analysis of opaque deep NLP models'"
    ],
    "url": [
      "predictions:"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "literal": "**\"Attention is not Explanation\"** - Jain, S., & Wallace, B. C."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper critically examines the use of attention mechanisms as explanations for model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Interpretable and Explainable Deep Learning: A Survey\"**"
    ],
    "editor": [
      {
        "family": "Zhang",
        "given": "Q."
      },
      {
        "family": "Zhu",
        "given": "S."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A comprehensive survey on interpretability and explainability in deep learning, including NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "literal": "**\"Rationalizing Neural Predictions\"** - Lei, T., Barzilay, R., & Jaakkola, T."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This work proposes a method to generate rationales for predictions made by neural models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "given": "L.I.M.E."
      }
    ],
    "title": [
      "Local Interpretable Model-Agnostic Explanations\"**"
    ],
    "editor": [
      {
        "family": "Ribeiro",
        "given": "M.T."
      },
      {
        "family": "Singh",
        "given": "S."
      },
      {
        "family": "Guestrin",
        "given": "C."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces LIME, a technique to explain individual predictions of any classifier"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "literal": "**\"Anchors: High-Precision Model-Agnostic Explanations\"** - Ribeiro, M. T., Singh, S., & Guestrin, C."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Extends LIME to provide high-precision explanations for model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions\"**"
    ],
    "editor": [
      {
        "family": "Koh",
        "given": "P.W."
      },
      {
        "family": "Liang",
        "given": "P."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Uses influence functions to understand model predictions and identify data artifacts"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Integrated Gradients: Axiomatic Attribution for Deep Networks\"**"
    ],
    "editor": [
      {
        "family": "Sundararajan",
        "given": "M."
      },
      {
        "family": "Taly",
        "given": "A."
      },
      {
        "family": "Yan",
        "given": "Q."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes Integrated Gradients, a method for attributing the prediction of deep networks to their input features"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "literal": "**\"SHAP: Shapley Additive Explanations\"** - Lundberg, S. M., & Lee, S.-I."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces SHAP values, a unified measure of feature importance based on cooperative game theory"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"A Unified Approach to Interpreting Model Predictions\"**"
    ],
    "author": [
      {
        "family": "Lundberg",
        "given": "S.M."
      },
      {
        "family": "Lee",
        "given": "S.-I."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses a unified framework for interpreting model predictions using SHAP values"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"The Building Blocks of Interpretability\"**"
    ],
    "author": [
      {
        "family": "Lipton",
        "given": "Z.C."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Analyzes different aspects and challenges of interpretability in machine learning models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Contextual Decomposition for Neural Network Interpretability\"**"
    ],
    "editor": [
      {
        "family": "Murdoch",
        "given": "W.J."
      },
      {
        "family": "Szlam",
        "given": "A."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a method for decomposing the contributions of individual neurons to model predictions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Learning Important Features Through Propagating Activation Differences\"**"
    ],
    "author": [
      {
        "family": "Shrikumar",
        "given": "A."
      },
      {
        "family": "Greenside",
        "given": "P."
      },
      {
        "family": "Kundaje",
        "given": "A."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces DeepLIFT, a method for attributing the output of a neural network to its input features"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "literal": "**\"Axiomatic Attribution for Deep Networks\"** - Sundararajan, M., Taly, A., & Yan, Q."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses axiomatic properties that attribution methods should satisfy and introduces Integrated Gradients"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Towards a Rigorous Science of Interpretable Machine Learning\"**"
    ],
    "editor": [
      {
        "family": "Doshi-Velez",
        "given": "F."
      },
      {
        "family": "Kim",
        "given": "B."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A position paper advocating for a more rigorous approach to the science of interpretability in machine learning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Learning to Explain: An Information-Theoretic Perspective on Model Interpretation\"**"
    ],
    "editor": [
      {
        "family": "Chen",
        "given": "J."
      },
      {
        "family": "Song",
        "given": "L."
      },
      {
        "family": "Wainwright",
        "given": "M.J."
      },
      {
        "family": "Jordan",
        "given": "M.I."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes an information-theoretic framework for model interpretation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "literal": "**\"Explaining and Harnessing Adversarial Examples\"** - Goodfellow, I. J., Shlens, J., & Szegedy, C."
      }
    ],
    "date": [
      "2015"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses adversarial examples and their implications for model interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "You?\"",
        "given": "Why Should I.Trust"
      }
    ],
    "title": [
      "Explaining the Predictions of Any Classifier\"**"
    ],
    "editor": [
      {
        "family": "Ribeiro",
        "given": "M.T."
      },
      {
        "family": "Singh",
        "given": "S."
      },
      {
        "family": "Guestrin",
        "given": "C."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces LIME and discusses its application to various classifiers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "literal": "**\"Visualizing and Understanding Neural Models in NLP\"** - Li, J., Chen, X., Hovy, E., & Jurafsky, D."
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Uses visualization techniques to understand neural models in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)\"**"
    ],
    "editor": [
      {
        "family": "Kim",
        "given": "B."
      },
      {
        "family": "Wattenberg",
        "given": "M."
      },
      {
        "family": "Gilmer",
        "given": "J."
      },
      {
        "family": "Cai",
        "given": "C."
      },
      {
        "family": "Wexler",
        "given": "J."
      },
      {
        "family": "Viegas",
        "given": "F."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes TCAV, a method for testing model sensitivity to high-level concepts"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI\"**"
    ],
    "director": [
      {
        "family": "Arrieta",
        "given": "A.B."
      },
      {
        "family": "Díaz-Rodríguez",
        "given": "N."
      },
      {
        "family": "Del Ser",
        "given": "J."
      },
      {
        "family": "Bennetot",
        "given": "A."
      },
      {
        "family": "Tabik",
        "given": "S."
      },
      {
        "family": "Barbado",
        "given": "A."
      },
      {
        "family": "Herrera",
        "given": "F."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A comprehensive review of XAI, including concepts, taxonomies, and challenges"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of methods and perspectives on interpreting and understanding the predictions of deep NLP models, from feature attribution techniques to broader discussions on the principles and challenges of interpretability"
    ],
    "type": null
  }
]
