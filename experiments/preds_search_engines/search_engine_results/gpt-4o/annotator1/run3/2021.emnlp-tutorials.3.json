[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2021 that focus on natural language generation (NLG) with implicit commonsense and background knowledge"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** by Jacob Devlin"
    ],
    "publisher": [
      "Ming-Wei Chang"
    ],
    "location": [
      "Kenton Lee, Kristina Toutanova"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "While primarily about language understanding, BERT's pre-training on large corpora provides a foundation for NLG with embedded commonsense knowledge"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"GPT-3: Language Models are Few-Shot Learners\"** by Tom B"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Brown et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the capabilities of GPT-3, a model that leverages vast amounts of data to generate text with implicit commonsense and background knowledge"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\"** by Bill Yuchen Lin et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a benchmark for evaluating NLG systems on their ability to generate text grounded in commonsense knowledge"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "given": "C.O.M.E.T."
      }
    ],
    "title": [
      "Commonsense Transformers for Automatic Knowledge Graph Construction\"** by Antoine Bosselut et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Describes a model that generates commonsense knowledge by leveraging pre-trained language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"** by Colin Raffel et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores a unified framework for NLG that can incorporate background knowledge through transfer learning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Neural Text Generation: A Practical Guide\"** by Angela Fan"
    ],
    "editor": [
      {
        "family": "Lewis",
        "given": "Mike"
      },
      {
        "family": "Dauphin",
        "given": "Yann"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Provides an overview of neural text generation techniques, including methods for incorporating background knowledge"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Language Models as Knowledge Bases?\"** by Fabio Petroni et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the extent to which pre-trained language models can serve as repositories of factual and commonsense knowledge"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"A Survey on Contextual Embeddings\"** by Emily Alsentzer et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Reviews various contextual embedding methods that can enhance NLG systems with implicit knowledge"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Generating Fact Checking Explanations\"** by Andreas Hanselowski et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses generating explanations for fact-checking, which requires integrating background knowledge into the generation process"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Plug and Play Language Models: A Simple Approach to Controlled Text Generation\"** by Yuntian Deng et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a method for controlled text generation that can incorporate specific knowledge constraints"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Knowledge-Enhanced Neural Conversational Model Using Sememe Memory\"** by Yiming Cui et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a conversational model that integrates external knowledge to enhance the generation of contextually relevant responses"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Towards Commonsense and Knowledgeable Topic Models\"** by Yao Fu et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores topic models that incorporate commonsense knowledge for more coherent and knowledgeable text generation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Commonsense Knowledge Mining from Pretrained Models\"** by Antoine Bosselut et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses techniques for extracting and utilizing commonsense knowledge from pre-trained language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Narrative Generation with a Fusion of Background Knowledge and Story Grammar\"** by Boyang Li et al"
    ],
    "date": [
      "2013"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "An early work on integrating background knowledge with narrative generation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Incorporating Commonsense Knowledge into Neural Machine Translation\"** by Duyu Tang et al"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines how commonsense knowledge can improve the quality of machine translation, which is closely related to NLG"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Commonsense Knowledge in Word Associations and ConceptNet\"** by Robyn Speer, Catherine Havasi"
    ],
    "date": [
      "2012"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the role of commonsense knowledge in word associations and its application in NLG"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation\"** by Jian Guan et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a pretraining model specifically designed to enhance story generation with commonsense knowledge"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Commonsense Reasoning for Natural Language Understanding: A Survey of Benchmarks, Resources, and Approaches\"** by Chitta Baral et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Provides a comprehensive survey of commonsense reasoning in NLU, which is crucial for NLG tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Learning to Generate Natural Language Rationales for Game Playing Agents\"** by Prithviraj Ammanabrolu et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Focuses on generating natural language rationales that require integrating background knowledge"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Towards Knowledge-Based Text Generation with Hypergraph Attention Networks\"** by Xiaoyang Wang et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a model that uses hypergraph attention networks to incorporate knowledge into text generation"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of approaches and techniques for integrating implicit commonsense and background knowledge into natural language generation, providing a solid foundation for understanding the current state of the field up to"
    ],
    "date": [
      "2021"
    ],
    "type": null
  }
]
