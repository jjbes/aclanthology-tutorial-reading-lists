[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here is a list of 20 articles up to 2020 that focus on interpreting the predictions of neural networks and understanding their decisions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Explaining Explanations: An Overview of Interpretability of Machine Learning\"** by Finale Doshi-Velez and Been Kim"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Axiomatic Attribution for Deep Networks\"** by Mukund Sundararajan, Ankur Taly, and Qiqi Yan"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"** by Ramprasaath R"
    ],
    "date": [
      "2017"
    ],
    "type": "article-journal",
    "container-title": [
      "Selvaraju et al"
    ]
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "given": "L.I.M.E."
      }
    ],
    "title": [
      "Local Interpretable Model-agnostic Explanations\"** by Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "given": "DeepLIFT"
      }
    ],
    "title": [
      "Learning Important Features Through Propagating Activation Differences\"** by Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"The Building Blocks of Interpretability\"** by Chris Olah et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Anchors: High-Precision Model-Agnostic Explanations\"** by Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "given": "Interpretable"
      },
      {
        "family": "Been Kim",
        "given": "Pedagogical Examples\"",
        "particle": "by"
      },
      {
        "family": "Khanna",
        "given": "Rajiv"
      },
      {
        "family": "Koyejo",
        "given": "Oluwasanmi"
      }
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Towards a Rigorous Science of Interpretable Machine Learning\"** by Finale Doshi-Velez and Been Kim"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Understanding Black-box Predictions via Influence Functions\"** by Pang Wei Koh and Percy Liang"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Learning Important Features Through Propagating Activation Differences\"** by Avanti Shrikumar et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "given": "Visualizing"
      },
      {
        "family": "Matthew D. Zeiler",
        "given": "Understanding Convolutional Networks\"",
        "particle": "by"
      },
      {
        "family": "Fergus",
        "given": "Rob"
      }
    ],
    "date": [
      "2014"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "family": "Ashish Vaswani",
        "given": "Attention",
        "particle": "is All You Need\" by"
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "title": [
      "- While primarily about the Transformer model, it introduces the concept of attention which is crucial for interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"PatternNet and PatternAttribution: Improving the Interpretability of Neural Networks\"** by Alexander Kindermans et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"The Tree of Thoughts: Understanding Neural Network Decisions with Hierarchical Visualizations\"** by David Alvarez-Melis and Tommi Jaakkola"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Feature Importance Explanations for Temporal Black Box Models\"** by Sara Hooker et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "Explainable",
        "given": "A.I."
      }
    ],
    "title": [
      "The New 42?\"** by Wojciech Samek, Thomas Wiegand, and Klaus-Robert MÃ¼ller"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Interpreting and Understanding Deep Neural Networks by Means of Complex Network Analysis\"** by Enrico Scalas et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Towards Robust Interpretability with Self-Explaining Neural Networks\"** by David Alvarez-Melis and"
    ],
    "author": [
      {
        "family": "Jaakkola",
        "given": "Tommi S."
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"A Unified Approach to Interpreting Model Predictions\"** by Scott M"
    ],
    "publisher": [
      "Lundberg and Su-In Lee"
    ],
    "date": [
      "2017"
    ],
    "type": "book"
  },
  {
    "title": [
      "These articles cover a range of methods and perspectives on interpreting neural networks, from gradient-based techniques to model-agnostic approaches and visualization methods. They should provide a comprehensive foundation for understanding how to interpret and explain the decisions made by neural networks"
    ],
    "type": null
  }
]
