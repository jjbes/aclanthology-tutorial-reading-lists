[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here is a reading list of 20 articles on the topic of hallucinations in large language models (LLMs). These articles span from foundational concepts to the latest research up to"
    ],
    "date": [
      "2024"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** - Devlin et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Foundational paper introducing BERT, discussing early observations of model behavior"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "literal": "**\"Attention is All You Need\"** - Vaswani et al."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces the Transformer architecture, crucial for understanding LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Language Models are Few-Shot Learners\"**"
    ],
    "publisher": [
      "Brown et al"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses GPT-3 and its capabilities, including hallucination phenomena"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Understanding and Mitigating the Uncertainty of Language Models for Reliable Text Generation\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Holtzman et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores the uncertainty and reliability of text generated by LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Evaluating the Factual Consistency of Abstractive Text Summarization\"** - Kryscinski et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Focuses on the factual accuracy of summaries, relevant to hallucination issues"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "container-title": [
      "**\"Detecting Hallucinated Content in Conditional Neural Sequence Generation\"** - Maynez et al"
    ],
    "date": [
      "2020"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates methods for detecting hallucinated content in generated text"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "literal": "**\"Faithfulness and Abstractiveness in Summarization\"** - Falke et al."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines the balance between faithfulness and abstractiveness in text summarization"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\"** - Bender et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Critiques the scale of LLMs and discusses ethical implications, including hallucinations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"TruthfulQA: Measuring How Models Mimic Human Falsehoods\"** - Lin et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a benchmark for evaluating the truthfulness of LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Mitigating Misinformation in Abstractive Summarization by Leveraging the Wisdom of the Crowd\"**"
    ],
    "date": [
      "2021"
    ],
    "type": "article-journal",
    "container-title": [
      "Zellers et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses methods to reduce misinformation in generated summaries"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Factual Error Correction for Abstractive Summarization Models\"** - Dong et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes techniques for correcting factual errors in summaries"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Improving Factual Consistency of Abstractive Summarization via Question Answering\"** - Wang et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Uses question answering to improve the factual consistency of summaries"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "container-title": [
      "**\"Hallucination in Neural Machine Translation\"** - Lee et al"
    ],
    "date": [
      "2019"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates hallucination issues in the context of machine translation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Faithful to the Original: Fact Aware Neural Abstractive Summarization\"** - Zhao et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Focuses on maintaining factual accuracy in neural abstractive summarization"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Evaluating the Factual Consistency of Text Generated by Large Language Models\"**"
    ],
    "date": [
      "2022"
    ],
    "type": "article-journal",
    "container-title": [
      "Chen et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Provides a comprehensive evaluation framework for factual consistency in LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Fact-Checking in the Era of Large Language Models\"** - Thorne et al"
    ],
    "date": [
      "2022"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the role of fact-checking in mitigating hallucinations in LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Reducing Hallucination in Neural Machine Translation: A Model-Level Approach\"** - Zhang et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes model-level solutions to reduce hallucinations in machine translation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints\"** - Chen et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Addresses hallucination in the context of table-to-text generation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "container-title": [
      "**\"Mitigating Hallucinations in Neural Machine Translation with Model Uncertainty\"** - Ott et al"
    ],
    "date": [
      "2021"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores the use of model uncertainty to mitigate hallucinations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Hallucinations in Large Language Models: A Survey\"**"
    ],
    "publisher": [
      "Forthcoming"
    ],
    "date": [
      "2024"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A comprehensive survey of hallucinations in LLMs, summarizing recent advances and future directions"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a broad and deep understanding of the hallucination phenomenon in LLMs, covering various aspects from detection and mitigation to ethical considerations"
    ],
    "type": null
  }
]
