[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2021 that focus on neural machine translation (NMT) using pre-training methods. These articles cover a range of topics including pre-training strategies, model architectures, and applications in NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "family": "Vaswani",
        "given": "A."
      },
      {
        "family": "Shazeer",
        "given": "N."
      },
      {
        "family": "Parmar",
        "given": "N."
      },
      {
        "family": "Uszkoreit",
        "given": "J."
      },
      {
        "family": "Jones",
        "given": "L."
      },
      {
        "family": "Gomez",
        "given": "A.N."
      },
      {
        "family": "Polosukhin",
        "given": "I."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "title": [
      "Attention is All You Need"
    ],
    "container-title": [
      "*Advances in Neural Information Processing Systems*"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This seminal paper introduces the Transformer model, which is foundational for many pre-training methods in NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "family": "Devlin",
        "given": "J."
      },
      {
        "family": "Chang",
        "given": "M.W."
      },
      {
        "family": "Lee",
        "given": "K."
      },
      {
        "family": "Toutanova",
        "given": "K."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "publisher": [
      "*NAACL-HLT*"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "While primarily focused on language understanding, BERT's pre-training methods have influenced NMT approaches"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "family": "Lample",
        "given": "G."
      },
      {
        "family": "Conneau",
        "given": "A."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Cross-lingual Language Model Pretraining"
    ],
    "type": "article-journal",
    "container-title": [
      "*NeurIPS*"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper discusses XLM, a model that pre-trains on multiple languages for improved cross-lingual understanding and translation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "family": "Song",
        "given": "K."
      },
      {
        "family": "Tan",
        "given": "X."
      },
      {
        "family": "Qin",
        "given": "T."
      },
      {
        "family": "Lu",
        "given": "J."
      },
      {
        "family": "Liu",
        "given": "T."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "MASS: Masked Sequence to Sequence Pre-training for Language Generation"
    ],
    "container-title": [
      "*ICML*"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces MASS, a pre-training method specifically designed for sequence-to-sequence tasks like NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "family": "Ott",
        "given": "M."
      },
      {
        "family": "Goyal",
        "given": "N."
      },
      {
        "family": "Du",
        "given": "J."
      },
      {
        "family": "Joshi",
        "given": "M."
      },
      {
        "family": "Chen",
        "given": "D."
      },
      {
        "family": "Stoyanov",
        "given": "V."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
    ],
    "note": [
      "*arXiv preprint arXiv:1907.11692*."
    ],
    "arxiv": [
      "1907.11692"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses improvements to BERT's pre-training that can be beneficial for NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "family": "Lewis",
        "given": "M."
      },
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "family": "Goyal",
        "given": "N."
      },
      {
        "family": "Ghazvininejad",
        "given": "M."
      },
      {
        "family": "Mohamed",
        "given": "A."
      },
      {
        "family": "Levy",
        "given": "O."
      },
      {
        "family": "Zettlemoyer",
        "given": "L."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "container-title": [
      "\"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.\" *ACL*"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces BART, a model that combines BERT and GPT for sequence-to-sequence tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "family": "Conneau",
        "given": "A."
      },
      {
        "family": "Khandelwal",
        "given": "K."
      },
      {
        "family": "Goyal",
        "given": "N."
      },
      {
        "family": "Chaudhary",
        "given": "V."
      },
      {
        "family": "Wenzek",
        "given": "G."
      },
      {
        "family": "Guzm√°n",
        "given": "F."
      },
      {
        "family": "Stoyanov",
        "given": "V."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Unsupervised Cross-lingual Representation Learning at Scale"
    ],
    "publisher": [
      "*ACL*"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the XLM-R model, which scales cross-lingual pre-training to improve NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "family": "Raffel",
        "given": "C."
      },
      {
        "family": "Shazeer",
        "given": "N."
      },
      {
        "family": "Roberts",
        "given": "A."
      },
      {
        "family": "Lee",
        "given": "K."
      },
      {
        "family": "Narang",
        "given": "S."
      },
      {
        "family": "Matena",
        "given": "M."
      },
      {
        "family": "Liu",
        "given": "P.J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
    ],
    "publisher": [
      "*JMLR*"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "The T5 model, which treats all NLP tasks as text-to-text transformations, including NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "family": "Brown",
        "given": "T.B."
      },
      {
        "family": "Mann",
        "given": "B."
      },
      {
        "family": "Ryder",
        "given": "N."
      },
      {
        "family": "Subbiah",
        "given": "M."
      },
      {
        "family": "Kaplan",
        "given": "J."
      },
      {
        "family": "Dhariwal",
        "given": "P."
      },
      {
        "family": "Amodei",
        "given": "D."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Language Models are Few-Shot Learners"
    ],
    "type": "article-journal",
    "container-title": [
      "*NeurIPS*"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces GPT-3, which, while not specifically for NMT, has significant implications for translation tasks through few-shot learning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "family": "Zhang",
        "given": "J."
      },
      {
        "family": "Zhao",
        "given": "Y."
      },
      {
        "family": "Saleh",
        "given": "M."
      },
      {
        "family": "Liu",
        "given": "P.J."
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"
    ],
    "container-title": [
      "*ICML*"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses a pre-training method that can be adapted for NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "family": "He",
        "given": "D."
      },
      {
        "family": "Xia",
        "given": "Y."
      },
      {
        "family": "Qin",
        "given": "T."
      },
      {
        "family": "Wang",
        "given": "L."
      },
      {
        "family": "Yu",
        "given": "N."
      },
      {
        "family": "Liu",
        "given": "T.Y."
      },
      {
        "family": "Ma",
        "given": "W.Y."
      }
    ],
    "date": [
      "2016"
    ],
    "publisher": [
      "\"Dual Learning for Machine Translation.\" *NeurIPS*"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces dual learning, a method that can be combined with pre-training for NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "family": "Edunov",
        "given": "S."
      },
      {
        "family": "Ott",
        "given": "M."
      },
      {
        "family": "Auli",
        "given": "M."
      },
      {
        "family": "Grangier",
        "given": "D."
      }
    ],
    "date": [
      "2018"
    ],
    "title": [
      "Understanding Back-Translation at Scale"
    ],
    "publisher": [
      "*EMNLP*"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses back-translation, a technique often used in conjunction with pre-training for NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "family": "Lample",
        "given": "G."
      },
      {
        "family": "Ott",
        "given": "M."
      },
      {
        "family": "Conneau",
        "given": "A."
      },
      {
        "family": "Denoyer",
        "given": "L."
      },
      {
        "family": "Ranzato",
        "given": "M."
      }
    ],
    "date": [
      "2018"
    ],
    "publisher": [
      "\"Phrase-Based & Neural Unsupervised Machine Translation.\" *EMNLP*"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores unsupervised NMT, which benefits from pre-training methods"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "author": [
      {
        "family": "Artetxe",
        "given": "M."
      },
      {
        "family": "Labaka",
        "given": "G."
      },
      {
        "family": "Agirre",
        "given": "E."
      }
    ],
    "date": [
      "2018"
    ],
    "title": [
      "Unsupervised Statistical Machine Translation"
    ],
    "type": "article-journal",
    "container-title": [
      "*EMNLP*"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Another approach to unsupervised NMT that leverages pre-training"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "family": "Wu",
        "given": "J."
      },
      {
        "family": "Child",
        "given": "R."
      },
      {
        "family": "Luan",
        "given": "D."
      },
      {
        "family": "Amodei",
        "given": "D."
      },
      {
        "family": "Sutskever",
        "given": "I."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Language Models are Unsupervised Multitask Learners"
    ],
    "publisher": [
      "*OpenAI Blog*"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses GPT-2, which has applications in NMT through its pre-training"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "family": "Nguyen",
        "given": "K."
      },
      {
        "family": "Tuan Nguyen",
        "given": "T."
      },
      {
        "family": "Nguyen",
        "given": "T."
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Data Augmentation for Neural Machine Translation"
    ],
    "note": [
      "*arXiv preprint arXiv:2004.08792*."
    ],
    "arxiv": [
      "2004.08792"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores data augmentation techniques that can be combined with pre-training for NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "Zhu",
        "given": "J."
      },
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "family": "Fu",
        "given": "J."
      },
      {
        "family": "Wang",
        "given": "Y."
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Incorporating BERT into Neural Machine Translation"
    ],
    "note": [
      "*arXiv preprint arXiv:2002.06823*."
    ],
    "arxiv": [
      "2002.06823"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses methods to integrate BERT into NMT systems"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "family": "Lapata",
        "given": "M."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Text Summarization with Pretrained Encoders"
    ],
    "publisher": [
      "*EMNLP-IJCNLP*"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "While focused on summarization, the pre-training techniques are relevant for NMT"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "family": "Yang",
        "given": "Z."
      },
      {
        "family": "Dai",
        "given": "Z."
      },
      {
        "family": "Yang",
        "given": "Y."
      },
      {
        "family": "Carbonell",
        "given": "J."
      },
      {
        "family": "Salakhutdinov",
        "given": "R."
      },
      {
        "family": "Le",
        "given": "Q.V."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
    ],
    "type": "article-journal",
    "container-title": [
      "*NeurIPS*"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces XLNet, a model with implications for NMT pre-training"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "family": "Sun",
        "given": "Y."
      },
      {
        "family": "Wang",
        "given": "S."
      },
      {
        "family": "Li",
        "given": "Y."
      },
      {
        "family": "Feng",
        "given": "S."
      },
      {
        "family": "Tian",
        "given": "H."
      },
      {
        "family": "Wu",
        "given": "H."
      },
      {
        "family": "Wang",
        "given": "H."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "ERNIE: Enhanced Representation through Knowledge Integration"
    ],
    "publisher": [
      "*ACL*"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses ERNIE, a model that integrates knowledge into pre-training, useful for NMT"
    ],
    "type": null
  },
  {
    "title": [
      "These articles should provide a comprehensive overview of the advancements in neural machine translation using pre-training methods up to"
    ],
    "date": [
      "2021"
    ],
    "type": null
  }
]
