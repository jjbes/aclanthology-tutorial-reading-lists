[{"authors": ["Carlini, Nicholas", "Athalye, Anish", "Papernot, Nicolas", "Goodfellow, Ian", "Shlens, Jonathan", "Szegedy, Christian"], "title": "Evaluating adversarial robustness", "year": 2017}, {"authors": ["Wallace, Eric", "Lazaridou, Angeliki", "Sutskever, Ilya", "Pineau, Joelle"], "title": "Universal adversarial triggers for attacking and analyzing NLP", "year": 2019}, {"authors": ["Carlini, Nicholas", "Wagner, David"], "title": "Towards evaluating the robustness of neural networks", "year": 2017}, {"authors": ["Liu, Shiyu", "Li, Xiang", "Zhang, Fuyu", "Liu, Yanhui", "Zhang, Bo", "Wang, Haijun", "Zhang, Jun", "Wang, Zhi", "Li, Jun"], "title": "Data poisoning attacks against deep learning: A survey", "year": 2021}, {"authors": ["Carlini, Nicholas", "Wagner, David"], "title": "Adversarial examples are not bugs, they are features", "year": 2017}, {"authors": ["Papernot, Nicolas", "McDaniel, Patrick", "Jha, Somesh", "Fredrikson, Matt", "Cui, Z. Berkay", "Swami, Ananthram"], "title": "The limitations of deep learning in adversarial settings", "year": 2016}, {"authors": ["Shokri, Reza", "Stronati, Marco", "Song, Song", "Shmatikov, Vitaly"], "title": "Membership inference attacks against machine learning models", "year": 2017}, {"authors": ["Fredrikson, Matt", "Jha, Somesh", "Ristenpart, Thomas"], "title": "Model inversion attacks that exploit confidence information and basic countermeasures", "year": 2015}, {"authors": ["Geiping, Jonas", "Bauer, Hartmut", "Moeller, Michael", "Dror, Ron"], "title": "Inverting gradients - how easy is it to break privacy in federated learning?", "year": 2020}, {"authors": ["Nasr, Mohammad", "Shokri, Reza", "Houmansadr, Amir"], "title": "Comprehensive privacy analysis of deep learning: A case study of generative adversarial networks", "year": 2019}, {"authors": ["Carlini, Nicholas", "Wagner, David"], "title": "Defensive distillation is not robust to adversarial examples", "year": 2017}, {"authors": ["Madry, Aleksander", "Makelov, Aleksandar", "Schmidt, Ludwig", "Tsipras, Dimitris", "Vladu, Adrian"], "title": "Towards deep learning models resistant to adversarial attacks", "year": 2017}, {"authors": ["Goodfellow, Ian", "Shlens, Jonathan", "Szegedy, Christian"], "title": "Explaining and harnessing adversarial examples", "year": 2014}, {"authors": ["Ebrahimi, Javid", "Rao, Aditya", "Zhou, Daniel", "Li, Jun"], "title": "Adversarial training for natural language processing: A survey", "year": 2021}, {"authors": ["Li, B., Li, J., & Wang, S. (2021).", "Privacy-preserving federated learning: A survey", "arXiv preprint arXiv:2103.00925"], "title": "Privacy-preserving federated learning: A survey", "year": 2021}, {"authors": ["Liu, Shiyu", "Li, Xiang", "Zhang, Fuyu", "Liu, Yanhui", "Zhang, Bo", "Wang, Haijun", "Zhang, Jun", "Wang, Zhi", "Li, Jun"], "title": "Data poisoning attacks against deep learning: A survey", "year": 2021}, {"authors": ["Carlini, Nicholas", "Wagner, David"], "title": "Towards evaluating the robustness of neural networks", "year": 2017}, {"authors": ["Papernot, Nicolas", "McDaniel, Patrick", "Jha, Somesh", "Fredrikson, Matt", "Cui, Z. Berkay", "Swami, Ananthram"], "title": "The limitations of deep learning in adversarial settings", "year": 2016}, {"authors": ["Shokri, Reza", "Stronati, Marco", "Song, Song", "Shmatikov, Vitaly"], "title": "Membership inference attacks against machine learning models", "year": 2017}, {"authors": ["Fredrikson, Matt", "Jha, Somesh", "Ristenpart, Thomas"], "title": "Model inversion attacks that exploit confidence information and basic countermeasures", "year": 2015}]
