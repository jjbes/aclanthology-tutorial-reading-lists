[{"authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton", "Toutanova, Kristina"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Radford, Alec", "Wu, Jeffrey", "Child, Rewon", "Lu, David", "Amodei, Dario"], "title": "Language Models are Unsupervised Multitask Learners", "year": 2019}, {"authors": ["Brown, Tom", "Mann, Benjamin", "Rytting, Nick", "Roberts,  Melanie", "Bamford,  Sasha", "Chess,  Lucas", "Child,  Rewon", "Gray,  Scott", "Le,  Sander", "McCann,  Benjamin", "Radford,  Alec", "Wu,  Jeffrey", "Amodei,  Dario", "Sutskever,  Ilya"], "title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", " Joshi, Mandar", "Chen, Danqi", "Levy, Omer", "Lewis, Mike", "Zettlemoyer, Luke"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Lan, Zhilin", "Chen, Mingda", "Lu, Sheng", "Wang, Xueqi", "Zhang, Yun", "Gu, Qun", "Xu, Jing", "Wang, Haisong", "Liu, Zhiyuan"], "title": "ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations", "year": 2019}, {"authors": ["Raffel, Colin", "Shazeer, Noam", "Roberts, Adam", "Lee, Katherine", "Narang, Sharan", "Matena,  Michael", "Zhou, Yanqi", "Li, Wei", "Liu, Peter"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"authors": ["Sanh, Victor", "Debruyne, Lysandre", "Lachaux,  Thomas", "Buolamwini, Joy", "Ramesh,  Ashish", "Odena,  Aäron", "Bartunov,  Sergey", "Xu,  Wei", "Zhang,  Yiding", "Kong,  Ludwig", "Zou,  Fang", "Piot,  Clement",  "Touvron,  Hugo", "Wojna,  Zora", "Jaffar,  Manuela", "von Platen,  Leonard", "Liao,  Thomas", "Lavin,  Alexandre", "Karpukhin,  Vladimir", "Dauphin,  Yann", "Conneau,  Alexis", "Goyal,  Naman",  "Khandelwal,  Karan", "Lewis,  Mike", "Wang,  Luke", "Mu,  Yujia", "Liu,  Xiaodong", "Zettlemoyer,  Luke"], "title": "Multilingual Language Model Pre-training for Low-Resource Languages", "year": 2020}, {"authors": ["Tay, Yi", "Dai,  Anh",  "Maharjan,  Sushant",  "Lee,  Zhenzhong",  "Guo,  Daochen",  "Liu,  Yelong"], "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"authors": ["Clark, Kevin", "Khan,  Urvashi",  "Wang,  William",  "Mou,  Lili",  "Lewis,  Mike",  "Zettlemoyer,  Luke"], "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "year": 2020},  {"authors": ["Liu,  Xiaodong",  "Zhang,  Furu",  "Yang,  Nianwen",  "Zhou,  Ming",  "Li,  Zhiyuan",  "Wang,  Bing",  "Zhang,  Lei"], "title": "ERNIE 2.0: A Multilingual Pre-trained Language Model", "year": 2019}, {"authors": ["Wang,  Alex",  "Singh,  Amanpreet",  "Gupta,  Julian",  "Le,  Li",  "Sutskever,  Ilya"], "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data", "year": 2017}, {"authors": ["Peters,  Matthew",  "Neumann,  Mark",  "Iyyer,  Mohit",  "Gardner,  Matt",  "Clark,  Christopher",  "Lee,  Kenton",  "Zettlemoyer,  Luke"], "title": "Deep Contextualized Word Representations", "year": 2018}, {"authors": ["Dai,  Andrew",  "Yang,  Zihang",  "Yang,  Yiming",  "Carbonell,  Jaime",  "Salakhutdinov,  Ruslan"], "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "year": 2019}, {"authors": ["Howard,  Jeremy",  "Goyal,  Naman",  "Salazar,  Abhishek",  "Chan,  Waleed",  "Chen,  Zihang",  "Liu,  Chris",  "Parmar,  Nitish",  "Sachan,  Ilya",  "Zhang,  Mostafa",  "Serdyuk,  David",  "Kadam,  Naveen",  "Liu,  Yiming",  "Child,  Rewon",  "Glass,  Michael",  "Lee,  Zhengdong",  "Liu,  Scott",  "Shi,  Luke",  "Wang,  Zhenzhong",  "Wang,  Alex",  "Wu,  Yonghui",  "Dai,  Andrew",  "Yogatama,  Denny",  "Clark,  Chris",  "Neumann,  Mark",  "Smith,  Noah"], "title": "Universal Language Model Fine-tuning for Text Classification", "year": 2018}, {"authors": ["Conneau,  Alexis",  "Khandelwal,  Karan",  "Goyal,  Naman",  "Artetxe,  Miryam",  "Schwenk,  Holger",  "Barrault,  Loïc",  "Lewis,  Mike",  "Zettlemoyer,  Luke"], "title": "Unsupervised Cross-Lingual Representation Learning at Scale", "year": 2019}, {"authors": ["Liu,  Yelong",  "Zhou,  Peng",  "Zhao,  Zhenyu",  "Wang,  Zhihua",  "Ju,  Chang",  "Liu,  Huan",  "Zhang,  Haifeng",  "Liu,  Zichao"], "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "year": 2019}, {"authors": ["Gururangan,  Suchin",  "Marasović,  Ana",  "Belanger,  Katharina",  "Swayamdipta,  Swabha",  "Smith,  Noah"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2020}, {"authors": ["Raffel,  Colin",  "Shazeer,  Noam",  "Roberts,  Adam",  "Lee,  Katherine",  "Narang,  Sharan",  "Matena,  Michael",  "Zhou,  Yanqi",  "Li,  Wei",  "Liu,  Peter"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"authors": ["Joshi,  Mandar",  "Chen,  Danqi",  "Liu,  Yinhan",  "Levy,  Omer",  "Zettlemoyer,  Luke"], "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "year": 2019}, {"authors": ["Wolf,  Thomas",  "Sanh,  Victor",  "Debruyne,  Lysandre",  "Chaumond,  Julien",  "Delangue,  Clement",  "Ramesh,  Ashish",  "Popic,  Mark",  "Wu,  Yifan",  "Courtois,  Quentin",  "Bau,  Thibault",  "Gouws,  Sebastian",  "Lewis,  Mike",  "Furukawa,  Mike",  "Perez,  Yacine",  "Stoyanov,  Victor",  "Lachaux,  Thomas",  "Conneau,  Alexis",  "Karpukhin,  Vladimir",  "Goyal,  Naman",  "Khandelwal,  Karan",  "Wang,  Luke",  "Mu,  Yujia",  "Liu,  Xiaodong",  "Zettlemoyer,  Luke"], "title": "Hugging Face's Transformers: State-of-the-art Natural Language Processing", "year": 2020}]
