[{"authors": ["Wang, Alex, et al.", "ACL 2023"], "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2023}, {"authors": ["Liu, Pengfei, et al.", "ACL 2023"], "title": "Improving Robustness of Language Models via Contrastive Learning with Semantic Augmentation", "year": 2023}, {"authors": ["Minervini, Paul, et al.", "ACL 2023"], "title": "Reasoning with Language Models: Towards a Unified Framework for Evaluating and Improving Reasoning Abilities", "year": 2023}, {"authors": ["Zhao, Tianyi, et al.", "ACL 2023"], "title": "Towards Robust and Explainable Language Models: A Contrastive Learning Approach", "year": 2023}, {"authors": ["Schick, Tim, et al.", "EMNLP 2022"], "title": "Improving Reasoning in Language Models with Contrastive Learning", "year": 2022}, {"authors": ["Zhou, Danyang, et al.", "EMNLP 2022"], "title": "Reasoning with Language Models: A Survey", "year": 2022}, {"authors": ["Gehrmann, Sebastian, et al.", "NAACL 2022"], "title": "Improving Robustness of Language Models by Learning to Detect and Correct Errors", "year": 2022}, {"authors": ["McCann, Bryan, et al.", "ICLR 2022"], "title": "Improving Language Model Robustness with Adversarial Training", "year": 2022}, {"authors": ["Hendrycks, Dan, et al.", "ICLR 2022"], "title": "Measuring and Improving the Robustness of Language Models", "year": 2022}, {"authors": ["Liu, Yelong, et al.", "ACL 2021"], "title": "Pre-training Language Models with Contrastive Sentence-Pair Learning", "year": 2021}, {"authors": ["Gururangan, Suchin, et al.", "ACL 2020"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2020}, {"authors": ["Radford, Alec, et al.", "OpenAI 2019"], "title": "Language Models are Unsupervised Multitask Learners", "year": 2019}, {"authors": ["Devlin, Jacob, et al.", "NAACL 2019"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019}, {"authors": ["Peters, Matthew E., et al.", "EMNLP 2018"], "title": "Deep Contextualized Word Representations", "year": 2018}, {"authors": ["Dai, Andrew M., et al.", "ICLR 2018"], "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "year": 2018}, {"authors": ["Vaswani, Ashish, et al.", "NIPS 2017"], "title": "Attention Is All You Need", "year": 2017}, {"authors": ["Bahdanau, Dzmitry, et al.", "ICLR 2015"], "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "year": 2015}, {"authors": ["Sutskever, Ilya, et al.", "NIPS 2014"], "title": "Sequence to Sequence Learning with Neural Networks", "year": 2014}, {"authors": ["Mikolov, Tomas, et al.", "ICLR 2013"], "title": "Efficient Estimation of Word Representations in Vector Space", "year": 2013}, {"authors": ["Collobert, Ronan, et al.", "ICML 2011"], "title": "Natural Language Processing (Almost) from Scratch", "year": 2011}]
