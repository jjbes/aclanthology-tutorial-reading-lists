[{"authors": ["Nicholas Carlini", "David Wagner"], "title": "Towards Evaluating the Robustness of Deep Learning", "year": 2017}, {"authors": ["Anish Athalye", "Nicholas Carlini", "David Wagner"], "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples", "year": 2018}, {"authors": ["Micah Goldblum", "Jamie Hayes", "Bo Li", "Sergey Levine", "Dawn Song"], "title": "Spurious Correlations in Neural Networks", "year": 2020}, {"authors": ["Yossi Adi", "Jonathan Berant", "Jacob Goldberger", "Ofer Shapira", "Ori Ram"], "title": "Fine-grained Analysis of Sentence Embeddings for Textual Adversarial Attacks", "year": 2019}, {"authors": ["Moustapha Cisse", "Yossi Adi", "Eduardo Peixoto", "Tal Hassner"], "title": "Robustness via Curriculum Learning", "year": 2017}, {"authors": ["Nicholas Carlini", "David Wagner"], "title": "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods", "year": 2017}, {"authors": ["Florian Tramèr", "Dan Boneh", "Nicholas Carlini"], "title": "Stealing Machine Learning Models via Gradient Leakage", "year": 2016}, {"authors": ["Aleksander Madry", "Aleksander Makelov", "Ludwig Schmidt", "Dimitris Tsipras", "Adrian Vladu"], "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}, {"authors": ["Ian Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "title": "Explaining and Harnessing Adversarial Examples", "year": 2014}, {"authors": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z. Berkay Celik", "Ananthram Swami"], "title": "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks", "year": 2016}, {"authors": ["Nicholas Carlini", "David Wagner"], "title": "Defensive Distillation Is Not Robust to Adversarial Examples", "year": 2017}, {"authors": ["Florian Tramèr", "Fan Zhang", "Ari Juels", "Michael K. Reiter", "Thomas Ristenpart"], "title": "Stealing Machine Learning Models via Prediction APIs", "year": 2016}, {"authors": ["Shiqi Wang", "Yisen Wang", "Hengshuai Zhao", "Jingdong Shen", "Dinggang Shen", "Lawrence Carin"], "title": "Learning to Defend Against Adversarial Attacks with Gradient-Based Adversarial Training", "year": 2018}, {"authors": ["Jonas Geiping", "Micah Goldblum", "Jonas Rauber", "Thomas Bock", "Michael Moeller", "Florian Tramèr"], "title": "Analyzing and Improving the Robustness of Textual Adversarial Attacks", "year": 2020}, {"authors": ["Nicholas Carlini", "Chun-Chen Tu", "David Wagner"], "title": "Label-Only Poisoning of Deep Neural Networks", "year": 2017}, {"authors": ["Aleksander Madry", "Aleksander Makelov", "Ludwig Schmidt", "Dimitris Tsipras", "Adrian Vladu"], "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}, {"authors": ["Florian Tramèr", "Fan Zhang", "Ari Juels", "Michael K. Reiter", "Thomas Ristenpart"], "title": "Stealing Machine Learning Models via Prediction APIs", "year": 2016}, {"authors": ["Shiqi Wang", "Yisen Wang", "Hengshuai Zhao", "Jingdong Shen", "Dinggang Shen", "Lawrence Carin"], "title": "Learning to Defend Against Adversarial Attacks with Gradient-Based Adversarial Training", "year": 2018}, {"authors": ["Jonas Geiping", "Micah Goldblum", "Jonas Rauber", "Thomas Bock", "Michael Moeller", "Florian Tramèr"], "title": "Analyzing and Improving the Robustness of Textual Adversarial Attacks", "year": 2020}, {"authors": ["Nicholas Carlini", "Chun-Chen Tu", "David Wagner"], "title": "Label-Only Poisoning of Deep Neural Networks", "year": 2017}]
