[{"authors": ["Radford, Alec", "Wu, Jeffrey", "Child, Rewon", "Lu, David", "Amodei, Dario", "Sutskever, Ilya"], "title": "Language Models are Unsupervised Multitask Learners", "year": 2019}, {"authors": ["Brown, Tom", "Mann, Benjamin", "Rytting, Nick", "Kabra, Sandeep", "Dhariwal, Pieter", "Amodei, Dario", "Akhter, Ilia", "Barham, Paul", "Cai, Geoffrey", "Chung, Jing", "Daulton, S.,", "DeVito, Z.", "Du, S.", "Evans, B.", "Feyer, A.", "Ganguli, S.", "Goyal, N.", "Graves, A.", "Hinton, G.", "Jain, T.", "Karthik, K.", "Khanna, R.", "Liao, P.", "Lu, J.", "Maddison, C.", "Mirza, M.", "Monga, R.", "Olah, C.", "Parker, L.", "Patel, A.", "Perez, M.", "Pfeiffer, J.", "Prakash, A.", "Ramaswamy, S.", "Rao, A.", "Rosenbaum, D.", "Sastry, S.", "Schatzman, J.", "Sharma, V.", "Sidor, S.", "Smith, O.", "So, A.", "Sutskever, I.", "Swersky, K.", "Syed, M.", "Tang, M.", "Vaswani, A.", "Viegas, F.", "Vincent, J.", "Wang, L.", "Young, C.", "Zaremba, W.", "Zhang, A.", "Zhao, K.", "Zheng, H.", "Zhou, M.", "Zhu, Y.", "Zoph, B.", "Le, Q. V.", "Li, Y.", "Liu, Z.", "Bengio, Y.", "Manning, C. D.", "Lake, R.", "Clark, P.", "Htut, P.", "Duvenaud, D.", "Sachdeva, S.", "Talwar, K.", "NeurIPS", "2020"], "title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Raffel, Colin", "Shazeer, Noam", "Roberts, Adam", "Lee, Katherine", "Narang, Sharan", "Matena, Michael", "Zhou, Yan", "Li, Wei", "Liu, Peter J.", "Huang, Jihun", "Young, David", "Goyal, Neel", "Liu, Xiaodong", "Kannan, Aravind", "Shlens, Jon", "Zhang, Jie", "Zhao, Bo", "Prabhakar, Zhifeng", "Mehta, Ankur", "Faghri, Faraz", "NeurIPS", "2020"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2020}, {"authors": ["Zhang, Sheng", "Zhao, Junbo", "LeCun, Yann"], "title": "Character-Level Convolutional Networks for Text Classification", "year": 2015}, {"authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton", "Toutanova, Kristina"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Vaswani, Ashish", "Shazeer, Noam", "Parmar, Niki", "Uszkoreit, Jakob", "Jones, Llion", "Gomez, Aidan N.", "Kaiser, Lukasz", "Polosukhin, Illia"], "title": "Attention Is All You Need", "year": 2017}, {"authors": ["Dai, Andrew M.", "Yang, Zihang", "Yang, Yiming", "Carbonell, Jaime G.", "Salakhutdinov, Ruslan"], "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "year": 2019}, {"authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", "Fan, Amanda", "Ji, Zhilin", "Qiu, Xian", "Chen, Peter", "Goldberg, Yossi", "Yang, Jie"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Clark, Kevin", "Khan, Urvashi", "Chang, Michael W.", "Manning, Christopher D.", "Surya, Aakanksha"], "title": "Electra: Pre-training Text Encoders as Discriminators Rather Than Generators", "year": 2020}, {"authors": ["Sanh, Victor", "Debruyne, Lysandre", "Dupont, Julien", "Lachaux, Sylvain", "Fauquet, Quentin", "Rousseau, Adam", "Pasupat, Piotr", "So, Yacine", "Wolf, Thomas"], "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "year": 2019}, {"authors": ["Liu, Pengfei", "Zhou, Mike", "Zhao, Zhiyuan", "Wang, Zhifang", "Ju, Qun", "Htut, Peter", "Cheng, Xuanjing", "Liu, Yelong"], "title": "ERNIE 2.0: A Multilingual Pre-trained Language Model", "year": 2019}, {"authors": ["Wang, Alex", "Singh, Amanpreet", "Michael, Julian", "Hill, Felix", "Levy, Omer", "Bowman, Samuel R."],"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "year": 2018}, {"authors": ["Radford, Alec", "Narasimhan, Karthik", "Sutskever, Ilya", "Amodei, Dario"], "title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"authors": ["Howard, Jeremy", "Goyal, Naman", "Pennington, Jonathan", "Dandekar, Vinay", "Phelps, Sam", "Liu, Ziang", "Gross, Ryan", "Shuster, Kenneth", "Stoica, Ion", "Fan, Ankur"], "title": "Universal Language Model Fine-tuning for Text Classification", "year": 2018}, {"authors": ["Peters, Matthew E.", "Neumann, Mark", "Iyyer, Mohit", "Gardner, Matt", "Clark, Christopher", "Lee, Kenton", "Zettlemoyer, Luke"], "title": "Deep contextualized word representations", "year": 2018}, {"authors": ["Dai, Andrew M.", "Yang, Zihang", "Yang, Yiming", "Carbonell, Jaime G.", "Salakhutdinov, Ruslan"], "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "year": 2019}, {"authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", "Fan, Amanda", "Ji, Zhilin", "Qiu, Xian", "Chen, Peter", "Goldberg, Yossi", "Yang, Jie"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Clark, Kevin", "Khan, Urvashi", "Chang, Michael W.", "Manning, Christopher D.", "Surya, Aakanksha"], "title": "Electra: Pre-training Text Encoders as Discriminators Rather Than Generators", "year": 2020}, {"authors": ["Sanh, Victor", "Debruyne, Lysandre", "Dupont, Julien", "Lachaux, Sylvain", "Fauquet, Quentin", "Rousseau, Adam", "Pasupat, Piotr", "So, Yacine", "Wolf, Thomas"], "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "year": 2019}, {"authors": ["Liu, Pengfei", "Zhou, Mike", "Zhao, Zhiyuan", "Wang, Zhifang", "Ju, Qun", "Htut, Peter", "Cheng, Xuanjing", "Liu, Yelong"], "title": "ERNIE 2.0: A Multilingual Pre-trained Language Model", "year": 2019}]
