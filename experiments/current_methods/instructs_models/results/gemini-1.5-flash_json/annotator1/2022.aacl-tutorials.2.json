[{"authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton", "Toutanova, Kristina"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Howard, Jeremy", "Ruder, Sebastian"], "title": "Universal Language Model Fine-tuning for Text Classification", "year": 2018}, {"authors": ["Peters, Matthew E.", "Neumann, Mark", "Iyyer, Mohit", "Gardner, Matt", "Clark, Christopher", "Lee, Kenton", "Zettlemoyer, Luke"], "title": "Deep contextualized word representations", "year": 2018}, {"authors": ["Radford, Alec", "Narasimhan, Karthik", "Salimans, Tim", "Sutskever, Ilya"], "title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jing", "Fan, Amanda", "Jia, Zhiheng", "Chen, Danqi", "Guo, Min", "Phang, Jiasen", "Wang, Xian", "Zhang, Guoyin", "Young, Christopher", "Smith, Jason", "Wang, Jie"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Lan, Zhilin", "Chen, Mingda", "Lu, Sheng", "Wang, Xuan", "Zhou, Jing", "Zhang, Qun", "Liu, Peng", "Zhang, Sijie"], "title": "ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations", "year": 2019}, {"authors": ["Raffel, Colin", "Shazeer, Noam", "Roberts, Adam", "Lee, Katherine", "Narang, Sharan", "Matena, Michael", "Zhou, Yanqi", "Li, Wei", "Liu, Peter J"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"authors": ["Brown, Tom", "Mann, Benjamin", "Rytting, Nick", "Kabra, Sanjeeva", "Jafarnia, Daniel", "Faghri, Fahim", "Lewis, Scott", "Child, Ben", "Sumner, Geoffrey", "So, Christopher D", "Chess, James", "Ganguli, Surya", "Dhariwal, Prafulla", "Amodei, Dario", "Sutskever, Ilya"], "title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Sanh, Victor", "Dehaene, Lysandre", "Wolf, Thomas"], "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "year": 2019}, {"authors": ["Tay, Yi", "Dai, Anh Tuan", "Maharjan, Siddhartha", "Lee, K. H.", "Liu, Zhilin", "Yang, Jie", "Guo, Min", "Wang, Wei", "Zhang, Jie"], "title": "Efficient Transformers: A Survey", "year": 2022}, {"authors": ["Liu, Xiaodong", "Zhou, Peng", "Zhao, Zhiyuan", "Wang, Zhiheng", "Ju, Qiming", "Wang, Hongyun"], "title": "ERNIE 2.0: A Multilingual Pre-trained Language Model", "year": 2019}, {"authors": ["Wang, Alex", "Singh, Amanpreet", "Michael, Julian", "Hill, Felix", "Levy, Omer"], "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "year": 2018}, {"authors": ["Conneau, Alexis", "Kiela, Douwe", "Schwenk, Holger", "Barrault, Loïc", "Lecun, Yann"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2019}, {"authors": ["Gururangan, Suchin", "Marasović, Ana", "Beltagy, Ido", "Dodge, Jacob", "Smith, Noah A"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2020}, {"authors": ["Liu, Pengfei", "Zhou, Zhilin", "Zhao, Zhiyuan", "Wang, Zhiheng", "Ju, Qiming", "Wang, Hongyun"], "title": "ERNIE 2.0: A Multilingual Pre-trained Language Model", "year": 2019}, {"authors": ["Wang, Alex", "Singh, Amanpreet", "Michael, Julian", "Hill, Felix", "Levy, Omer"], "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "year": 2018}, {"authors": ["Conneau, Alexis", "Kiela, Douwe", "Schwenk, Holger", "Barrault, Loïc", "Lecun, Yann"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2019}, {"authors": ["Gururangan, Suchin", "Marasović, Ana", "Beltagy, Ido", "Dodge, Jacob", "Smith, Noah A"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2020}, {"authors": ["Liu, Pengfei", "Zhou, Zhilin", "Zhao, Zhiyuan", "Wang, Zhiheng", "Ju, Qiming", "Wang, Hongyun"], "title": "ERNIE 2.0: A Multilingual Pre-trained Language Model", "year": 2019}]
