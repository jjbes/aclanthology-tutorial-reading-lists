[{"authors": ["Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Janvin"], "title": "A Neural Probabilistic Language Model", "year": 2003}, {"authors": ["Mikolov, Tomas; Sutskever, Ilya; Chen, Kai; Corrado, Greg S.; Dean, Jeff"], "title": "Distributed Representations of Words and Phrases and their Compositionality", "year": 2013}, {"authors": ["Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua"], "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "year": 2014}, {"authors": ["Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V"], "title": "Sequence to Sequence Learning with Neural Networks", "year": 2014}, {"authors": ["Jean, Sébastien; Cho, Kyunghyun; Memisevic, Roland; Bengio, Yoshua"], "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "year": 2014}, {"authors": ["Cho, Kyunghyun; Van Merriënboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua"], "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "year": 2014}, {"authors": ["Luong, Minh-Thang; Sutskever, Ilya; Le, Quoc V; Vinyals, Oriol"], "title": "Addressing the Rare Word Problem in Neural Machine Translation", "year": 2015}, {"authors": ["Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin; Macherey, Klaus; et al."],"title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "year": 2016}, {"authors": ["Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Lukasz;  Polosukhin, Illia"], "title": "Attention Is All You Need", "year": 2017}, {"authors": ["Radford, Alec; Narasimhan, Karthik; Salimans, Tim; Sutskever, Ilya"], "title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"authors": ["Liu, Yinhan; Ott, Myle; Goyal, Naman; Du, Jing; Joshi, Manzil; Chen, Danqi;  Levy, Omer; Lewis, Mike; Zettlemoyer, Luke; Stoyanov, Veselin"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind;  Amodei, Dario;  Akhter,  S;  Alekseev,  Alex;  et al."],"title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine;  Narang, Sharan;  Matena,  Michael;  Zhou, Yan;  Li, Wei;  Liu, Peter J"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"authors": ["Joshi, Manzil; Chen, Danqi;  Xu,  Weizhu;  Stoyanov, Veselin"], "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "year": 2019}, {"authors": ["Clark, Kevin;  Fox,  Matthew;  Dhingra,  Bhuwan;  Chang,  Michael;  Guestrin,  Carlos;  Zettlemoyer,  Luke"], "title": "Deep Learning for Dialogue Generation", "year": 2018}, {"authors": ["Dai, Andrew;  Yang,  Zihang;  Yang,  Yiming;  Carbonell,  Jaime;  Salakhutdinov,  Ruslan"], "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "year": 2019}, {"authors": ["Liu,  Yang;  Guo,  Jingjing;  Zhou,  Peng;  Zhao,  Zhilin;  Wang,  Weizhu;  Ju,  Zhengdong;  Wang,  Haifeng"], "title": "Pre-Training with Whole Word Masking for Chinese BERT", "year": 2019}, {"authors": ["Al-Rfou, Rami;  Choe,  Desmond;  Yang,  Michael;  Luong,  Minh-Thang;  Sutskever,  Ilya"], "title": "Character-Level Language Modeling with Deeper Self-Attention", "year": 2018}, {"authors": ["Conneau, Alexis;  Kiela, Douwe;  Schwenk, Holger;  Barrault,  Loïc;  Lecun,  Yann"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2019}]
