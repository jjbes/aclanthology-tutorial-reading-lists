[{"authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton", "Toutanova, Kristina"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jing", "Fan, Amanda", "Jia, Zhilin", "Chen, Danqi", "Guo, Min", "Phang, Jiasen", "Wang, Xian", "Zhang, Yun", "Zhao, Jing", "Wang, Wei", "Liu, Zheng", "Jiang, Jianfeng"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Conneau, Alexis", "Kiela, Douwe", "Schwenk, Holger", "Barrault, Loïc", "Lecun, Yann"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2019}, {"authors": ["XLM-R: Multilingual and Cross-lingual Language Representation", "Conneau, Alexis", "Lample, Guillaume", "Ranzato, Marc'Aurelio", "Denoyer, Ludovic", "Jégou, Hervé"], "title": "XLM-R: Multilingual and Cross-lingual Language Representation", "year": 2020}, {"authors": ["Wu, Wei", "Deng, Jie", "Chen, Danqi", "Song, Yelong", "Liu, Feifei"], "title": "Cross-Lingual Language Model Pretraining for Low-Resource Neural Machine Translation", "year": 2020}, {"authors": ["Gururangan, Suchin", "Marasović, Ana", "Beltagy, Ido", "Smith, Noah A.", "Hovy, Eduard"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2020}, {"authors": ["Fan, Anyi", "Zhong, Victor", "Devlin, Jacob", "Chang, Ming-Wei", "Toutanova, Kristina"], "title": "XNLI: Cross-Lingual Natural Language Inference", "year": 2019}, {"authors": ["Liu, Pengfei", "Deng, Jie", "Wu, Wei", "Song, Yelong", "Liu, Feifei"], "title": "Cross-Lingual Language Model Pretraining for Low-Resource Neural Machine Translation", "year": 2020}, {"authors": ["Bapna, Aakanksha", "Firat, Orhan", "Schwenk, Holger", "Neubig, Graham"], "title": "Zero-Shot Cross-Lingual Transfer with Multilingual BERT", "year": 2020}, {"authors": ["Lample, Guillaume", "Conneau, Alexis", "Denoyer, Ludovic", "Ranzato, Marc'Aurelio"], "title": "Cross-Lingual Language Model Pretraining", "year": 2019}, {"authors": ["Artetxe, Mikel", "Labaka, Gorka", "Agirre, Eneko"], "title": "Multilingual BERT for Cross-Lingual Transfer Learning", "year": 2019}, {"authors": ["Plumer, David", "Wang, Sheng", "Liu, Yang", "Eisenstein, Jacob"], "title": "Unsupervised Cross-Lingual Representation Learning for Code-Switching", "year": 2020}, {"authors": ["Ruder, Sebastian", "Cotterell, Ryan", "Smith, Noah A.", "Eisenstein, Jacob"], "title": "Bridging the Gap Between Human and Machine Translation", "year": 2019}, {"authors": ["Liu, Yang", "Plumer, David", "Eisenstein, Jacob"], "title": "Unsupervised Cross-Lingual Representation Learning for Code-Switching", "year": 2020}, {"authors": ["Schwenk, Holger", "Barrault, Loïc", "Conneau, Alexis", "Kiela, Douwe", "Lecun, Yann"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2019}, {"authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton", "Toutanova, Kristina"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jing", "Fan, Amanda", "Jia, Zhilin", "Chen, Danqi", "Guo, Min", "Phang, Jiasen", "Wang, Xian", "Zhang, Yun", "Zhao, Jing", "Wang, Wei", "Liu, Zheng", "Jiang, Jianfeng"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Conneau, Alexis", "Kiela, Douwe", "Schwenk, Holger", "Barrault, Loïc", "Lecun, Yann"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2019}, {"authors": ["XLM-R: Multilingual and Cross-lingual Language Representation", "Conneau, Alexis", "Lample, Guillaume", "Ranzato, Marc'Aurelio", "Denoyer, Ludovic", "Jégou, Hervé"], "title": "XLM-R: Multilingual and Cross-lingual Language Representation", "year": 2020}, {"authors": ["Wu, Wei", "Deng, Jie", "Chen, Danqi", "Song, Yelong", "Liu, Feifei"], "title": "Cross-Lingual Language Model Pretraining for Low-Resource Neural Machine Translation", "year": 2020}]
