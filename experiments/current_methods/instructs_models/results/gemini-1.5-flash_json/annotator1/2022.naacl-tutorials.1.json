[{"authors": ["Radford, Alec, et al.", "OpenAI"], "title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"authors": ["Devlin, Jacob, et al.", "Google AI"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Vaswani, Ashish, et al.", "Google Brain"], "title": "Attention Is All You Need", "year": 2017}, {"authors": ["Brown, Tom, et al.", "OpenAI"], "title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Raffel, Colin, et al.", "Google AI"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"authors": ["Liu, Yinhan, et al.", "Google AI"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Clark, Kevin, et al.", "Google AI"], "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "year": 2020}, {"authors": ["Zhang, Yinhan, et al.", "Google AI"], "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"authors": ["Dai, Hanxiao, et al.", "Google AI"], "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "year": 2019}, {"authors": ["Lewis, Mike, et al.", "Google AI"], "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}, {"authors": ["Sennrich, Rico, et al.", "University of Edinburgh"], "title": "Neural Machine Translation of Rare Words with Subword Units", "year": 2016}, {"authors": ["Wu, Yonghui, et al.", "Google AI"], "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "year": 2016}, {"authors": ["Bahdanau, Dzmitry, et al.", "University of Montreal"], "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "year": 2014}, {"authors": ["Cho, Kyunghyun, et al.", "University of Montreal"], "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "year": 2014}, {"authors": ["Sutskever, Ilya, et al.", "University of Toronto"], "title": "Sequence to Sequence Learning with Neural Networks", "year": 2014}, {"authors": ["Graves, Alex, et al.", "University of Toronto"], "title": "Generating Sequences With Recurrent Neural Networks", "year": 2013}, {"authors": ["Mikolov, Tomas, et al.", "Google"], "title": "Efficient Estimation of Word Representations in Vector Space", "year": 2013}, {"authors": ["Bengio, Yoshua, et al.", "University of Montreal"], "title": "A Neural Probabilistic Language Model", "year": 2003}, {"authors": ["Rumelhart, David E., et al.", "University of California, San Diego"], "title": "Learning Representations by Back-propagating Errors", "year": 1986}, {"authors": ["Hopfield, John J.", "California Institute of Technology"], "title": "Neural networks and physical systems with emergent collective computational abilities", "year": 1982}]
