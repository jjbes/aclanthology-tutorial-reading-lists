[{"title": "Transfer Learning for Text Generation: A Survey", "year": 2020}, {"title": "Contextualized Embeddings in Natural Language Generation: A Survey", "year": 2019}, {"title": "Deep Contextualized Word Representations", "year": 2018}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"title": "GPT-2: Language Models are Unsupervised Multitask Learners", "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}, {"title": "T5: Text-To-Text Transfer Transformer", "year": 2019}, {"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2020}, {"title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "year": 2019}, {"title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "year": 2019}, {"title": "UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation", "year": 2019}, {"title": "CTRL: A Conditional Transformer Language Model for Controllable Generation", "year": 2019}, {"title": "GPT-3: Language Models are Few-Shot Learners", "year": 2020}, {"title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"title": "Language Models are Few-Shot Learners", "year": 2020}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "year": 2014}, {"title": "Sequence to Sequence Learning with Neural Networks", "year": 2014}, {"title": "Generating Sequences With Recurrent Neural Networks", "year": 2013}]
