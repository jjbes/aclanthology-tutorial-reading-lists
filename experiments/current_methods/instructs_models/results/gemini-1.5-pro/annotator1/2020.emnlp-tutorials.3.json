[{"title": "The Myth of Model Interpretability", "year": 2016}, {"title": "Axiomatic Attribution for Deep Networks", "year": 2017}, {"title": "Local Interpretable Model-Agnostic Explanations (LIME)", "year": 2016}, {"title": "SHAP (SHapley Additive exPlanations)", "year": 2017}, {"title": "Visualizing and Understanding Convolutional Networks", "year": 2014}, {"title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "year": 2014}, {"title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization", "year": 2017}, {"title": "SmoothGrad: removing noise by adding noise", "year": 2017}, {"title": "Understanding Black-box Predictions via Influence Functions", "year": 2017}, {"title": "Distilling the Knowledge in a Neural Network", "year": 2015}, {"title": "Learning Important Features Through Propagating Activation Differences", "year": 2017}, {"title": "Interpretation of Neural Networks is fragile", "year": 2018}, {"title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "year": 2018}, {"title": "Explaining Explanations: An Overview of Interpretability of Machine Learning", "year": 2018}, {"title": "Towards A Rigorous Science of Interpretable Machine Learning", "year": 2017}, {"title": "Sanity Checks for Saliency Maps", "year": 2018}, {"title": "On Calibration of Modern Neural Networks", "year": 2017}, {"title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "year": 2019}, {"title": "This Looks Like That: Deep Learning for Interpretable Image Recognition", "year": 2019}, {"title": "Neural Networks are Surprisingly Modular", "year": 2020}]
