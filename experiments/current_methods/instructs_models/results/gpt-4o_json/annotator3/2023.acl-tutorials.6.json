[{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "year": 2019}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2019}, {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "year": 2020}, {"title": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "year": 2020}, {"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "authors": ["Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning"], "year": 2020}, {"title": "GPT-3: Language Models are Few-Shot Learners", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "Tom Henighan", "Rewon Child", "Aditya Ramesh", "Daniel M. Ziegler", "Jeffrey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "year": 2020}, {"title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "authors": ["Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf"], "year": 2019}, {"title": "ERNIE: Enhanced Representation through Knowledge Integration", "authors": ["Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Hao Tian", "Hua Wu", "Haifeng Wang"], "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "year": 2019}, {"title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "authors": ["Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy"], "year": 2020}, {"title": "UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation", "authors": ["Li Dong", "Furu Wei", "Wenhui Wang", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon"], "year": 2019}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "authors": ["Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer"], "year": 2020}, {"title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "authors": ["Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen"], "year": 2021}, {"title": "Big Bird: Transformers for Longer Sequences", "authors": ["Manzil Zaheer", "Guru Guruganesh", "Kumar Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "Santiago Ontanon", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang", "Amr Ahmed"], "year": 2020}, {"title": "Reformer: The Efficient Transformer", "authors": ["Nikolaus P. Schmid", "Jonas Kohn", "Stefan Braun", "Stefan Riezler"], "year": 2020}, {"title": "Longformer: The Long-Document Transformer", "authors": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"], "year": 2020}, {"title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization", "authors": ["Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu"], "year": 2020}, {"title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "authors": ["Yu Sun", "Hao Tian", "Shengyu Wang", "Shikun Feng", "Hao Tian", "Hua Wu", "Haifeng Wang"], "year": 2020}, {"title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer", "authors": ["Xueguang Ma", "Yinhan Liu", "Luke Zettlemoyer", "Ming-Wei Chang"], "year": 2021}, {"title": "LaBSE: Language-agnostic BERT Sentence Embedding", "authors": ["Fangxiaoyu Feng", "Yinfei Yang", "Daniel Cer", "Naman Goyal", "Ming-Wei Chang", "Rochelle L. Rivlin", "Matt Peters", "Ming-Wei Chang", "Weizhu Chen"], "year": 2020}]