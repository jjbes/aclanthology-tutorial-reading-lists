[{"title": "Language Models are Few-Shot Learners", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder"], "year": 2020}, {"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts"], "year": 2020}, {"title": "Zero-Shot Text Classification with Generative Language Models", "authors": ["Yinhan Liu", "Jiatao Gu", "Xiaodong Liu"], "year": 2021}, {"title": "Few-Shot Learning with Contextualized Word Vectors", "authors": ["Matthew Peters", "Mark Neumann", "Luke Zettlemoyer"], "year": 2019}, {"title": "Unsupervised Data Augmentation for Consistency Training", "authors": ["Qizhe Xie", "Zihang Dai", "Eduard Hovy"], "year": 2020}, {"title": "GPT-3: Language Models are Few-Shot Learners", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder"], "year": 2020}, {"title": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts"], "year": 2020}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee"], "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang"], "year": 2019}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal"], "year": 2019}, {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman"], "year": 2020}, {"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "authors": ["Kevin Clark", "Minh-Thang Luong", "Quoc V. Le"], "year": 2020}, {"title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "authors": ["Pengcheng He", "Xiaodong Liu", "Jianfeng Gao"], "year": 2021}, {"title": "ERNIE: Enhanced Representation through Knowledge Integration", "authors": ["Yu Sun", "Shuohuan Wang", "Yukun Li"], "year": 2019}, {"title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "authors": ["Mandar Joshi", "Danqi Chen", "Yasaman Samei"], "year": 2020}, {"title": "Few-Shot Text Classification with Distributional Signatures", "authors": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang"], "year": 2021}, {"title": "Meta-Learning for Few-Shot Natural Language Processing: A Survey", "authors": ["Fei Xia", "Pascale Fung"], "year": 2021}, {"title": "LEOPARD: Language Model Pre-training with Latent Structure", "authors": ["Suchin Gururangan", "Tam Dang", "Dallas Card"], "year": 2021}, {"title": "Zero-Shot Learning with Pretrained Language Models", "authors": ["Suchin Gururangan", "Dallas Card", "Noah A. Smith"], "year": 2021}, {"title": "Few-Shot Learning with Pretrained Language Models", "authors": ["Suchin Gururangan", "Dallas Card", "Noah A. Smith"], "year": 2021}]