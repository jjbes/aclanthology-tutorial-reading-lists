[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2023 that cover various security challenges in natural language processing (NLP) models, including different types of attacks and defenses"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Adversarial Examples for Evaluating Reading Comprehension Systems\"**"
    ],
    "editor": [
      {
        "family": "Jia",
        "given": "Robin"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses adversarial attacks on reading comprehension systems"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"HotFlip: White-Box Adversarial Examples for Text Classification\"** - Ebrahimi, Javid, et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a method for generating white-box adversarial examples for text classification"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Universal Adversarial Triggers for Attacking and Analyzing NLP\"** - Wallace, Eric, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores universal adversarial triggers that can fool NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"TextFooler: A Model for Evaluating the Robustness of NLP Models\"** - Jin, Di, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes TextFooler, a method for generating adversarial text to test NLP model robustness"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Robustness and Reliability of NLP Models: A Survey\"** - Zhang, Tianyi, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A comprehensive survey on the robustness and reliability of NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review\"** - Sun, Lichao, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Reviews adversarial attacks and defenses across different domains, including text"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"On the Robustness of Language Encoders against Grammatical Errors\"** - Pruthi, Danish, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the robustness of language models against grammatical errors"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Mitigating Adversarial Effects Through Randomization in NLP Models\"** - Xie, Cihang, et al"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses randomization techniques to mitigate adversarial effects in NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "literal": "**\"Certified Robustness to Adversarial Word Substitutions\"** - Jia, Robin, et al."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a method for certifying the robustness of NLP models to adversarial word substitutions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Training for Free!\"** - Shafahi, Ali, et al."
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a method for adversarial training that is computationally efficient"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Exploring the Vulnerability of Deep Neural Networks: A Study of Parameter Corruption\"** - He, Kaiming, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines the vulnerability of deep neural networks to parameter corruption"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Attacks on Neural Network Policies\"** - Huang, Sandy, et al."
      }
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses adversarial attacks on neural network policies, including those used in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Adversarial Examples Are Not Bugs, They Are Features\"** - Ilyas, Andrew, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Argues that adversarial examples exploit features inherent in the data"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Improving the Robustness of NLP Models to Adversarial Attacks\"** - Wang, Alex, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes methods to improve the robustness of NLP models against adversarial attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Adversarial Training for Large Neural Language Models\"** - Zhu, Chengxu, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses adversarial training techniques for large neural language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Defending Against Neural Fake News\"**"
    ],
    "editor": [
      {
        "family": "Zellers",
        "given": "Rowan"
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores methods to defend against neural fake news generation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Adversarial Attacks on Text Generators: A Survey\"** - Zhang, Tianyi, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Surveys adversarial attacks specifically targeting text generation models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Adversarial Robustness of Neural-Statistical Learning Models for Text Classification\"** - Wang, Yifan, et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the adversarial robustness of neural-statistical learning models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Adversarial Examples in Modern Machine Learning: A Review\"** - Akhtar, Naveed, and Ajmal Mian"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A review of adversarial examples in machine learning, including NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Adversarial Attacks and Defenses in Deep Learning\"** - Yuan, Xiaoyong, et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Comprehensive review of adversarial attacks and defenses in deep learning, with sections on NLP"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a broad overview of the current state of research on security challenges in NLP models, including various types of attacks and defense mechanisms"
    ],
    "type": null
  }
]
