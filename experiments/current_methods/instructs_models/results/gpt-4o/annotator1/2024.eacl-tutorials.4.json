[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here is a reading list of 20 articles on the interpretability of transformer models, covering a range of topics from foundational concepts to the latest research up to 2024"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "family": "Vaswani",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "title": [
      "Attention is All You Need.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "NeurIPS"
      }
    ],
    "type": null
  },
  {
    "title": [
      "This foundational paper introduces the Transformer model and its attention mechanism"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "family": "Clark",
        "given": "K."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "What Does BERT Look At? An Analysis of BERT's Attention.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "E.M.N.L.P."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Analyzes the attention heads in BERT to understand what the model focuses on"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "family": "Rogers",
        "given": "A."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "A Primer in BERTology: What We Know About How BERT Works.\"**"
    ],
    "type": null
  },
  {
    "title": [
      "*Transactions of the Association for Computational Linguistics.*"
    ],
    "type": null
  },
  {
    "title": [
      "A comprehensive survey of research on BERT's interpretability"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "family": "Jain",
        "given": "S."
      },
      {
        "family": "Wallace",
        "given": "B.C."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Attention is not Explanation.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "N.A.A.C.L.-H.L.T."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Challenges the notion that attention weights can be directly interpreted as explanations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "family": "Kovaleva",
        "given": "O."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Revealing the Dark Secrets of BERT.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "E.M.N.L.P."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Investigates the interpretability of BERT by probing its attention heads and layers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "family": "Tenney",
        "given": "I."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "BERT Rediscovers the Classical NLP Pipeline.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "A.C.L."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Probes BERT to understand how it captures linguistic structures"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "family": "Michel",
        "given": "P."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Are Sixteen Heads Really Better than One?\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "NeurIPS"
      }
    ],
    "type": null
  },
  {
    "title": [
      "Examines the redundancy and importance of attention heads in Transformers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "family": "Vig",
        "given": "J."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "A Multiscale Visualization of Attention in the Transformer Model.\"**"
    ],
    "type": null
  },
  {
    "note": [
      "*arXiv preprint arXiv:1906.05714.*"
    ],
    "arxiv": [
      "1906.05714"
    ],
    "type": null
  },
  {
    "title": [
      "Proposes a visualization tool for understanding attention in Transformers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "family": "Voita",
        "given": "E."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "A.C.L."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Analyzes the roles of different attention heads in Transformers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "family": "Hewitt",
        "given": "J."
      },
      {
        "family": "Manning",
        "given": "C.D."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "A Structural Probe for Finding Syntax in Word Representations.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "N.A.A.C.L.-H.L.T."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Introduces a method to probe syntactic information in Transformer models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "family": "Lin",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Open Sesame: Getting Inside BERT's Linguistic Knowledge.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "A.C.L."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Investigates the linguistic knowledge encoded in BERT's layers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "family": "Brunner",
        "given": "G."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "On Identifiability in Transformers.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "I.C.L.R."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Studies the identifiability of parameters in Transformer models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "family": "Kassner",
        "given": "N."
      },
      {
        "family": "Sch√ºtze",
        "given": "H."
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "A.C.L."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Proposes new probing techniques to test the robustness of language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "author": [
      {
        "family": "Ethayarajh",
        "given": "K."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "E.M.N.L.P."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Compares the contextualization of word embeddings in different models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "family": "Geva",
        "given": "M."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Transformer Feed-Forward Layers Are Key-Value Memories.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "E.M.N.L.P."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Analyzes the role of feed-forward layers in Transformers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "family": "Elhage",
        "given": "N."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "A Mathematical Framework for Transformer Circuits.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Distill"
      }
    ],
    "type": null
  },
  {
    "title": [
      "Proposes a mathematical framework to understand the circuits in Transformer models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "Meng",
        "given": "Z."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Locating and Editing Factual Associations in GPT.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "NeurIPS"
      }
    ],
    "type": null
  },
  {
    "title": [
      "Studies how factual information is stored and can be edited in GPT models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "family": "Ravfogel",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Linear Adversarial Concept Erasure.\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "NeurIPS"
      }
    ],
    "type": null
  },
  {
    "title": [
      "Introduces a method to erase specific concepts from Transformer models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "family": "Belinkov",
        "given": "Y."
      },
      {
        "family": "Glass",
        "given": "J."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Analysis Methods in Neural Language Processing: A Survey.\"**"
    ],
    "type": null
  },
  {
    "title": [
      "*Transactions of the Association for Computational Linguistics.*"
    ],
    "type": null
  },
  {
    "title": [
      "Surveys various methods for analyzing neural language models, including Transformers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "family": "Tay",
        "given": "Y."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Are Pre-trained Convolutions Better than Pre-trained Transformers?\"**"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "I.C.L.R."
      }
    ],
    "type": null
  },
  {
    "title": [
      "Compares the interpretability and performance of pre-trained convolutional models and Transformers"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a broad overview of the interpretability of Transformer models, from foundational concepts to cutting-edge research"
    ],
    "type": null
  }
]
