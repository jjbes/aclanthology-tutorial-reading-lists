[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2024 that focus on the evaluation of large language models"
    ],
    "note": [
      "LLMs), particularly on benchmarks and frameworks:"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** - Devlin et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\"**"
    ],
    "date": [
      "2019"
    ],
    "type": "article-journal",
    "container-title": [
      "Wang et al"
    ]
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\"** - Wang et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Evaluating Large Language Models Trained on Code\"** - Chen et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList\"** - Ribeiro et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\"**"
    ],
    "date": [
      "2021"
    ],
    "type": "article-journal",
    "container-title": [
      "Gehrmann et al"
    ]
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Dynabench: Rethinking Benchmarking in NLP\"** - Kiela et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"ELEVATER: A Benchmark and Toolkit for Evaluating Language Models on a Diverse Set of Tasks\"**"
    ],
    "date": [
      "2022"
    ],
    "type": "article-journal",
    "container-title": [
      "Liu et al"
    ]
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"BIG-bench: A Large-scale Evaluation of Language Models on Diverse Tasks\"**"
    ],
    "date": [
      "2022"
    ],
    "type": "article-journal",
    "container-title": [
      "Srivastava et al"
    ]
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Holistic Evaluation of Language Models\"** - Liang et al"
    ],
    "date": [
      "2022"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models\"**"
    ],
    "date": [
      "2022"
    ],
    "type": "article-journal",
    "container-title": [
      "Wei et al"
    ]
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"The HELM Benchmark: Holistic Evaluation of Language Models\"**"
    ],
    "date": [
      "2023"
    ],
    "type": "article-journal",
    "container-title": [
      "Li et al"
    ]
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"LM-BFF: Better Few-shot Fine-tuning of Language Models\"** - Gao et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Measuring Massive Multitask Language Understanding\"** - Hendrycks et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Gao et al"
    ]
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Scaling Laws for Neural Language Models\"** - Kaplan et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Evaluating the Robustness of Language Models to Input Perturbations\"**"
    ],
    "date": [
      "2019"
    ],
    "type": "article-journal",
    "container-title": [
      "Pruthi et al"
    ]
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"The State of AI Ethics Report\"** - McGregor et al"
    ],
    "date": [
      "2022"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"The Benchmarking Toolkit: A Framework for Evaluating Language Models\"** - Zhang et al"
    ],
    "date": [
      "2023"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Towards a Comprehensive Evaluation of Language Models: A Survey of Benchmarks and Metrics\"**"
    ],
    "date": [
      "2023"
    ],
    "type": "article-journal",
    "container-title": [
      "Johnson et al"
    ]
  },
  {
    "title": [
      "These articles cover a range of topics from the development of specific benchmarks and evaluation frameworks to broader surveys and analyses of the state of LLM evaluation. They should provide a comprehensive overview of the current landscape and emerging trends in the evaluation of large language models"
    ],
    "type": null
  }
]
