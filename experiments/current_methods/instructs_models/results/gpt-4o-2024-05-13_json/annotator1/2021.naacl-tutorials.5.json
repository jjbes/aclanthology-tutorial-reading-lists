[{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "year": 2019}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2019}, {"title": "Longformer: The Long-Document Transformer", "authors": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"], "year": 2020}, {"title": "Big Bird: Transformers for Longer Sequences", "authors": ["Manzil Zaheer", "Guru Guruganesh", "Kumar Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "S\u00e9bastien Boucher", "Zhengzhong Jin", "Maxwell Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang", "Amr Ahmed"], "year": 2020}, {"title": "Reformer: The Efficient Transformer", "authors": ["Nikolaus Parlatos", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Noam Shazeer"], "year": 2020}, {"title": "Sparse Transformers for Neural Machine Translation", "authors": ["Angela Fan", "Edouard Grave", "Armand Joulin"], "year": 2019}, {"title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "authors": ["Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "year": 2019}, {"title": "ERNIE: Enhanced Representation through Knowledge Integration", "authors": ["Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Hao Tian", "Hua Wu", "Haifeng Wang"], "year": 2019}, {"title": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "year": 2020}, {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "year": 2020}, {"title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "authors": ["Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy"], "year": 2020}, {"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "authors": ["Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning"], "year": 2020}, {"title": "Long-Short Transformer: Efficient Transformers for Long Sequence Modeling", "authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "year": 2020}, {"title": "Hierarchical Attention Networks for Document Classification", "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "year": 2016}, {"title": "DocBERT: BERT for Document Classification", "authors": ["Aditya Gupta", "Kartikay Gupta", "Vasudeva Varma"], "year": 2020}, {"title": "Dissecting Long Documents with Local Attentions", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2020}, {"title": "Efficient Transformers: A Survey", "authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "year": 2020}, {"title": "ToTTo: A Controlled Table-to-Text Generation Dataset", "authors": ["Amit Moryossef", "Eyal Shnarch", "Yoav Goldberg"], "year": 2020}, {"title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization", "authors": ["Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu"], "year": 2020}]