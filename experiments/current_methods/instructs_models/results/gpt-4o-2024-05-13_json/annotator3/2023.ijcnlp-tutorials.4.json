[{"title": "Fine-Tuning Pre-trained Language Models", "authors": ["Jacob Devlin", "Ming-Wei Chang"], "year": 2019}, {"title": "Parameter-Efficient Transfer Learning for NLP", "authors": ["Neil Houlsby", "Andrei Giurgiu"], "year": 2019}, {"title": "Adapters: Lightweight Transfer Learning for NLP", "authors": ["Jonas Pfeiffer", "Andreas R\u00fcckl\u00e9"], "year": 2020}, {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "authors": ["Edward J. Hu", "Yelong Shen"], "year": 2021}, {"title": "Prompt Tuning for GPT-3", "authors": ["Brian Lester", "Rami Al-Rfou"], "year": 2021}, {"title": "P-Tuning: Prompt-based Tuning for Pre-trained Models", "authors": ["Xiao Liu", "Yanan Zheng"], "year": 2021}, {"title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "authors": ["Xiang Lisa Li", "Percy Liang"], "year": 2021}, {"title": "Efficient Fine-Tuning of Large-Scale Models", "authors": ["Zhenzhong Lan", "Mingda Chen"], "year": 2020}, {"title": "Hyperparameter Optimization for Large Language Models", "authors": ["James Bergstra", "Yoshua Bengio"], "year": 2012}, {"title": "Knowledge Distillation for BERT Models", "authors": ["Victor Sanh", "Lysandre Debut"], "year": 2019}, {"title": "DistilBERT: A Smaller, Faster, Cheaper Version of BERT", "authors": ["Victor Sanh", "Lysandre Debut"], "year": 2019}, {"title": "Quantization and Pruning for Efficient Inference", "authors": ["Song Han", "Jeff Pool"], "year": 2015}, {"title": "Sparse Transformers for Language Modeling", "authors": ["Rewon Child", "Scott Gray"], "year": 2019}, {"title": "LayerDrop: Structured Dropout for Transformer Models", "authors": ["Fan Yin", "Angela Fan"], "year": 2020}, {"title": "Reformer: The Efficient Transformer", "authors": ["Nikolaus P. Joulin", "Aidan N. Gomez"], "year": 2020}, {"title": "Switch Transformers: Scaling to Trillion Parameter Models", "authors": ["William Fedus", "Barret Zoph"], "year": 2021}, {"title": "Megatron-LM: Training Multi-Billion Parameter Language Models", "authors": ["Mohammad Shoeybi", "Mostofa Patwary"], "year": 2019}, {"title": "ZeRO: Memory Optimization for Large Models", "authors": ["Samyam Rajbhandari", "Jeff Rasley"], "year": 2020}, {"title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference", "authors": ["Haojun Huang", "Hao Zhang"], "year": 2020}, {"title": "T5: Text-To-Text Transfer Transformer", "authors": ["Colin Raffel", "Noam Shazeer"], "year": 2020}]