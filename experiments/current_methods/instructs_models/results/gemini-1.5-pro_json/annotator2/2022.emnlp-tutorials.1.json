[{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"title": "GloVe: Global Vectors for Word Representation", "year": 2014}, {"title": "Word2Vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method", "year": 2016}, {"title": "ELMo Embeddings in Practice: Training and Evaluation", "year": 2018}, {"title": "Universal Sentence Encoder", "year": 2018}, {"title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "year": 2019}, {"title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"title": "Language Models are Few-Shot Learners", "year": 2020}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}, {"title": "T5: Text-To-Text Transfer Transformer", "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "year": 2019}, {"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "year": 2020}, {"title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "year": 2019}, {"title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "year": 2019}, {"title": "TinyBERT: Distilling BERT for Natural Language Understanding", "year": 2019}, {"title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing", "year": 2018}, {"title": "BPEmb: Tokenization-free Pre-training for Neural Machine Translation", "year": 2018}, {"title": "FastText.zip: Compressing text classification models", "year": 2016}]
