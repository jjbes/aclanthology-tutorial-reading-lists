[{"title": "Finetuned Language Models Are Zero-Shot Learners", "year": 2021}, {"title": "Language Models are Few-Shot Learners", "year": 2020}, {"title": "Training Language Models to Follow Instructions with Human Feedback", "year": 2022}, {"title": "Learning to Follow Instructions in Text", "year": 2017}, {"title": "Self-Instruct: Aligning Language Model with Self Generated Instructions", "year": 2022}, {"title": "FLAN: Fine-tuned Language Models are Zero-Shot Learners", "year": 2021}, {"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2020}, {"title": "Prompt Programming for Large Language Models: A Survey", "year": 2023}, {"title": "Large Language Models are Human-Level Prompt Engineers", "year": 2022}, {"title": "SupernaturalInstructions: Generalization Tests for Instruction Following with Language Models", "year": 2022}, {"title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models", "year": 2022}, {"title": "Benchmarking Instruction Following for Large Language Models", "year": 2022}, {"title": "Towards Unified Prompting for Text Generation", "year": 2022}, {"title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "year": 2021}, {"title": "Prompt Engineering for Text Summarization: An Empirical Study of Large Language Models", "year": 2023}, {"title": "Can Large Language Models Be Instructed to Follow Instructions?", "year": 2022}, {"title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions", "year": 2021}, {"title": "Learning to Generate Task-Specific Instructions", "year": 2022}, {"title": "Measuring and Improving Instruction Following for End-to-End Dialogue Systems", "year": 2020}, {"title": "Task-Oriented Dialogue as Data Augmentation for Language Modeling", "year": 2020}]
