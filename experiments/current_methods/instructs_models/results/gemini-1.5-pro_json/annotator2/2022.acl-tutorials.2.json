[{"title": "On the Reproducibility of Neural Machine Translation", "year": 2022}, {"title": "Reproducibility in NLP: A Survey", "year": 2021}, {"title": "Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches", "year": 2019}, {"title": "You Can't Always Get What You Want: Analyzing the Variance of BERT", "year": 2021}, {"title": "Rethinking Generalization Requires Revisiting Old Ideas: Statistical Significance Testing Helps", "year": 2018}, {"title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines", "year": 2021}, {"title": "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks", "year": 2019}, {"title": "What's in a Name? Reducing Bias in Bios Without Access to Protected Attributes", "year": 2020}, {"title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data", "year": 2020}, {"title": "The Measure of All Things: Evaluating Language Models", "year": 2022}, {"title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?", "year": 2021}, {"title": "Data Augmentation for Low-Resource Text Classification: A Survey", "year": 2021}, {"title": "ALBEF: Aligning Text and Images Though Contrastive Learning of Text-Derived Visual Regions", "year": 2021}, {"title": "Emerging Trends in Natural Language Processing: A Review", "year": 2022}, {"title": "Reproducibility, Replicability and Robustness in Deep Learning", "year": 2020}, {"title": "Hyperparameter Optimization in Machine Learning: A Survey", "year": 2021}, {"title": "The Importance of Being Unbiased: Investigating Fairness in Pre-trained Language Models", "year": 2021}, {"title": "On the Robustness of Language Models to Input Perturbations", "year": 2020}, {"title": "A Survey on Evaluation Metrics for Image Captioning", "year": 2019}, {"title": "BERT Rediscovers the Classical NLP Pipeline", "year": 2021}]
