[{"title": "Exploring Vulnerabilities in Machine Learning", "year": 2014}, {"title": "Adversarial Examples in the Physical World", "year": 2016}, {"title": "Explaining and Harnessing Adversarial Examples", "year": 2015}, {"title": "Towards Evaluating the Robustness of Neural Networks", "year": 2017}, {"title": "Threat of Adversarial Attacks on Deep Learning Systems in Natural Language Processing: A Survey", "year": 2019}, {"title": "On the Security of Machine Learning in Natural Language Processing", "year": 2019}, {"title": "Adversarial Attacks against Text Classifiers", "year": 2017}, {"title": "Generating Natural Language Adversarial Examples", "year": 2018}, {"title": "Certified Robustness to Adversarial Word Substitutions", "year": 2019}, {"title": "On the Robustness of Text Classification with LSTM Networks", "year": 2016}, {"title": "Adversarial Training Methods for Semi-Supervised Text Classification", "year": 2019}, {"title": "Defensive Distillation is Not Robust to Adversarial Examples", "year": 2016}, {"title": "Towards Robust Neural Networks for NLP: A Survey", "year": 2020}, {"title": "Adversarial Attacks and Defenses in Natural Language Processing", "year": 2021}, {"title": "Black-Box Adversarial Attacks on Commercial Speech Recognition Systems", "year": 2020}, {"title": "Robustness to Backdoor Attacks in Natural Language Processing", "year": 2021}, {"title": "Privacy and Security in Natural Language Processing", "year": 2022}, {"title": "Fairness and Bias in Natural Language Processing", "year": 2023}, {"title": "Ethical Considerations in Natural Language Processing", "year": 2023}, {"title": "Emerging Security Challenges in Large Language Models", "year": 2023}]
