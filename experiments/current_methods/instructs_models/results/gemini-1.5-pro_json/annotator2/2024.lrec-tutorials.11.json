[{"title": "How to Detect LLM Hallucination", "year": 2023}, {"title": "Surveying Hallucination in Large Language Models", "year": 2023}, {"title": "Improving Factuality and Reasoning in Large Language Models", "year": 2023}, {"title": "Hallucination in Large Language Models: A Survey", "year": 2023}, {"title": "Detecting Hallucinations in Neural Machine Translation", "year": 2022}, {"title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?", "year": 2021}, {"title": "Evaluating the Factual Consistency of Abstractive Text Summarization", "year": 2021}, {"title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods", "year": 2021}, {"title": "Mitigating Neural Response Generation Bias with Diversity-Promoting Training Objectives", "year": 2020}, {"title": "Fact-Checking with External Sources for Neural Question Answering", "year": 2020}, {"title": "Neural Fact Verification: Challenges and Perspectives", "year": 2019}, {"title": "Defense Against Adversarial Attacks on Question Answering Systems", "year": 2019}, {"title": "Evaluating the Robustness of Neural Question Answering Systems", "year": 2018}, {"title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "year": 2017}, {"title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories", "year": 2016}, {"title": "Teaching Machines to Read and Comprehend", "year": 2015}, {"title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "year": 2015}, {"title": "Reasoning about Entailment with Neural Attention", "year": 2015}, {"title": "Large Language Models are Few-Shot Learners", "year": 2020}, {"title": "Language Models are Unsupervised Multitask Learners", "year": 2019}]
