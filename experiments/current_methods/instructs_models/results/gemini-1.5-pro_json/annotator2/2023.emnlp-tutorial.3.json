[{"title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList", "year": 2020}, {"title": "Quantifying Qualitative Open-Ended Probing of Language Models", "year": 2021}, {"title": "Dynabench: Rethinking Benchmarking in NLP", "year": 2021}, {"title": "User Evaluation of Language Model Dialogue Systems", "year": 2022}, {"title": "Measuring Conversational Quality: A Systematic Review of NLP Evaluation Metrics", "year": 2022}, {"title": "Towards a Holistic Evaluation of Language Models", "year": 2023}, {"title": "Evaluating the User Experience of NLP Systems: A Case Study on Text Summarization", "year": 2022}, {"title": "Human Evaluation of Text Generation: A Survey", "year": 2021}, {"title": "Beyond Task Accuracy: Evaluating Situated Language Agents", "year": 2021}, {"title": "Measuring the Reliability of Human Evaluation in NLP", "year": 2022}, {"title": "Crowdsourcing NLP Annotations: A Survey", "year": 2020}, {"title": "Evaluating Dialogue Systems: A Review of Methods and Metrics", "year": 2020}, {"title": "Towards Faithful Evaluation of Open-Domain Dialogue Systems", "year": 2021}, {"title": "Evaluating NLP Models for Accessibility", "year": 2022}, {"title": "Measuring Social Biases in NLP Models", "year": 2021}, {"title": "Evaluating the Factual Consistency of Text Generation", "year": 2022}, {"title": "Towards Robust Evaluation of NLP Systems", "year": 2023}, {"title": "Human-in-the-Loop NLP: A Survey", "year": 2021}, {"title": "Evaluating Explainability in NLP", "year": 2022}, {"title": "Towards User-Centric Evaluation of NLP Systems", "year": 2023}]
