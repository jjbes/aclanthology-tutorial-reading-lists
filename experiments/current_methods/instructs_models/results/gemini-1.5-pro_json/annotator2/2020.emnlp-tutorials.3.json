[{"title": "Attention is not Explanation", "year": 2019}, {"title": "Axiomatic Attribution for Deep Networks", "year": 2017}, {"title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList", "year": 2020}, {"title": "Black Box NLP: The Need for Interpretability", "year": 2018}, {"title": "Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens", "year": 2020}, {"title": "Examining CNN Representations with respect to Dataset Bias", "year": 2019}, {"title": "Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions", "year": 2020}, {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "year": 2016}, {"title": "How Can We Know What Language Models Know?", "year": 2020}, {"title": "Input Reduction Methods for Debugging and Interpreting Text Classifiers", "year": 2019}, {"title": "Interpretability and Analysis in Neural NLP", "year": 2019}, {"title": "Learning to Generate Reviews and Discovering Sentiment", "year": 2015}, {"title": "Lime: Why Should I Trust You?", "year": 2016}, {"title": "Manipulating and Measuring Model Interpretability", "year": 2019}, {"title": "On the Real Dangers of Interpretability", "year": 2019}, {"title": "Probing Neural Network Comprehension of Natural Language Arguments", "year": 2019}, {"title": "Quantifying Attention Flow in Transformers", "year": 2019}, {"title": "Towards a Deep and Unified Understanding of Deep Neural Models in NLP", "year": 2018}, {"title": "Visualizing and Understanding Neural Models in NLP", "year": 2015}, {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "year": 2019}]
