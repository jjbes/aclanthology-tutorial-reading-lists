[{"title": "Adaptive Span Extraction for Long Documents with Limited Memory", "year": 2021}, {"title": "Efficient Content-Based Sparse Attention with Routing Transformers", "year": 2020}, {"title": "Longformer: The Long-Document Transformer", "year": 2020}, {"title": "Reformer: The Efficient Transformer", "year": 2020}, {"title": "Big Bird: Transformers for Longer Sequences", "year": 2020}, {"title": "Cluster-Former: Clustering-based Sparse Transformer for Long Text Summarization", "year": 2021}, {"title": "Text Summarization with Pretrained Encoders", "year": 2019}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}, {"title": "Hierarchical Transformers for Long Document Processing", "year": 2019}, {"title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "year": 2019}, {"title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "year": 2019}, {"title": "Graph Convolutional Networks for Text Classification", "year": 2019}, {"title": "Neural Speed Reading via Skimming", "year": 2017}, {"title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents", "year": 2018}, {"title": "Get To The Point: Summarization with Pointer-Generator Networks", "year": 2017}, {"title": "Extractive Summarization Using Deep Learning", "year": 2017}, {"title": "Teaching Machines to Read and Comprehend", "year": 2015}, {"title": "Recursive Deep Models for Discourse Parsing", "year": 2014}]
