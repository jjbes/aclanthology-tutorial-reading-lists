[{"title": "BERT Rediscovers the Classical NLP Pipeline", "year": 2019}, {"title": "Are Sixteen Heads Really Better than One?", "year": 2019}, {"title": "What Does BERT Look At? An Analysis of BERT's Attention", "year": 2019}, {"title": "Visualizing and Measuring the Geometry of BERT", "year": 2019}, {"title": "Open Sesame: Getting Inside BERT's Linguistic Knowledge", "year": 2020}, {"title": "Unsupervised Analysis of Syntactic Structures Learned by Transformers", "year": 2020}, {"title": "Quantifying Attention Flow in Transformers", "year": 2020}, {"title": "ExBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models", "year": 2020}, {"title": "Tree-Structured Attention with Hierarchical Accumulation for Source Code Representation Learning", "year": 2020}, {"title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList", "year": 2020}, {"title": "The Right Tool for the Job: Matching Model Explanation Techniques to User Needs", "year": 2021}, {"title": "Position Information in Transformers: An Analysis of Layer-Wise Probes and Decoding Performance", "year": 2021}, {"title": "Transformer Interpretability Beyond Attention Visualization", "year": 2021}, {"title": "Revealing the Dark Secrets of BERT", "year": 2021}, {"title": "Analyzing the Role of Context in Transformer Language Models", "year": 2022}, {"title": "Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?", "year": 2022}, {"title": "Understanding the Limits of Explainable AI for Text Classification", "year": 2023}, {"title": "On the Importance of Data Size for Interpretability in NLP", "year": 2023}, {"title": "Interpretability Methods for Graph Neural Networks: A Comprehensive Survey", "year": 2023}, {"title": "Explainable AI for Natural Language Processing: A Survey", "year": 2023}]
