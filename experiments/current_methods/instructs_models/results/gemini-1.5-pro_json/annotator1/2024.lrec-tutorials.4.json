[{"title": "Benchmarking Large Language Models for Code", "year": 2021}, {"title": "Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models", "year": 2021}, {"title": "EleutherAI: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?", "year": 2021}, {"title": "Measuring Massive Multitask Language Understanding", "year": 2020}, {"title": "OpenAI's GPT-3: A Comprehensive Evaluation of Capabilities and Limitations", "year": 2020}, {"title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "year": 2019}, {"title": "The GEM Benchmark: Natural Language Generation, Evaluation, and Metrics", "year": 2021}, {"title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization", "year": 2020}, {"title": "BIG-bench: Towards an Industry Standard Benchmark for General-purpose Large Language Models", "year": 2022}, {"title": "Evaluating Large Language Models Trained on Code", "year": 2021}, {"title": "Towards Faithful Evaluation of Text Generation", "year": 2020}, {"title": "Dynabench: An Evaluation Platform for Dynamic Benchmarking", "year": 2021}, {"title": "HELM: Holistic Evaluation of Language Models", "year": 2022}, {"title": "Language Model Evaluation Harness", "year": 2022}, {"title": "PromptBench: Towards Evaluating the Robustness of Prompt-based Text Generation", "year": 2022}, {"title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList", "year": 2020}, {"title": "Factual Language Evaluation", "year": 2021}, {"title": "Measuring the Factual Accuracy of Generated Text", "year": 2021}, {"title": "Evaluating and Minimizing Bias in Language Models", "year": 2021}, {"title": "On the Dangers of Evaluating Artificial Intelligence", "year": 2021}]
