[{"authors": ["Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Minh-Thang Luong", "Quoc V. Le"], "title": "Unsupervised Data Augmentation for Consistency Training", "year": 2020}, {"authors": ["Jason Wei", "Kai Zou"], "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks", "year": 2019}, {"authors": ["Steven Y. Feng", "Varun Gangal", "Jason Wei", "Sarath Chandar", "Soroush Vosoughi", "Hari Subramoni"], "title": "A Survey of Data Augmentation Approaches for NLP", "year": 2021}, {"authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "title": "Character-level Convolutional Networks for Text Classification", "year": 2015}, {"authors": ["Jeremy Howard", "Sebastian Ruder"], "title": "Universal Language Model Fine-tuning for Text Classification", "year": 2018}, {"authors": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer"], "title": "Deep Contextualized Word Representations", "year": 2018}, {"authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019}, {"authors": ["Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever"], "title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"authors": ["Andrew M. Dai", "Quoc V. Le"], "title": "Semi-supervised Sequence Learning", "year": 2015}, {"authors": ["Thorsten Joachims"], "title": "Transductive Inference for Text Classification using Support Vector Machines", "year": 1999}, {"authors": ["Xiaojin Zhu"], "title": "Semi-Supervised Learning Literature Survey", "year": 2005}, {"authors": ["Yutai Hou", "Wanxiang Che", "Ting Liu", "Bing Qin", "Tyler Derr"], "title": "Improving BERT Fine-tuning via Self-Ensemble and Self-Distillation", "year": 2020}, {"authors": ["Suchin Gururangan", "Ana MarasoviÄ‡", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2020}, {"authors": ["Tianyu Gao", "Adam Fisch", "Danqi Chen"], "title": "Making Pre-trained Language Models Better Few-shot Learners", "year": 2021}, {"authors": ["Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin de Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly"], "title": "Parameter-Efficient Transfer Learning for NLP", "year": 2019}, {"authors": ["Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf"], "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "year": 2020}, {"authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"authors": ["Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer"], "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2020}, {"authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2020}]
