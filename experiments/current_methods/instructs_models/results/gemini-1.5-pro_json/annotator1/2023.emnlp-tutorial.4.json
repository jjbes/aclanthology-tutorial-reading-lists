[{"title": "Large Language Models Can Be Easily Distracted by Irrelevant Instruction Words", "year": 2023}, {"title": "Instruction Tuning with GPT-3", "year": 2023}, {"title": "Learning to Follow Instructions with Adversarial Reward Induction", "year": 2022}, {"title": "Measuring and Improving Instruction Following for End-to-End Spoken Dialogue Systems", "year": 2022}, {"title": "FLAN: Exploring Transfer Learning with Unified Text-to-Text Transformers", "year": 2022}, {"title": "SupernaturalInstructions: Comprehensive Instruction Benchmark for Large Language Models", "year": 2022}, {"title": "Cross-Task Generalization via Multi-Task Finetuning in BERT", "year": 2021}, {"title": "Finetuned Language Models Are Zero-Shot Learners", "year": 2021}, {"title": "Evaluating Large Language Models Trained on Code", "year": 2021}, {"title": "Language Models are Few-Shot Learners", "year": 2020}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"title": "GPT-3: Language Models are Few-Shot Learners", "year": 2020}, {"title": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "year": 2019}, {"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "year": 2020}, {"title": "Longformer: Extending Transformers to Long Documents", "year": 2020}, {"title": "Reformulator: A Grounded Vision-and-Language Navigation Agent that Reasons with Language", "year": 2020}]
