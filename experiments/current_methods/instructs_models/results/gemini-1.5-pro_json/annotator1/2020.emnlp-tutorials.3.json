[{"title": "The Mythos of Model Interpretability", "year": 2016}, {"title": "Axiomatic Attribution for Deep Networks", "year": 2017}, {"title": "Local Interpretable Model-Agnostic Explanations (LIME): An Introduction", "year": 2016}, {"title": "SHAP (SHapley Additive exPlanations): A Unified Approach to Interpreting Model Predictions", "year": 2017}, {"title": "Visualizing and Understanding Convolutional Networks", "year": 2014}, {"title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "year": 2014}, {"title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization", "year": 2017}, {"title": "SmoothGrad: removing noise by adding noise", "year": 2017}, {"title": "Understanding Black-box Predictions via Influence Functions", "year": 2017}, {"title": "Distilling a Neural Network Into a Soft Decision Tree", "year": 2017}, {"title": "Beyond Sparsity: Tree Regularization of Deep Models for Interpretability", "year": 2018}, {"title": "Learning Important Features Through Propagating Activation Differences", "year": 2017}, {"title": "Interpretation of Neural Networks is Fragile", "year": 2018}, {"title": "Sanity Checks for Saliency Maps", "year": 2018}, {"title": "Explaining and Harnessing Adversarial Examples", "year": 2015}, {"title": "On the Interpretation of Deep Neural Networks for Tasks Involving Time Series", "year": 2019}, {"title": "Attention is All You Need", "year": 2017}, {"title": "Quantifying Attention Flow in Transformers", "year": 2019}, {"title": "A Survey on Neural Network Interpretability", "year": 2020}, {"title": "Towards A Rigorous Science of Interpretable Machine Learning", "year": 2017}]
