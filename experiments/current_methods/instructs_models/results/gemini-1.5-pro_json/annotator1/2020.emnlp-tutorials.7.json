[{"title": "Transfer Learning for Text Generation: A Survey", "year": 2020}, {"title": "Contextualized Word Embeddings for Text Generation", "year": 2019}, {"title": "Deep Contextual Representation Learning for Neural Language Generation", "year": 2018}, {"title": "Transfer Learning in Natural Language Processing", "year": 2019}, {"title": "A Survey on Contextual Embeddings", "year": 2019}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"title": "GPT-2: Language Models are Unsupervised Multitask Learners", "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"title": "T5: Text-to-Text Transfer Transformer", "year": 2019}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}, {"title": "CTRL: A Conditional Transformer Language Model for Controllable Generation", "year": 2019}, {"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"title": "How to Fine-Tune BERT for Text Generation?", "year": 2019}, {"title": "Leveraging Pre-trained Language Models for Neural Text Generation", "year": 2020}, {"title": "Fine-tuning Pretrained Language Models for Text Generation: A Case Study on Abstractive Summarization", "year": 2020}, {"title": "Contextual Embeddings: When Are They Worth It?", "year": 2019}, {"title": "Improving Neural Language Generation with Pre-trained Language Models", "year": 2019}, {"title": "A Comprehensive Survey on Transfer Learning for Natural Language Processing", "year": 2020}, {"title": "Deep Contextualized Word Representations", "year": 2018}]
