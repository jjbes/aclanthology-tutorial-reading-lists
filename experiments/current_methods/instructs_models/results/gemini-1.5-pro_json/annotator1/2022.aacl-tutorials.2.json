[{"authors": ["Tommaso Pasini", "Alessandro Lazaric", "Denis Krompaß"], "title": "Learning to Prompt for Vision-Language Models", "year": 2021}, {"authors": ["Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin de Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly"], "year": 2019}, {"authors": ["Tianyu Gao", "Adam Fisch", "Danqi Chen"], "title": "Making Pre-trained Language Models Better Few-shot Learners", "year": 2021}, {"authors": ["Timo Schick", "Hinrich Schütze"], "title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "year": 2021}, {"authors": ["Victor Sanh", "Albert Webson", "Colin Raffel", "Stephen H. Bach", "Lintang Sutawika", "Zaid Alyafeai", "Antoine Chaffin", "Arnaud Stiegler", "Teven Le Scao", "Arun Raja", "Manan Dey", "M Saiful Bari", "Pawan Sasanka", "Sumon Kumar Bose", "Ram Loomba", "Abhishek Thakur", "Philippe Cudré-Mauroux", "Yacine Jernite", "Julien Launay", "Stella DeLaughter", "Edward Beeching"], "title": "Multi-Task Deep Neural Networks for Natural Language Understanding", "year": 2019}, {"authors": ["Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2020}, {"authors": ["Weijie Liu", "Peng Zhou", "Zhe Zhao", "Zhiruo Wang", "Qi Ju", "Haotang Deng", "Ping Wang"], "title": "Pre-trained Language Model for Text Generation: A Survey", "year": 2021}, {"authors": ["Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych"], "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning", "year": 2021}, {"authors": ["Jiaxin Huang", "Yuxian Gu", "Graham Neubig"], "title": "Improving Low-Resource Cross-lingual Transfer via Multilingual Template Generation", "year": 2021}, {"authors": ["Xiang Lisa Li", "Kaitao Song", "Wenhao Li", "Honglak Lee", "Dinesh Manocha"], "title": "VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks", "year": 2022}, {"authors": ["Yujia Qin", "Yankai Lin", "Ryuichi Takanobu", "Zhiyuan Liu"], "title": "Learning and Evaluating Contextual Embedding of Source Code", "year": 2019}, {"authors": ["Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer"], "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2020}, {"authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2020}, {"authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019}, {"authors": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer"], "title": "Deep Contextualized Word Representations", "year": 2018}, {"authors": ["Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever"], "title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "title": "GloVe: Global Vectors for Word Representation", "year": 2014}, {"authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "title": "Distributed Representations of Words and Phrases and their Compositionality", "year": 2013}, {"authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "title": "Efficient Estimation of Word Representations in Vector Space", "year": 2013}]
