[{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"title": "GPT-3: Language Models Are Few-Shot Learners", "year": 2020}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}, {"title": "T5: Text-To-Text Transfer Transformer", "year": 2019}, {"title": "ELECTRA: Pre-training Electrostatic Potential Maps for Molecular and Protein Structure Prediction", "year": 2020}, {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "year": 2019}, {"title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "year": 2019}, {"title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "year": 2019}, {"title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "year": 2019}, {"title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "year": 2019}, {"title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training", "year": 2020}, {"title": "Longformer: The Long-Document Transformer", "year": 2020}, {"title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "year": 2020}, {"title": "Big Bird: Transformers for Longer Sequences", "year": 2020}, {"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2020}, {"title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "year": 2019}, {"title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "year": 2021}, {"title": "Efficient Large Scale Language Modeling with Mixture-of-Experts", "year": 2022}]
