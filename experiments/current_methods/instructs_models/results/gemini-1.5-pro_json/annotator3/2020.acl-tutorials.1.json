[{"title": "Visualizing and Understanding Recurrent Networks", "year": 2015}, {"title": "Attention Is All You Need", "year": 2017}, {"title": "Getting Inside the Black Box: Understanding Neural Networks Through Information Theory", "year": 2017}, {"title": "Lime: Why Should I Trust You?", "year": 2016}, {"title": "Axiomatic Attribution for Deep Networks", "year": 2017}, {"title": "Learning to Generate Reviews and Discovering Sentiment", "year": 2017}, {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "year": 2016}, {"title": "A Unified Approach to Interpreting Model Predictions", "year": 2017}, {"title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs for Text Classification", "year": 2018}, {"title": "Interpretation of Neural Networks is Fragile", "year": 2018}, {"title": "Semantically Equivalent Adversarial Rules for Debugging NLP Models", "year": 2018}, {"title": "Fine-Grained Sentence Functions for Short-Text Classification", "year": 2016}, {"title": "Blackbox NLP: Accessing and Understanding BERT's Internal Representations", "year": 2019}, {"title": "Are Sixteen Heads Really Better than One?", "year": 2019}, {"title": "What Does BERT Look At? An Analysis of BERT's Attention", "year": 2019}, {"title": "Quantifying Attention Flow in Transformers", "year": 2019}, {"title": "Visualizing the Geometry of BERT", "year": 2019}, {"title": "Is BERT Really Robust? A Comprehensive Study on the Impact of Data Augmentation on BERT's Robustness", "year": 2019}, {"title": "Exposing and Harnessing Spatial Bias in Word Embeddings", "year": 2016}, {"title": "On the Dangers of Gender Bias in Word Embeddings", "year": 2016}]
