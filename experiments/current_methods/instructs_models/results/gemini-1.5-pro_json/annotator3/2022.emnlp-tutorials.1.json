[{"title": "BERT rediscovers the classical NLP pipeline", "year": 2019}, {"title": "Language Models as Knowledge Bases?", "year": 2019}, {"title": "How Context Affects Word Embeddings: A Similarity-based Distributional Perspective", "year": 2019}, {"title": "Probing Neural Network Comprehension of Natural Language Arguments", "year": 2019}, {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "year": 2019}, {"title": "Designing and Interpreting Probes with Control Tasks", "year": 2020}, {"title": "Inducing Relational Knowledge from BERT", "year": 2020}, {"title": "SenseBERT: Driving Some Sense into BERT", "year": 2020}, {"title": "Evaluating BERT for Propositional Reasoning: Implications for Downstream NLP Tasks", "year": 2020}, {"title": "Measuring Compositional Generalization: A Comprehensive Method on Realistic Data", "year": 2021}, {"title": "Measuring Massive Multitask Language Understanding", "year": 2020}, {"title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList", "year": 2020}, {"title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data", "year": 2020}, {"title": "Analyzing the Effects of Data Augmentation and Scale on BERT", "year": 2020}, {"title": "Emerging Trends in Natural Language Processing", "year": 2020}, {"title": "What Can We Learn from Collective Human Opinions on Natural Language Inference Data?", "year": 2020}, {"title": "With Little Help from My Friends: Few-Shot Learning with Retrieval Augmented Language Model", "year": 2020}, {"title": "Language Models are Few-Shot Learners", "year": 2020}, {"title": "The Right Tool for the Job: Matching Model and Instance Complexities", "year": 2021}, {"title": "Measuring and Improving Consistency in Pretrained Language Models", "year": 2020}]
