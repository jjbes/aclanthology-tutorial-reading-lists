[{"title": "Reproducibility in NLP: A Survey", "year": 2022}, {"title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?", "year": 2021}, {"title": "Will We Ever Stop Training? An Analysis of Replicability Properties of BERT", "year": 2021}, {"title": "Are NLP Models Really As Good As They Seem? On the Robustness of Neural Transfer Learning for NLP", "year": 2020}, {"title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data", "year": 2020}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019}, {"title": "Reproducibility in Empirical NLP: A Survey", "year": 2019}, {"title": "On the State of the Art of Evaluation in Neural Language Models", "year": 2019}, {"title": "Show Your Work: Improved Reporting of Experimental Results", "year": 2018}, {"title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond", "year": 2018}, {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "year": 2018}, {"title": "Deep contextualized word representations", "year": 2018}, {"title": "Attention Is All You Need", "year": 2017}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "year": 2015}, {"title": "GloVe: Global Vectors for Word Representation", "year": 2014}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "year": 2013}, {"title": "Natural Language Processing (Almost) from Scratch", "year": 2011}, {"title": "A Fast and Simple Algorithm for Training Neural Probabilistic Language Models", "year": 2011}, {"title": "Statistical Machine Translation", "year": 2010}]
