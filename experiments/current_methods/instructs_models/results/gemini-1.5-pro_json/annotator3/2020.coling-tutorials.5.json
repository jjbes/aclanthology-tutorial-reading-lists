[{"title": "The Stanford Question Answering Dataset", "year": 2016}, {"title": "A Large-Scale Dataset for Multiple-Choice Natural Language Inference", "year": 2015}, {"title": "The Winograd Schema Challenge", "year": 2012}, {"title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "year": 2017}, {"title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "year": 2019}, {"title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "year": 2019}, {"title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering", "year": 2018}, {"title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs", "year": 2019}, {"title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge", "year": 2019}, {"title": "SocialIQA: Commonsense Reasoning about Social Interactions", "year": 2019}, {"title": "HellaSwag: Can a Machine Really Finish Your Sentence?", "year": 2019}, {"title": "PIQA: Physical Interaction: Reasoning about Actions, Objects, and Physics", "year": 2020}, {"title": "WinoGrande: An Adversarial Winograd Schema Challenge at Scale", "year": 2019}, {"title": "QuAC: Question Answering in Context", "year": 2018}, {"title": "MultiRC: A Multi-Sentence Reasoning Challenge", "year": 2018}, {"title": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension", "year": 2018}, {"title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference", "year": 2018}, {"title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "year": 2016}, {"title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations", "year": 2018}, {"title": "NarrativeQA: A New Dataset for Narrative Comprehension", "year": 2018}]
