[{"title": "Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models", "year": 2021}, {"title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?", "year": 2021}, {"title": "Measuring Massive Multitask Language Understanding", "year": 2020}, {"title": "Evaluating NLP Models via Contrast Sets", "year": 2020}, {"title": "BERT Rediscovers the Classical NLP Pipeline", "year": 2019}, {"title": "The Winograd Schema Challenge", "year": 2016}, {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "year": 2018}, {"title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "year": 2019}, {"title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization", "year": 2020}, {"title": "Dynabench: Rethinking Benchmarking in NLP", "year": 2020}, {"title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList", "year": 2020}, {"title": "Towards Faithful and Robust Human Evaluation of Open-domain Dialogue Systems", "year": 2020}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "year": 2002}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "year": 2004}, {"title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "year": 2005}, {"title": "CIDEr: Consensus-based Image Description Evaluation", "year": 2015}, {"title": "SPICE: Semantic Propositional Image Caption Evaluation", "year": 2016}, {"title": "Evaluating Text Generation: A Survey", "year": 2020}, {"title": "Ethical Considerations in Evaluating Language Models", "year": 2021}, {"title": "On the Limits of Evaluating Artificial Intelligence", "year": 2021}]
