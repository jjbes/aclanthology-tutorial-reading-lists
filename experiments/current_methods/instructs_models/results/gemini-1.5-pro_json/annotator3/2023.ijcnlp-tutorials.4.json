[{"title": "Scaling Laws for Reward Model Overoptimization", "year": 2022}, {"title": "Discovering Language Model Behaviors with Model-Written Evaluations", "year": 2022}, {"title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned", "year": 2022}, {"title": "Constitutional AI: Harmlessness from AI Alignment", "year": 2022}, {"title": "Training Language Models to Follow Instructions with Human Feedback", "year": 2022}, {"title": "Learning to Summarize from Human Feedback", "year": 2022}, {"title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm", "year": 2021}, {"title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}, {"title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "year": 2021}, {"title": "Making Pre-trained Language Models Better Few-shot Learners", "year": 2021}, {"title": "Improving Language Model Behavior by Training on a Curated Dataset of Human Feedback", "year": 2020}, {"title": "REALM: Retrieval-Augmented Language Model Pre-Training", "year": 2020}, {"title": "Towards Socially Acceptable Language Generation: Methods and Evaluations", "year": 2023}, {"title": "MEGATRON-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "year": 2019}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"title": "GPT-3: Language Models are Few-Shot Learners", "year": 2020}, {"title": "T5: Text-to-Text Transfer Transformer", "year": 2019}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}]
