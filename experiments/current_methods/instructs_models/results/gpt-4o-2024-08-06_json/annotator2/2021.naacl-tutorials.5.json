[{"title": "Attention is All You Need", "authors": ["Vaswani, A.", "Shazeer, N.", "Parmar, N.", "Uszkoreit, J.", "Jones, L.", "Gomez, A. N.", "Kaiser, \u0141.", "Polosukhin, I."], "year": 2017}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Devlin, J.", "Chang, M. W.", "Lee, K.", "Toutanova, K."], "year": 2019}, {"title": "Longformer: The Long-Document Transformer", "authors": ["Beltagy, I.", "Peters, M. E.", "Cohan, A."], "year": 2020}, {"title": "Reformer: The Efficient Transformer", "authors": ["Kitaev, N.", "Kaiser, \u0141.", "Levskaya, A."], "year": 2020}, {"title": "Efficient Transformers: A Survey", "authors": ["Tay, Y.", "Dehghani, M.", "Bahri, D.", "Metzler, D."], "year": 2020}, {"title": "Hierarchical Attention Networks for Document Classification", "authors": ["Yang, Z.", "Yang, D.", "Dyer, C.", "He, X.", "Smola, A.", "Hovy, E."], "year": 2016}, {"title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "authors": ["Dai, Z.", "Yang, Z.", "Yang, Y.", "Carbonell, J.", "Le, Q. V.", "Salakhutdinov, R."], "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": ["Yang, Z.", "Dai, Z.", "Yang, Y.", "Carbonell, J.", "Salakhutdinov, R.", "Le, Q. V."], "year": 2019}, {"title": "Big Bird: Transformers for Longer Sequences", "authors": ["Zaheer, M.", "Guruganesh, G.", "Dubey, K. A.", "Ainslie, J.", "Alberti, C.", "Ontanon, S.", "Pham, P.", "Ravula, A.", "Srinivasan, K.", "Wang, L.", "Yang, A."], "year": 2020}, {"title": "Sparse Transformers for Long Sequence Modeling", "authors": ["Child, R.", "Gray, S.", "Radford, A.", "Sutskever, I."], "year": 2019}, {"title": "Long-Short Transformer: Efficient Transformers for Language and Vision", "authors": ["Zhu, L.", "Li, J.", "Liu, Y.", "Zhu, C.", "Cui, L.", "Zhang, T.", "Zhang, D.", "Wang, Y.", "Zhang, B.", "Huang, F."], "year": 2021}, {"title": "Linformer: Self-Attention with Linear Complexity", "authors": ["Wang, S.", "Zhang, B.", "Shou, L.", "Jin, H.", "Zhang, H.", "Zhou, M."], "year": 2020}, {"title": "Routing Transformer: Learning to Route Queries in Transformers", "authors": ["Roy, A.", "Saffar, M.", "Vaswani, A.", "Grangier, D."], "year": 2020}, {"title": "Synthesizer: Rethinking Self-Attention in Transformer Models", "authors": ["Tay, Y.", "Bahri, D.", "Metzler, D.", "Juan, D. C.", "Zhao, Z.", "Anil, R."], "year": 2021}, {"title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Rae, J. W.", "Potapenko, A.", "Jayakumar, S. M.", "Lillicrap, T. P."], "year": 2020}, {"title": "Long Range Arena: A Benchmark for Efficient Transformers", "authors": ["Tay, Y.", "Dehghani, M.", "Abnar, S.", "Shen, Y.", "Bahri, D.", "Pham, P.", "Rae, J.", "Metzler, D."], "year": 2020}, {"title": "Linear Transformers Are Secretly Fast Weight Programmers", "authors": ["Schlag, I.", "Irie, K.", "Schmidhuber, J."], "year": 2021}, {"title": "Set Transformers: A Framework for Attention-based Permutation-Invariant Neural Networks", "authors": ["Lee, J.", "Lee, Y.", "Kim, J.", "Kosiorek, A. R.", "Choi, S.", "Teh, Y. W."], "year": 2019}, {"title": "Memory-Augmented Neural Networks for Machine Reading with Long Contexts", "authors": ["Sukhbaatar, S.", "Weston, J.", "Fergus, R."], "year": 2015}, {"title": "Adaptive Attention Span in Transformers", "authors": ["Sukhbaatar, S.", "Fan, A.", "Grave, E.", "Joulin, A."], "year": 2019}]