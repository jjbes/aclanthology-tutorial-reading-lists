[{"title": "Attention is not Explanation", "authors": ["Sarthak Jain", "Byron C. Wallace"], "year": 2019}, {"title": "A Survey of Methods for Explaining Black Box Models", "authors": ["Riccardo Guidotti", "Anna Monreale", "Salvatore Ruggieri", "Franco Turini", "Fosca Giannotti", "Dino Pedreschi"], "year": 2018}, {"title": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable", "authors": ["Christoph Molnar"], "year": 2019}, {"title": "Explaining Black Box Models and Their Predictions: A Survey of Local Interpretation Methods", "authors": ["P. Hall", "N. Gill", "B. Schmidt"], "year": 2020}, {"title": "Towards a Rigorous Science of Interpretable Machine Learning", "authors": ["Finale Doshi-Velez", "Been Kim"], "year": 2017}, {"title": "Evaluating the Interpretability of Generative Models by Interactive Reconstruction", "authors": ["David Bau", "Hendrik Strobelt", "Jonas Wulff", "Bolei Zhou", "Jun-Yan Zhu", "Antonio Torralba"], "year": 2019}, {"title": "The Building Blocks of Interpretability", "authors": ["Ruth C. Fong", "Andrea Vedaldi"], "year": 2018}, {"title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)", "authors": ["Been Kim", "Martin Wattenberg", "Justin Gilmer", "Carrie Cai", "James Wexler", "Fernanda Vi\u00e9gas"], "year": 2018}, {"title": "Attention Interpretability Across NLP Tasks", "authors": ["Sarah Wiegreffe", "Yuval Pinter"], "year": 2019}, {"title": "Transformer Interpretability Beyond Attention Visualization", "authors": ["Jesse Vig", "Yonatan Belinkov"], "year": 2020}, {"title": "Analyzing the Structure of Attention in a Transformer Language Model", "authors": ["Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning"], "year": 2019}, {"title": "Dissecting Contextual Word Embeddings: Architecture and Representation", "authors": ["John Hewitt", "Christopher D. Manning"], "year": 2019}, {"title": "Visualizing and Understanding Neural Models in NLP", "authors": ["Matthew D. Zeiler", "Rob Fergus"], "year": 2014}, {"title": "Understanding Attention in Transformers: A Visual Analysis", "authors": ["Sofia Serrano", "Noah A. Smith"], "year": 2019}, {"title": "Towards Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks", "authors": ["Yingzhen Yang", "Yongxin Yang", "Tim Hospedales", "Tao Xiang"], "year": 2019}, {"title": "Interpretability of Deep Learning Models: A Survey of Results", "authors": ["Zhi-Hua Zhou"], "year": 2021}, {"title": "A Survey on the Interpretability of Deep Learning Models", "authors": ["Qinglong Wang", "Feng Liu", "Yong Shi"], "year": 2020}, {"title": "Understanding Neural Networks Through Deep Visualization", "authors": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "year": 2015}, {"title": "Interpreting Deep Learning Models for NLP", "authors": ["Sebastian Ruder"], "year": 2020}, {"title": "The Shapley Value in Machine Learning", "authors": ["Scott M. Lundberg", "Su-In Lee"], "year": 2017}]