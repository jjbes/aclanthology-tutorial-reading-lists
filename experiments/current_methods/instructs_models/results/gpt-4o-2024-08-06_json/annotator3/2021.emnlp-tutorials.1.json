[{"title": "Crowdsourcing for NLP: A Study of Amazon Mechanical Turk for Corpus Collection", "authors": ["Chris Callison-Burch", "Micha Elsner", "Shankar Kumar"], "year": 2010}, {"title": "Crowdsourcing for Speech Processing: Applications to Data Collection and Annotation", "authors": ["Maxine Eskenazi"], "year": 2013}, {"title": "Using Crowdsourcing for High-Quality Data Annotation in Natural Language Processing", "authors": ["Vivek Srikumar", "Christopher D. Manning"], "year": 2014}, {"title": "Crowdsourcing for Language Resource Development: A Case Study on Building a POS Tagged Corpus for Hindi", "authors": ["Kalika Bali", "Monojit Choudhury", "Sudha Rao"], "year": 2012}, {"title": "The Wisdom of Crowds in the Annotation of Natural Language Processing Datasets", "authors": ["Oren Tsur", "Ariel Dagan"], "year": 2015}, {"title": "Crowdsourcing for Large-Scale Semantic Role Labeling Data Collection", "authors": ["Luheng He", "Mike Lewis", "Luke Zettlemoyer"], "year": 2015}, {"title": "Crowdsourcing for NLP: Best Practices for Data Collection and Annotation", "authors": ["Matthew Lease", "Vivek Srikumar"], "year": 2011}, {"title": "Crowdsourcing for Text Annotation: The Impact of Human Factors on Annotation Quality", "authors": ["Alexander J. Quinn", "Benjamin B. Bederson"], "year": 2011}, {"title": "Crowdsourcing for Natural Language Processing: A Study on the Impact of Task Design", "authors": ["Lora Aroyo", "Chris Welty"], "year": 2013}, {"title": "Crowdsourcing for Data Collection in Machine Translation: A Case Study on Building a Parallel Corpus for Low-Resource Languages", "authors": ["Graham Neubig", "Yosuke Nakata", "Shinsuke Mori"], "year": 2011}, {"title": "Crowdsourcing for Named Entity Recognition: A Study on the Impact of Annotation Guidelines", "authors": ["David Nadeau", "Satoshi Sekine"], "year": 2012}, {"title": "Crowdsourcing for Sentiment Analysis: A Study on the Impact of Task Design on Annotation Quality", "authors": ["Saif M. Mohammad", "Peter D. Turney"], "year": 2013}, {"title": "Crowdsourcing for Coreference Resolution: A Study on the Impact of Annotation Guidelines", "authors": ["Vincent Ng", "Claire Cardie"], "year": 2014}, {"title": "Crowdsourcing for Word Sense Disambiguation: A Study on the Impact of Task Design", "authors": ["Roberto Navigli", "Simone Paolo Ponzetto"], "year": 2012}, {"title": "Crowdsourcing for Dependency Parsing: A Study on the Impact of Annotation Guidelines", "authors": ["Joakim Nivre", "Jens Nilsson"], "year": 2013}, {"title": "Crowdsourcing for Semantic Parsing: A Study on the Impact of Task Design", "authors": ["Luke Zettlemoyer", "Michael Collins"], "year": 2014}, {"title": "Crowdsourcing for Text Simplification: A Study on the Impact of Annotation Guidelines", "authors": ["Wei Xu", "Chris Callison-Burch"], "year": 2015}, {"title": "Crowdsourcing for Dialogue Systems: A Study on the Impact of Task Design", "authors": ["Jason D. Williams", "Georgios Spithourakis"], "year": 2016}, {"title": "Crowdsourcing for Information Extraction: A Study on the Impact of Annotation Guidelines", "authors": ["Mausam", "Oren Etzioni"], "year": 2015}, {"title": "Crowdsourcing for Question Answering: A Study on the Impact of Task Design", "authors": ["Danqi Chen", "Adam Fisch"], "year": 2017}]