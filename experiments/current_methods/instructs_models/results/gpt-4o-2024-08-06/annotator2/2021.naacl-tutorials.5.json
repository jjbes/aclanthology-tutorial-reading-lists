[
  {
    "title": [
      "Creating a reading list of articles on handling long documents in natural language processing (NLP) involves selecting a mix of foundational papers, recent advancements, and practical approaches. Hereâ€™s a list of 20 articles up to 2021 that cover various aspects of this topic"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "family": "Vaswani",
        "given": "Attention Is All You Need\"",
        "particle": "by"
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "title": [
      "- Introduces the Transformer model, which is foundational for many NLP tasks, including handling long documents"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** by Devlin et al"
    ],
    "date": [
      "2019"
    ],
    "note": [
      "- Discusses BERT, which has been adapted for long document processing."
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "given": "Longformer"
      }
    ],
    "title": [
      "The Long-Document Transformer\"** by Beltagy et al",
      "- Proposes a Transformer variant designed for long documents"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "given": "Reformer"
      }
    ],
    "title": [
      "The Efficient Transformer\"** by Kitaev et al",
      "- Introduces techniques to handle long sequences efficiently"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Big Bird: Transformers for Longer Sequences\"** by Zaheer et al",
      "- Presents a model that extends Transformers to longer sequences"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Hierarchical Attention Networks for Document Classification\"** by Yang et al",
      "- Explores hierarchical models for document classification"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Efficient Transformers: A Survey\"** by Tay et al",
      "- Surveys various efficient Transformer models, including those for long documents"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Sparse Transformers for Long Sequence Modeling\"** by Child et al",
      "- Discusses sparse attention mechanisms for long sequences"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Long-Range Arena: A Benchmark for Efficient Transformers\"** by Tay et al",
      "- Introduces a benchmark for evaluating long-sequence models"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\"** by Dai et al",
      "- Proposes a model that extends context length for language modeling"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Compressive Transformers for Long-Range Sequence Modelling\"** by Rae et al",
      "- Introduces a method for compressing past hidden states"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Hierarchical Transformers for Long Document Classification\"** by Zhang et al",
      "- Discusses a hierarchical approach to document classification"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks\"** by Guo et al"
    ],
    "date": [
      "2019"
    ],
    "note": [
      "- Focuses on summarization techniques for long documents."
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Dissecting Long Documents with Local Coherence Networks\"** by Xu et al",
      "- Explores local coherence in long document processing"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Efficient Attention: Attention with Linear Complexities\"** by Katharopoulos et al",
      "- Proposes a linear complexity attention mechanism"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Structured Self-Attention for Text Representation\"** by Lin et al",
      "- Discusses self-attention mechanisms for text representation"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Hierarchical Transformers Are More Efficient Language Models\"** by Ainslie et al",
      "- Explores hierarchical structures in Transformers"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Long Document Coreference Resolution\"** by Joshi et al",
      "- Focuses on coreference resolution in long documents"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Efficient Content-Based Sparse Attention with Routing Transformers\"** by Roy et al",
      "- Introduces routing mechanisms for sparse attention"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Beyond Sequential Modeling: A Survey of Deep Learning Approaches for Long-Range Dependence\"** by Bai et al",
      "- Surveys deep learning methods for long-range dependencies"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the challenges and solutions in processing long documents in NLP, covering both theoretical advancements and practical implementations"
    ],
    "type": null
  }
]
