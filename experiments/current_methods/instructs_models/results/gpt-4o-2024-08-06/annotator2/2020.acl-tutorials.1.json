[
  {
    "title": [
      "Creating a reading list of articles on interpreting and explaining NLP neural models is a great way to delve into the complexities of these systems. Hereâ€™s a list of 20 influential articles and papers up to 2020 that cover various aspects of this topic"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "family": "Vaswani",
        "given": "Attention",
        "particle": "is All You Need\" by"
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "title": [
      "- Introduces the Transformer model, which is foundational for understanding attention mechanisms in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** by Devlin et al",
      "- Discusses BERT, a model that has significantly influenced NLP and its interpretability"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"A Survey of Methods for Explaining Black Box Models\"** by Guidotti et al",
      "- Provides a comprehensive overview of techniques for explaining complex models, including those used in NLP"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Interpretable Machine Learning: A Guide for Making Black Box Models Explainable\"** by Christoph Molnar",
      "- A book that offers insights into making machine learning models, including NLP models, more interpretable"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Visualizing and Understanding Neural Models in NLP\"** by Li et al",
      "- Explores visualization techniques to understand neural models in NLP"
    ],
    "date": [
      "2015"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "given": "L.I.M.E."
      }
    ],
    "title": [
      "Local Interpretable Model-Agnostic Explanations\"** by Ribeiro et al",
      "- Introduces LIME, a method for explaining predictions of any machine learning model, applicable to NLP"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"The Building Blocks of Interpretability\"** by Olah et al",
      "- Discusses interpretability techniques that can be applied to neural networks, including those used in NLP"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Evaluating the Interpretability of Generative Models by Interactive Reconstruction\"** by Kim et al",
      "- Focuses on evaluating the interpretability of generative models, relevant for understanding NLP models"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Deep Learning for NLP and Speech Recognition\"** by Deng and Liu",
      "- Provides insights into deep learning models used in NLP and their interpretability"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Explaining Black Box Models and Their Predictions with Counterfactuals\"** by Wachter et al",
      "- Discusses the use of counterfactuals to explain model predictions, applicable to NLP"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)\"** by Kim et al",
      "- Introduces TCAV, a method for interpreting neural networks by testing with concept activation vectors"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Attention is not Explanation\"** by Jain and Wallace",
      "- Critically examines the use of attention mechanisms as explanations in NLP models"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Towards a Rigorous Science of Interpretable Machine Learning\"** by Doshi-Velez and Kim (2017) - Discusses the principles of interpretability in machine learning, relevant for NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Anchors: High-Precision Model-Agnostic Explanations\"** by Ribeiro et al",
      "- Proposes Anchors, a method for generating high-precision explanations for model predictions"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Interpretability of Neural Networks: A Survey\"** by Montavon et al",
      "- Surveys various methods for interpreting neural networks, including those used in NLP"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"A Unified Approach to Interpreting Model Predictions\"** by Lundberg and Lee",
      "- Introduces SHAP values, a method for interpreting model predictions, applicable to NLP"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"The Mythos of Model Interpretability\"** by Lipton",
      "- Discusses the challenges and misconceptions surrounding model interpretability"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Explaining Explanations: An Overview of Interpretability of Machine Learning\"** by Gilpin et al",
      "- Provides an overview of interpretability methods in machine learning, including NLP"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"What Do You Mean by 'Interpretability'? A Formal Framework for Understanding Interpretability of Machine Learning Models\"** by Lipton",
      "- Proposes a formal framework for understanding interpretability in machine learning"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Interpretable and Explainable Deep Learning: A Survey\"** by Zhang and Zhu",
      "- Surveys methods for making deep learning models interpretable and explainable, relevant for NLP"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "title": [
      "These articles and papers provide a solid foundation for understanding the interpretability and explainability of NLP neural models. They cover a range of techniques and perspectives, from theoretical frameworks to practical methods"
    ],
    "type": null
  }
]
