Creating a reading list of articles on interpreting and explaining NLP neural models is a great way to delve into the complexities of these systems. Hereâ€™s a list of 20 influential articles and papers up to 2020 that cover various aspects of this topic:

1. **"Attention is All You Need"** by Vaswani et al. (2017) - Introduces the Transformer model, which is foundational for understanding attention mechanisms in NLP.

2. **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** by Devlin et al. (2018) - Discusses BERT, a model that has significantly influenced NLP and its interpretability.

3. **"A Survey of Methods for Explaining Black Box Models"** by Guidotti et al. (2018) - Provides a comprehensive overview of techniques for explaining complex models, including those used in NLP.

4. **"Interpretable Machine Learning: A Guide for Making Black Box Models Explainable"** by Christoph Molnar (2019) - A book that offers insights into making machine learning models, including NLP models, more interpretable.

5. **"Visualizing and Understanding Neural Models in NLP"** by Li et al. (2015) - Explores visualization techniques to understand neural models in NLP.

6. **"LIME: Local Interpretable Model-Agnostic Explanations"** by Ribeiro et al. (2016) - Introduces LIME, a method for explaining predictions of any machine learning model, applicable to NLP.

7. **"The Building Blocks of Interpretability"** by Olah et al. (2018) - Discusses interpretability techniques that can be applied to neural networks, including those used in NLP.

8. **"Evaluating the Interpretability of Generative Models by Interactive Reconstruction"** by Kim et al. (2016) - Focuses on evaluating the interpretability of generative models, relevant for understanding NLP models.

9. **"Deep Learning for NLP and Speech Recognition"** by Deng and Liu (2018) - Provides insights into deep learning models used in NLP and their interpretability.

10. **"Explaining Black Box Models and Their Predictions with Counterfactuals"** by Wachter et al. (2017) - Discusses the use of counterfactuals to explain model predictions, applicable to NLP.

11. **"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"** by Kim et al. (2018) - Introduces TCAV, a method for interpreting neural networks by testing with concept activation vectors.

12. **"Attention is not Explanation"** by Jain and Wallace (2019) - Critically examines the use of attention mechanisms as explanations in NLP models.

13. **"Towards a Rigorous Science of Interpretable Machine Learning"** by Doshi-Velez and Kim (2017) - Discusses the principles of interpretability in machine learning, relevant for NLP.

14. **"Anchors: High-Precision Model-Agnostic Explanations"** by Ribeiro et al. (2018) - Proposes Anchors, a method for generating high-precision explanations for model predictions.

15. **"Interpretability of Neural Networks: A Survey"** by Montavon et al. (2018) - Surveys various methods for interpreting neural networks, including those used in NLP.

16. **"A Unified Approach to Interpreting Model Predictions"** by Lundberg and Lee (2017) - Introduces SHAP values, a method for interpreting model predictions, applicable to NLP.

17. **"The Mythos of Model Interpretability"** by Lipton (2016) - Discusses the challenges and misconceptions surrounding model interpretability.

18. **"Explaining Explanations: An Overview of Interpretability of Machine Learning"** by Gilpin et al. (2018) - Provides an overview of interpretability methods in machine learning, including NLP.

19. **"What Do You Mean by 'Interpretability'? A Formal Framework for Understanding Interpretability of Machine Learning Models"** by Lipton (2018) - Proposes a formal framework for understanding interpretability in machine learning.

20. **"Interpretable and Explainable Deep Learning: A Survey"** by Zhang and Zhu (2018) - Surveys methods for making deep learning models interpretable and explainable, relevant for NLP.

These articles and papers provide a solid foundation for understanding the interpretability and explainability of NLP neural models. They cover a range of techniques and perspectives, from theoretical frameworks to practical methods.