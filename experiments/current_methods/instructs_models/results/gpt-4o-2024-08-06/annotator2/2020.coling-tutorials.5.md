Creating a reading list of articles that discuss benchmarking datasets in question answering (QA), natural language inference (NLI), and commonsense reasoning, particularly focusing on the problems of these datasets, is a great way to delve into the challenges and limitations of these fields. Here’s a list of 20 articles up to 2020 that you might find useful:

1. **"SQuAD: 100,000+ Questions for Machine Comprehension of Text"** by Rajpurkar et al. (2016)
   - Discusses the Stanford Question Answering Dataset (SQuAD) and its limitations.

2. **"The Stanford Question Answering Dataset"** by Rajpurkar et al. (2018)
   - An extended discussion on SQuAD and its challenges.

3. **"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"** by Wang et al. (2018)
   - Introduces the GLUE benchmark and discusses its limitations.

4. **"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"** by Wang et al. (2019)
   - Discusses the SuperGLUE benchmark and its challenges.

5. **"Adversarial Examples for Evaluating Reading Comprehension Systems"** by Jia and Liang (2017)
   - Explores adversarial examples in QA datasets.

6. **"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"** by Zellers et al. (2018)
   - Discusses the SWAG dataset and its limitations.

7. **"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"** by Talmor et al. (2019)
   - Introduces CommonsenseQA and discusses its challenges.

8. **"QuAC: Question Answering in Context"** by Choi et al. (2018)
   - Discusses the QuAC dataset and its limitations.

9. **"Natural Questions: A Benchmark for Question Answering Research"** by Kwiatkowski et al. (2019)
   - Introduces the Natural Questions dataset and its challenges.

10. **"MultiNLI: The Stanford Natural Language Inference Corpus"** by Williams et al. (2018)
    - Discusses the MultiNLI dataset and its limitations.

11. **"RACE: Large-scale ReAding Comprehension Dataset From Examinations"** by Lai et al. (2017)
    - Discusses the RACE dataset and its challenges.

12. **"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"** by Dua et al. (2019)
    - Introduces the DROP dataset and discusses its limitations.

13. **"BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"** by Clark et al. (2019)
    - Discusses the BoolQ dataset and its challenges.

14. **"The Story Cloze Test: A New Benchmark for Story Understanding"** by Mostafazadeh et al. (2016)
    - Discusses the Story Cloze Test and its limitations.

15. **"Probing Neural Network Comprehension of Natural Language Arguments"** by Glockner et al. (2018)
    - Explores the limitations of NLI datasets.

16. **"On the Limitations of Unsupervised Bilingual Dictionary Induction"** by Søgaard et al. (2018)
    - Discusses challenges in cross-lingual datasets.

17. **"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"** by Ribeiro et al. (2020)
    - Discusses the limitations of current NLP benchmarks.

18. **"ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning"** by Yu et al. (2020)
    - Introduces ReClor and discusses its challenges.

19. **"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"** by Gao et al. (2020)
    - Discusses the Pile dataset and its limitations.

20. **"Adversarial NLI: A New Benchmark for Natural Language Understanding"** by Nie et al. (2020)
    - Introduces the Adversarial NLI dataset and discusses its challenges.

These articles provide a comprehensive overview of the datasets used in QA, NLI, and commonsense reasoning, along with a critical analysis of their limitations and challenges.