[
  {
    "title": [
      "Creating a reading list on complex reasoning in NLP with a focus on pretrained language models (PLMs) involves selecting influential and recent articles that cover various aspects of the topic. Here’s a list of 20 articles up to 2023 that you might find useful"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** by Jacob Devlin et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This foundational paper introduces BERT, a model that has significantly influenced the development of PLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Language Models are Few-Shot Learners\"** by Tom B. Brown et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper presents GPT-3, highlighting its capabilities in few-shot learning and reasoning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"** by Colin Raffel et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the T5 model and its approach to treating all NLP tasks as text-to-text problems"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"** by Yinhan Liu et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores improvements over BERT, focusing on training strategies that enhance reasoning capabilities"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\"** by Zhenzhong Lan et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces ALBERT, which optimizes BERT for efficiency and performance"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"DeBERTa: Decoding-enhanced BERT with Disentangled Attention\"** by Pengcheng He et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes enhancements to BERT’s architecture to improve reasoning and understanding"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "given": "E.L.E.C.T.R.A."
      }
    ],
    "title": [
      "Pre-training Text Encoders as Discriminators Rather Than Generators\"** by Kevin Clark et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces a novel pretraining method that improves efficiency and performance in reasoning tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\"** by Zhilin Yang et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Combines the strengths of autoregressive and autoencoding models for better reasoning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View\"** by Zhenhai Zhu et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Offers insights into transformer models from a novel perspective, enhancing reasoning capabilities"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Rethinking Attention with Performers\"** by Krzysztof Choromanski et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Proposes a new attention mechanism that improves scalability and reasoning in transformers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Longformer: The Long-Document Transformer\"** by Iz Beltagy et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Addresses the challenge of processing long documents, crucial for complex reasoning tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Reformer: The Efficient Transformer\"** by Nikita Kitaev et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces techniques to make transformers more efficient, aiding in complex reasoning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "given": "B.A.R.T."
      }
    ],
    "title": [
      "Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\"** by Mike Lewis et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses BART, a model that excels in tasks requiring complex reasoning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets\"** by Alethea Power et al"
    ],
    "date": [
      "2022"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores how models can generalize complex reasoning tasks beyond overfitting"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"UnifiedQA: Crossing Format Boundaries with a Single QA System\"** by Daniel Khashabi et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Presents a QA system that handles multiple formats, showcasing reasoning across diverse tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Improving Language Understanding by Generative Pre-Training\"** by Alec Radford et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "The original GPT paper, laying the groundwork for generative pretraining in reasoning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Reasoning with Language Model Prompting: A Survey\"** by Xinyi Wang et al"
    ],
    "date": [
      "2022"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "A comprehensive survey on using prompts to enhance reasoning in language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Chain of Thought Prompting Elicits Reasoning in Large Language Models\"** by Jason Wei et al"
    ],
    "date": [
      "2022"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates how specific prompting techniques can improve reasoning in PLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Self-Supervised Learning: Generative or Contrastive\"** by Philip Bachman et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses self-supervised learning approaches that impact reasoning in PLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"The Power of Scale for Parameter-Efficient Prompt Tuning\"** by Brian Lester et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores parameter-efficient tuning methods that enhance reasoning capabilities in large models"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of the advancements in complex reasoning within NLP, focusing on the development and application of pretrained language models"
    ],
    "type": null
  }
]
