[
  {
    "title": [
      "Creating a reading list on dataset annotation aggregation and interpreting multiple expert annotators involves selecting articles that cover various methodologies, challenges, and solutions in this area. Hereâ€™s a list of 20 articles and papers up to 2021 that you might find useful"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"The Wisdom of Crowds\" by James Surowiecki** - While not a research article, this book provides foundational insights into collective decision-making, relevant to understanding multiple annotator scenarios"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Label Aggregation Methods for Crowdsourced Data: A Survey\" by Y. Zheng et al.** - A comprehensive survey of methods used to aggregate labels from multiple annotators"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Crowdsourcing Annotations for Visual Object Detection\" by Sorokin and Forsyth** - Discusses the use of crowdsourcing for gathering annotations and the challenges involved"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Beyond Majority Voting: Label Aggregation Methods for Crowdsourcing\" by Ipeirotis et al.** - Explores various label aggregation techniques beyond simple majority voting"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"A Bayesian Approach to Crowdsourcing\" by Raykar et al.** - Introduces a Bayesian framework for aggregating annotations from multiple sources"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Modeling Annotator Expertise: Learning When Everybody Knows a Bit of Something\" by Welinder et al.** - Discusses models that account for varying levels of annotator expertise"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Truth Inference in Crowdsourcing: Is the Problem Solved?\" by Zheng et al.** - Reviews the state of truth inference in crowdsourcing, a key aspect of annotation aggregation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Learning from Crowds\" by Sheng et al.** - A seminal paper on learning from data annotated by multiple, potentially noisy, annotators"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Crowdsourcing Subjective Fashion Advice Using VizWiz: Challenges in Data Collection and Interpretation\" by Bigham et al.** - Examines the challenges of collecting and interpreting subjective data from crowds"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Active Learning with Multiple Annotators\" by Yan et al.** - Discusses active learning strategies in the context of multiple annotators"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"The Role of Expertise and Annotator Confidence in Label Aggregation\" by Snow et al.** - Investigates how annotator expertise and confidence can be used to improve label aggregation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Combining Human and Machine Intelligence to Large-Scale Crowdsourcing\" by Branson et al.** - Explores hybrid approaches that combine human and machine intelligence for annotation tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Crowdsourcing with Endogenous Difficulty Control\" by Ho and Vaughan** - Discusses methods for dynamically adjusting task difficulty in crowdsourcing environments"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"A Survey of Methods for Data Quality Assessment in Crowdsourcing\" by Daniel et al.** - Reviews techniques for assessing and ensuring data quality in crowdsourced annotations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Probabilistic Models for Crowdsourcing\" by Karger et al.** - Introduces probabilistic models to better handle the variability in crowdsourced data"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Learning from Multiple Annotators with Varying Expertise\" by Whitehill et al.** - Proposes models that account for varying levels of annotator expertise"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Crowdsourcing and the Evolution of Human Computation\" by Quinn and Bederson** - Provides a broad overview of crowdsourcing and its applications, including annotation tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"The Impact of Task and Worker Characteristics on Crowdsourced Work Quality\" by Difallah et al.** - Analyzes factors that influence the quality of crowdsourced annotations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Quality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques, and Assurance Actions\" by Allahbakhsh et al.** - Surveys quality control mechanisms in crowdsourcing"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Consensus and Disagreement in Crowdsourced Labeling\" by Sheng et al.** - Examines how consensus and disagreement among annotators can be managed and interpreted"
    ],
    "type": null
  },
  {
    "title": [
      "These articles and papers provide a mix of theoretical insights, practical methodologies, and case studies that can help you understand and implement effective dataset annotation aggregation strategies"
    ],
    "type": null
  }
]
