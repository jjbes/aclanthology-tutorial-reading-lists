Creating a reading list of 20 articles on unsupervised neural machine translation (UNMT) up to 2021 involves selecting key papers that have contributed significantly to the field. Hereâ€™s a list that includes foundational works, improvements, and various approaches to UNMT:

1. **"Unsupervised Machine Translation Using Monolingual Corpora Only"** - Lample et al. (2018)
   - This paper is one of the pioneering works in UNMT, introducing a method that relies solely on monolingual data.

2. **"Phrase-Based & Neural Unsupervised Machine Translation"** - Artetxe et al. (2018)
   - This work explores both phrase-based and neural approaches to unsupervised translation, providing a comprehensive analysis.

3. **"On the Cross-lingual Transferability of Monolingual Representations"** - Conneau et al. (2018)
   - This paper investigates the transferability of monolingual embeddings across languages, which is crucial for UNMT.

4. **"Unsupervised Neural Machine Translation with SMT as Posterior Regularization"** - Ren et al. (2019)
   - The authors propose using statistical machine translation (SMT) to regularize neural models in an unsupervised setting.

5. **"Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder"** - Lample et al. (2018)
   - This paper enhances word-by-word translation using language models and denoising autoencoders.

6. **"Unsupervised Neural Machine Translation with Weight Sharing"** - Yang et al. (2018)
   - The authors introduce a weight-sharing mechanism to improve the performance of UNMT models.

7. **"Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges"** - Arivazhagan et al. (2019)
   - This paper discusses the challenges and findings from deploying multilingual NMT systems, relevant for understanding UNMT in diverse settings.

8. **"Unsupervised Neural Machine Translation with Generative Language Models Only"** - Song et al. (2019)
   - The authors propose a method that leverages generative language models for UNMT.

9. **"Unsupervised Neural Machine Translation with SMT"** - Marie et al. (2019)
   - This work integrates SMT into the training of neural models for unsupervised translation.

10. **"Pre-training via Paraphrasing"** - Wieting et al. (2019)
    - This paper explores the use of paraphrasing as a pre-training task for improving translation models.

11. **"Unsupervised Neural Machine Translation with Denoising Autoencoders"** - Lample et al. (2018)
    - The authors use denoising autoencoders to improve the robustness of UNMT models.

12. **"Cross-lingual Language Model Pretraining"** - Conneau and Lample (2019)
    - This influential work introduces the XLM model, which pretrains language models for cross-lingual tasks, including UNMT.

13. **"Unsupervised Neural Machine Translation with Back-Translation"** - Sennrich et al. (2016)
    - Although primarily about back-translation, this technique is crucial for improving UNMT systems.

14. **"Unsupervised Neural Machine Translation with Monolingual Data"** - Artetxe et al. (2019)
    - This paper further explores the use of monolingual data in training UNMT models.

15. **"Improving Unsupervised Neural Machine Translation with Pseudo-Parallel Data"** - Kim et al. (2020)
    - The authors propose generating pseudo-parallel data to enhance UNMT performance.

16. **"Unsupervised Neural Machine Translation with Cross-lingual Language Model Pretraining"** - Lample and Conneau (2019)
    - This work combines cross-lingual pretraining with UNMT, showing significant improvements.

17. **"Unsupervised Neural Machine Translation with Adversarial Training"** - Wu et al. (2019)
    - The paper introduces adversarial training techniques to improve the quality of UNMT.

18. **"Unsupervised Neural Machine Translation with Multilingual Pretraining"** - Liu et al. (2020)
    - This research explores the benefits of multilingual pretraining for UNMT.

19. **"Unsupervised Neural Machine Translation with Language Model Fusion"** - He et al. (2020)
    - The authors propose a method that fuses language models with translation models in an unsupervised setting.

20. **"Unsupervised Neural Machine Translation with Iterative Back-Translation"** - Edunov et al. (2018)
    - This paper discusses iterative back-translation, a technique that iteratively refines translations in an unsupervised manner.

These articles provide a comprehensive overview of the developments in unsupervised neural machine translation up to 2021. They cover various methodologies, improvements, and applications in the field.