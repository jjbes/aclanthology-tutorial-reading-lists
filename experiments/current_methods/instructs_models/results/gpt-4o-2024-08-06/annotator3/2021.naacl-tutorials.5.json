[
  {
    "title": [
      "Creating a reading list of articles on document-level representation learning involves selecting influential and relevant papers that have contributed to the field. Here is a list of 20 articles up to 2021 that you might find useful"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** by Jacob Devlin et al"
    ],
    "date": [
      "2019"
    ],
    "note": [
      "- Introduces BERT, a model that has significantly influenced document-level representation learning."
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\"** by Zhilin Yang et al",
      "- Proposes XLNet, which improves upon BERT by using a permutation-based training approach"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"** by Yinhan Liu et al",
      "- Enhances BERT by optimizing its pretraining process"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "given": "E.R.N.I.E."
      }
    ],
    "title": [
      "Enhanced Language Representation with Informative Entities\"** by Yu Sun et al",
      "- Introduces ERNIE, which incorporates knowledge graphs into language models"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Longformer: The Long-Document Transformer\"** by Iz Beltagy et al",
      "- Adapts transformers for long documents by using a sparse attention mechanism"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Big Bird: Transformers for Longer Sequences\"** by Manzil Zaheer et al",
      "- Proposes Big Bird, which extends transformers to handle longer sequences efficiently"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Hierarchical Attention Networks for Document Classification\"** by Zichao Yang et al",
      "- Introduces a hierarchical attention mechanism for document classification"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "given": "DocBERT"
      },
      {
        "given": "Das"
      },
      {
        "others": true
      }
    ],
    "title": [
      "BERT for Document Classification\"** by Aditya P",
      "- Adapts BERT for document-level classification tasks"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"A Hierarchical Neural Autoencoder for Paragraphs and Documents\"** by Jiwei Li et al",
      "- Proposes a hierarchical autoencoder for document representation"
    ],
    "date": [
      "2015"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "family": "Ashish Vaswani",
        "given": "Attention",
        "particle": "is All You Need\" by"
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "title": [
      "- While not specific to document-level tasks, this paper introduces the transformer architecture, foundational for many document-level models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling\"** by Pengfei Liu et al",
      "- Discusses improvements in text classification using LSTM and pooling techniques"
    ],
    "date": [
      "2017"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Universal Language Model Fine-tuning for Text Classification\"** by Jeremy Howard and Sebastian Ruder",
      "- Introduces ULMFiT, a transfer learning approach for text classification"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis\"** by Li Dong et al",
      "- Proposes a hierarchical model for sentiment analysis at the document level"
    ],
    "date": [
      "2014"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Hierarchical Attention Networks for Document Classification\"** by Zichao Yang et al"
    ],
    "date": [
      "2016"
    ],
    "note": [
      "- Focuses on hierarchical attention mechanisms for document classification."
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Document Modeling with Gated Recurrent Neural Network for Sentiment Classification\"** by Yoon Kim et al",
      "- Uses GRNNs for document-level sentiment classification"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Deep contextualized word representations\"** by Matthew Peters et al"
    ],
    "date": [
      "2018"
    ],
    "note": [
      "- Introduces ELMo, which provides deep contextualized word representations."
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\"** by Zihang Dai et al",
      "- Extends transformers to capture longer contexts"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Reformer: The Efficient Transformer\"** by Nikita Kitaev et al",
      "- Proposes an efficient transformer model for handling long sequences"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Hierarchical Transformers for Long Document Classification\"** by Wei Xiao et al",
      "- Adapts transformers for hierarchical document classification"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Pre-trained Language Model for Text Generation and Classification\"** by Alec Radford et al",
      "- Discusses the use of pre-trained models for text generation and classification"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of approaches and innovations in document-level representation learning, from foundational models to specific adaptations for handling long documents and hierarchical structures"
    ],
    "type": null
  }
]
