Here's a reading list of 20 articles on interpretability and analysis of neural network models in natural language processing (NLP) up to 2020:

1. **"Attention is All You Need"** by Vaswani et al. (2017)  
   - Introduces the Transformer model, which uses self-attention mechanisms, and discusses interpretability aspects of attention.

2. **"A Structural Probe for Finding Syntax in Word Representations"** by Hewitt and Manning (2019)  
   - Proposes a method to probe syntactic information in word embeddings.

3. **"Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder"** by Zhang et al. (2018)  
   - Discusses a method for making relation learning models more interpretable.

4. **"Visualizing and Understanding Neural Models in NLP"** by Li et al. (2016)  
   - Explores visualization techniques to understand neural models in NLP.

5. **"What Does BERT Look at? An Analysis of BERT's Attention"** by Clark et al. (2019)  
   - Analyzes the attention mechanisms in BERT to understand what it focuses on.

6. **"LIME: Local Interpretable Model-Agnostic Explanations"** by Ribeiro et al. (2016)  
   - Introduces a technique for explaining the predictions of any classifier, applicable to NLP models.

7. **"Deep Learning for NLP and Speech Recognition"** by Deng and Liu (2018)  
   - Provides insights into the interpretability of deep learning models in NLP and speech.

8. **"Evaluating the Interpretability of Generative Models by Interactive Reconstruction"** by Kim et al. (2016)  
   - Discusses methods for evaluating the interpretability of generative models.

9. **"Probing Neural Network Comprehension of Natural Language Arguments"** by McCoy et al. (2019)  
   - Investigates how well neural networks understand natural language arguments.

10. **"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"** by Kim et al. (2018)  
    - Proposes a method for testing the influence of high-level concepts on model predictions.

11. **"On the Importance of Single Directions for Generalization"** by Morcos et al. (2018)  
    - Examines the role of individual neurons in neural network generalization.

12. **"Analyzing and Interpreting Neural Networks for NLP: A Report on the First BlackboxNLP Workshop"** by Belinkov and Glass (2019)  
    - Summarizes findings from a workshop focused on neural network interpretability in NLP.

13. **"Attention is not Explanation"** by Jain and Wallace (2019)  
    - Critically examines the use of attention weights as explanations for model predictions.

14. **"A Survey of Methods for Explaining Black Box Models"** by Guidotti et al. (2018)  
    - Provides a comprehensive overview of methods for explaining black-box models, including those used in NLP.

15. **"The Building Blocks of Interpretability"** by Olah et al. (2018)  
    - Discusses fundamental concepts in interpretability, applicable to NLP models.

16. **"Contextual Decomposition for Neural Network Interpretability"** by Murdoch et al. (2018)  
    - Introduces a method for decomposing neural network predictions to improve interpretability.

17. **"Learning to Explain: An Information-Theoretic Perspective on Model Interpretation"** by Chen et al. (2018)  
    - Proposes an information-theoretic framework for model interpretation.

18. **"Interpretability of Neural Networks: A Survey"** by Montavon et al. (2018)  
    - Surveys various approaches to neural network interpretability, including applications in NLP.

19. **"Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions"** by Koh and Liang (2017)  
    - Introduces influence functions to understand model predictions and data artifacts.

20. **"Towards a Rigorous Science of Interpretable Machine Learning"** by Doshi-Velez and Kim (2017)  
    - Discusses the challenges and methodologies for achieving interpretability in machine learning models.

These articles cover a range of topics related to the interpretability and analysis of neural network models in NLP, providing a solid foundation for understanding this important area of research.