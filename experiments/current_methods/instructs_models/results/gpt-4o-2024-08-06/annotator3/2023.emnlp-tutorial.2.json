[
  {
    "title": [
      "Here's a reading list of articles and papers up to 2023 that focus on security concerns associated with NLP models, particularly backdoor attacks, private data leakage, and imitation attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Backdoor Attacks on Deep Learning Models: A Survey\"** - This paper provides a comprehensive overview of backdoor attacks, including those targeting NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain\"** - A foundational paper discussing backdoor attacks in machine learning, applicable to NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "family": "Neural Networks\"",
        "given": "Trojaning Attack",
        "particle": "on"
      }
    ],
    "title": [
      "Explores the concept of trojaning attacks, which are a form of backdoor attack, and their implications for NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review\"** - This review includes sections on adversarial attacks in NLP, including backdoor and imitation attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Privacy-Preserving Machine Learning: Threats and Solutions\"** - Discusses various privacy concerns, including data leakage in NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Membership Inference Attacks Against Machine Learning Models\"** - Explores how attackers can infer whether a particular data point was part of the training set, relevant to private data leakage"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Model Extraction and Stealing Attacks in Machine Learning: Taxonomy and Survey\"** - Provides insights into imitation attacks where adversaries attempt to replicate NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Data Poisoning Attacks in Contextualized Word Embeddings\"** - Focuses on how data poisoning can lead to backdoor vulnerabilities in NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Mitigating Unintended Memorization in Language Models\"** - Discusses techniques to reduce private data leakage in NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"A Survey on Model Watermarking for Neural Networks\"** - While focused on watermarking, this paper discusses techniques to prevent model theft and imitation attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Exploring the Vulnerability of Deep Neural Networks: A Study of Parameter Corruption\"** - Investigates how parameter corruption can lead to backdoor vulnerabilities"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Adversarial Examples Are Not Bugs, They Are Features\"** - Discusses the nature of adversarial examples, which can be related to imitation attacks in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks\"** - Examines how NLP models can unintentionally memorize and leak private data"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Defending Against Neural Network Model Stealing Attacks Using Deceptive Perturbations\"** - Proposes methods to defend against imitation attacks on NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Backdoor Attacks on Sequence-to-Sequence Models\"** - Specifically addresses backdoor attacks in sequence-to-sequence NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Privacy Risks of Securing Machine Learning Models Against Adversarial Attacks\"** - Discusses the trade-offs between securing models and potential privacy risks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"A Survey on Adversarial Attacks and Defenses for Text\"** - Provides a comprehensive overview of adversarial attacks, including backdoor and imitation attacks in NLP"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Understanding and Mitigating the Security Risks of Machine Learning\"** - A broad overview of security risks, with sections relevant to NLP models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures\"** - Discusses how attackers can reconstruct input data, relevant to private data leakage"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Neural Network Inversion in Adversarial Setting via Background Knowledge Alignment\"** - Explores inversion attacks that can lead to data leakage in NLP models"
    ],
    "type": null
  },
  {
    "title": [
      "These articles and papers provide a solid foundation for understanding the security challenges associated with NLP models, focusing on backdoor attacks, private data leakage, and imitation attacks"
    ],
    "type": null
  }
]
