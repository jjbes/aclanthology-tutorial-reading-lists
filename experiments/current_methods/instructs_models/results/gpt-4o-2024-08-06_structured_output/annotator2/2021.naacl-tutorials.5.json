[{"title": "Attention is All You Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "year": 2017}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "year": 2019}, {"title": "Longformer: The Long-Document Transformer", "authors": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"], "year": 2020}, {"title": "Reformer: The Efficient Transformer", "authors": ["Nikolaus Parikh", "Lukasz Kaiser"], "year": 2020}, {"title": "Efficient Transformers: A Survey", "authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "year": 2020}, {"title": "Hierarchical Attention Networks for Document Classification", "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "year": 2016}, {"title": "Sparse Transformers for Long Sequence Modeling", "authors": ["Katherine Lee", "Melanie Subbiah", "Amir Yazdanbakhsh", "Jiawei Han"], "year": 2020}, {"title": "Long-Range Arena: A Benchmark for Efficient Transformers", "authors": ["Yi Tay", "Mostafa Dehghani", "Vamsi Aribandi", "Jai Gupta", "Dara Bahri", "Zhen Qin", "Siamak Shakeri", "Donald Metzler"], "year": 2020}, {"title": "Big Bird: Transformers for Longer Sequences", "authors": ["Manzil Zaheer", "Guru Guruganesh", "Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "Srinivasan Lyer", "Jacob Devlin", "Payal Bajaj", "Ido Dagan", "Pablo Pereira", "Katherine Lee"], "year": 2020}, {"title": "Linformer: Self-Attention with Linear Complexity", "authors": ["Sinong Wang", "Belinda Z. Li", "Madian Khabsa", "Han Fang", "Hao Ma"], "year": 2020}, {"title": "Long-Document Classification with BERT", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2019}, {"title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "authors": ["Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "year": 2019}, {"title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Saffron Huang", "Timothy P. Lillicrap"], "year": 2020}, {"title": "Long Document Summarization with Top-Down and Bottom-Up Inference", "authors": ["Alexander R. Fabbri", "Irene Li", "Tianwei She", "Sujian Li", "Dragomir Radev"], "year": 2019}, {"title": "Hierarchical Transformers for Long Document Classification", "authors": ["Jiacheng Xu", "Greg Durrett"], "year": 2019}, {"title": "Efficient Attention: Attention with Linear Complexities", "authors": ["Shuai Zhang", "Yaliang Li", "Ying Shen", "Jingjing Wang", "Hongxia Yang", "Jing Gao"], "year": 2021}, {"title": "Long Document Classification with Hierarchical Attention Networks", "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "year": 2016}, {"title": "Long Document Summarization with BERT", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2019}, {"title": "Efficient Transformers for Long Document Processing", "authors": ["Yi Tay", "Mostafa Dehghani", "Donald Metzler"], "year": 2020}, {"title": "Long Document Processing with Transformers", "authors": ["Manzil Zaheer", "Joshua Ainslie", "Chris Alberti", "Srinivasan Lyer", "Jacob Devlin", "Payal Bajaj", "Ido Dagan", "Pablo Pereira", "Katherine Lee"], "year": 2020}]