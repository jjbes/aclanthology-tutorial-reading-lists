[{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "year": 2019}, {"title": "GPT-2: Language Models are Unsupervised Multitask Learners", "authors": ["Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever"], "year": 2019}, {"title": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "year": 2020}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2019}, {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "year": 2020}, {"title": "ERNIE: Enhanced Representation through Knowledge Integration", "authors": ["Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Hao Tian", "Hua Wu", "Haifeng Wang"], "year": 2019}, {"title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "authors": ["Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf"], "year": 2019}, {"title": "CTRL: A Conditional Transformer Language Model for Controllable Generation", "authors": ["Nitish Shirish Keskar", "Bryan McCann", "Lauri Ramstedt", "Matthew R. Ziegler", "Abhinav K. Gupta", "Richard Socher"], "year": 2019}, {"title": "UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation", "authors": ["Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon"], "year": 2019}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "authors": ["Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer"], "year": 2020}, {"title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization", "authors": ["Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu"], "year": 2020}, {"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "authors": ["Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning"], "year": 2020}, {"title": "Turing-NLG: A 17-billion-parameter language model by Microsoft", "authors": ["Microsoft Research"], "year": 2020}, {"title": "Meena: Towards an Open-Domain Chatbot", "authors": ["Daniel Adiwardana", "Minh-Thang Luong", "David R. So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zora Tung", "Xiaodan Liang", "Quoc V. Le", "Ed Chi"], "year": 2020}, {"title": "Reformer: The Efficient Transformer", "authors": ["Nikolaus Parlos", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Noam Shazeer"], "year": 2020}, {"title": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "year": 2020}, {"title": "GPT-3: Language Models are Few-Shot Learners", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "Tom Henighan", "Rewon Child", "Aditya Ramesh", "Daniel M. Ziegler", "Jeffrey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "year": 2020}, {"title": "Language Models as Knowledge Bases?", "authors": ["Fabio Petroni", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller"], "year": 2019}, {"title": "Language Models are Few-Shot Learners", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "Tom Henighan", "Rewon Child", "Aditya Ramesh", "Daniel M. Ziegler", "Jeffrey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "year": 2020}]