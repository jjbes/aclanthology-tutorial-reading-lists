[{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "year": 2019}, {"title": "Longformer: The Long-Document Transformer", "authors": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"], "year": 2020}, {"title": "Reformer: The Efficient Transformer", "authors": ["Nikolaus Parikh", "Lukasz Kaiser"], "year": 2020}, {"title": "Big Bird: Transformers for Longer Sequences", "authors": ["Manzil Zaheer", "Guru Guruganesh", "Kumar Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "S\u00e9bastien Boucher", "Alexandre Lourie", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang", "Amr Ahmed"], "year": 2020}, {"title": "Hierarchical Attention Networks for Document Classification", "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "year": 2016}, {"title": "Attention is All You Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "year": 2017}, {"title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "authors": ["Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "year": 2019}, {"title": "ERNIE: Enhanced Language Representation with Informative Entities", "authors": ["Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Hao Tian", "Hua Wu", "Haifeng Wang"], "year": 2019}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2019}, {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "year": 2020}, {"title": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "year": 2020}, {"title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "authors": ["Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy"], "year": 2020}, {"title": "Long-Range Arena: A Benchmark for Efficient Transformers", "authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "year": 2020}, {"title": "Linformer: Self-Attention with Linear Complexity", "authors": ["Sinong Wang", "Belinda Z. Li", "Madian Khabsa", "Han Fang", "Hao Ma"], "year": 2020}, {"title": "Synthesizer: Rethinking Self-Attention in Transformer Models", "authors": ["Yi Tay", "Dara Bahri", "Luu Anh Tuan", "Cong Yu", "Donald Metzler"], "year": 2021}, {"title": "Long-Document Transformer", "authors": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"], "year": 2020}, {"title": "Efficient Transformers: A Survey", "authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "year": 2020}, {"title": "Document-Level Neural Machine Translation with Hierarchical Attention Networks", "authors": ["Yong Cheng", "Shiqi Shen", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "year": 2016}, {"title": "Hierarchical Transformers for Long Document Classification", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2019}]