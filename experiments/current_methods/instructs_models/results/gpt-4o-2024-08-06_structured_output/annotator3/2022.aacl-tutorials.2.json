[{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "year": 2019}, {"title": "GPT-3: Language Models are Few-Shot Learners", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "Tom Henighan", "Rewon Child", "Aditya Ramesh", "Daniel M. Ziegler", "Jeffrey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "year": 2020}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2019}, {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "year": 2019}, {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "year": 2019}, {"title": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "year": 2020}, {"title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "authors": ["Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf"], "year": 2019}, {"title": "ERNIE: Enhanced Representation through Knowledge Integration", "authors": ["Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Hao Tian", "Hua Wu", "Haifeng Wang"], "year": 2019}, {"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "authors": ["Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning"], "year": 2020}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "authors": ["Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer"], "year": 2020}, {"title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "authors": ["Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy"], "year": 2020}, {"title": "UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation", "authors": ["Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon"], "year": 2019}, {"title": "CTRL: A Conditional Transformer Language Model for Controllable Generation", "authors": ["Nitish Shirish Keskar", "Bryan McCann", "Lav R. Varshney", "Caiming Xiong", "Richard Socher"], "year": 2019}, {"title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "authors": ["Mohammad Shoeybi", "Mostofa Patwary", "Patrick Puri", "Patrick LeGresley", "Jared Casper", "Bryan Catanzaro"], "year": 2019}, {"title": "Reformer: The Efficient Transformer", "authors": ["Nikolaus Parlatone", "Lukasz Kaiser", "Jonas Kornblith", "Mohammad Norouzi", "Geoffrey Hinton"], "year": 2020}, {"title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "authors": ["Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen"], "year": 2020}, {"title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "authors": ["Yu Sun", "Hao Tian", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Xuyi Chen", "Hongkun Zhang", "Xiaolong Zhu", "Danxiang Tian", "Hua Wu", "Haifeng Wang"], "year": 2020}, {"title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing", "authors": ["Zihang Dai", "Guokun Lai", "Yiming Yang", "Quoc V. Le"], "year": 2020}, {"title": "Longformer: The Long-Document Transformer", "authors": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"], "year": 2020}, {"title": "Big Bird: Transformers for Longer Sequences", "authors": ["Manzil Zaheer", "Guru Guruganesh", "Kumar Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "Santiago Ontanon", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang", "Amr Ahmed"], "year": 2020}]