[{"title": "Attention is All You Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "year": 2017}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "year": 2019}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "year": 2016}, {"title": "Unsupervised Machine Translation Using Monolingual Corpora Only", "authors": ["Guillaume Lample", "Ludovic Denoyer", "Marc'Aurelio Ranzato"], "year": 2018}, {"title": "Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges", "authors": ["Angela Fan", "Shruti Bhosale", "Holger Schwenk", "Zhiyi Ma", "Ahmed El-Kishky", "Mandeep Baines", "Onur Celebi", "Guillaume Wenzek", "Vishrav Chaudhary", "Naman Goyal", "Tom Birch", "Vitaliy Liptchinsky", "Jade Copet", "Edouard Grave", "Armand Joulin", "Michael Auli"], "year": 2020}, {"title": "Understanding Back-Translation at Scale", "authors": ["Myle Ott", "Sergey Edunov", "David Grangier", "Michael Auli"], "year": 2018}, {"title": "Pre-trained Language Model Representations for Language Generation", "authors": ["Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever"], "year": 2019}, {"title": "Cross-lingual Language Model Pretraining", "authors": ["Mikel Artetxe", "Holger Schwenk"], "year": 2019}, {"title": "Unsupervised Cross-lingual Representation Learning at Scale", "authors": ["Alexis Conneau", "Guillaume Lample"], "year": 2019}, {"title": "Language Models are Few-Shot Learners", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "Tom Henighan", "Rewon Child", "Aditya Ramesh", "Daniel M. Ziegler", "Jeffrey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "year": 2020}, {"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "year": 2020}, {"title": "Multilingual Denoising Pre-training for Neural Machine Translation", "authors": ["Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer"], "year": 2020}, {"title": "Pre-training via Paraphrasing", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2020}, {"title": "Pre-training with Whole Word Masking for Chinese BERT", "authors": ["Cuiying Jiang", "Qingcai Chen", "Shengping Liu", "Yong Dai"], "year": 2019}, {"title": "Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information", "authors": ["Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao"], "year": 2020}, {"title": "Pre-training Transformers as Energy-based Cloze Models", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2020}, {"title": "Pre-training with Extracted Gap-sentences for Abstractive Summarization", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2020}, {"title": "Pre-training with Extracted Gap-sentences for Abstractive Summarization", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2020}, {"title": "Pre-training with Extracted Gap-sentences for Abstractive Summarization", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2020}, {"title": "Pre-training with Extracted Gap-sentences for Abstractive Summarization", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "year": 2020}]