[{"title": "Fine-Tuning Language Models from Human Preferences", "authors": ["Paul Christiano", "Jan Leike", "Tom Brown"], "year": 2020}, {"title": "Parameter-Efficient Transfer Learning for NLP", "authors": ["Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski"], "year": 2019}, {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "authors": ["Edward J. Hu", "Yelong Shen", "Phillip Wallach"], "year": 2021}, {"title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning", "authors": ["Jonas Pfeiffer", "Andreas R\u00fcckl\u00e9", "Clifton Poth"], "year": 2020}, {"title": "Efficient Fine-Tuning of Transformer Models", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts"], "year": 2020}, {"title": "Prompt Tuning for Large Language Models", "authors": ["Brian Lester", "Rami Al-Rfou", "Noah Constant"], "year": 2021}, {"title": "Revisiting Few-shot Learning for Large Language Models", "authors": ["Jason Wei", "Maarten Bosma", "Vincent Y. Zhao"], "year": 2022}, {"title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "authors": ["Xiang Lisa Li", "Percy Liang"], "year": 2021}, {"title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "authors": ["Xiang Lisa Li", "Percy Liang"], "year": 2021}, {"title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks", "authors": ["Xiao Liu", "Yanan Zheng", "Zhuoyi Yang"], "year": 2021}, {"title": "Adapting Language Models for Zero-Shot Learning by Meta-Learning", "authors": ["Yinhan Liu", "Jiatao Gu", "Naman Goyal"], "year": 2021}, {"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts"], "year": 2020}, {"title": "Parameter-Efficient Transfer Learning with Diff Pruning", "authors": ["Xiaodong Liu", "Pengcheng He", "Weizhu Chen"], "year": 2021}, {"title": "Efficiently Adapting Pretrained Language Models for Dialogue Systems", "authors": ["Chia-Hsuan Lee", "Yi-Lin Tuan", "Hung-yi Lee"], "year": 2021}, {"title": "Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks", "authors": ["Yinhan Liu", "Jiatao Gu", "Naman Goyal"], "year": 2021}, {"title": "Efficient Transfer Learning for NLP with Adapters", "authors": ["Jonas Pfeiffer", "Andreas R\u00fcckl\u00e9", "Clifton Poth"], "year": 2020}, {"title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "authors": ["Jonathan Frankle", "Michael Carbin"], "year": 2019}, {"title": "Sparse Fine-Tuning for Large Language Models", "authors": ["Zhuohan Li", "Eric Wallace", "Shafiq Joty"], "year": 2022}, {"title": "Efficient Large-Scale Language Model Fine-Tuning with Pre-Layer Norm", "authors": ["Zihang Dai", "Zhilin Yang", "Yiming Yang"], "year": 2021}, {"title": "Efficient Transfer Learning with a Language Model", "authors": ["Jeremy Howard", "Sebastian Ruder"], "year": 2018}]