[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here is a reading list of 20 articles up to 2022 that focus on the usage of pre-trained language models (PLMs) in downstream tasks, particularly under data scarcity and parameter efficiency scenarios"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "family": "Howard",
        "given": "J."
      },
      {
        "family": "Ruder",
        "given": "S."
      }
    ],
    "date": [
      "2018",
      "2018"
    ],
    "title": [
      "Universal Language Model Fine-tuning for Text Classification"
    ],
    "type": "article-journal",
    "container-title": [
      "*ACL"
    ]
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "family": "Devlin",
        "given": "J."
      },
      {
        "family": "Chang",
        "given": "M.-W."
      },
      {
        "family": "Lee",
        "given": "K."
      },
      {
        "family": "Toutanova",
        "given": "K."
      }
    ],
    "date": [
      "2019",
      "2019"
    ],
    "title": [
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "type": "article-journal",
    "container-title": [
      "*NAACL-HLT"
    ]
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "family": "Ott",
        "given": "M."
      },
      {
        "family": "Goyal",
        "given": "N."
      },
      {
        "family": "Du",
        "given": "J."
      },
      {
        "family": "Joshi",
        "given": "M."
      },
      {
        "family": "Chen",
        "given": "D."
      },
      {
        "family": "Stoyanov",
        "given": "V."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
    ],
    "note": [
      "*arXiv preprint arXiv:1907.11692.*"
    ],
    "arxiv": [
      "1907.11692"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "family": "Lan",
        "given": "Z."
      },
      {
        "family": "Chen",
        "given": "M."
      },
      {
        "family": "Goodman",
        "given": "S."
      },
      {
        "family": "Gimpel",
        "given": "K."
      },
      {
        "family": "Sharma",
        "given": "P."
      },
      {
        "family": "Soricut",
        "given": "R."
      }
    ],
    "date": [
      "2020",
      "2020"
    ],
    "title": [
      "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
    ],
    "type": "article-journal",
    "container-title": [
      "*ICLR"
    ]
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "family": "Sanh",
        "given": "V."
      },
      {
        "family": "Debut",
        "given": "L."
      },
      {
        "family": "Chaumond",
        "given": "J."
      },
      {
        "family": "Wolf",
        "given": "T."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
    ],
    "note": [
      "*arXiv preprint arXiv:1910.01108.*"
    ],
    "arxiv": [
      "1910.01108"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "family": "Sun",
        "given": "C."
      },
      {
        "family": "Qiu",
        "given": "X."
      },
      {
        "family": "Xu",
        "given": "Y."
      },
      {
        "family": "Huang",
        "given": "X."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "How to Fine-Tune BERT for Text Classification?"
    ],
    "container-title": [
      "*China National Conference on Chinese Computational Linguistics.*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "family": "Houlsby",
        "given": "N."
      },
      {
        "family": "Giurgiu",
        "given": "A."
      },
      {
        "family": "Jastrzebski",
        "given": "S."
      },
      {
        "family": "Morrone",
        "given": "B."
      },
      {
        "family": "Laroussilhe",
        "given": "Q.",
        "particle": "de"
      },
      {
        "family": "Gesmundo",
        "given": "A."
      },
      {
        "family": "Gelly",
        "given": "S."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Parameter-Efficient Transfer Learning for NLP"
    ],
    "container-title": [
      "*ICML 2019.*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "family": "Peters",
        "given": "M.E."
      },
      {
        "family": "Ruder",
        "given": "S."
      },
      {
        "family": "Smith",
        "given": "N.A."
      }
    ],
    "date": [
      "2019",
      "2019"
    ],
    "title": [
      "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "family": "Raffel",
        "given": "C."
      },
      {
        "family": "Shazeer",
        "given": "N."
      },
      {
        "family": "Roberts",
        "given": "A."
      },
      {
        "family": "Lee",
        "given": "K."
      },
      {
        "family": "Narang",
        "given": "S."
      },
      {
        "family": "Matena",
        "given": "M."
      },
      {
        "family": "Liu",
        "given": "P.J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020",
      "2020"
    ],
    "title": [
      "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
    ],
    "type": "article-journal",
    "container-title": [
      "*JMLR"
    ]
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "family": "Gururangan",
        "given": "S."
      },
      {
        "family": "Marasović",
        "given": "A."
      },
      {
        "family": "Swayamdipta",
        "given": "S."
      },
      {
        "family": "Lo",
        "given": "K."
      },
      {
        "family": "Beltagy",
        "given": "I."
      },
      {
        "family": "Downey",
        "given": "D."
      },
      {
        "family": "Smith",
        "given": "N.A."
      }
    ],
    "date": [
      "2020",
      "2020"
    ],
    "title": [
      "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks"
    ],
    "type": "article-journal",
    "container-title": [
      "*ACL"
    ]
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "family": "Zhang",
        "given": "Z."
      },
      {
        "family": "Yang",
        "given": "K."
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Revisiting Few-sample BERT Fine-tuning"
    ],
    "container-title": [
      "*ICLR 2021.*"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "family": "Schick",
        "given": "T."
      },
      {
        "family": "Schütze",
        "given": "H."
      }
    ],
    "date": [
      "2021",
      "2021"
    ],
    "title": [
      "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"
    ],
    "type": "article-journal",
    "container-title": [
      "*EACL"
    ]
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "family": "Gao",
        "given": "T."
      },
      {
        "family": "Fisch",
        "given": "A."
      },
      {
        "family": "Chen",
        "given": "D."
      }
    ],
    "date": [
      "2021",
      "2021"
    ],
    "title": [
      "Making Pre-trained Language Models Better Few-shot Learners"
    ],
    "type": "article-journal",
    "container-title": [
      "*ACL"
    ]
  },
  {
    "citation-number": [
      "14."
    ],
    "author": [
      {
        "family": "Lester",
        "given": "B."
      },
      {
        "family": "Al-Rfou",
        "given": "R."
      },
      {
        "family": "Constant",
        "given": "N."
      }
    ],
    "date": [
      "2021",
      "2021"
    ],
    "title": [
      "The Power of Scale for Parameter-Efficient Prompt Tuning"
    ],
    "type": "article-journal",
    "container-title": [
      "*EMNLP"
    ]
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "family": "Li",
        "given": "X.L."
      },
      {
        "family": "Liang",
        "given": "P."
      }
    ],
    "date": [
      "2021",
      "2021"
    ],
    "title": [
      "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
    ],
    "type": "article-journal",
    "container-title": [
      "*ACL"
    ]
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "family": "He",
        "given": "J."
      },
      {
        "family": "Liu",
        "given": "M."
      },
      {
        "family": "Liu",
        "given": "J."
      }
    ],
    "date": [
      "2021",
      "2021"
    ],
    "title": [
      "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
    ],
    "type": "article-journal",
    "container-title": [
      "*ICLR"
    ]
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "Zaken",
        "given": "E."
      },
      {
        "family": "Ravfogel",
        "given": "S."
      },
      {
        "family": "Goldberg",
        "given": "Y."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"
    ],
    "note": [
      "*arXiv preprint arXiv:2106.10199.*"
    ],
    "arxiv": [
      "2106.10199"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "P."
      },
      {
        "family": "Yuan",
        "given": "W."
      },
      {
        "family": "Fu",
        "given": "J."
      },
      {
        "family": "Jiang",
        "given": "Z."
      },
      {
        "family": "Hayashi",
        "given": "H."
      },
      {
        "family": "Neubig",
        "given": "G."
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
    ],
    "note": [
      "*arXiv preprint arXiv:2107.13586.*"
    ],
    "arxiv": [
      "2107.13586"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "family": "Zhang",
        "given": "T."
      },
      {
        "family": "Roller",
        "given": "S."
      },
      {
        "family": "Goyal",
        "given": "N."
      },
      {
        "family": "Artetxe",
        "given": "M."
      },
      {
        "family": "Chen",
        "given": "M."
      },
      {
        "family": "Chen",
        "given": "S."
      },
      {
        "family": "Zettlemoyer",
        "given": "L."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "OPT: Open Pre-trained Transformer Language Models"
    ],
    "note": [
      "*arXiv preprint arXiv:2205.01068.*"
    ],
    "arxiv": [
      "2205.01068"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "family": "Wei",
        "given": "J."
      },
      {
        "family": "Wang",
        "given": "X."
      },
      {
        "family": "Schuurmans",
        "given": "D."
      },
      {
        "family": "Bosma",
        "given": "M."
      },
      {
        "family": "Ichter",
        "given": "B."
      },
      {
        "family": "Xia",
        "given": "F."
      },
      {
        "family": "Le",
        "given": "Q.V."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2022"
    ],
    "title": [
      "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
    ],
    "note": [
      "*arXiv preprint arXiv:2201.11903.*"
    ],
    "arxiv": [
      "2201.11903"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of topics including fine-tuning strategies, parameter-efficient methods, and approaches to handling data scarcity in the context of pre-trained language models"
    ],
    "type": null
  }
]
