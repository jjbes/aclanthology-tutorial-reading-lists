[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here's a reading list of 20 influential articles on neural language generation (NLG) with a focus on deep contextual models and transfer learning, up to the year 2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "family": "Vaswani",
        "given": "A."
      },
      {
        "family": "Shazeer",
        "given": "N."
      },
      {
        "family": "Parmar",
        "given": "N."
      },
      {
        "family": "Uszkoreit",
        "given": "J."
      },
      {
        "family": "Jones",
        "given": "L."
      },
      {
        "family": "Gomez",
        "given": "A.N."
      },
      {
        "family": "Polosukhin",
        "given": "I."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "title": [
      "Attention is All You Need"
    ],
    "container-title": [
      "*Advances in Neural Information Processing Systems (NeurIPS)*"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces the Transformer model, which has become foundational for many NLG tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "family": "Narasimhan",
        "given": "K."
      },
      {
        "family": "Salimans",
        "given": "T."
      },
      {
        "family": "Sutskever",
        "given": "I."
      }
    ],
    "date": [
      "2018"
    ],
    "title": [
      "Improving Language Understanding by Generative Pre-Training"
    ],
    "type": "article-journal",
    "container-title": [
      "*OpenAI*"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper presents GPT, a generative pre-trained transformer model that significantly advances NLG"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "family": "Devlin",
        "given": "J."
      },
      {
        "family": "Chang",
        "given": "M.W."
      },
      {
        "family": "Lee",
        "given": "K."
      },
      {
        "family": "Toutanova",
        "given": "K."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "container-title": [
      "*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*"
    ],
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "BERT introduces bidirectional training of transformers, which has been highly influential in contextual language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "family": "Wu",
        "given": "J."
      },
      {
        "family": "Child",
        "given": "R."
      },
      {
        "family": "Luan",
        "given": "D."
      },
      {
        "family": "Amodei",
        "given": "D."
      },
      {
        "family": "Sutskever",
        "given": "I."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Language Models are Unsupervised Multitask Learners"
    ],
    "type": "article-journal",
    "container-title": [
      "*OpenAI*"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces GPT-2, which demonstrates the power of large-scale unsupervised language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "family": "Brown",
        "given": "T.B."
      },
      {
        "family": "Mann",
        "given": "B."
      },
      {
        "family": "Ryder",
        "given": "N."
      },
      {
        "family": "Subbiah",
        "given": "M."
      },
      {
        "family": "Kaplan",
        "given": "J."
      },
      {
        "family": "Dhariwal",
        "given": "P."
      },
      {
        "family": "Amodei",
        "given": "D."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Language Models are Few-Shot Learners"
    ],
    "note": [
      "*arXiv preprint arXiv:2005.14165*."
    ],
    "arxiv": [
      "2005.14165"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "GPT-3 is introduced, showcasing the capabilities of even larger language models in few-shot learning scenarios"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "author": [
      {
        "family": "Peters",
        "given": "M.E."
      },
      {
        "family": "Neumann",
        "given": "M."
      },
      {
        "family": "Iyyer",
        "given": "M."
      },
      {
        "family": "Gardner",
        "given": "M."
      },
      {
        "family": "Clark",
        "given": "C."
      },
      {
        "family": "Lee",
        "given": "K."
      },
      {
        "family": "Zettlemoyer",
        "given": "L."
      }
    ],
    "date": [
      "2018"
    ],
    "title": [
      "Deep Contextualized Word Representations"
    ],
    "container-title": [
      "*Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*"
    ],
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces ELMo, which provides deep contextualized word representations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "family": "Lample",
        "given": "G."
      },
      {
        "family": "Conneau",
        "given": "A."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Cross-lingual Language Model Pretraining"
    ],
    "container-title": [
      "*Advances in Neural Information Processing Systems (NeurIPS)*"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This work explores cross-lingual pretraining for language models, enhancing transfer learning across languages"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "family": "Yang",
        "given": "Z."
      },
      {
        "family": "Dai",
        "given": "Z."
      },
      {
        "family": "Yang",
        "given": "Y."
      },
      {
        "family": "Carbonell",
        "given": "J."
      },
      {
        "family": "Salakhutdinov",
        "given": "R."
      },
      {
        "family": "Le",
        "given": "Q.V."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
    ],
    "container-title": [
      "*Advances in Neural Information Processing Systems (NeurIPS)*"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "XLNet improves upon BERT by using a permutation-based training objective"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "author": [
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "family": "Ott",
        "given": "M."
      },
      {
        "family": "Goyal",
        "given": "N."
      },
      {
        "family": "Du",
        "given": "J."
      },
      {
        "family": "Joshi",
        "given": "M."
      },
      {
        "family": "Chen",
        "given": "D."
      },
      {
        "family": "Stoyanov",
        "given": "V."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
    ],
    "note": [
      "*arXiv preprint arXiv:1907.11692*."
    ],
    "arxiv": [
      "1907.11692"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "RoBERTa optimizes BERT's pretraining process, leading to improved performance on various benchmarks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "author": [
      {
        "family": "Lewis",
        "given": "M."
      },
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "family": "Goyal",
        "given": "N."
      },
      {
        "family": "Ghazvininejad",
        "given": "M."
      },
      {
        "family": "Mohamed",
        "given": "A."
      },
      {
        "family": "Levy",
        "given": "O."
      },
      {
        "family": "Zettlemoyer",
        "given": "L."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
    ],
    "container-title": [
      "*Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)*"
    ],
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "BART combines the best of BERT and GPT for sequence-to-sequence tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "family": "Raffel",
        "given": "C."
      },
      {
        "family": "Shazeer",
        "given": "N."
      },
      {
        "family": "Roberts",
        "given": "A."
      },
      {
        "family": "Lee",
        "given": "K."
      },
      {
        "family": "Narang",
        "given": "S."
      },
      {
        "family": "Matena",
        "given": "M."
      },
      {
        "family": "Liu",
        "given": "P.J."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
    ],
    "container-title": [
      "*Journal of Machine Learning Research (JMLR)*"
    ],
    "type": "article-journal"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces T5, a model that frames all NLP tasks as text-to-text transformations"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "family": "Dong",
        "given": "L."
      },
      {
        "family": "Yang",
        "given": "N."
      },
      {
        "family": "Wang",
        "given": "W."
      },
      {
        "family": "Wei",
        "given": "F."
      },
      {
        "family": "Zhou",
        "given": "M."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Unified Language Model Pre-training for Natural Language Understanding and Generation"
    ],
    "container-title": [
      "*Advances in Neural Information Processing Systems (NeurIPS)*"
    ],
    "type": "chapter"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This work presents UniLM, a unified pretraining model for both NLU and NLG tasks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "family": "Zhang",
        "given": "Y."
      },
      {
        "family": "Sun",
        "given": "S."
      },
      {
        "family": "Galley",
        "given": "M."
      },
      {
        "family": "Chen",
        "given": "Y.C."
      },
      {
        "family": "Brockett",
        "given": "C."
      },
      {
        "family": "Gao",
        "given": "X."
      },
      {
        "family": "Dolan",
        "given": "B."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Dialogpt: Large-scale generative pre-training for conversational response generation"
    ],
    "container-title": [
      "*Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)*"
    ],
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "DialoGPT adapts GPT-2 for conversational response generation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "author": [
      {
        "family": "Keskar",
        "given": "N.S."
      },
      {
        "family": "McCann",
        "given": "B."
      },
      {
        "family": "Varshney",
        "given": "L.R."
      },
      {
        "family": "Xiong",
        "given": "C."
      },
      {
        "family": "Socher",
        "given": "R."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "CTRL: A Conditional Transformer Language Model for Controllable Generation"
    ],
    "note": [
      "*arXiv preprint arXiv:1909.05858*."
    ],
    "arxiv": [
      "1909.05858"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "CTRL allows for controllable text generation by conditioning on control codes"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "family": "Metz",
        "given": "L."
      },
      {
        "family": "Chintala",
        "given": "S."
      }
    ],
    "date": [
      "2016"
    ],
    "title": [
      "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"
    ],
    "note": [
      "*arXiv preprint arXiv:1511.06434*."
    ],
    "arxiv": [
      "1511.06434"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces DCGANs, which have influenced many generative models, including those for text"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "family": "Zellers",
        "given": "R."
      },
      {
        "family": "Holtzman",
        "given": "A."
      },
      {
        "family": "Bisk",
        "given": "Y."
      },
      {
        "family": "Farhadi",
        "given": "A."
      },
      {
        "family": "Choi",
        "given": "Y."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "HellaSwag: Can a Machine Really Finish Your Sentence?"
    ],
    "container-title": [
      "*Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)*"
    ],
    "type": "paper-conference"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "HellaSwag tests the commonsense reasoning capabilities of language models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "author": [
      {
        "family": "Wolf",
        "given": "T."
      },
      {
        "family": "Sanh",
        "given": "V."
      },
      {
        "family": "Chaumond",
        "given": "J."
      },
      {
        "family": "Delangue",
        "given": "C."
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents"
    ],
    "note": [
      "*arXiv preprint arXiv:1901.08149*."
    ],
    "arxiv": [
      "1901.08149"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper explores transfer learning for conversational agents using transformers"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "family": "Ziegler",
        "given": "D.M."
      },
      {
        "family": "Stiennon",
        "given": "N."
      },
      {
        "family": "Wu",
        "given": "J."
      },
      {
        "family": "Brown",
        "given": "T.B."
      },
      {
        "family": "Radford",
        "given": "A."
      },
      {
        "family": "Amodei",
        "given": "D."
      },
      {
        "family": "Irving",
        "given": "G."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2019"
    ],
    "title": [
      "Fine-Tuning Language Models from Human Preferences"
    ],
    "note": [
      "*arXiv preprint arXiv:1909.08593*."
    ],
    "arxiv": [
      "1909.08593"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This work focuses on fine-tuning language models based on human feedback"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "family": "Khandelwal",
        "given": "U."
      },
      {
        "family": "Levy",
        "given": "O."
      },
      {
        "family": "Jurafsky",
        "given": "D."
      },
      {
        "family": "Zettlemoyer",
        "given": "L."
      },
      {
        "family": "Lewis",
        "given": "M."
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "Generalization through Memorization: Nearest Neighbor Language Models"
    ],
    "note": [
      "*arXiv preprint arXiv:1911.00172*."
    ],
    "arxiv": [
      "1911.00172"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "This paper introduces a nearest neighbor approach to enhance language model generalization"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "family": "Clark",
        "given": "K."
      },
      {
        "family": "Luong",
        "given": "M.T."
      },
      {
        "family": "Le",
        "given": "Q.V."
      },
      {
        "family": "Manning",
        "given": "C.D."
      }
    ],
    "date": [
      "2020"
    ],
    "title": [
      "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
    ],
    "note": [
      "*arXiv preprint arXiv:2003.10555*."
    ],
    "arxiv": [
      "2003.10555"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "ELECTRA proposes a new pretraining method that trains text encoders as discriminators, leading to more efficient training"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a broad spectrum of advancements in neural language generation, deep contextual models, and transfer learning, providing a comprehensive foundation for understanding the field up to"
    ],
    "date": [
      "2020"
    ],
    "type": null
  }
]
