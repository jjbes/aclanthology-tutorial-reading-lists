[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2023 that focus on security concerns associated with NLP models, particularly on backdoor attacks, private data leakage, and imitation attacks"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Attacks",
        "given": "Backdoor"
      }
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "author": [
      {
        "literal": "**\"Backdoor Attacks on Deep Learning Models\"** - Gu, T., Dolan-Gavitt, B., & Garg, S."
      }
    ],
    "date": [
      "2017"
    ],
    "note": [
      "*arXiv preprint arXiv:1708.06733*."
    ],
    "arxiv": [
      "1708.06733"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain\"**"
    ],
    "editor": [
      {
        "family": "Liu",
        "given": "Y."
      },
      {
        "family": "Ma",
        "given": "S."
      },
      {
        "family": "Aafer",
        "given": "Y."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2018"
    ],
    "note": [
      "*arXiv preprint arXiv:1708.06733*."
    ],
    "arxiv": [
      "1708.06733"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "author": [
      {
        "literal": "**\"Trojaning Attack on Neural Networks\"** - Liu, Y., Ma, S., Bailey, J., & Lu, F."
      }
    ],
    "date": [
      "2017"
    ],
    "container-title": [
      "*Network and Distributed System Security Symposium (NDSS)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Backdoor Attacks Against Transfer Learning with Pre-trained Deep Learning Models\"**"
    ],
    "editor": [
      {
        "family": "Yao",
        "given": "Y."
      },
      {
        "family": "Li",
        "given": "H."
      },
      {
        "family": "Zheng",
        "given": "H."
      },
      {
        "family": "Zhao",
        "given": "B.Y."
      }
    ],
    "date": [
      "2019"
    ],
    "note": [
      "*arXiv preprint arXiv:1901.02710*."
    ],
    "arxiv": [
      "1901.02710"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "literal": "**\"Neural Trojans\"** - Chen, X., Liu, C., Li, B., Lu, K., & Song, D."
      }
    ],
    "date": [
      "2017"
    ],
    "publisher": [
      "*IEEE Security and Privacy Workshops (SPW)*"
    ],
    "type": "book"
  },
  {
    "container-title": [
      "### Private Data Leakage"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Membership Inference Attacks Against Machine Learning Models\"**"
    ],
    "author": [
      {
        "family": "Shokri",
        "given": "R."
      },
      {
        "family": "Stronati",
        "given": "M."
      },
      {
        "family": "Song",
        "given": "C."
      },
      {
        "family": "Shmatikov",
        "given": "V."
      }
    ],
    "date": [
      "2017"
    ],
    "container-title": [
      "*IEEE Symposium on Security and Privacy (SP)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning\"**"
    ],
    "author": [
      {
        "family": "Hitaj",
        "given": "B."
      },
      {
        "family": "Ateniese",
        "given": "G."
      },
      {
        "family": "Perez-Cruz",
        "given": "F."
      }
    ],
    "date": [
      "2017"
    ],
    "container-title": [
      "*ACM SIGSAC Conference on Computer and Communications Security (CCS)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Auditing Data Provenance in Text-Generation Models\"**"
    ],
    "author": [
      {
        "family": "Carlini",
        "given": "N."
      },
      {
        "family": "Tramer",
        "given": "F."
      },
      {
        "family": "Wallace",
        "given": "E."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "note": [
      "*arXiv preprint arXiv:2107.03396*."
    ],
    "arxiv": [
      "2107.03396"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Extracting Training Data from Large Language Models\"**"
    ],
    "author": [
      {
        "family": "Carlini",
        "given": "N."
      },
      {
        "family": "Tramer",
        "given": "F."
      },
      {
        "family": "Wallace",
        "given": "E."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2021"
    ],
    "publisher": [
      "*USENIX Security Symposium*"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Privacy-Preserving Machine Learning: Threats and Solutions\"**"
    ],
    "author": [
      {
        "family": "Abadi",
        "given": "M."
      },
      {
        "family": "Chu",
        "given": "A."
      },
      {
        "family": "Goodfellow",
        "given": "I."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2016"
    ],
    "publisher": [
      "*IEEE Security and Privacy*"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "family": "Attacks",
        "given": "Imitation"
      }
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "author": [
      {
        "literal": "**\"Stealing Machine Learning Models via Prediction APIs\"** - Tramer, F., Zhang, F., Juels, A., Reiter, M. K., & Ristenpart, T."
      }
    ],
    "date": [
      "2016"
    ],
    "publisher": [
      "*USENIX Security Symposium*"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Model Extraction Attacks Against Machine Learning Models\"**"
    ],
    "editor": [
      {
        "family": "Papernot",
        "given": "N."
      },
      {
        "family": "McDaniel",
        "given": "P."
      },
      {
        "family": "Goodfellow",
        "given": "I."
      },
      {
        "others": true
      }
    ],
    "date": [
      "2017"
    ],
    "publisher": [
      "*IEEE European Symposium on Security and Privacy (EuroS&P)*"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "13."
    ],
    "author": [
      {
        "literal": "**\"Knockoff Nets: Stealing Functionality of Black-Box Models\"** - Orekondy, T., Schiele, B., & Fritz, M."
      }
    ],
    "date": [
      "2019"
    ],
    "container-title": [
      "*IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks\"**"
    ],
    "author": [
      {
        "family": "Carlini",
        "given": "N."
      },
      {
        "family": "Liu",
        "given": "C."
      },
      {
        "family": "Erlingsson",
        "given": "U."
      },
      {
        "family": "Kos",
        "given": "J."
      },
      {
        "family": "Song",
        "given": "D."
      }
    ],
    "date": [
      "2019"
    ],
    "publisher": [
      "*USENIX Security Symposium*"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "literal": "**\"Model Extraction and Adversarial Attacks on Machine Learning as a Service\"** - Juuti, M., Szyller, S., Marchal, S., & Asokan, N."
      }
    ],
    "date": [
      "2019"
    ],
    "note": [
      "*Proceedings on Privacy Enhancing Technologies*."
    ],
    "type": null
  },
  {
    "title": [
      "### General Security Concerns in NLP Models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Examples in the Physical World\"** - Kurakin, A., Goodfellow, I., & Bengio, S."
      }
    ],
    "date": [
      "2016"
    ],
    "note": [
      "*arXiv preprint arXiv:1607.02533*."
    ],
    "arxiv": [
      "1607.02533"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review\"**"
    ],
    "author": [
      {
        "family": "Sun",
        "given": "X."
      },
      {
        "family": "Wang",
        "given": "J."
      },
      {
        "family": "Tang",
        "given": "X."
      }
    ],
    "date": [
      "2018"
    ],
    "note": [
      "*arXiv preprint arXiv:1812.10528*."
    ],
    "arxiv": [
      "1812.10528"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Attacks on Neural Networks for Graph Data\"** - Zugner, D., Akbarnejad, A., & Gunnemann, S."
      }
    ],
    "date": [
      "2018"
    ],
    "container-title": [
      "*Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*"
    ],
    "type": "paper-conference"
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "literal": "**\"Adversarial Attacks on Neural Network Policies\"** - Huang, S., Papernot, N., Goodfellow, I., Duan, Y., & Abbeel, P."
      }
    ],
    "date": [
      "2017"
    ],
    "note": [
      "*arXiv preprint arXiv:1702.02284*."
    ],
    "arxiv": [
      "1702.02284"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Adversarial Attacks on Machine Learning Systems for High-Frequency Trading\"**"
    ],
    "editor": [
      {
        "family": "Feng",
        "given": "Y."
      },
      {
        "family": "Yan",
        "given": "W."
      }
    ],
    "date": [
      "2019"
    ],
    "note": [
      "*arXiv preprint arXiv:1902.01879*."
    ],
    "arxiv": [
      "1902.01879"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of topics related to security concerns in NLP models, including backdoor attacks, private data leakage, and imitation attacks. They provide a comprehensive overview of the current state of research in these areas"
    ],
    "type": null
  }
]
