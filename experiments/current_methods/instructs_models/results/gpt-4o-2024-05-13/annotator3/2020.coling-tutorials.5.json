[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here is a reading list of 20 articles up to 2020 that cover existing datasets for Question Answering (QA"
    ],
    "container-title": [
      "Natural Language Inference (NLI), and commonsense reasoning"
    ],
    "type": "chapter"
  },
  {
    "note": [
      "### Question Answering (QA"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"SQuAD: 100,000+ Questions for Machine Comprehension of Text\"** - Rajpurkar et al"
    ],
    "date": [
      "2016"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\"**"
    ],
    "date": [
      "2017"
    ],
    "type": "article-journal",
    "container-title": [
      "Joshi et al"
    ]
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"QuAC: Question Answering in Context\"** - Choi et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\"**"
    ],
    "date": [
      "2018"
    ],
    "type": "article-journal",
    "container-title": [
      "Yang et al"
    ]
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Natural Questions: A Benchmark for Question Answering Research\"**"
    ],
    "date": [
      "2019"
    ],
    "type": "article-journal",
    "container-title": [
      "Kwiatkowski et al"
    ]
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\"**"
    ],
    "date": [
      "2019"
    ],
    "type": "article-journal",
    "container-title": [
      "Dua et al"
    ]
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"CoQA: A Conversational Question Answering Challenge\"** - Reddy et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "author": [
      {
        "given": "BoolQ"
      }
    ],
    "title": [
      "Exploring the Surprising Difficulty of Natural Yes/No Questions\"**"
    ],
    "publisher": [
      "Clark et al"
    ],
    "date": [
      "2019"
    ],
    "type": "book"
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Yu et al"
    ]
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"TWEETQA: A Social Media Focused Question Answering Dataset\"** - Xiong et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "note": [
      "### Natural Language Inference (NLI"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"A Large Annotated Corpus for Learning Natural Language Inference\"**"
    ],
    "note": [
      "Bowman et al., 2015 (SNLI"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"MultiNLI: The Stanford Natural Language Inference Corpus\"** - Williams et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"SciTail: A Textual Entailment Dataset from Science Question Answering\"** - Khot et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Adversarial NLI: A New Benchmark for Natural Language Understanding\"** - Nie et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\"** - Zellers et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"ANLI: A New Benchmark for Natural Language Understanding\"** - Nie et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "note": [
      "### Commonsense Reasoning"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge\"** - Talmor et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"SocialIQA: Commonsense Reasoning about Social Interactions\"** - Sap et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "author": [
      {
        "literal": "**\"Winograd Schema Challenge\"** - Levesque et al."
      }
    ],
    "date": [
      "2012"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "family": "Cosmos",
        "given": "Q.A."
      }
    ],
    "title": [
      "Machine Reading Comprehension with Contextual Commonsense Reasoning\"** - Huang et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "title": [
      "These articles provide a comprehensive overview of various datasets that have been influential in advancing research in QA, NLI, and commonsense reasoning"
    ],
    "type": null
  }
]
