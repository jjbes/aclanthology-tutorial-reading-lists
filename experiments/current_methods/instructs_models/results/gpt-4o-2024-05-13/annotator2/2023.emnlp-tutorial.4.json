[
  {
    "author": [
      {
        "given": "Sure!"
      }
    ],
    "title": [
      "Here's a reading list of 20 articles up to 2023 that discuss conditioning large language models (LLMs) using task instructions, including various methods for creating such instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Language Models are Few-Shot Learners\"**"
    ],
    "publisher": [
      "Brown et al"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces GPT-3 and its ability to perform tasks with few-shot learning using task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Raffel et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the T5 model and its approach to converting all NLP tasks into a text-to-text format"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Instruction Tuning for Few-Shot Learning\"** - Wei et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores how instruction tuning can improve few-shot learning performance in LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm\"**"
    ],
    "publisher": [
      "Reynolds and McDonell"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines different prompt programming techniques for LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Unsupervised Translation of Programming Languages\"** - Lachaux et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the use of task instructions for translating between programming languages"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Multitask Prompted Training Enables Zero-Shot Task Generalization\"** - Sanh et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates how multitask prompted training can enable zero-shot generalization"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Learning to Summarize with Human Feedback\"** - Stiennon et al"
    ],
    "date": [
      "2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores the use of human feedback to improve task instructions for summarization"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Zero-Shot Text Classification with Generative Language Models\"** - Yin et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses zero-shot text classification using generative LLMs and task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Raffel et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Details the T5 model and its approach to task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Language Models as Knowledge Bases?\"** - Petroni et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates the potential of LLMs to act as knowledge bases using task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Improving Language Understanding by Generative Pre-Training\"** - Radford et al"
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Introduces GPT and discusses the use of pre-training for task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding\"** - Devlin et al"
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses BERT and its approach to pre-training and task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"UnifiedQA: Crossing Format Boundaries with a Single QA System\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Khashabi et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores a unified approach to question answering using task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Few-Shot Text Generation with Pattern-Exploiting Training\"**"
    ],
    "publisher": [
      "Schick and Sch√ºtze"
    ],
    "date": [
      "2021"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates few-shot text generation using pattern-exploiting training and task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Language Models as Few-Shot Learners\"**"
    ],
    "publisher": [
      "Brown et al"
    ],
    "date": [
      "2020"
    ],
    "type": "book"
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the few-shot learning capabilities of GPT-3 using task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "literal": "**\"Prompting GPT-3 To Be Reliable\"** - Perez et al."
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines methods to improve the reliability of GPT-3 using task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"The Power of Scale for Parameter-Efficient Prompt Tuning\"** - Lester et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Investigates parameter-efficient prompt tuning for LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Adapting Language Models for Zero-Shot Learning by Meta-Learning\"**"
    ],
    "date": [
      "2020"
    ],
    "type": "article-journal",
    "container-title": [
      "Bansal et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Explores meta-learning approaches for zero-shot learning using task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\"**"
    ],
    "date": [
      "2022"
    ],
    "type": "article-journal",
    "container-title": [
      "Ahn et al"
    ]
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Discusses the use of LLMs for zero-shot planning in embodied agents using task instructions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Prompting for a Conversation: How to Control a Dialog Model\"** - Roller et al"
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "-"
      }
    ],
    "title": [
      "Examines methods for controlling dialog models using task instructions"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a range of topics related to conditioning LLMs with task instructions, including few-shot learning, zero-shot learning, prompt tuning, and the creation of effective task instructions"
    ],
    "type": null
  }
]
