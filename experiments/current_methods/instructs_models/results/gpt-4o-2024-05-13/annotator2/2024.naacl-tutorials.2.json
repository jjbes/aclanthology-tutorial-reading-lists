[
  {
    "author": [
      {
        "given": "Certainly!"
      }
    ],
    "title": [
      "Here's a curated reading list of 20 articles up to 2024 that cover various aspects of security in large language models (LLMs), including attacks during training and inference"
    ],
    "note": [
      "as well as privacy concerns:"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Adversarial Attacks on Machine Learning Models: A Comprehensive Review\"** - This article provides an overview of adversarial attacks on machine learning models, including LLMs, and discusses various defense mechanisms"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"Privacy-Preserving Machine Learning: Threats and Solutions\"** - A detailed examination of privacy issues in machine learning, with a focus on LLMs and techniques to mitigate these concerns"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Poisoning Attacks on Machine Learning Models: A Survey\"** - This survey covers various poisoning attacks that can occur during the training phase of LLMs and discusses potential countermeasures"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"Membership Inference Attacks Against Machine Learning Models\"** - An in-depth look at how attackers can infer whether a particular data point was used in the training set of an LLM"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "**\"Model Extraction Attacks on Machine Learning Models: Taxonomy and Realization\"** - This article categorizes and explains different types of model extraction attacks, which can be particularly relevant for LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Differential Privacy in Machine Learning: A Survey and Review\"** - A comprehensive review of differential privacy techniques and their application to LLMs to protect sensitive information"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Adversarial Examples in the Physical World\"** - This paper explores how adversarial examples can be crafted to fool LLMs in real-world scenarios"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Securing Machine Learning Algorithms Against Adversarial Attacks\"** - Discusses various strategies to secure LLMs against adversarial attacks during both training and inference phases"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Federated Learning: Challenges, Methods, and Future Directions\"** - Explores the concept of federated learning and its implications for privacy and security in LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"Backdoor Attacks on Deep Learning Models: A Survey\"** - A survey of backdoor attacks that can compromise LLMs during the training phase and methods to detect and prevent them"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Data Poisoning Attacks in Federated Learning\"** - Focuses on how data poisoning attacks can be executed in federated learning environments, which are increasingly used for training LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Robustness of Deep Learning Models: A Survey\"** - Reviews the robustness of deep learning models, including LLMs, against various types of attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Privacy Attacks on Machine Learning Models: A Survey and Taxonomy\"** - Provides a taxonomy of privacy attacks on machine learning models, with a focus on LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Adversarial Training: A Review and Practical Guide\"** - Discusses adversarial training as a method to improve the robustness of LLMs against adversarial attacks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"Machine Learning Security: Threats, Countermeasures, and Evaluations\"** - A comprehensive overview of security threats to machine learning models, including LLMs, and the effectiveness of various countermeasures"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Model Inversion Attacks Against Machine Learning Models\"** - Explores how attackers can reconstruct input data from the outputs of LLMs, posing significant privacy risks"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Secure Multi-Party Computation for Machine Learning\"** - Discusses how secure multi-party computation can be used to enhance the privacy and security of LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Adversarial Robustness of Neural Networks: A Survey\"** - Reviews the state of adversarial robustness in neural networks, including LLMs, and discusses future research directions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Privacy-Preserving Deep Learning: Opportunities and Challenges\"** - Examines the opportunities and challenges in implementing privacy-preserving techniques in deep learning models, including LLMs"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Evaluating the Robustness of Neural Networks: An Empirical Study\"** - An empirical study that evaluates the robustness of various neural network architectures, including LLMs, against different types of attacks"
    ],
    "type": null
  },
  {
    "title": [
      "These articles should provide a comprehensive understanding of the current landscape of security and privacy in large language models, covering both theoretical and practical aspects"
    ],
    "type": null
  }
]
