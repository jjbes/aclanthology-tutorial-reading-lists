Sure! Here's a reading list of 20 influential articles on word embeddings in natural language processing (NLP) up to 2020. This list covers conventional embeddings, sense embeddings, graph-based embeddings, and contextual embeddings.

### Conventional Word Embeddings
1. **Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013).** "Efficient Estimation of Word Representations in Vector Space." arXiv preprint arXiv:1301.3781.
2. **Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013).** "Distributed Representations of Words and Phrases and their Compositionality." Advances in Neural Information Processing Systems (NeurIPS).
3. **Pennington, J., Socher, R., & Manning, C. D. (2014).** "GloVe: Global Vectors for Word Representation." Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).

### Sense Embeddings
4. **Huang, E. H., Socher, R., Manning, C. D., & Ng, A. Y. (2012).** "Improving Word Representations via Global Context and Multiple Word Prototypes." Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL).
5. **Neelakantan, A., Shankar, J., Passos, A., & McCallum, A. (2014).** "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space." Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).
6. **Iacobacci, I., Pilehvar, M. T., & Navigli, R. (2015).** "Sensembed: Learning Sense Embeddings for Word and Relational Similarity." Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL).

### Graph-based Embeddings
7. **Levy, O., & Goldberg, Y. (2014).** "Neural Word Embedding as Implicit Matrix Factorization." Advances in Neural Information Processing Systems (NeurIPS).
8. **Perozzi, B., Al-Rfou, R., & Skiena, S. (2014).** "DeepWalk: Online Learning of Social Representations." Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD).
9. **Grover, A., & Leskovec, J. (2016).** "node2vec: Scalable Feature Learning for Networks." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD).

### Contextual Embeddings
10. **Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018).** "Deep Contextualized Word Representations." Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).
11. **Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019).** "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).
12. **Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018).** "Improving Language Understanding by Generative Pre-Training." OpenAI.
13. **Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).** "Language Models are Unsupervised Multitask Learners." OpenAI.
14. **Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019).** "RoBERTa: A Robustly Optimized BERT Pretraining Approach." arXiv preprint arXiv:1907.11692.
15. **Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019).** "XLNet: Generalized Autoregressive Pretraining for Language Understanding." Advances in Neural Information Processing Systems (NeurIPS).
16. **Clark, K., Luong, M. T., Le, Q. V., & Manning, C. D. (2020).** "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators." Proceedings of the 8th International Conference on Learning Representations (ICLR).

### Other Types of Embeddings
17. **Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017).** "Enriching Word Vectors with Subword Information." Transactions of the Association for Computational Linguistics (TACL).
18. **Pinter, Y., Guthrie, R., & Eisenstein, J. (2017).** "Mimicking Word Embeddings using Subword RNNs." Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).
19. **Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Urtasun, R., Torralba, A., & Fidler, S. (2015).** "Skip-Thought Vectors." Advances in Neural Information Processing Systems (NeurIPS).
20. **Le, Q., & Mikolov, T. (2014).** "Distributed Representations of Sentences and Documents." Proceedings of the 31st International Conference on Machine Learning (ICML).

This list should provide a comprehensive overview of the development and variety of word embeddings in NLP up to 2020.