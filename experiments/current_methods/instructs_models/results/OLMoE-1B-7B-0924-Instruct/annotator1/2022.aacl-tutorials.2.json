[
  {
    "author": [
      {
        "given": "endoftext|><|user|>"
      }
    ],
    "type": null
  },
  {
    "title": [
      "Give me a reading list of 20 articles up to 2022 about pre-trained language models (PLMs) usage on downstream tasks under data scarcity and parameter efficiency scenarios"
    ],
    "type": null
  },
  {
    "title": [
      "<|assistant|>"
    ],
    "type": null
  },
  {
    "title": [
      "Here's a reading list of 20 articles up to 2022 that discuss pre-trained language models (PLMs) usage on downstream tasks under data scarcity and parameter efficiency scenarios"
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "author": [
      {
        "family": "Jacob Devlin",
        "particle": "by"
      },
      {
        "family": "Chang",
        "given": "Ming-Wei"
      },
      {
        "family": "Lee",
        "given": "Kenton"
      },
      {
        "family": "Toutanova",
        "given": "Kristina"
      }
    ],
    "date": [
      "2018"
    ],
    "note": [
      "This paper introduced BERT, a powerful PLM that has since been widely adopted for various NLP tasks."
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "BERT: A Beginner's Guide",
      "This article provides a comprehensive introduction to BERT and its applications"
    ],
    "author": [
      {
        "family": "Sergi Abadal",
        "particle": "by"
      },
      {
        "family": "Alarcón",
        "given": "Eduard"
      },
      {
        "family": "Alarcón",
        "given": "Eduard"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "Transformers: State-of-the-Art Natural Language Processing in Python",
      "This article provides an overview of the Transformer architecture and its implementation in the popular NLP library, Hugging Face Transformers"
    ],
    "author": [
      {
        "family": "Jacob Devlin",
        "particle": "by"
      },
      {
        "family": "Chang",
        "given": "Ming-Wei"
      },
      {
        "family": "Lee",
        "given": "Kenton"
      },
      {
        "family": "Toutanova",
        "given": "Kristina"
      }
    ],
    "date": [
      "2019"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "BERT: A Survey of Pre-training Strategies and Their Performance on Natural Language Understanding",
      "This paper provides a comprehensive survey of pre-training strategies for PLMs and their performance on various NLP tasks"
    ],
    "editor": [
      {
        "family": "Yonatan Bisk",
        "particle": "by"
      },
      {
        "family": "Goldberg",
        "given": "Yoav"
      }
    ],
    "author": [
      {
        "family": "Keshavarz",
        "given": "Nitish Shirish"
      }
    ],
    "date": [
      "2021"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "Pre-training of Language Models: A Survey",
      "This paper provides a comprehensive survey of pre-training strategies for language models"
    ],
    "author": [
      {
        "family": "Alec Radford",
        "particle": "by"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      }
    ],
    "date": [
      "2018"
    ],
    "type": null
  },
  {
    "title": [
      "These articles cover a wide range of topics related to pre-trained language models, including their applications, limitations, and recent developments. They provide valuable insights into the state of the art in PLM research and can help researchers and practitioners better understand the potential of these models for solving various NLP tasks under data scarcity and parameter efficiency scenarios.<|endoftext|>"
    ],
    "type": null
  }
]
