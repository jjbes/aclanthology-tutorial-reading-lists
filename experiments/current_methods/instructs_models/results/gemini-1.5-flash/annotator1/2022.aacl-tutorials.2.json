[{"authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton", "Toutanova, Kristina"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Howard, Jeremy", "Ruder, Sebastian"], "title": "Universal Language Model Fine-tuning for Text Classification", "year": 2018}, {"authors": ["Peters, Matthew E.", "Neumann, Mark", "Iyyer, Mohit", "Gardner, Matt", "Clark, Christopher", "Lee, Kenton", "Zettlemoyer, Luke"], "title": "Deep contextualized word representations", "year": 2018}, {"authors": ["Radford, Alec", "Narasimhan, Karthik", "Salimans, Tim", "Sutskever, Ilya"], "title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jing", "Fan, Amanda", "Jia, Zhiheng", "Chen, Danqi", "Guo, Min", "Phang, Jiasen", "Wang, Xian", "Zhang, Guoyin", "Young, Michael", "Smith, Jason", "Zhang, Jie"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Lan, Zhilin", "Chen, Mingda", "Lu, Sheng", "Wang, Xuan", "Zhang, Bin"], "title": "ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations", "year": 2019}, {"authors": ["Raffel, Colin", "Shazeer, Noam", "Roberts, Adam", "Lee, Katherine", "Narang, Sharan", "Matena, Michael", "Zhou, Yanqi", "Li, Wei", "Liu, Peter J"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"authors": ["Brown, Tom", "Mann, Benjamin", "Rytting, Nick", "Kabra, Sanjeeva", "Jafarnia, Daniel", "Faghri, Fahim", "Lewis, Scott", "Child, Ben", "Sumner, Geoffrey", "So, Christopher D.", "Chess, James", "Ganguli, Surya", "Amodei, Dario", "Zhokhov, Vlad", "Chen, Lu", "Lu,  Denny", "Perez, Chris", "Lee, Jessica", "Zettlemoyer, Luke", "Antol, Stanislaw", "Merrill,  Thomas", "Herzig, Jared", "Madaan, Aakanksha", "Slama,  Tara", "Gross,  Sasha", "Aguilar,  Rodrigo", "Leike,  Jan", "Ilyas,  Andrew", "Goh,  Geoffrey", "OpenAI", "Bard,  Jack", "Sutskever,  Ilya"], "title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Sanh, Victor", "Dehaene,  Etienne", "Strub,  Thomas", "Winata,  Genta", "Abadi,  Martin", "Duval,  Victor", "Liao,  Yacine", "Furukawa,  Thomas", "Guu,  Kelvin", "Lample,  Guillaume", "Ballesteros,  Miguel", "Dauphin,  Yann", "Conneau,  Alexis"], "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "year": 2020}, {"authors": ["Tay, Yi", "Dai,  Anh",  "Maharjan,  Sushant", "Lee,  Kenton", "Guo,  Zhenzhong", "Liu,  Yiming"], "title": "Efficient Transformers: A Survey", "year": 2020}, {"authors": ["Liu,  Xiaodong", "Zhou,  Peng", "Zhao,  Zhilin", "Wang,  Zhiyuan", "Ju,  Qiming", "Wang,  Huan", "Feng,  Zhan", "Yu,  Tao"], "title": "Pre-trained Models for Natural Language Processing: A Survey", "year": 2020}, {"authors": ["Wang,  Alex", "Singh,  Amanpreet",  "Gupta,  Julian", "Li,  Manzil", "Suganuma,  Michael", "Chen,  William", "Wang,  Lijun", "Jurafsky,  Dan"], "title": "Supervised Fine-Tuning of BERT for Extractive Question Answering", "year": 2019}, {"authors": ["Gururangan, Suchin", "Marasović, Ana", "Beltagy, Ido", "Dodge, Jacob", "Smith, Noah A.", "Downey,  David"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2020}, {"authors": ["Liu,  Weijie", "Zhou,  Peng",  "Zhao,  Zhilin", "Wang,  Zhiyuan", "Ju,  Qiming", "Wang,  Huan", "Feng,  Zhan", "Yu,  Tao"], "title": "Pre-trained Models for Natural Language Processing: A Survey", "year": 2020}, {"authors": ["Sun,  Sheng", "Wang,  Yi", "Li,  Jiajun", "Zhao,  Jun", "Wang,  Yang", "Zhou,  Michael R.", "Li,  Lei"], "title": "Patient-Specific Fine-tuning of BERT for Medical Text Classification", "year": 2020}, {"authors": ["Joshi,  Mandar", "Chen,  Wen-tau Y.", "Liu,  Yelong", "Stoyanov,  Veselin"], "title": "BERT-based Cross-Lingual Transfer for Low-Resource Text Classification", "year": 2020}, {"authors": ["Wang,  Yujia", "Liu,  Jingjing", "Wang,  Zhen", "Zhang,  Jun", "Liu,  Yelong", "Zhao,  Jun"], "title": "Few-Shot Text Classification with Pre-Trained Language Models", "year": 2020}, {"authors": ["Li,  Jiajun", "Sun,  Sheng", "Wang,  Yi", "Zhao,  Jun", "Wang,  Yang", "Zhou,  Michael R.", "Li,  Lei"], "title": "Fine-tuning Pre-trained Language Models for Medical Text Classification with Limited Labeled Data", "year": 2020}, {"authors": ["Zhang,  Xiang", "Zhao,  Jun", "LeCun,  Yann"], "title": "Character-Level Convolutional Networks for Text Classification", "year": 2015}, {"authors": ["Conneau,  Alexis", "Kiela,  Douwe", "Schwenk,  Holger", "Barrault,  Loïc", "Lecun,  Yann"], "title": "A Neural Network Approach to Context-Sensitive Meaning Representation", "year": 2017}]
