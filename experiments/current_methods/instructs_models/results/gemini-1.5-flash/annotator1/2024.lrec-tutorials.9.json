[{"authors": ["Conneau, Alexis", "Kiela, Douwe", "Schwenk, Holger", "Barrault, Lo√Øc", "Dabrowski,  Arnauld"], "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2020}, {"authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton", "Toutanova, Kristina"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Peters, Matthew E.", "Neumann, Mark", "Iyyer, Mohit", "Gardner, Matt", "Clark, Christopher", "Lee, Kenton", "Zettlemoyer, Luke"], "title": "Deep contextualized word representations", "year": 2018}, {"authors": ["Radford, Alec", "Narasimhan, Karthik", "Sutskever, Ilya", "Sohl-Dickstein, Josh"], "title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jing", "Fan, Amanda", "Jia, Zhilin", "Chen, Danqi", "Guo, Min", "Phang, James", "Zhang, Jie", "etal"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Brown, Tom", "Mann, Benjamin", "Rytting, Nick", "Kabra, Sanjeevani", "Jafarnia,  Ali", "Faghri, Fahim", "Lewis,  Scott", "Gross,  Philip", "Krishna,  Rajesh", "Shankar,  Sharan", "etal"], "title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Sanh, Victor", "Dehaene,  Etienne", "Strub,  Fanny", "Winata,  Genta", "Abadi,  Martin", "Duval,  Thomas", "Liao,  Yacine", "Furukawa,  Thomas", "Jernite,  Yoon Kim", "Dauphin,  Yann", "etal"], "title": "Multilingual BERT: Pre-training a Multilingual Language Understanding Model", "year": 2019}, {"authors": ["Clark, Kevin", "Dinu, George", "Gross,  Philip", "Hassan,  Sam", "Lee,  Kenton", "Manning,  Christopher D.", "Miller,  Taylor"], "title": "Electra: Pre-training Text Encoders as Discriminators Rather Than Generators", "year": 2020}, {"authors": ["Raffel, Colin", "Shazeer, Noam", "Roberts, Adam", "Lee, Katherine", "Narang, Sharan", "Matena,  Michael", "Zhou, Yanqi", "Li, Wei", "Liu, Peter J.", "Huang,  Zhilin", "etal"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"authors": ["Dai, Andrew M.", "Yang, Zihang", "Yang, Yiming", "Carbonell, Jaime G.", "Salakhutdinov, Ruslan"], "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "year": 2019}, {"authors": ["Wang, Alex", "Singh, Amanpreet", "Michael,  Julian", "Hill, Felix", "Levy, Omer", "Bowman, Samuel R." ], "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "year": 2018}, {"authors": ["Liu, Pengfei", "Zhou,  Peter", "Zhao,  Zhilin", "Wang, Zhiyuan", "Ju,  Qiming", "Liu,  Huan", "Zhang,  Lei", "Zhao,  Jie", "Yan,  Rui", "Wang,  Weizhu", "etal"], "title": "ERNIE 2.0: A Multilingual Pre-trained Language Model", "year": 2019}, {"authors": ["Zhang,  Xiang", "Zhao,  Junbo", "LeCun,  Yann"], "title": "Character-level Convolutional Networks for Text Classification", "year": 2015}, {"authors": ["Kim, Yoon"], "title": "Convolutional Neural Networks for Sentence Classification", "year": 2014}, {"authors": ["Mikolov, Tomas", "Sutskever, Ilya", "Chen, Kai", "Corrado, Greg S.", "Dean, Jeff"], "title": "Distributed Representations of Words and Phrases and their Compositionality", "year": 2013}, {"authors": ["Pennington, Jeffrey", "Socher, Richard",  "Manning,  Christopher D." ], "title": "Glove: Global Vectors for Word Representation", "year": 2014}, {"authors": ["Bahdanau, Dzmitry", "Cho, Kyunghyun", "Bengio, Yoshua"], "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "year": 2014}, {"authors": ["Vaswani, Ashish", "Shazeer, Noam", "Parmar, Niki", "Uszkoreit, Jakob", "Jones,  Llion", "Gomez,  Aidan N.", "Kaiser,  Lukasz", "Polosukhin,  Illia"], "title": "Attention Is All You Need", "year": 2017}, {"authors": ["Howard, Jeremy", "Goyal,  Aakash",  "Stocke,  Abhishek", "Chen,  Zhenzhong", "Htut,  Ankur", "Sachan,  Serban",  "Pennington,  Jeffrey",  "Le,  Quoc V.", "Sankar,  Ilya", "Deng,  Jason"], "title": "Universal Language Model Fine-tuning for Text Classification", "year": 2018}]
