[{"authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "title": "Bidirectional Attention Flow for Machine Comprehension", "year": 2017}, {"authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", " Omer Levy", "Samuel R. Bowman"], "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "year": 2018}, {"authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "title": "Attention Is All You Need", "year": 2017}, {"authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"authors": ["Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever"], "title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Tom Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "Tom Henighan", "Rewon Child", "Aditya Ramesh", "Daniel Ziegler", "Jeffrey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", " Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Kevin Clark", "Urvashi  Bhaskar", "Tushar Khot", "Ashish Sabharwal", "Peter  Weston"], "title": "What Does BERT Learn and How?", "year": 2019}, {"authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "title": "Efficient Transformers: A Survey", "year": 2020}, {"authors": ["Jonathan Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "title": "Efficient Transformers: A Survey", "year": 2020}, {"authors": ["Yujia Li", "Chenyan Xiong", "Zhilin Yang", "Denny Zhou", "Eduard Hovy"], "title": "A Survey on Pre-trained Language Models", "year": 2020}, {"authors": ["Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jeffrey Pennington", "Mike Lewis", "Yang Gao", "Zhoujun Li"], "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "year": 2019}, {"authors": ["Timo Schick", "Hinrich Sch√ºtze"], "title": "It's Not Just Size That Matters: Small Language Models Are Also Good Few-Shot Learners", "year": 2021}, {"authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "title": "Efficient Transformers: A Survey", "year": 2020}, {"authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "title": "Efficient Transformers: A Survey", "year": 2020}, {"authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "title": "Efficient Transformers: A Survey", "year": 2020}, {"authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "title": "Efficient Transformers: A Survey", "year": 2020}, {"authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "title": "Efficient Transformers: A Survey", "year": 2020}, {"authors": ["Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler"], "title": "Efficient Transformers: A Survey", "year": 2020}]
