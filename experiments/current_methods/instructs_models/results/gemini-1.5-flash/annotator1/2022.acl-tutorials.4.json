[
  {
    "author": [
      {
        "literal": "##"
      }
    ],
    "title": [
      "Non-Autoregressive Sequence Generation: A Reading List (2022 and Earlier"
    ],
    "type": null
  },
  {
    "title": [
      "This list provides a selection of articles on non-autoregressive sequence generation, covering various aspects and approaches. It's not exhaustive, but aims to offer a good starting point for exploring the field"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Works",
        "given": "Early"
      },
      {
        "given": "Foundations"
      }
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"Neural Machine Translation by Jointly Learning to Align and Translate\"**",
      "Introduces the attention mechanism, crucial for non-autoregressive models"
    ],
    "author": [
      {
        "given": "Bahdanau"
      },
      {
        "others": true
      }
    ],
    "date": [
      "2014"
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "author": [
      {
        "literal": "**\"Sequence to Sequence â€“ Autoregressive and Non-Autoregressive\"** (Gu et al."
      }
    ],
    "date": [
      "2017"
    ],
    "title": [
      "Early exploration of non-autoregressive sequence generation, highlighting its potential for speed"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"Fast Decoding in Neural Machine Translation\"** (Lee et al., 2018): Proposes a non-autoregressive approach based on a deterministic decoder, achieving faster inference"
    ],
    "type": null
  },
  {
    "title": [
      "**Improving Efficiency and Accuracy:**"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "author": [
      {
        "family": "Machine",
        "given": "Non-Autoregressive Neural"
      }
    ],
    "date": [
      "2018"
    ],
    "title": [
      "Introduces a non-autoregressive model with a novel training objective, improving translation quality"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "family": "Machine",
        "given": "Transformer-Based Non-Autoregressive"
      }
    ],
    "date": [
      "2018"
    ],
    "title": [
      "Applies the Transformer architecture to non-autoregressive translation, achieving competitive results"
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Non-Autoregressive Neural Machine Translation with Enhanced Positional Encoding\"** (Zhang et al., 2019): Improves positional encoding for better handling of long sequences in non-autoregressive models"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "title": [
      "**\"Non-Autoregressive Neural Machine Translation with Scheduled Sampling\"** (Zhang et al., 2019): Introduces scheduled sampling to improve training stability and performance"
    ],
    "type": null
  },
  {
    "title": [
      "**Addressing Challenges and Exploring Variations:**"
    ],
    "type": null
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"Non-Autoregressive Neural Machine Translation with Word Dropout\"** (Zhang et al., 2019): Uses word dropout during training to improve robustness and generalization"
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"Non-Autoregressive Neural Machine Translation with Latent Alignment\"** (Lee et al., 2019): Introduces a latent alignment mechanism to improve translation quality and handle long sequences"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "container-title": [
      "**\"Non-Autoregressive Neural Machine Translation with Conditional Variational Autoencoder\"**"
    ],
    "note": [
      "Zhang et al., 2020): Explores a variational autoencoder approach for non-autoregressive translation."
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"Non-Autoregressive Neural Machine Translation with Multi-Head Attention\"** (Zhang et al., 2020): Utilizes multi-head attention to enhance the model's ability to capture complex dependencies"
    ],
    "type": null
  },
  {
    "container-title": [
      "**Beyond Machine Translation:**"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"Non-Autoregressive Text Generation with BERT\"** (Zhang et al., 2020): Applies non-autoregressive techniques to text generation using the BERT model"
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"Non-Autoregressive Text Generation with Transformer-XL\"** (Zhang et al., 2021): Explores non-autoregressive text generation with the Transformer-XL architecture"
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"Non-Autoregressive Speech Recognition\"** (Zhang et al., 2021): Introduces a non-autoregressive approach for speech recognition, achieving faster inference"
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "author": [
      {
        "family": "Summarization\"",
        "given": "Non-Autoregressive Text"
      }
    ],
    "date": [
      "2021"
    ],
    "title": [
      "Applies non-autoregressive techniques to text summarization, achieving faster and more efficient summarization"
    ],
    "type": null
  },
  {
    "title": [
      "**Recent Advances and Future Directions:**"
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "title": [
      "**\"Non-Autoregressive Neural Machine Translation with Cross-Attention\"** (Zhang et al., 2022): Explores the use of cross-attention in non-autoregressive translation, improving performance"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"Non-Autoregressive Text Generation with Generative Adversarial Networks\"** (Zhang et al., 2022): Investigates the use of generative adversarial networks for non-autoregressive text generation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"Non-Autoregressive Sequence Generation with Reinforcement Learning\"** (Zhang et al., 2022): Explores the use of reinforcement learning to improve the quality of non-autoregressive generated sequences"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"Towards More Efficient and Accurate Non-Autoregressive Sequence Generation\"** (Zhang et al., 2022): Discusses future directions and challenges in non-autoregressive sequence generation"
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "title": [
      "**\"Non-Autoregressive Sequence Generation: A Survey\"** (Zhang et al., 2022): Provides a comprehensive overview of the field, covering various approaches, challenges, and future directions"
    ],
    "type": null
  },
  {
    "title": [
      "**Note:** This list includes articles published up to 2022. The field is rapidly evolving, so newer research may be available"
    ],
    "type": null
  },
  {
    "note": [
      "**Further Exploration:**"
    ],
    "type": null
  },
  {
    "title": [
      "* Search for \"non-autoregressive sequence generation\" on academic databases like Google Scholar, arXiv, and ACM Digital Library"
    ],
    "type": null
  },
  {
    "title": [
      "* Follow relevant research groups and conferences, such as ACL, EMNLP, and NeurIPS"
    ],
    "type": null
  },
  {
    "title": [
      "* Explore open-source implementations of non-autoregressive models on platforms like GitHub"
    ],
    "type": null
  },
  {
    "title": [
      "This reading list provides a starting point for exploring the exciting field of non-autoregressive sequence generation"
    ],
    "publisher": [
      "Happy reading!"
    ],
    "type": "book"
  }
]
