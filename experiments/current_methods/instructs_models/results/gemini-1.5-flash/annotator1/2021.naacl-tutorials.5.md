## Reading List: Long Sequences & Document-Level Representation Learning (up to 2021)

This list covers a range of topics, from foundational work to recent advancements in long sequence and document-level representation learning. It includes both theoretical and practical aspects, with a focus on deep learning approaches.

**Foundational Work:**

1. **"Recurrent Neural Networks" by Elman (1990):** A seminal work introducing the concept of recurrent neural networks (RNNs) for processing sequential data.
2. **"Long Short-Term Memory" by Hochreiter & Schmidhuber (1997):** Introduces the LSTM architecture, addressing the vanishing gradient problem in RNNs and enabling learning long-term dependencies.
3. **"A Neural Probabilistic Language Model" by Bengio et al. (2003):** Demonstrates the effectiveness of neural networks for language modeling, paving the way for modern NLP applications.
4. **"Distributed Representations of Words and Phrases and their Compositionality" by Mikolov et al. (2013):** Introduces word2vec, a popular method for learning distributed word representations.
5. **"Sequence to Sequence Learning with Neural Networks" by Sutskever et al. (2014):** Introduces the encoder-decoder framework for sequence-to-sequence learning, enabling tasks like machine translation.

**Long Sequence Modeling:**

6. **"Attention Is All You Need" by Vaswani et al. (2017):** Introduces the Transformer architecture, which uses attention mechanisms to process long sequences without relying on recurrent connections.
7. **"Universal Transformers" by Dehghani et al. (2019):** Extends the Transformer architecture to handle arbitrarily long sequences by introducing a novel attention mechanism.
8. **"Reformer: The Efficient Transformer" by Kitaev et al. (2020):** Proposes a more efficient Transformer architecture by using locality-sensitive hashing and reversible layers.
9. **"Longformer: The Long-Document Transformer" by Beltagy et al. (2020):** Introduces a Transformer variant with a novel attention mechanism that allows it to process long documents efficiently.
10. **"BigBird: Transformers for Longer Sequences" by Zaheer et al. (2020):** Presents a Transformer architecture that uses a sparse attention mechanism to handle long sequences.

**Document-Level Representation Learning:**

11. **"Sentence Embeddings: A Survey" by Arora et al. (2017):** Provides a comprehensive overview of different approaches for learning sentence embeddings.
12. **"Universal Sentence Encoder" by Cer et al. (2018):** Introduces a pre-trained model for learning sentence embeddings that can be used for various downstream tasks.
13. **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" by Devlin et al. (2018):** Introduces BERT, a powerful pre-trained language model that achieves state-of-the-art results on various NLP tasks.
14. **"XLNet: Generalized Autoregressive Pretraining for Language Understanding" by Yang et al. (2019):** Presents XLNet, a pre-trained language model that uses a generalized autoregressive approach to learn better representations.
15. **"RoBERTa: A Robustly Optimized BERT Pretraining Approach" by Liu et al. (2019):** Introduces RoBERTa, a variant of BERT that achieves improved performance through better training techniques.
16. **"Document-Level Representation Learning with Transformers" by Liu et al. (2020):** Explores the use of Transformers for learning document-level representations, achieving state-of-the-art results on various tasks.
17. **"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks" by Reimers & Gurevych (2019):** Introduces Sentence-BERT, a method for learning sentence embeddings using Siamese BERT networks.
18. **"A Survey of Document Embeddings" by Arora et al. (2021):** Provides a comprehensive overview of different approaches for learning document embeddings.

**Applications:**

19. **"Neural Machine Translation by Jointly Learning to Align and Translate" by Bahdanau et al. (2014):** Demonstrates the use of attention mechanisms for machine translation.
20. **"Deep Learning for Text Summarization: A Comprehensive Survey" by Nallapati et al. (2017):** Provides a comprehensive overview of deep learning approaches for text summarization.

This list is not exhaustive, but it provides a good starting point for exploring the field of long sequences and document-level representation learning. You can find more articles and resources by searching online databases like Google Scholar and arXiv.