[
  {
    "title": [
      "## Neural Language Generation: Deep Contextual Models & Transfer Learning (Pre-2020"
    ],
    "type": null
  },
  {
    "title": [
      "This reading list focuses on articles published before 2020 that explore the intersection of deep contextual models and transfer learning in neural language generation (NLG"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**1."
    ],
    "title": [
      "** **Sequence to Sequence Learning with Neural Networks**"
    ],
    "note": [
      "Sutskever et al., 2014"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces the foundational Seq2Seq architecture for NLG, paving the way for deep contextual models"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**2."
    ],
    "author": [

    ],
    "title": [
      "**Neural Machine Translation by Jointly Learning to Align and Translate**"
    ],
    "note": [
      "Bahdanau et al., 2014"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces the attention mechanism, enabling models to focus on relevant parts of the input sequence, crucial for capturing context"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**3."
    ],
    "note": [
      "** **Neural Machine Translation in Linear Time** (Luong et al., 2015"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Proposes efficient attention mechanisms, improving the speed and performance of NLG models"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1508.04025](https://arxiv.org/abs/1508.04025"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**4."
    ],
    "note": [
      "** **Effective Approaches to Attention-Based Neural Machine Translation** (Luong et al., 2015"
    ],
    "type": null
  },
  {
    "title": [
      "* **Contribution:** Explores different attention mechanisms and their impact on NLG performance"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1508.04025](https://arxiv.org/abs/1508.04025"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**5."
    ],
    "note": [
      "** **Neural Machine Translation with Attention** (Bahdanau et al., 2015"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Further develops the attention mechanism, demonstrating its effectiveness in capturing long-range dependencies"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**6."
    ],
    "title": [
      "** **A Neural Probabilistic Language Model**"
    ],
    "note": [
      "Bengio et al., 2003"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces the concept of neural language models, laying the groundwork for deep contextual models"
    ],
    "type": null
  },
  {
    "author": [
      {
        "literal": "*"
      }
    ],
    "url": [
      "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf",
      "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**7."
    ],
    "title": [
      "** **Recurrent Neural Network Based Language Model**"
    ],
    "note": [
      "Mikolov et al., 2010"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces RNNs for language modeling, enabling the capture of sequential dependencies in text"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://www.researchgate.net/publication/221252988_Recurrent_Neural_Network_Based_Language_Model](https://www.researchgate.net/publication/221252988_Recurrent_Neural_Network_Based_Language_Model"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**8."
    ],
    "author": [

    ],
    "title": [
      "**Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation**"
    ],
    "note": [
      "Cho et al., 2014"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces the encoder-decoder framework for NLG, enabling the representation of input sequences as fixed-length vectors"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**9."
    ],
    "author": [

    ],
    "title": [
      "**Neural Machine Translation by Jointly Learning to Align and Translate**"
    ],
    "note": [
      "Bahdanau et al., 2014"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces the attention mechanism, enabling models to focus on relevant parts of the input sequence, crucial for capturing context"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**10."
    ],
    "note": [
      "** **Attention Is All You Need** (Vaswani et al., 2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces the Transformer architecture, a powerful deep contextual model that revolutionized NLG"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**11."
    ],
    "title": [
      "** **Deep Neural Networks for YouTube Recommendations**"
    ],
    "note": [
      "Covington et al., 2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Demonstrates the effectiveness of deep learning for recommendation systems, a related field to NLG"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**12."
    ],
    "note": [
      "** **Neural Machine Translation with Learned Universal Sentence Representations** (Lample et al., 2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Explores the use of universal sentence representations for NLG, enabling transfer learning across different tasks"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1703.03906](https://arxiv.org/abs/1703.03906"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**13."
    ],
    "note": [
      "** **Universal Sentence Encoder** (Cer et al., 2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces a pre-trained sentence encoder that can be used for various NLG tasks, facilitating transfer learning"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1803.11175](https://arxiv.org/abs/1803.11175"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**14."
    ],
    "author": [
      {
        "given": "B.E.R.T."
      }
    ],
    "title": [
      "Pre-training of Deep Bidirectional Transformers for Language Understanding**"
    ],
    "note": [
      "Devlin et al., 2018"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces BERT, a powerful pre-trained language model that significantly improves NLG performance"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**15."
    ],
    "author": [

    ],
    "title": [
      "**GPT-2: Language Modeling at Scale**"
    ],
    "note": [
      "Radford et al., 2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces GPT-2, a large-scale language model that demonstrates impressive NLG capabilities"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**16."
    ],
    "author": [
      {
        "given": "XLNet"
      }
    ],
    "title": [
      "Generalized Autoregressive Pretraining for Language Understanding**"
    ],
    "note": [
      "Yang et al., 2019"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces XLNet, a pre-trained language model that outperforms BERT in various NLG tasks"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**17."
    ],
    "author": [

    ],
    "title": [
      "**Transfer Learning from Pre-trained Language Models for Neural Machine Translation**"
    ],
    "note": [
      "Britz et al., 2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Demonstrates the effectiveness of transfer learning from pre-trained language models for NLG tasks like machine translation"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1703.03906](https://arxiv.org/abs/1703.03906"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**18."
    ],
    "author": [

    ],
    "title": [
      "**Neural Machine Translation with Language-Specific Encoder and Decoder**"
    ],
    "note": [
      "Lee et al., 2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Explores the use of language-specific encoders and decoders for NLG, improving performance in cross-lingual tasks"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1703.03906](https://arxiv.org/abs/1703.03906"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**19."
    ],
    "note": [
      "** **Zero-Shot Text Generation** (Hu et al., 2017"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Explores the concept of zero-shot text generation, where models can generate text in unseen domains without explicit training"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1703.03906](https://arxiv.org/abs/1703.03906"
    ],
    "type": null
  },
  {
    "citation-number": [
      "**20."
    ],
    "title": [
      "** **Neural Text Generation with Content Planning**"
    ],
    "note": [
      "Lebret et al., 2016"
    ],
    "type": null
  },
  {
    "author": [
      {
        "given": "Contribution"
      }
    ],
    "title": [
      "Introduces content planning techniques for NLG, enabling models to generate more coherent and informative text"
    ],
    "type": null
  },
  {
    "note": [
      "* **Link:** [https://arxiv.org/abs/1603.06393](https://arxiv.org/abs/1603.06393"
    ],
    "type": null
  },
  {
    "title": [
      "This list provides a starting point for exploring the exciting field of neural language generation with deep contextual models and transfer learning. Remember to explore further based on your specific interests and research goals"
    ],
    "type": null
  }
]
