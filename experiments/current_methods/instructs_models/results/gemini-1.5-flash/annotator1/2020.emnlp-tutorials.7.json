[{"authors": ["Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever"], "title": "Language models are unsupervised multitask learners", "year": 2019}, {"authors": ["Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova"], "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "year": 2018}, {"authors": ["Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer"], "title": "Deep contextualized word representations", "year": 2018}, {"authors": ["Dai, Andrew M., and Quoc V. Le"], "title": "Semi-supervised sequence learning", "year": 2015}, {"authors": ["Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio"], "title": "Neural machine translation by jointly learning to align and translate", "year": 2014}, {"authors": ["Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le"], "title": "Sequence to sequence learning with neural networks", "year": 2014}, {"authors": ["Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean"], "title": "Distributed representations of words and phrases and their compositionality", "year": 2013}, {"authors": ["Collobert, Ronan, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa"], "title": "Natural language processing (nlp) with deep learning", "year": 2011}, {"authors": ["Bengio, Yoshua, Réjean Ducharme, Pascal Vincent, and Christian Janvin"], "title": "A neural probabilistic language model", "year": 2003}, {"authors": ["Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al.", "Language models are few-shot learners"], "year": 2020}, {"authors": ["Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, et al.", "Exploring the limits of transfer learning with a unified text-to-text transformer"], "year": 2020}, {"authors": ["Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, et al.", "Roberta: A robustly optimized bert pretraining approach"], "year": 2019}, {"authors": ["Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le"], "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "year": 2019}, {"authors": ["Howard, Jeremy, and Sebastian Ruder"], "title": "Universal language model fine-tuning for text classification", "year": 2018}, {"authors": ["Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman"], "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "year": 2018}, {"authors": ["Conneau, Alexis, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes"], "title": "A study of cross-lingual word embedding mappings", "year": 2018}, {"authors": ["McCann, Bryan, James Bradbury, Caiming Xiong, and Richard Socher"], "title": "Learned in translation: Contextualized word vectors", "year": 2017}, {"authors": ["Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, et al.", "Attention is all you need"], "year": 2017}, {"authors": ["Kingma, Diederik P., and Jimmy Ba"], "title": "Adam: A method for stochastic optimization", "year": 2014}, {"authors": ["Goodfellow, Ian, Yoshua Bengio, and Aaron Courville"], "title": "Deep learning", "year": 2016}]
