[{"authors": ["Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever"], "title": "Language Models are Unsupervised Multitask Learners", "year": 2019}, {"authors": ["Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin"], "title": "Attention Is All You Need", "year": 2017}, {"authors": ["Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"], "title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Martin R. Oswald, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"], "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "year": 2020}, {"authors": ["Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov"], "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"authors": ["Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Kevin Clark, Minh-Thang Luong, Quoc V. Le"], "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "year": 2020}, {"authors": ["Yujia Li, Daniel Cer, Omer Levy, Mike Lewis"], "title": "Universal Sentence Encoder", "year": 2018}, {"authors": ["Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"], "title": "Deep contextualized word representations", "year": 2018}, {"authors": ["Sebastian Ruder, Ivan Vulić, Anders Søgaard"], "title": "A Survey of Cross-Lingual Pre-trained Language Models", "year": 2020}, {"authors": ["Iz Beltagy, Matthew E. Peters, Arman Cohan"], "title": "Longformer: The Long-Document Transformer", "year": 2020}, {"authors": ["Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao"], "title": "Pre-Training with Whole Word Masking for Chinese BERT", "year": 2019}, {"authors": ["Tianyu Gao, Xingxing Zhang, Jingjing Li, Yelong Shen, Xu Sun, Zhengdong Lu, Wei Lu"], "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "year": 2021}, {"authors": ["Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Li, Zhengdong Lu, Donald Metzler"], "title": "Efficient Transformers: A Survey", "year": 2020}, {"authors": ["Nils Reimers, Iryna Gurevych"], "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "year": 2019}, {"authors": ["David R. So, Chenyan Xiong, Jamie Callan"], "title": "A Survey of Large Language Models", "year": 2022}, {"authors": ["Wei Lu, Yelong Shen, Jingjing Li, Xu Sun, Wei Li, Zhoujun Li, Honglak Lee, Jianfeng Gao"], "title": "Understanding and Improving Information Retrieval with Pre-trained Language Models", "year": 2021}, {"authors": ["Jonathan Tay, Mostafa Dehghani, Samira Abnar, Yikang Li, Zhengdong Lu, Donald Metzler"], "title": "Efficient Transformers: A Survey", "year": 2020}, {"authors": ["Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever"], "title": "Language Models are Unsupervised Multitask Learners", "year": 2019}]
