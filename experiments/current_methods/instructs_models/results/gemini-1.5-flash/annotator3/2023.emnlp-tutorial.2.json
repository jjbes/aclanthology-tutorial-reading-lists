[{"authors": ["Nicholas Carlini", "David Wagner"], "title": "Towards Evaluating the Robustness of Deep Learning", "year": 2017}, {"authors": ["Anish Athalye", "Nicholas Carlini", "David Wagner"], "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples", "year": 2018}, {"authors": ["Micah Goldblum", "Jamie Hayes", "Bo Li", "Sergey Levine", "Dawn Song"], "title": "Spurious Correlations in Neural Networks", "year": 2020}, {"authors": ["Yossi Adi", "Jonathan Berant", "Jacob Goldberger", "Ofer Shapira", "Ori Ram"], "title": "Fine-grained Analysis of Sentence Embeddings for Textual Adversarial Attacks", "year": 2019}, {"authors": ["Moustapha Cisse", "Yossi Adi", "Eduardo Peixoto", "Tal Hassner"], "title": "Robustness via Curriculum Learning", "year": 2017}, {"authors": ["Nicholas Carlini", "David Wagner"], "title": "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods", "year": 2017}, {"authors": ["Florian Tramèr", "Nicolas Papernot", "Ian Goodfellow", "Dan Boneh", "Patrick McDaniel"], "title": "The Space of Transferable Adversarial Examples", "year": 2017}, {"authors": ["Aleksander Madry", "Aleksander Makelov", "Ludwig Schmidt", "Dimitris Papailiopoulos", "Abbas El Gamal"], "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}, {"authors": ["Ian Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "title": "Explaining and Harnessing Adversarial Examples", "year": 2014}, {"authors": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "title": "Intriguing properties of neural networks", "year": 2013}, {"authors": ["Nicholas Carlini", "David Wagner"], "title": "Defensive Distillation: A Simple and Effective Defense Against Adversarial Examples", "year": 2016}, {"authors": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z. Berkay Celik", "Ananthram Swami"], "title": "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks", "year": 2016}, {"authors": ["Florian Tramèr", "Aditya Talwar", "Ian Goodfellow", "Dan Boneh", "Patrick McDaniel"], "title": "Stealing Machine Learning Models via Gradient Leakage", "year": 2016}, {"authors": ["Matthew Jagielski", "Jonathan Ullman", "Alina Oprea", "Arvind Narayanan"], "title": "Auditing Membership Inference Attacks Against Machine Learning Models", "year": 2020}, {"authors": ["Jonas Geiping", "Micah Goldblum", "Jonas Rauber", "Thomas Bock", "Michael Moeller", "Florian Tramèr"], "title": "Inverting Gradients - How easy is it to steal a model?", "year": 2020}, {"authors": ["Battista Biggio", "Igino Corona", "Giorgio Giacinto", "Benjamin Nelson", "Paolo L.  Rizzo"], "title": "Poisoning Attacks Against Support Vector Machines", "year": 2012}, {"authors": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z. Berkay Celik", "Ananthram Swami"], "title": "The Limitations of Deep Learning in Adversarial Settings", "year": 2016}, {"authors": ["Florian Tramèr", "Dan Boneh", "Patrick McDaniel"], "title": "Adversarial Training and Robustness for Multiple Perturbations", "year": 2017}, {"authors": ["Aleksander Madry", "Jamie Hayes", "Thomas P.  Dietrich", "George  Vorontsov", "Andrew  Stevens"], "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}, {"authors": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z. Berkay Celik", "Ananthram Swami"], "title": "Practical Black-box Attacks Against Machine Learning", "year": 2017}]
