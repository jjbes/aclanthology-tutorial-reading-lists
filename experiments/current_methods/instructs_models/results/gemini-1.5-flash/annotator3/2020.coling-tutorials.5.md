## Reading List: Datasets for QA, NLI, and Commonsense Reasoning (up to 2020)

**Question Answering (QA)**

1. **"SQuAD: 100,000+ Questions for Machine Comprehension of Text"** (Rajpurkar et al., 2016) - Introduces the Stanford Question Answering Dataset (SQuAD), a large-scale reading comprehension dataset.
2. **"TriviaQA: A Large Scale Distantly Supervised Question Answering Dataset"** (Joshi et al., 2017) - Presents TriviaQA, a dataset with diverse question types and answer sources.
3. **"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"** (Yang et al., 2018) - Introduces HotpotQA, a dataset requiring multi-hop reasoning to answer questions.
4. **"The NarrativeQA Reading Comprehension Challenge"** (Kociský et al., 2018) - Describes NarrativeQA, a dataset focused on answering questions about stories.
5. **"NQ: Open Domain Question Answering with a Large-Scale, Diverse Dataset"** (Kwiatkowski et al., 2019) - Introduces NQ, a large-scale dataset for open-domain question answering.
6. **"Natural Questions: A Benchmark for Question Answering Research"** (Dua et al., 2019) - Presents Natural Questions, a dataset with real user questions from Google Search.
7. **"The Open Book Question Answering Dataset"** (Clark et al., 2018) - Introduces OpenBookQA, a dataset designed to assess commonsense reasoning in QA.

**Natural Language Inference (NLI)**

8. **"SNLI: A Large Dataset for Natural Language Inference"** (Bowman et al., 2015) - Introduces the Stanford Natural Language Inference (SNLI) dataset, a large-scale dataset for NLI.
9. **"MultiNLI: A New Dataset for Natural Language Inference"** (Williams et al., 2018) - Presents MultiNLI, a dataset with more diverse sentence pairs and genres than SNLI.
10. **"The SciTail Dataset for Scientific Natural Language Inference"** (Kociský et al., 2018) - Introduces SciTail, a dataset specifically designed for NLI in scientific text.
11. **"RTE: A Corpus for Evaluating NLP Systems: An Experiment Using the RTE Test Collection"** (Dagan et al., 2005) - Presents the Recognizing Textual Entailment (RTE) dataset, an early dataset for NLI.
12. **"The FEVER Dataset: A Large-Scale Dataset for Fact Verification"** (Thorne et al., 2018) - Introduces FEVER, a dataset for fact verification, which can be considered a form of NLI.

**Commonsense Reasoning**

13. **"ATOMIC: A Dataset for Reasoning about Actions, Entities, and Time"** (Sap et al., 2019) - Introduces ATOMIC, a dataset focused on commonsense reasoning about actions, entities, and time.
14. **"The Commonsense Reasoning Corpus (CoSMoS): A Large-Scale Dataset for Commonsense Reasoning"** (Talmor et al., 2019) - Presents CoSMoS, a large-scale dataset for commonsense reasoning.
15. **"The Social IQA Dataset: A Benchmark for Commonsense Reasoning in Social Interactions"** (Rashkin et al., 2019) - Introduces Social IQA, a dataset focused on commonsense reasoning in social interactions.
16. **"The Winograd Schema Challenge"** (Levesque et al., 2011) - Presents the Winograd Schema Challenge, a benchmark for evaluating commonsense reasoning in language understanding.
17. **"The bAbI Dataset: Towards AI-Complete Question Answering"** (Weston et al., 2015) - Introduces bAbI, a dataset designed to assess reasoning abilities in QA.
18. **"The Story Cloze Test: A New Challenge for Open-Ended Commonsense Reasoning"** (Mostafazadeh et al., 2016) - Presents the Story Cloze Test, a dataset for evaluating commonsense reasoning in story comprehension.
19. **"The CommonsenseQA Dataset: A Question Answering Benchmark for Commonsense Reasoning"** (Talmor et al., 2019) - Introduces CommonsenseQA, a dataset specifically designed for commonsense reasoning in QA.
20. **"The ConceptNet Numberbatch Dataset"** (Speer et al., 2017) - Presents ConceptNet Numberbatch, a dataset containing a large number of commonsense knowledge triples.

**Note:** This list is not exhaustive and there are many other datasets available for QA, NLI, and commonsense reasoning. This list provides a starting point for exploring these datasets and their applications in NLP research.