[
  {
    "note": [
      "## Reading List: Datasets for QA, NLI, and Commonsense Reasoning (up to 2020"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Answering",
        "given": "Question"
      }
    ],
    "type": null
  },
  {
    "citation-number": [
      "1."
    ],
    "title": [
      "**\"SQuAD: 100,000+ Questions for Machine Comprehension of Text\"**"
    ],
    "note": [
      "Rajpurkar et al., 2016) - Introduces the Stanford Question Answering Dataset (SQuAD), a large-scale reading comprehension dataset."
    ],
    "type": null
  },
  {
    "citation-number": [
      "2."
    ],
    "title": [
      "**\"TriviaQA: A Large Scale Distantly Supervised Question Answering Dataset\"** (Joshi et al., 2017) - Presents TriviaQA, a dataset with diverse question types and answer sources"
    ],
    "type": null
  },
  {
    "citation-number": [
      "3."
    ],
    "title": [
      "**\"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\"** (Yang et al., 2018) - Introduces HotpotQA, a dataset requiring multi-hop reasoning to answer questions"
    ],
    "type": null
  },
  {
    "citation-number": [
      "4."
    ],
    "title": [
      "**\"The NarrativeQA Reading Comprehension Challenge\"** (Kociský et al., 2018) - Describes NarrativeQA, a dataset focused on answering questions about stories"
    ],
    "type": null
  },
  {
    "citation-number": [
      "5."
    ],
    "author": [
      {
        "given": "N.Q."
      }
    ],
    "title": [
      "Open Domain Question Answering with a Large-Scale, Diverse Dataset\"**"
    ],
    "note": [
      "Kwiatkowski et al., 2019) - Introduces NQ, a large-scale dataset for open-domain question answering."
    ],
    "type": null
  },
  {
    "citation-number": [
      "6."
    ],
    "title": [
      "**\"Natural Questions: A Benchmark for Question Answering Research\"** (Dua et al., 2019) - Presents Natural Questions, a dataset with real user questions from Google Search"
    ],
    "type": null
  },
  {
    "citation-number": [
      "7."
    ],
    "author": [
      {
        "family": "Dataset\"",
        "given": "The Open Book Question Answering"
      }
    ],
    "date": [
      "2018"
    ],
    "title": [
      "- Introduces OpenBookQA, a dataset designed to assess commonsense reasoning in QA"
    ],
    "type": null
  },
  {
    "container-title": [
      "**Natural Language Inference (NLI)**"
    ],
    "type": "chapter"
  },
  {
    "citation-number": [
      "8."
    ],
    "title": [
      "**\"SNLI: A Large Dataset for Natural Language Inference\"**"
    ],
    "note": [
      "Bowman et al., 2015) - Introduces the Stanford Natural Language Inference (SNLI) dataset, a large-scale dataset for NLI."
    ],
    "type": null
  },
  {
    "citation-number": [
      "9."
    ],
    "title": [
      "**\"MultiNLI: A New Dataset for Natural Language Inference\"** (Williams et al., 2018) - Presents MultiNLI, a dataset with more diverse sentence pairs and genres than SNLI"
    ],
    "type": null
  },
  {
    "citation-number": [
      "10."
    ],
    "title": [
      "**\"The SciTail Dataset for Scientific Natural Language Inference\"**"
    ],
    "note": [
      "Kociský et al., 2018) - Introduces SciTail, a dataset specifically designed for NLI in scientific text."
    ],
    "type": null
  },
  {
    "citation-number": [
      "11."
    ],
    "title": [
      "**\"RTE: A Corpus for Evaluating NLP Systems: An Experiment Using the RTE Test Collection\"**"
    ],
    "note": [
      "Dagan et al., 2005) - Presents the Recognizing Textual Entailment (RTE) dataset, an early dataset for NLI."
    ],
    "type": null
  },
  {
    "citation-number": [
      "12."
    ],
    "title": [
      "**\"The FEVER Dataset: A Large-Scale Dataset for Fact Verification\"** (Thorne et al., 2018) - Introduces FEVER, a dataset for fact verification, which can be considered a form of NLI"
    ],
    "type": null
  },
  {
    "author": [
      {
        "family": "Reasoning",
        "given": "Commonsense"
      }
    ],
    "type": null
  },
  {
    "citation-number": [
      "13."
    ],
    "title": [
      "**\"ATOMIC: A Dataset for Reasoning about Actions, Entities, and Time\"**"
    ],
    "note": [
      "Sap et al., 2019) - Introduces ATOMIC, a dataset focused on commonsense reasoning about actions, entities, and time."
    ],
    "type": null
  },
  {
    "citation-number": [
      "14."
    ],
    "title": [
      "**\"The Commonsense Reasoning Corpus (CoSMoS): A Large-Scale Dataset for Commonsense Reasoning\"**"
    ],
    "note": [
      "Talmor et al., 2019) - Presents CoSMoS, a large-scale dataset for commonsense reasoning."
    ],
    "type": null
  },
  {
    "citation-number": [
      "15."
    ],
    "title": [
      "**\"The Social IQA Dataset: A Benchmark for Commonsense Reasoning in Social Interactions\"**"
    ],
    "note": [
      "Rashkin et al., 2019) - Introduces Social IQA, a dataset focused on commonsense reasoning in social interactions."
    ],
    "type": null
  },
  {
    "citation-number": [
      "16."
    ],
    "author": [
      {
        "family": "Challenge\"",
        "given": "The Winograd Schema"
      }
    ],
    "date": [
      "2011"
    ],
    "title": [
      "- Presents the Winograd Schema Challenge, a benchmark for evaluating commonsense reasoning in language understanding"
    ],
    "type": null
  },
  {
    "citation-number": [
      "17."
    ],
    "title": [
      "**\"The bAbI Dataset: Towards AI-Complete Question Answering\"** (Weston et al., 2015) - Introduces bAbI, a dataset designed to assess reasoning abilities in QA"
    ],
    "type": null
  },
  {
    "citation-number": [
      "18."
    ],
    "title": [
      "**\"The Story Cloze Test: A New Challenge for Open-Ended Commonsense Reasoning\"** (Mostafazadeh et al., 2016) - Presents the Story Cloze Test, a dataset for evaluating commonsense reasoning in story comprehension"
    ],
    "type": null
  },
  {
    "citation-number": [
      "19."
    ],
    "title": [
      "**\"The CommonsenseQA Dataset: A Question Answering Benchmark for Commonsense Reasoning\"**"
    ],
    "note": [
      "Talmor et al., 2019) - Introduces CommonsenseQA, a dataset specifically designed for commonsense reasoning in QA."
    ],
    "type": null
  },
  {
    "citation-number": [
      "20."
    ],
    "author": [
      {
        "family": "Dataset\"",
        "given": "The ConceptNet Numberbatch"
      }
    ],
    "date": [
      "2017"
    ],
    "title": [
      "- Presents ConceptNet Numberbatch, a dataset containing a large number of commonsense knowledge triples"
    ],
    "type": null
  },
  {
    "title": [
      "**Note:** This list is not exhaustive and there are many other datasets available for QA, NLI, and commonsense reasoning. This list provides a starting point for exploring these datasets and their applications in NLP research"
    ],
    "type": null
  }
]
