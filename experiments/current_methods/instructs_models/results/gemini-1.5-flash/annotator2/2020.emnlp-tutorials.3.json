[{"authors": ["Li, Jiwei", "Chen, Xin", "Hovy, Eduard", "Jurafsky, Dan"], "title": "Visualizing and Understanding Neural Models in NLP", "year": 2016}, {"authors": ["Belinkov, Yonatan", "Bisk, Yonatan"], "title": "Neural Machine Translation and the Curse of Dimensionality", "year": 2017}, {"authors": ["Jain, Sarthak", "Bhattacharya, Piyush", "Chang, Michael W.", "Das, Dipanjan"], "title": "Attention is not Explanation: Interpreting Attention Mechanisms", "year": 2019}, {"authors": ["Wiegreffe, Sarah", "Papp, Gabor", "Clark, Peter"], "title": "Attention is not Explanation: A Critical Review of Attention Mechanisms in NLP", "year": 2020}, {"authors": ["Liu, Pengfei", "Lee, Jie", "Huang, Xiaoxiao", "Zhao, Zhiyuan", "Zhou, Min", "Zhao, Lei"], "title": "Towards Better Understanding of Black-box Predictions in Text Classification", "year": 2018}, {"authors": ["Yogatama, Daniel", "Smith, Noah A.", "Dhingra, Bhaskar", "Cohen, William W.", "Jurafsky, Dan"], "title": "Learning to Explain: An Information-Theoretic Perspective on Explanatory Neural Networks", "year": 2019}, {"authors": ["Rudin, Cynthia"], "title": "Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead", "year": 2019}, {"authors": ["Ribeiro, Marco Tulio", "Singh, Sameer", "Guestrin, Carlos"], "title": "Why Should I Trust You?: Explaining the Predictions of Any Classifier", "year": 2016}, {"authors": ["Lundberg, Scott M.", "Lee, Su-In"], "title": "A Unified Approach to Interpreting Model Predictions", "year": 2017}, {"authors": ["Zeiler, Matthew D.", " Fergus, Rob"], "title": "Visualizing and Understanding Convolutional Networks", "year": 2014}, {"authors": ["Simonyan, Karen", "Vedaldi, Andrea", "Zisserman, Andrew"], "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "year": 2013}, {"authors": ["Bahdanau, Dzmitry", "Cho, Kyunghyun", "Bengio, Yoshua"], "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "year": 2014}, {"authors": ["Vaswani, Ashish", "Shazeer, Noam", "Parmar, Niki", "Uszkoreit, Jakob", "Jones, Llion", "Gomez, Aidan N.", "Kaiser, Lukasz", "Polosukhin, Illia"], "title": "Attention Is All You Need", "year": 2017}, {"authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton", "Toutanova, Kristina"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"authors": ["Radford, Alec", "Narasimhan, Karthik", "Salimans, Tim", "Sutskever, Ilya"], "title": "Improving Language Understanding by Generative Pre-Training", "year": 2018}, {"authors": ["Brown, Tom B.", "Mann, Benjamin", "Rytting, Nick", "Kabra, Sanjeeva", "Jafarnia, Daniel", "Faghri, Fahim", "Lewis, Scott", "Child, Ben", "Sumner, Geoffrey", "So, Christopher D.", "Chess, James", "Ganguli, Surya", "Dhariwal, Prafulla", "Amodei, Dario", "Sutskever, Ilya"], "title": "Language Models are Few-Shot Learners", "year": 2020}, {"authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", " Joshi, Mandar", "Chen, Danqi", "Levy, Omer", "Lewis, Mike", "Zettlemoyer, Luke"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"authors": ["Clark, Kevin", "Doran, Christopher", "Grover, Alexander", "Kwiatkowski, Tom", "Lee, Kensen", "Lewis, Michael", "Zettlemoyer, Luke"], "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "year": 2020}, {"authors": ["Dai, Andrew M.", "Yang, Zihang", "Yang, Yiming", "Carbonell, Jaime G.", "Salakhutdinov, Ruslan"], "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "year": 2019}, {"authors": ["Raffel, Colin", "Shazeer, Noam", "Roberts, Adam", "Lee, Katherine", "Narang, Sharan", "Matena, Michael", "Zhou, Yanqi", "Li, Wei", "Liu, Peter J.", "Huang, Jiasen", "Deng, Jiameng", "Yu, Zhifeng", "Deng, Xiaobing"], "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}]
