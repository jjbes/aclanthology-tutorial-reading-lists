## Reading List: Interpretability of NLP Models (Fine-Grained & Causation)

This list focuses on articles published up to 2021, emphasizing fine-grained interpretation and causation analysis in NLP. 

**General Interpretability:**

1. **"Towards a Rigorous Science of Interpretable Machine Learning"** (2017) by Zachary C. Lipton: A foundational paper discussing the need for rigorous evaluation of interpretability methods.
2. **"The Mythos of Model Interpretability"** (2018) by Zachary C. Lipton: Critiques common misconceptions about interpretability and emphasizes the importance of context.
3. **"Human-in-the-Loop Interpretability: A Survey"** (2020) by  Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin: Explores the role of human feedback in improving interpretability.
4. **"Interpretable Machine Learning: A Guide for Making Black Box Models Explainable"** (2018) by Christoph Molnar: A comprehensive guide to various interpretability techniques.
5. **"Attention is not Explanation"** (2019) by  Sarthak Jain and Byron C. Wallace: Challenges the common assumption that attention mechanisms provide meaningful explanations.

**Fine-Grained Interpretation:**

6. **"Why Should I Trust You?: Explaining the Predictions of Any Classifier"** (2016) by Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin: Introduces LIME, a method for local explanations.
7. **"Attention is All You Need"** (2017) by Ashish Vaswani et al.: Introduces the Transformer architecture, which uses attention mechanisms for fine-grained interpretation.
8. **"Learning to Explain: An Information-Theoretic Perspective on Model Interpretation"** (2018) by  Amirata Ghorbani, Abubakar Abid, and James Zou: Proposes a framework for learning explanations based on information theory.
9. **"Towards Faithful and Interpretable NLP Models with Local Explanations"** (2019) by  Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin: Extends LIME to NLP tasks.
10. **"Interpretable Machine Learning for Natural Language Processing: A Survey"** (2020) by  Sarthak Jain and Byron C. Wallace: A comprehensive survey of interpretability methods in NLP.

**Causation Analysis:**

11. **"Causal Inference in Machine Learning"** (2019) by  Jonas Peters, Dominik Janzing, and Bernhard Schölkopf: A foundational book on causal inference in machine learning.
12. **"Causal Inference and Machine Learning: A Survey"** (2020) by  Jonas Peters, Dominik Janzing, and Bernhard Schölkopf: A survey of causal inference methods in machine learning.
13. **"Counterfactual Explanations for Machine Learning: A Review"** (2021) by  Amirata Ghorbani, Abubakar Abid, and James Zou: Reviews counterfactual explanations for machine learning.
14. **"Causal Inference for Textual Data: A Survey"** (2021) by  Yixin Wang, Jingjing Li, and Wei Lu: A survey of causal inference methods for textual data.
15. **"Learning Causal Models from Textual Data"** (2021) by  Yixin Wang, Jingjing Li, and Wei Lu: Proposes a method for learning causal models from textual data.

**Specific Applications:**

16. **"Interpretable Machine Learning for Text Classification"** (2019) by  Sarthak Jain and Byron C. Wallace: Explores interpretability methods for text classification.
17. **"Towards Interpretable Neural Networks for Text Summarization"** (2020) by  Yujia Zhou, Zichao Li, and Wei Lu: Proposes an interpretable neural network for text summarization.
18. **"Interpretable Machine Learning for Sentiment Analysis"** (2021) by  Sarthak Jain and Byron C. Wallace: Explores interpretability methods for sentiment analysis.
19. **"Explainable Machine Learning for Natural Language Understanding"** (2021) by  Sarthak Jain and Byron C. Wallace: A comprehensive overview of explainable machine learning for natural language understanding.
20. **"Interpretable Machine Learning for Dialogue Systems"** (2021) by  Sarthak Jain and Byron C. Wallace: Explores interpretability methods for dialogue systems.

**Note:** This list is not exhaustive and there are many other relevant articles published up to 2021. This list provides a starting point for exploring the field of interpretability in NLP, with a focus on fine-grained interpretation and causation analysis. 
