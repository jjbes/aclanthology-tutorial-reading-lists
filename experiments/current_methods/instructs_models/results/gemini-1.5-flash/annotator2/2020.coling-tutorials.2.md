## Reading List: Word Embeddings in NLP (up to 2020)

This list covers conventional, sense, graph, and contextual embeddings, aiming to provide a comprehensive overview of the field.

**Conventional Embeddings:**

1. **Distributed Representations of Words and Phrases and their Compositionality** (Mikolov et al., 2013): Introduces Word2Vec, a popular method for learning word embeddings.
2. **GloVe: Global Vectors for Word Representation** (Pennington et al., 2014): Presents GloVe, another widely used word embedding model based on global word co-occurrence statistics.
3. **Efficient Estimation of Word Representations in Vector Space** (Mikolov et al., 2013): Discusses the CBOW and Skip-gram architectures used in Word2Vec.
4. **Neural Word Embedding as Implicit Matrix Factorization** (Levy & Goldberg, 2014): Analyzes the relationship between word embeddings and matrix factorization.
5. **FastText: Efficient Learning of Word Representations** (Bojanowski et al., 2016): Introduces FastText, an extension of Word2Vec that considers subword information.

**Sense Embeddings:**

6. **Learning Distributed Representations of Sentences from Unlabelled Data** (Le & Mikolov, 2014): Explores sentence embeddings using a bag-of-words approach.
7. **A Neural Probabilistic Language Model** (Bengio et al., 2003): Introduces a neural language model that learns word representations.
8. **Learning to Represent Sentences** (Socher et al., 2011): Proposes a recursive neural network for learning sentence representations.
9. **A Simple but Tough-to-Beat Baseline for Sentence Embeddings** (Conneau et al., 2017): Presents a simple yet effective baseline for sentence embedding using a mean of word embeddings.
10. **Universal Sentence Encoder** (Cer et al., 2018): Introduces a transformer-based model for learning sentence embeddings.

**Graph Embeddings:**

11. **DeepWalk: Online Learning of Social Representations** (Perozzi et al., 2014): Introduces DeepWalk, a method for learning graph embeddings using random walks.
12. **LINE: Large-scale Information Network Embedding** (Tang et al., 2015): Presents LINE, another graph embedding method that considers both first-order and second-order proximities.
13. **Node2Vec: Scalable Feature Learning for Networks** (Grover & Leskovec, 2016): Introduces Node2Vec, a flexible framework for learning graph embeddings by combining BFS and DFS.
14. **Graph Convolutional Networks for Text Classification** (Yao et al., 2019): Explores the use of graph convolutional networks for text classification.
15. **Attributed Graph Embedding** (Zhang et al., 2018): Discusses graph embedding methods that incorporate node attributes.

**Contextual Embeddings:**

16. **Neural Machine Translation by Jointly Learning to Align and Translate** (Bahdanau et al., 2014): Introduces the attention mechanism in neural machine translation.
17. **Attention Is All You Need** (Vaswani et al., 2017): Presents the Transformer architecture, which relies solely on attention mechanisms.
18. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (Devlin et al., 2018): Introduces BERT, a powerful language model that learns contextualized word representations.
19. **XLNet: Generalized Autoregressive Pretraining for Language Understanding** (Yang et al., 2019): Presents XLNet, another powerful language model that improves upon BERT by using a generalized autoregressive approach.
20. **RoBERTa: A Robustly Optimized BERT Pretraining Approach** (Liu et al., 2019): Introduces RoBERTa, an optimized version of BERT that achieves state-of-the-art results on various NLP tasks.

This list provides a starting point for exploring the vast field of word embeddings in NLP. It is important to note that this is not an exhaustive list, and many other important works exist. 

**Further Exploration:**

* **ACL Anthology:** A comprehensive database of NLP research papers.
* **arXiv:** A repository of preprints in various fields, including NLP.
* **Google Scholar:** A search engine for academic literature.

By exploring these resources, you can delve deeper into the fascinating world of word embeddings and their applications in NLP.