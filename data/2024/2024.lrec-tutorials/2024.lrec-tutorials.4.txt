Reading List

1. Surveys on evaluation of LLMs
(Chang et al., 2023;
Ziyu et al., 2023; 
Gehrmann et al., 2023)

2. Pre-training paradigms
(Min et al., 2023)

3. Current benchmarks:
HELM (Liang et al., 2022), 
big-bench (Srivastava et al., 2022),
LM-evaluation-harness (Gao et al., 2021)

4. Prompts:
creating paraphrases (Lester et al.,2021; Gonen et al., 2022; Honovich et al., 2022), 
robustness to paraphrases (Gu et al., 2022; Sun et al., 2023; Mizrahi et al., 2024)

5. Metrics:
survey (Sai et al., 2022), 
models as evaluators (Zheng et al., 2023)

6. Efficient-benchmarking:
(Perlitz et al., 2023a;
Vivek et al., 2023; 
Liang et al., 2022),

7. Manual Evaluation:
survey (Bojic et al., 2023),
reproducibility (Belz et al., 2023)