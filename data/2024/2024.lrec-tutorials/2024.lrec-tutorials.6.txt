Reading list  

• “A Comprehensive Study of Knowledge Editing for Large Language Models”, (Zhang et al., 2024) 
• “Editing Large Language Models: Problems, Methods, and Opportunities”, (Yao et al., 2023b) 
• “Detoxifying Large Language Models via Knowledge Editing”, (Wang et al., 2024a) 
• “Editing Conceptual Knowledge for Large Lan- guage Models”, (Wang et al., 2024b) 
• “Evaluating the Ripple Effects of Knowledge Editing in Language Models”, (Cohen et al., 2023a) 
• “Can We Edit Multimodal Large Language Models?”, (Cheng et al., 2023a) 
• “Unveiling the Pitfalls of Knowledge Editing for Large Language Models”, (Li et al., 2023) 
• “Editing Personality for LLMs”, (Mao et al., 2023) 
• “Editing Language Model-based Knowledge Graph Embeddings”, (Cheng et al., 2023b) 
• “Memory-Based Model Editing at Scale”, (Mitchell et al., 2022c) 
• “Calibrating Factual Knowledge in Pretrained Language Models”, (Dong et al., 2022) 
• “Transformer-Patcher: One Mistake worth One Neuron”, (Huang et al., 2023) 
• “Can We Edit Factual Knowledge by In-Context Learning?”, (Zheng et al., 2023) 
• “Editing Factual Knowledge in Language Mod- els”, (Cao et al., 2021) 
• “Fast Model Editing at Scale”, (Mitchell et al., 2022a) 
• “Knowledge Neurons in Pretrained Transform- ers”, (Dai et al., 2022a) 
• “Locating and Editing Factual Associations in GPT”, (Meng et al., 2022a) 
• “Mass-Editing Memory in a Transformer”, (Meng et al., 2023) 
• “MQUAKE: Assessing Knowledge Editing in- Language Models via Multi-Hop Questions”, (Zhong et al., 2023) 
• “Can LMs Learn New Entities from Descrip- tions? Challenges in Propagating Injected Knowledge”, (Gupta et al., 2023) 
• “Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark”, (Hoelscher-Obermaier et al., 2023) 
• “Editing Commonsense Knowledge in GPT”, (Gupta et al., 2023)