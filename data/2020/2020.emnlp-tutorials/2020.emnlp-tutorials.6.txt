Small Reading List  

Only the last two (33%) were co-authored by the presenters.  

• Alvin Grissom II, He He, Jordan Boyd- Graber, John Morgan, and Hal Daum´e III, Don’t Until the Final Verb Wait: Reinforce- ment Learning for Simultaneous Machine Translation, EMNLP 2014. When source and target language have drastically word orders difference, e.g., from verb-final languages (German) to verb- medial languages (English), the final inal verb is predicted in advance on source side to avoid long latency.
• He He, Alvin Grissom II, Jordan Boyd- Graber and Hal Daum´e III, Syntax-based Rewriting for Simultaneous Machine Trans- lation, EMNLP 2015. A sentence rewriting method is proposed to generates more monotonic translations to improve the speed-accuracy tradeof. Sev- eral grammaticality and meaning-preserving syntactic transformation rules are applied to paraphrase reference translations to make their word order closer to the source language word order.
• Kyunghyun Cho and Masha Esipova, Can neural machine translation do simultaneous translation?, arXiv:1606.02012, 2016. Several waiting criteria are manually de- signed to serve as translation polices to de- cide wait or read.
• Jiatao Gu, Graham Neubig, Kyunghyun Cho and Victor O.K. Li, Learning to Translate in Real-time with Neural Machine Translation, EACL 2017. The authors proposed a NMT framework for simultaneous translation with a agent which learn to make decisions on when to translate or wait by interacting with a pre- trained NMT environment.
• Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu and Haifeng Wang, STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework, ACL 2019. Prefix-to-prefix framework is proposed for simultaneous translation which implic- itly learns to anticipate in a single translation model. Within this framework, “wait-k” pol- icy is trained to generate the target sentence simultaneously with the source sentence with k word delay.
• Naveen Arivazhagan, Colin Cherry, Wolf- gang Macherey, Chung-Cheng Chiu, Semih Yavuz, Ruoming Pang Wei Li and Colin Raf- fel, Monotonic Infinite Lookback Attention for Simultaneous Machine Translation, ACL 2019. A Monotonic Infinite Lookback (MILk) technique is proposed to maintain both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head to extend from the monotonic head back to the beginning of the source. MILk is trained to learn a adaptive schedule by balancing the latency-quality trade-offs.