Small Reading List.

1. (Gatt and Krahmer, 2018): traditional methods on natural language generation
2. (Radford et al., 2019): large-scale language models as unsupervised multitask learners with generative capabilities
3. (Khandelwal et al., 2019): example high lighting the rise of pretrained language models for neural text generation
4. (Holtzman et al., 2019): studying the dramatic effect of decoding strategies on the quality of machine text
5. (Kusner et al., 2015): going beyond n-gram matching, using representation learning to evaluate generation
6. (Ranzato et al., 2015): introduction to exposure bias and training with sequence-level objective functions
7. (Bowman et al., 2016): variational autoencoders for language generation
8. (Holtzman et al., 2018): designing neural networks as scoring functions during decoding