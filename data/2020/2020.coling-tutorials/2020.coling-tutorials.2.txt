Small reading list

In addition to the main reference book (Pilehvar and Camacho-Collados, 2020), in the following we present some references that may be helpful for understanding the tutorial. Nonetheless, they are not required to read in advance as their main ideas are also covered as part of the tutorial.

• Schütze (1992): Dimensions of meaning
• Turney and Pantel (2010): From Frequency to Meaning: Vector Space Models of Semantics
• Mikolov et al. (2013): Efficient Estimation of Word Representations in Vector Space
• Pennington et al. (2014): GloVe: Global vectors for word representation
• Melamud et al. (2016): Context2vec: Learning Generic Context Embedding with Bidirectional LSTM
• Peters et al. (2018a): Deep contextualized word representations
• Camacho-Collados and Pilehvar (2018): From word to sense embeddings: A survey on vector representations of meaning
• Peters et al. (2018b): Dissecting Contextual Word Embeddings: Architecture and Representation
• Devlin et al. (2018): BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding