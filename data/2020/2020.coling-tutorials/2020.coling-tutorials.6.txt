Prerequisites and reading list

A basic knowledge of machine learning (e.g. familiarity with precision, recall, F-score metrics, and ensemble methods) and neural approaches to natural language processing (e.g. the concept of sequence-to-sequence encoder-decoder neural models) will be helpful to understand the content of the tutorial.

Suggested reading list:

1. Automated grammatical error detection for language learners. Leacock, Chodorow, Gamon, and Tetreault (2010).
2. The CoNLL-2014 shared task on grammatical error correction. Ng, Wu, Briscoe, Hadiwinoto, Susanto, and Bryant (2014).
3. Building a state-of-the-art grammatical error correction system. Rozovskaya and Roth (2014).
4. Phrase-based machine translation is state-of-the-art for automatic grammatical error correction. Junczys-Dowmunt and Grundkiewicz (2016).
5. A multilayer convolutional encoder-decoder neural network for grammatical error correction. Chollampatt and Ng (2018).
6. Neural grammatical error correction systems with unsupervised pre-training on synthetic data. Grundkiewicz, Junczys-Dowmunt, and Heafield (2019).
7. An empirical study of incorporating pseudo data into grammatical error correction. Kiyono, Suzuki, Mita, Mizumoto, and Inui (2019).