{
    "2021.naacl-tutorials.1": {
        "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
        "author": "Andrew Yates, Rodrigo Nogueira, Jimmy Lin",
        "year": 2021,
        "url": "https://aclanthology.org/2021.naacl-tutorials.1",
        "doi": "10.18653/v1/2021.naacl-tutorials.1",
        "abstract": "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly.",
        "readingList": [
            {
                "sectionName": null,
                "subsectionName": null,
                "referencesIds": []
            }
        ]
    }
}