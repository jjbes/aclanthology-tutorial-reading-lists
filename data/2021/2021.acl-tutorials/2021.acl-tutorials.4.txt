Prerequisites

The tutorial is self-contained. We will address the background, the technical details and the examples. Basic knowledge about neural networks are required, including word embeddings, attention, and encoder-decoder models. Prior NLP courses and familarity with the machine translation task are preferred.
It is recommended (and optional) that audience to read the following papers before the tutorial:

1. Basic MT model: Attention is all you need (Vaswani et al., 2017).
2. Googleâ€™s multilingual neural machine translation system (Johnson et al., 2017).
3. Text pre-training with BERT (Devlin et al., 2019) and GPT (Radford et al., 2019).
4. Audio pre-training with Wav2vec and Wav2vec2.0 (Schneider et al., 2019; Baevski et al., 2020b).
5. Pre-training multilingual NMT (Lin et al., 2020; Liu et al., 2020).