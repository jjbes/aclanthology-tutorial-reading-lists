{
    "2023.ijcnlp-tutorials.4": {
        "title": "Editing Large Language Models",
        "author": "Ningyu Zhang, Yunzhi Yao, Shumin Deng",
        "year": 2023,
        "url": "https://aclanthology.org/2023.ijcnlp-tutorials.4",
        "doi": "10.18653/v1/2023.ijcnlp-tutorials.4",
        "abstract": "Even with their impressive abilities, Large Language Models (LLMs) such as ChatGPT are not immune to issues of factual or logically consistent. Concretely, the key concern is how to seamlessly update those LLMs to correct mistakes without resorting to an exhaustive retraining or continuous training procedure, both of which can demand significant computational resources and time. Thus, the capability to edit LLMs offers an efficient solution to alter a modelâ€™s behavior, notably within a distinct area of interest, without negatively impacting its performance on other tasks. Through this tutorial, we strive to acquaint interested NLP researchers with recent and emerging techniques for editing LLMs. Specifically, we aim to present a systematic and current overview of cutting-edge methods, supplemented with practical tools, and unveil new research opportunities for our audiences. All the valuable resources can be accessed at https://github.com/zjunlp/KnowledgeEditingPapers.",
        "readingList": [
            {
                "sectionName": null,
                "subsectionName": null,
                "referencesIds": [
                    258833129,
                    249642147,
                    252762125,
                    256194369,
                    258832407,
                    233289412,
                    239050360,
                    233296761,
                    255825985,
                    252873467,
                    258865984,
                    258437155,
                    258960406,
                    258865393
                ]
            }
        ]
    }
}