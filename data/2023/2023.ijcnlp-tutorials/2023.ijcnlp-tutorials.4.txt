Reading list

• “Editing Large Language Models: Problems, Methods, and Opportunities”, (Yao et al., 2023b)
• “Memory-Based Model Editing at Scale”, (Mitchell et al., 2022b)
• “Calibrating Factual Knowledge in Pretrained Language Models”, (Dong et al., 2022)
• “Transformer-Patcher: One Mistake worth One Neuron”, (Huang et al., 2023)
• “Can We Edit Factual Knowledge by In-Context Learning?”, (Zheng et al., 2023)
• “Editing Factual Knowledge in Language Models”, (De Cao et al., 2021)
• “Fast Model Editing at Scale”, (Mitchell et al., 2022a)
• “Knowledge Neurons in Pretrained Transformers”, (Dai et al., 2022)
• “Locating and Editing Factual Associations in GPT”, (Meng et al., 2022)
• “Mass-Editing Memory in a Transformer”, (Meng et al., 2023)
• “MQUAKE: Assessing Knowledge Editing in-Language Models via Multi-Hop Questions”, (Zhong et al., 2023)
• “Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge”, (Gupta et al., 2023)
• “Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark”, (Hoelscher-Obermaier et al., 2023)
• “Editing Commonsense Knowledge in GPT”, (Gupta et al., 2023)