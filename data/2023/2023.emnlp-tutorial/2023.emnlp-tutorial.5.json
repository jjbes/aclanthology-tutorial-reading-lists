{
    "2023.emnlp-tutorial.5": {
        "title": "Mitigating Societal Harms in Large Language Models",
        "author": "Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos, Yulia Tsvetkov",
        "year": 2023,
        "url": "https://aclanthology.org/2023.emnlp-tutorial.5",
        "doi": "10.18653/v1/2023.emnlp-tutorial.5",
        "abstract": "Numerous recent studies have highlighted societal harms that can be caused by language technologies deployed in the wild. While several surveys, tutorials, and workshops have discussed the risks of harms in specific contexts – e.g., detecting and mitigating gender bias in NLP models – no prior work has developed a unified typology of technical approaches for mitigating harms of language generation models. Our tutorial is based on a survey we recently wrote that proposes such a typology. We will provide an overview of potential social issues in language generation, including toxicity, social biases, misinformation, factual inconsistency, and privacy violations. Our primary focus will be on how to systematically identify risks, and how eliminate them at various stages of model development, from data collection, to model development, to inference/language generation. Through this tutorial, we aim to equip NLP researchers and engineers with a suite of practical tools for mitigating safety risks from pretrained language generation models.",
        "readingList": [
            {
                "sectionName": null,
                "subsectionName": null,
                "referencesIds": []
            }
        ]
    }
}
