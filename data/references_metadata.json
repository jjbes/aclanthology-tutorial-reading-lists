{
  "8360910": {
    "paperId": "37efe2ef1b9d27cc598361a8013ec888a6f7c4d8",
    "externalIds": {
      "MAG": "1608322251",
      "DBLP": "conf/emnlp/BaroniZ10",
      "ACL": "D10-1115",
      "CorpusId": 8360910
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space",
    "abstract": "We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2010,
    "referenceCount": 36,
    "citationCount": 541,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145283199",
        "name": "Marco Baroni"
      },
      {
        "authorId": "2713535",
        "name": "Roberto Zamparelli"
      }
    ]
  },
  "5917203": {
    "paperId": "228d9e4b69926594fd26080f4cfaa9ecfca44eb3",
    "externalIds": {
      "DBLP": "journals/corr/abs-1003-4394",
      "MAG": "2793391459",
      "ArXiv": "1003.4394",
      "CorpusId": 5917203
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Mathematical Foundations for a Compositional Distributional Model of Meaning",
    "abstract": "We propose a mathematical framework for a unification of the distributional theory of meaning in terms of vector space models, and a compositional theory for grammatical types, for which we rely on the algebra of Pregroups, introduced by Lambek. This mathematical framework enables us to compute the meaning of a well-typed sentence from the meanings of its constituents. Concretely, the type reductions of Pregroups are `lifted' to morphisms in a category, a procedure that transforms meanings of constituents into a meaning of the (well-typed) whole. Importantly, meanings of whole sentences live in a single space, independent of the grammatical structure of the sentence. Hence the inner-product can be used to compare meanings of arbitrary sentences, as it is for comparing the meanings of words in the distributional model. The mathematical structure we employ admits a purely diagrammatic calculus which exposes how the information flows between the words in a sentence in order to make up the meaning of the whole sentence. A variation of our `categorical model' which involves constraining the scalars of the vector spaces to the semiring of Booleans results in a Montague-style Boolean-valued semantics.",
    "venue": "arXiv.org",
    "year": 2010,
    "referenceCount": 44,
    "citationCount": 545,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3326718",
        "name": "B. Coecke"
      },
      {
        "authorId": "1784777",
        "name": "M. Sadrzadeh"
      },
      {
        "authorId": "144523372",
        "name": "S. Clark"
      }
    ]
  },
  "11691908": {
    "paperId": "167d0e6bbb4199764773e7fb77882ce64e586e89",
    "externalIds": {
      "DBLP": "conf/coling/KartsaklisSP12",
      "MAG": "1549789254",
      "ACL": "C12-2054",
      "CorpusId": 11691908
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "A Unified Sentence Space for Categorical Distributional-Compositional Semantics: Theory and Experiments",
    "abstract": "This short paper summarizes a faithful implementation of the categorical framework of Coecke et al. (2010), the aim of which is to provide compositionality in distributional models of lexical semantics. Based on Frobenius Algebras, our method enable us to (1) have a unifying meaning space for phrases and sentences of different structure and word vectors, (2) stay faithful to the linguistic types suggested by the underlying type-logic, and (3) perform the concrete computations in lower dimensions by reducing the space complexity. We experiment with two different parameters of the model and apply the setting to a verb disambiguation and a term/definition classification task with promising results.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2012,
    "referenceCount": 25,
    "citationCount": 74,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2940780",
        "name": "Dimitri Kartsaklis"
      },
      {
        "authorId": "1784777",
        "name": "M. Sadrzadeh"
      },
      {
        "authorId": "50419262",
        "name": "S. Pulman"
      }
    ]
  },
  "26901423": {
    "paperId": "745d86adca56ec50761591733e157f84cfb19671",
    "externalIds": {
      "DBLP": "journals/cogsci/MitchellL10",
      "MAG": "1984052055",
      "DOI": "10.1111/j.1551-6709.2010.01106.x",
      "CorpusId": 26901423,
      "PubMed": "21564253"
    },
    "publicationVenue": {
      "id": "c33b01b0-31b4-470e-a9f9-8432e02c3cb9",
      "name": "Cognitive Sciences",
      "type": "journal",
      "alternate_names": [
        "Cognitive Science",
        "Cogn Sci"
      ],
      "issn": "1935-8059",
      "alternate_issns": [
        "0364-0213"
      ],
      "url": "http://www.informaworld.com/openurl?genre=journal&issn=1551-6709",
      "alternate_urls": [
        "http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1551-6709",
        "http://www3.interscience.wiley.com/cgi-bin/jtoc?ID=121670282",
        "https://onlinelibrary.wiley.com/journal/15516709",
        "http://www.sciencedirect.com/science/journal/03640213",
        "http://www.leaonline.com/loi/cog"
      ]
    },
    "title": "Composition in Distributional Models of Semantics",
    "abstract": "Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words. This article proposes a framework for representing the meaning of word combinations in vector space. Central to our approach is vector composition, which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models that we evaluate empirically on a phrase similarity task.",
    "venue": "Cognitive Sciences",
    "year": 2010,
    "referenceCount": 249,
    "citationCount": 987,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34902160",
        "name": "Jeff Mitchell"
      },
      {
        "authorId": "1747893",
        "name": "Mirella Lapata"
      }
    ]
  },
  "806709": {
    "paperId": "27e38351e48fe4b7da2775bf94341738bc4da07e",
    "externalIds": {
      "DBLP": "conf/emnlp/SocherHMN12",
      "MAG": "1889268436",
      "ACL": "D12-1110",
      "CorpusId": 806709
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces",
    "abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2012,
    "referenceCount": 44,
    "citationCount": 1380,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2166511",
        "name": "R. Socher"
      },
      {
        "authorId": "2570381",
        "name": "Brody Huval"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      },
      {
        "authorId": "34699434",
        "name": "A. Ng"
      }
    ]
  },
  "1500900": {
    "paperId": "3a0e788268fafb23ab20da0e98bb578b06830f7d",
    "externalIds": {
      "MAG": "1662133657",
      "ArXiv": "1003.1141",
      "DBLP": "journals/jair/TurneyP10",
      "DOI": "10.1613/jair.2934",
      "CorpusId": 1500900
    },
    "publicationVenue": {
      "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
      "name": "Journal of Artificial Intelligence Research",
      "type": "journal",
      "alternate_names": [
        "JAIR",
        "J Artif Intell Res",
        "The Journal of Artificial Intelligence Research"
      ],
      "issn": "1076-9757",
      "url": "http://www.jair.org/"
    },
    "title": "From Frequency to Meaning: Vector Space Models of Semantics",
    "abstract": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.",
    "venue": "Journal of Artificial Intelligence Research",
    "year": 2010,
    "referenceCount": 199,
    "citationCount": 2941,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1689647",
        "name": "Peter D. Turney"
      },
      {
        "authorId": "1990190",
        "name": "Patrick Pantel"
      }
    ]
  },
  "16196219": {
    "paperId": "b9921fb4d1448058642897797e77bdaf8f444404",
    "externalIds": {
      "MAG": "2095655043",
      "DOI": "10.1093/pan/mps028",
      "CorpusId": 16196219
    },
    "publicationVenue": {
      "id": "171758eb-39e1-4665-a00d-dd65ec7d465e",
      "name": "Political Analysis",
      "type": "journal",
      "alternate_names": [
        "Political Anal"
      ],
      "issn": "1047-1987",
      "url": "https://www.cambridge.org/core/journals/political-analysis",
      "alternate_urls": [
        "http://www.jstor.org/action/showPublication?journalCode=polianalysis",
        "https://www.jstor.org/journal/polianalysis"
      ]
    },
    "title": "Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts",
    "abstract": "Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods\u2014they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.",
    "venue": "Political Analysis",
    "year": 2013,
    "referenceCount": 109,
    "citationCount": 2480,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Political Science",
        "source": "external"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2361828",
        "name": "Justin Grimmer"
      },
      {
        "authorId": "28924497",
        "name": "Brandon M Stewart"
      }
    ]
  },
  "10274824": {
    "paperId": "7d9cc63dfbd34acf271e3a2c922ea1c07fb2f482",
    "externalIds": {
      "MAG": "2009659525",
      "DOI": "10.1017/S0003055403000698",
      "CorpusId": 10274824
    },
    "publicationVenue": {
      "id": "849d78f8-8160-4ca1-8cd8-84253f6abdf6",
      "name": "American Political Science Review",
      "type": "journal",
      "alternate_names": [
        "Am Political Sci Rev"
      ],
      "issn": "0003-0554",
      "url": "https://www.apsanet.org/apsr",
      "alternate_urls": [
        "https://www.cambridge.org/core/journals/american-political-science-review",
        "http://journals.cambridge.org/action/displayJournal?jid=PSR",
        "https://www.jstor.org/journal/amerpoliscierevi",
        "http://www.apsanet.org/apsr",
        "http://journals.cambridge.org/jid_PSR",
        "http://www.jstor.org/journals/00030554.html"
      ]
    },
    "title": "Extracting Policy Positions from Political Texts Using Words as Data",
    "abstract": "We present a new way of extracting policy positions from political texts that treats texts not as discourses to be understood and interpreted but rather, as data in the form of words. We compare this approach to previous methods of text analysis and use it to replicate published estimates of the policy positions of political parties in Britain and Ireland, on both economic and social policy dimensions. We \u201cexport\u201d the method to a non-English-language environment, analyzing the policy positions of German parties, including the PDS as it entered the former West German party system. Finally, we extend its application beyond the analysis of party manifestos, to the estimation of political positions from legislative speeches. Our \u201clanguage-blind\u201d word scoring technique successfully replicates published policy estimates without the substantial costs of time and labor that these require. Furthermore, unlike in any previous method for extracting policy positions from political texts, we provide uncertainty measures for our estimates, allowing analysts to make informed judgments of the extent to which differences between two estimated policy positions can be viewed as significant or merely as products of measurement error.We thank Raj Chari, Gary King, Michael McDonald, Gail McElroy, and three anonymous reviewers for comments on drafts of this paper.",
    "venue": "American Political Science Review",
    "year": 2003,
    "referenceCount": 35,
    "citationCount": 1276,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Sociology",
        "source": "external"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143758665",
        "name": "M. Laver"
      },
      {
        "authorId": "26916605",
        "name": "K. Benoit"
      },
      {
        "authorId": "80157164",
        "name": "John Garry"
      }
    ]
  },
  "17026162": {
    "paperId": "5109c519cd4442041a5d3915ca305eba6d68ee10",
    "externalIds": {
      "MAG": "2245173016",
      "DOI": "10.1111/J.1540-5907.2008.00338.X",
      "CorpusId": 17026162
    },
    "publicationVenue": null,
    "title": "A Scaling Model for Estimating Time-Series Party Positions from Texts",
    "abstract": "However, existing text-based methods face challenges in producing valid and reliable time-series data. This article proposes a scaling algorithm called WORDFISH to estimate policy positions based on word frequencies in texts. The technique allows researchers to locate parties in one or multiple elections. We demonstrate the algorithm by estimating the positions of German political parties from 1990 to 2005 using word frequencies in party manifestos. The extracted positions reflect changes in the party system more accurately than existing time-series estimates. In addition, the method allows researchers to examine which words are important for placing parties on the left and on the right. We find that words with strong political connotations are the best discriminators between parties. Finally, a series of robustness checks demonstrate that the estimated positions are insensitive to distributional assumptions and document selection.",
    "venue": "",
    "year": 2007,
    "referenceCount": 53,
    "citationCount": 678,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Political Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "70665044",
        "name": "Jonathan B. Slapin"
      },
      {
        "authorId": "145688599",
        "name": "Sven-Oliver Proksch"
      }
    ]
  },
  "56657817": {
    "paperId": "668f42a4d4094f0a66d402a16087e14269b31a1f",
    "externalIds": {
      "DBLP": "journals/corr/abs-1812-08951",
      "ACL": "Q19-1004",
      "MAG": "2954730351",
      "ArXiv": "1812.08951",
      "DOI": "10.1162/tacl_a_00254",
      "CorpusId": 56657817
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Analysis Methods in Neural Language Processing: A Survey",
    "abstract": "The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 205,
    "citationCount": 511,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "145898106",
        "name": "James R. Glass"
      }
    ]
  },
  "5013113": {
    "paperId": "f170fed9acd71bd5feb20901c7ec1fe395f3fae5",
    "externalIds": {
      "ArXiv": "1711.10203",
      "MAG": "2963430224",
      "DBLP": "journals/corr/abs-1711-10203",
      "DOI": "10.1613/jair.1.11196",
      "CorpusId": 5013113
    },
    "publicationVenue": {
      "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
      "name": "Journal of Artificial Intelligence Research",
      "type": "journal",
      "alternate_names": [
        "JAIR",
        "J Artif Intell Res",
        "The Journal of Artificial Intelligence Research"
      ],
      "issn": "1076-9757",
      "url": "http://www.jair.org/"
    },
    "title": "Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure",
    "abstract": "We investigate how neural networks can learn and process languages with hierarchical, compositional semantics. To this end, we define the artificial task of processing nested arithmetic expressions, and study whether different types of neural networks can learn to compute their meaning. We find that recursive neural networks can find a generalising solution to this problem, and we visualise this solution by breaking it up in three steps: project, sum and squash. As a next step, we investigate recurrent neural networks, and show that a gated recurrent unit, that processes its input incrementally, also performs very well on this task. To develop an understanding of what the recurrent network encodes, visualisation techniques alone do not suffice. Therefore, we develop an approach where we formulate and test multiple hypotheses on the information encoded and processed by the network. For each hypothesis, we derive predictions about features of the hidden state representations at each time step, and train 'diagnostic classifiers' to test those predictions. Our results indicate that the networks follow a strategy similar to our hypothesised 'cumulative strategy', which explains the high accuracy of the network on novel expressions, the generalisation to longer expressions than seen in training, and the mild deterioration with increasing length. This is turn shows that diagnostic classifiers can be a useful technique for opening up the black box of neural networks. We argue that diagnostic classification, unlike most visualisation techniques, does scale up from small networks in a toy domain, to larger and deeper recurrent networks dealing with real-life data, and may therefore contribute to a better understanding of the internal dynamics of current state-of-the-art models in natural language processing.",
    "venue": "Journal of Artificial Intelligence Research",
    "year": 2017,
    "referenceCount": 43,
    "citationCount": 234,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3449411",
        "name": "Dieuwke Hupkes"
      },
      {
        "authorId": "1787819",
        "name": "Willem H. Zuidema"
      }
    ]
  },
  "108300988": {
    "paperId": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
    "externalIds": {
      "DBLP": "journals/corr/abs-1905-06316",
      "MAG": "2908854766",
      "ArXiv": "1905.06316",
      "CorpusId": 108300988
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "What do you learn from context? Probing for sentence structure in contextualized word representations",
    "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "referenceCount": 57,
    "citationCount": 807,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "6117577",
        "name": "Ian Tenney"
      },
      {
        "authorId": "2465658",
        "name": "Patrick Xia"
      },
      {
        "authorId": "2108381400",
        "name": "Berlin Chen"
      },
      {
        "authorId": "144906624",
        "name": "Alex Wang"
      },
      {
        "authorId": "48926630",
        "name": "Adam Poliak"
      },
      {
        "authorId": "145534175",
        "name": "R. Thomas McCoy"
      },
      {
        "authorId": "8756748",
        "name": "Najoung Kim"
      },
      {
        "authorId": "7536576",
        "name": "Benjamin Van Durme"
      },
      {
        "authorId": "3644767",
        "name": "Samuel R. Bowman"
      },
      {
        "authorId": "143790066",
        "name": "Dipanjan Das"
      },
      {
        "authorId": "2949185",
        "name": "Ellie Pavlick"
      }
    ]
  },
  "14091946": {
    "paperId": "3aa52436575cf6768a0a1a476601825f6a62e58f",
    "externalIds": {
      "DBLP": "journals/tacl/LinzenDG16",
      "ACL": "Q16-1037",
      "MAG": "2949674892",
      "ArXiv": "1611.01368",
      "DOI": "10.1162/tacl_a_00115",
      "CorpusId": 14091946
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
    "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture\u2019s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 52,
    "citationCount": 858,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2467508",
        "name": "Tal Linzen"
      },
      {
        "authorId": "2202008",
        "name": "Emmanuel Dupoux"
      },
      {
        "authorId": "79775260",
        "name": "Yoav Goldberg"
      }
    ]
  },
  "49363457": {
    "paperId": "843c6b0a35b02e2c3d74bb545e74bc655e16e992",
    "externalIds": {
      "MAG": "2952909964",
      "ACL": "C18-1152",
      "ArXiv": "1809.03992",
      "DBLP": "journals/corr/abs-1809-03992",
      "CorpusId": 49363457
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Assessing Composition in Sentence Vector Representations",
    "abstract": "An important component of achieving language understanding is mastering the composition of sentence meaning, but an immediate challenge to solving this problem is the opacity of sentence vector representations produced by current neural sentence composition models. We present a method to address this challenge, developing tasks that directly target compositional meaning information in sentence vector representations with a high degree of precision and control. To enable the creation of these controlled tasks, we introduce a specialized sentence generation system that produces large, annotated sentence sets meeting specified syntactic, semantic and lexical constraints. We describe the details of the method and generation system, and then present results of experiments applying our method to probe for compositional information in embeddings from a number of existing sentence composition models. We find that the method is able to extract useful information about the differing capacities of these models, and we discuss the implications of our results with respect to these systems\u2019 capturing of sentence information. We make available for public use the datasets used for these experiments, as well as the generation system.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2018,
    "referenceCount": 45,
    "citationCount": 78,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "37907837",
        "name": "Allyson Ettinger"
      },
      {
        "authorId": "143718836",
        "name": "Ahmed Elgohary"
      },
      {
        "authorId": "143843506",
        "name": "C. Phillips"
      },
      {
        "authorId": "1680292",
        "name": "P. Resnik"
      }
    ]
  },
  "11212020": {
    "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
    "externalIds": {
      "MAG": "2133564696",
      "ArXiv": "1409.0473",
      "DBLP": "journals/corr/BahdanauCB14",
      "CorpusId": 11212020
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "venue": "International Conference on Learning Representations",
    "year": 2014,
    "referenceCount": 33,
    "citationCount": 26088,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3335364",
        "name": "Dzmitry Bahdanau"
      },
      {
        "authorId": "1979489",
        "name": "Kyunghyun Cho"
      },
      {
        "authorId": "1751762",
        "name": "Yoshua Bengio"
      }
    ]
  },
  "13017314": {
    "paperId": "4c41104e871bccbd56494350a71d77a7f1da5bb0",
    "externalIds": {
      "ArXiv": "1612.08220",
      "DBLP": "journals/corr/LiMJ16a",
      "MAG": "2562979205",
      "CorpusId": 13017314
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Understanding Neural Networks through Representation Erasure",
    "abstract": "While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.",
    "venue": "arXiv.org",
    "year": 2016,
    "referenceCount": 55,
    "citationCount": 536,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49298465",
        "name": "Jiwei Li"
      },
      {
        "authorId": "145768639",
        "name": "Will Monroe"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      }
    ]
  },
  "3085700": {
    "paperId": "63c4114bd373dd0fcfe0d25a605b353c62be2995",
    "externalIds": {
      "MAG": "2950694022",
      "DBLP": "journals/corr/Sennrich16",
      "ArXiv": "1612.04629",
      "ACL": "E17-2060",
      "DOI": "10.18653/V1/E17-2060",
      "CorpusId": 3085700
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs",
    "abstract": "Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of error. We present LingEval97, a large-scale data set of 97000 contrastive translation pairs based on the WMT English->German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 36,
    "citationCount": 160,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2082372",
        "name": "Rico Sennrich"
      }
    ]
  },
  "17362994": {
    "paperId": "78aa018ee7d52360e15d103390ea1cdb3a0beb41",
    "externalIds": {
      "DBLP": "journals/corr/PapernotMG16",
      "ArXiv": "1605.07277",
      "MAG": "2408141691",
      "CorpusId": 17362994
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples",
    "abstract": "Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.",
    "venue": "arXiv.org",
    "year": 2016,
    "referenceCount": 26,
    "citationCount": 1652,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1967156",
        "name": "Nicolas Papernot"
      },
      {
        "authorId": "144061974",
        "name": "P. Mcdaniel"
      },
      {
        "authorId": "153440022",
        "name": "I. Goodfellow"
      }
    ]
  },
  "21698802": {
    "paperId": "514e7fb769950dbe96eb519c88ca17e04dc829f6",
    "externalIds": {
      "DBLP": "conf/acl/EbrahimiRLD18",
      "MAG": "2799194071",
      "ACL": "P18-2006",
      "DOI": "10.18653/v1/P18-2006",
      "CorpusId": 21698802
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "HotFlip: White-Box Adversarial Examples for Text Classification",
    "abstract": "We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 25,
    "citationCount": 953,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39043512",
        "name": "J. Ebrahimi"
      },
      {
        "authorId": "36290866",
        "name": "Anyi Rao"
      },
      {
        "authorId": "3021654",
        "name": "Daniel Lowd"
      },
      {
        "authorId": "1721158",
        "name": "D. Dou"
      }
    ]
  },
  "26039972": {
    "paperId": "0e661bd2cfe94ed58e4e2abc1409c75b98c2582c",
    "externalIds": {
      "MAG": "2031932105",
      "DOI": "10.1007/s00005-008-0020-7",
      "CorpusId": 26039972,
      "PubMed": "18512027"
    },
    "publicationVenue": {
      "id": "7b7cf6ff-4022-4de8-8d4f-c40dd06a4342",
      "name": "Archivum Immunologiae et Therapiae Experimentalis",
      "type": "journal",
      "alternate_names": [
        "Arch Immunol Ther Exp",
        "Archivum Immunologiae Et Therapiae Experimentalis"
      ],
      "issn": "0004-069X",
      "url": "https://link.springer.com/journal/5"
    },
    "title": "Dual use and the ethical responsibility of scientists",
    "abstract": null,
    "venue": "Archivum Immunologiae et Therapiae Experimentalis",
    "year": 2008,
    "referenceCount": 10,
    "citationCount": 40,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Political Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3920554",
        "name": "Hans-J\u00f6rg Ehni"
      }
    ]
  },
  "52113954": {
    "paperId": "e8fa186444d98a39ee9139b1f5dd0c7618caef8f",
    "externalIds": {
      "ACL": "D18-1001",
      "MAG": "2889507104",
      "DBLP": "conf/emnlp/CoavouxNC18",
      "ArXiv": "1808.09408",
      "DOI": "10.18653/v1/D18-1001",
      "CorpusId": 52113954
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Privacy-preserving Neural Representations of Text",
    "abstract": "This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP), in the context of privacy protection. We study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. Such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user\u2019s device and sent to a cloud-based model. We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. Finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 28,
    "citationCount": 105,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3443469",
        "name": "Maximin Coavoux"
      },
      {
        "authorId": null,
        "name": "Shashi Narayan"
      },
      {
        "authorId": "40146204",
        "name": "Shay B. Cohen"
      }
    ]
  },
  "9460040": {
    "paperId": "f9acf607b858ac110c1bf83bf62835bcc1820e83",
    "externalIds": {
      "DBLP": "conf/chi/NathanKF07",
      "MAG": "2017538178",
      "DOI": "10.1145/1240866.1241046",
      "CorpusId": 9460040
    },
    "publicationVenue": null,
    "title": "Value scenarios: a technique for envisioning systemic effects of new technologies",
    "abstract": "In this paper we argue that there is a scarcity of methods which support critical, systemic, long-term thinking in current design practice, technology development and deployment. To address this need we introduce value scenarios, an extension of scenario-based design which can support envisioning the systemic effects of new technologies. We identify and describe five key elements of value scenarios; stakeholders, pervasiveness, time, systemic effects, and value implications. We provide two examples of value scenarios, which draw from our current work on urban simulation and human-robotic interaction . We conclude with suggestions for how value scenarios might be used by others.",
    "venue": "CHI Extended Abstracts",
    "year": 2007,
    "referenceCount": 9,
    "citationCount": 101,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Engineering",
        "source": "s2-fos-model"
      },
      {
        "category": "Environmental Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34869420",
        "name": "L. Nathan"
      },
      {
        "authorId": "2035680",
        "name": "P. Klasnja"
      },
      {
        "authorId": "144029598",
        "name": "Batya Friedman"
      }
    ]
  },
  "53782832": {
    "paperId": "c9fa1cb56feeeb5033aa7ba40fa035ca2b9018ce",
    "externalIds": {
      "MAG": "2901107694",
      "ArXiv": "1811.10104",
      "DBLP": "conf/fat/HutchinsonM19",
      "DOI": "10.1145/3287560.3287600",
      "CorpusId": 53782832
    },
    "publicationVenue": null,
    "title": "50 Years of Test (Un)fairness: Lessons for Machine Learning",
    "abstract": "Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.",
    "venue": "FAT",
    "year": 2018,
    "referenceCount": 82,
    "citationCount": 331,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2044655623",
        "name": "Ben Hutchinson"
      },
      {
        "authorId": "49501003",
        "name": "Margaret Mitchell"
      }
    ]
  },
  "2077168": {
    "paperId": "0fee3b6c72f7676b4934651e517d0a328048c600",
    "externalIds": {
      "DBLP": "journals/corr/FriedlerSV14",
      "MAG": "2952092676",
      "ArXiv": "1412.3756",
      "DOI": "10.1145/2783258.2783311",
      "CorpusId": 2077168
    },
    "publicationVenue": {
      "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
      "name": "Knowledge Discovery and Data Mining",
      "type": "conference",
      "alternate_names": [
        "KDD",
        "Knowl Discov Data Min"
      ],
      "url": "http://www.acm.org/sigkdd/"
    },
    "title": "Certifying and Removing Disparate Impact",
    "abstract": "What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process. When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses. We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.",
    "venue": "Knowledge Discovery and Data Mining",
    "year": 2014,
    "referenceCount": 31,
    "citationCount": 1840,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Law",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2053453944",
        "name": "Michael Feldman"
      },
      {
        "authorId": "34597147",
        "name": "Sorelle A. Friedler"
      },
      {
        "authorId": "144275618",
        "name": "John Moeller"
      },
      {
        "authorId": "1786183",
        "name": "C. Scheidegger"
      },
      {
        "authorId": "72563021",
        "name": "S. Venkatasubramanian"
      }
    ]
  },
  "153811205": {
    "paperId": "5a9cac54de14e58697d0315fe3c01f3dbe69c186",
    "externalIds": {
      "MAG": "1741471588",
      "DBLP": "books/others/91/ClarkB91",
      "DOI": "10.1037/10096-006",
      "CorpusId": 153811205
    },
    "publicationVenue": null,
    "title": "Grounding in communication",
    "abstract": "GROUNDING It takes two people working together to play a duet, shake hands, play chess, waltz, teach, or make love. To succeed, the two of them have to coordinate both the content and process of what they are doing. Alan and Barbara, on the piano, must come to play the same Mozart duet. This is coordination of content. They must also synchronize their entrances and exits, coordinate how loudly to play forte and pianissimo, and otherwise adjust to each other's tempo and dynamics. This is coordination of process. They cannot even begin to coordinate on content without assuming a vast amount of shared information or common ground-that is, mutual knowledge, mutual beliefs, and mutual assumptions And to coordinate on process, they need to update their common ground moment by moment. All collective actions are built on common ground and its accumulation. We thank many colleagues for discussion of the issues we take up here.",
    "venue": "Perspectives on socially shared cognition",
    "year": 1991,
    "referenceCount": 42,
    "citationCount": 4464,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "29224904",
        "name": "H. H. Clark"
      },
      {
        "authorId": "71463834",
        "name": "S. Brennan"
      }
    ]
  },
  "14623495": {
    "paperId": "06b6595034f6a8ea850ac12814030c0ef214d300",
    "externalIds": {
      "MAG": "2084617042",
      "DOI": "10.1007/S13164-014-0213-4",
      "CorpusId": 14623495
    },
    "publicationVenue": null,
    "title": "Meaning and Demonstration",
    "abstract": null,
    "venue": "",
    "year": 2015,
    "referenceCount": 36,
    "citationCount": 13,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144884556",
        "name": "Matthew Stone"
      },
      {
        "authorId": "3458697",
        "name": "Una Stojni\u0107"
      }
    ]
  },
  "10161834": {
    "paperId": "68922969c1b91cdfb4a13f1dab9b90d015179a9c",
    "externalIds": {
      "DBLP": "conf/sigdial/ManuvinakurikeD17",
      "ACL": "W17-5539",
      "MAG": "2774397635",
      "DOI": "10.18653/v1/W17-5539",
      "CorpusId": 10161834
    },
    "publicationVenue": null,
    "title": "Using Reinforcement Learning to Model Incrementality in a Fast-Paced Dialogue Game",
    "abstract": "We apply Reinforcement Learning (RL) to the problem of incremental dialogue policy learning in the context of a fast-paced dialogue game. We compare the policy learned by RL with a high-performance baseline policy which has been shown to perform very efficiently (nearly as well as humans) in this dialogue game. The RL policy outperforms the baseline policy in offline simulations (based on real user data). We provide a detailed comparison of the RL policy and the baseline policy, including information about how much effort and time it took to develop each one of them. We also highlight the cases where the RL policy performs better, and show that understanding the RL policy can provide valuable insights which can inform the creation of an even better rule-based policy.",
    "venue": "SIGDIAL Conference",
    "year": 2017,
    "referenceCount": 28,
    "citationCount": 20,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2175808",
        "name": "R. Manuvinakurike"
      },
      {
        "authorId": "144662324",
        "name": "David DeVault"
      },
      {
        "authorId": "3194430",
        "name": "Kallirroi Georgila"
      }
    ]
  },
  "51609464": {
    "paperId": "0e3c3599bf5dc2e24e724f097b80948f25c57d1d",
    "externalIds": {
      "MAG": "2876243767",
      "DBLP": "conf/ijcai/ChaiGSYSX18",
      "DOI": "10.24963/ijcai.2018/1",
      "CorpusId": 51609464
    },
    "publicationVenue": {
      "id": "67f7f831-711a-43c8-8785-1e09005359b5",
      "name": "International Joint Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Int Jt Conf Artif Intell",
        "IJCAI"
      ],
      "url": "http://www.ijcai.org/"
    },
    "title": "Language to Action: Towards Interactive Task Learning with Physical Agents",
    "abstract": "Language communication plays an important role in human learning and knowledge acquisition. With the emergence of a new generation of cognitive robots, empowering these robots to learn directly from human partners becomes increasingly important. This paper gives a brief introduction to interactive task learning where humans can teach physical agents new tasks through natural language communication and action demonstration. It discusses research challenges and opportunities in language and communication grounding that are critical in this process. It further highlights the importance of commonsense knowledge, particularly the very basic physical causality knowledge, in grounding language to perception and action.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2018,
    "referenceCount": 58,
    "citationCount": 87,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1707259",
        "name": "J. Chai"
      },
      {
        "authorId": "3193409",
        "name": "Qiaozi Gao"
      },
      {
        "authorId": "2720582",
        "name": "Lanbo She"
      },
      {
        "authorId": "47569745",
        "name": "Shaohua Yang"
      },
      {
        "authorId": "1411038811",
        "name": "S. Saba-Sadiya"
      },
      {
        "authorId": "49560239",
        "name": "Guangyue Xu"
      }
    ]
  },
  "14843216": {
    "paperId": "a2a4cc9bd34ed61383979edd365d29a32a74368e",
    "externalIds": {
      "DBLP": "conf/hri/HoughS17",
      "MAG": "2593459258",
      "DOI": "10.1145/2909824.3020214",
      "CorpusId": 14843216
    },
    "publicationVenue": {
      "id": "b49868ed-865c-4154-b70a-8d34e341cf68",
      "name": "IEEE/ACM International Conference on Human-Robot Interaction",
      "type": "conference",
      "alternate_names": [
        "Human-Robot Interaction",
        "HRI",
        "Human-robot Interact",
        "IEEE/ACM Int Conf Human-robot Interact"
      ],
      "url": "http://www.wikicfp.com/cfp/program?id=1232"
    },
    "title": "It's Not What You Do, It's How You Do It: Grounding Uncertainty for a Simple Robot",
    "abstract": "For effective HRI, robots must go beyond having good legibility of their intentions shown by their actions, but also ground the degree of uncertainty they have. We show how in simple robots which have spoken language understanding capacities, uncertainty can be communicated to users by principles of grounding in dialogue interaction even without natural language generation. We present a model which makes this possible for robots with limited communication channels beyond the execution of task actions themselves. We implement our model in a pick-and-place robot, and experiment with two strategies for grounding uncertainty. In an observer study, we show that participants observing interactions with the robot run by the two different strategies were able to infer the degree of understanding the robot had internally, and in the more uncertainty-expressive system, were also able to perceive the degree of internal uncertainty the robot had reliably.",
    "venue": "IEEE/ACM International Conference on Human-Robot Interaction",
    "year": 2017,
    "referenceCount": 31,
    "citationCount": 33,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144397346",
        "name": "Julian Hough"
      },
      {
        "authorId": "1817455",
        "name": "David Schlangen"
      }
    ]
  },
  "11824338": {
    "paperId": "440dd122c93f707c213bb3096449848ac7d1bda5",
    "externalIds": {
      "DBLP": "conf/acl/RieserL08",
      "ACL": "P08-1073",
      "MAG": "2155898337",
      "CorpusId": 11824338
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz Data: Bootstrapping and Evaluation",
    "abstract": "We address two problems in the field of automatic optimization of dialogue strategies: learning effective dialogue strategies when no initial data or system exists, and evaluating the result with real users. We use Reinforcement Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated environment which is \u201cbootstrapped\u201d from small amounts of Wizard-of-Oz (WOZ) data. This use of WOZ data allows development of optimal strategies for domains where no working prototype is available. We compare the RL-based strategy against a supervised strategy which mimics the wizards\u2019 policies. This comparison allows us to measure relative improvement over the training data. Our results show that RL significantly outperforms Supervised Learning when interacting in simulation as well as for interactions with real users. The RL-based policy gains on average 50-times more reward when tested in simulation, and almost 18-times more reward when interacting with real users. Users also subjectively rate the RL-based policy on average 10% higher.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2008,
    "referenceCount": 35,
    "citationCount": 97,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1681799",
        "name": "Verena Rieser"
      },
      {
        "authorId": "1782798",
        "name": "Oliver Lemon"
      }
    ]
  },
  "117398433": {
    "paperId": "39253602324619c130e140c4e5cef4af0d746880",
    "externalIds": {
      "MAG": "2785859276",
      "DBLP": "journals/ftrob/ChaKFM18",
      "DOI": "10.1561/2300000057",
      "CorpusId": 117398433
    },
    "publicationVenue": null,
    "title": "A Survey of Nonverbal Signaling Methods for Non-Humanoid Robots",
    "abstract": "This monograph surveys and informs the design and usage of nonverbal signals for human-robot interaction. With robots increasingly being utilized for tasks that require them to not only operate in close proximity to humans but to interact with them as well, there has been great interest in the communication challenges associated with the varying degrees of interaction in these environments. The success of such interactions depends on robots\u2019 ability to convey information about their knowledge, intent, and actions to co-located humans. The monograph presents a comprehensive review of literature related to the generation and usage of nonverbal signals that facilitate legibility of non-humanoid robot state and behavior. To motivate the need for these signaling behaviors, it surveys literature in human communication and psychology and outlines target use cases of non-humanoid robots. Specifically, the focus is on works that provide insight into the cognitive processes that enable humans to recognize, interpret, and exploit nonverbal signals. From these use cases, information is identified that is potentially important for non-humanoid robots to signal and organize it into three categories of robot state. The monograph then presents a review of signal design techniques to illustrate how signals conveying this information can be generated and utilized. It concludes by discussing issues that must be considered during nonverbal signaling and open research areas, with a focus on informing the design and usage of generalizable nonverbal signaling behaviors for task-oriented non-humanoid robots.",
    "venue": "Found. Trends Robotics",
    "year": 2018,
    "referenceCount": 272,
    "citationCount": 96,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Engineering",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3289717",
        "name": "Elizabeth Cha"
      },
      {
        "authorId": "1717667",
        "name": "Yunkyung Kim"
      },
      {
        "authorId": "145585047",
        "name": "T. Fong"
      },
      {
        "authorId": "1742183",
        "name": "M. Matari\u0107"
      }
    ]
  },
  "202588687": {
    "paperId": "4a5c3216f8aad40b17531e78df8cc18e2b5c73ff",
    "externalIds": {
      "CorpusId": 202588687
    },
    "publicationVenue": null,
    "title": "The Devil is in the Details: A Magnifying Glass for the GuessWhich Visual Dialogue Game",
    "abstract": "Grounded conversational agents are a fascinating research line on which important progress has beenmade lately thanks to the development of neural network models and to the release of visual dialogue datasets. The latter have been used to set visual dialogue games which are an interesting test bed to evaluate conversational agents. Researchers\u2019 attention is on building models of increasing complexity, trained with computationally costly machine learning paradigms that lead to higher task success scores. In this paper, we take a step back: We use a rather simple neural network architecture and we scrutinize theGuessWhich task, the dataset, and the quality of the generated dialogues. We show that our simple Questioner agent reaches state-of-the art performance, that the evaluation metric commonly used is too coarse to compare different models, and that high task success does not correspond to high quality of the dialogues. Our work shows the importance of running detailed analyses of the results to spot possible models\u2019 weaknesses rather than aiming to outperform state-of-the-art scores.",
    "venue": "",
    "year": 2019,
    "referenceCount": 26,
    "citationCount": 6,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "50829868",
        "name": "A. Testoni"
      },
      {
        "authorId": "145543514",
        "name": "Ravi Shekhar"
      },
      {
        "authorId": "144151273",
        "name": "R. Fern\u00e1ndez"
      }
    ]
  },
  "154339": {
    "paperId": "33ff45f364dac785b8bd4e3bf70fb169dc1d39b4",
    "externalIds": {
      "MAG": "2128186353",
      "DOI": "10.1126/science.342.6154.60",
      "CorpusId": 154339,
      "PubMed": "24092725"
    },
    "publicationVenue": {
      "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
      "name": "Science",
      "type": "journal",
      "issn": "0193-4511",
      "alternate_issns": [
        "0036-8075"
      ],
      "url": "https://www.jstor.org/journal/science",
      "alternate_urls": [
        "https://www.sciencemag.org/",
        "http://www.sciencemag.org/",
        "http://www.jstor.org/journals/00368075.html",
        "http://www.sciencemag.org/archive/"
      ]
    },
    "title": "Who's afraid of peer review?",
    "abstract": "Dozens of open-access journals targeted in an elaborate Science sting accepted a spoof research article, raising questions about peer-review practices in much of the open-access world.",
    "venue": "Science",
    "year": 2013,
    "referenceCount": 0,
    "citationCount": 883,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145179131",
        "name": "J. Bohannon"
      }
    ]
  },
  "8460592": {
    "paperId": "9ca5552008fe2c24e0541f6af47fd5110d4015b3",
    "externalIds": {
      "ACL": "J05-4006",
      "DOI": "10.1162/089120105775299131",
      "CorpusId": 8460592
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Last Words: Reviewing the Reviewers",
    "abstract": null,
    "venue": "International Conference on Computational Logic",
    "year": null,
    "referenceCount": 0,
    "citationCount": 48,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2272727361",
        "name": "K. Church"
      }
    ]
  },
  "16508456": {
    "paperId": "4fb5a17d4066116a8fc928e43aa558732d8b7cb2",
    "externalIds": {
      "MAG": "2558206625",
      "PubMedCentral": "5131510",
      "DOI": "10.1186/s40359-016-0167-7",
      "CorpusId": 16508456,
      "PubMed": "27903302"
    },
    "publicationVenue": {
      "id": "c76dbeff-4edf-44af-8ab2-0cf99f09a9bc",
      "name": "BMC Psychology",
      "type": "journal",
      "alternate_names": [
        "BMC Psychol"
      ],
      "issn": "2050-7283",
      "url": "http://www.biomedcentral.com/bmcpsychol/",
      "alternate_urls": [
        "http://www.biomedcentral.com/bmcpsychol/content",
        "http://rave.ohiolink.edu/ejournals/issn/20507283",
        "https://link.springer.com/journal/40359"
      ]
    },
    "title": "Preventing the ends from justifying the means: withholding results to address publication bias in peer-review",
    "abstract": null,
    "venue": "BMC Psychology",
    "year": 2016,
    "referenceCount": 49,
    "citationCount": 42,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "4058655",
        "name": "K. Button"
      },
      {
        "authorId": "38974348",
        "name": "Liz Bal"
      },
      {
        "authorId": "145879163",
        "name": "A. Clark"
      },
      {
        "authorId": "19854097",
        "name": "Tim Shipley"
      }
    ]
  },
  "53149927": {
    "paperId": "a772589606f9880d74ac79519ccef073eefd5519",
    "externalIds": {
      "MAG": "2091326296",
      "DOI": "10.1016/j.anbehav.2008.05.023",
      "CorpusId": 53149927
    },
    "publicationVenue": {
      "id": "faadb72e-cdca-4729-a66d-93ff09ec21bf",
      "name": "Animal Behaviour",
      "type": "journal",
      "alternate_names": [
        "Anim Behav"
      ],
      "issn": "0003-3472",
      "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622782/description#description",
      "alternate_urls": [
        "http://asab.nottingham.ac.uk/pubs/journal.php",
        "http://www.sciencedirect.com/science/journal/00033472"
      ]
    },
    "title": "Double-blind peer review and gender publication bias",
    "abstract": null,
    "venue": "Animal Behaviour",
    "year": 2008,
    "referenceCount": 8,
    "citationCount": 34,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3145400",
        "name": "L. Engqvist"
      },
      {
        "authorId": "2149229",
        "name": "Joachim G. Frommen"
      }
    ]
  },
  "7350256": {
    "paperId": "d8cf5c798397b6a0be1b41f18f979f0988f1ece7",
    "externalIds": {
      "MAG": "2164136002",
      "DOI": "10.1007/BF01173636",
      "CorpusId": 7350256
    },
    "publicationVenue": {
      "id": "65835f11-8059-479f-a4b4-68d0f725ac13",
      "name": "Cognitive Therapy and Research",
      "type": "journal",
      "alternate_names": [
        "Cogn Ther Res"
      ],
      "issn": "0147-5916",
      "url": "http://www.kluweronline.com/issn/0147-5916/contents",
      "alternate_urls": [
        "https://link.springer.com/journal/10608",
        "http://www.springer.com/dal/home?SGWID=1-102-70-35753187-0"
      ]
    },
    "title": "Publication prejudices: An experimental study of confirmatory bias in the peer review system",
    "abstract": null,
    "venue": "Cognitive Therapy and Research",
    "year": 1977,
    "referenceCount": 27,
    "citationCount": 668,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35256346",
        "name": "M. Mahoney"
      }
    ]
  },
  "155600340": {
    "paperId": "8d75051e8151fa5b7bd7c863102d0c4be7608c93",
    "externalIds": {
      "MAG": "2315362078",
      "DOI": "10.1038/nature.2014.16629",
      "CorpusId": 155600340
    },
    "publicationVenue": {
      "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
      "name": "Nature",
      "type": "journal",
      "issn": "0028-0836",
      "url": "https://www.nature.com/",
      "alternate_urls": [
        "http://www.nature.com/nature/",
        "https://www.nature.com/nature/",
        "http://www.nature.com/nature/archive/index.html"
      ]
    },
    "title": "Peer review \u2014 reviewed",
    "abstract": null,
    "venue": "Nature",
    "year": 2014,
    "referenceCount": 0,
    "citationCount": 3,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Sociology",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": []
  },
  "256659588": {
    "paperId": "87e849787dcfda83d7315c7d3d5c54851c82d264",
    "externalIds": {
      "DOI": "10.1002/ceas.12266",
      "CorpusId": 256659588
    },
    "publicationVenue": {
      "id": "637cc0f2-b4c4-4684-9f4c-0d900da8c96a",
      "name": "Counselor Education and Supervision",
      "type": "journal",
      "alternate_names": [
        "Couns Educ Superv"
      ],
      "issn": "0011-0035",
      "url": "http://www.ohiou.edu/che/ces/index.html"
    },
    "title": "On becoming a discipline",
    "abstract": null,
    "venue": "Counselor Education and Supervision",
    "year": 2023,
    "referenceCount": 22,
    "citationCount": 3,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Education",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "5922478",
        "name": "Melissa J. Fickling"
      }
    ]
  },
  "1570550": {
    "paperId": "830ab38207bd40189752a301967b865c38dab591",
    "externalIds": {
      "ACL": "J07-4009",
      "DOI": "10.1162/coli.2007.33.4.607",
      "CorpusId": 1570550
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Last Words: Breaking News: Changing Attitudes and Practices",
    "abstract": null,
    "venue": "International Conference on Computational Logic",
    "year": null,
    "referenceCount": 0,
    "citationCount": 3,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1736049",
        "name": "B. Webber"
      }
    ]
  },
  "522864": {
    "paperId": "cf222293e2447365ad25e603bfdd064646ef6652",
    "externalIds": {
      "MAG": "2132736842",
      "DOI": "10.1038/387341A0",
      "CorpusId": 522864,
      "PubMed": "9163412"
    },
    "publicationVenue": {
      "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
      "name": "Nature",
      "type": "journal",
      "issn": "0028-0836",
      "url": "https://www.nature.com/",
      "alternate_urls": [
        "http://www.nature.com/nature/",
        "https://www.nature.com/nature/",
        "http://www.nature.com/nature/archive/index.html"
      ]
    },
    "title": "Nepotism and sexism in peer-review",
    "abstract": null,
    "venue": "Nature",
    "year": 1997,
    "referenceCount": 11,
    "citationCount": 1357,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Biology",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "6447164",
        "name": "C. Wenner\u00e5s"
      },
      {
        "authorId": "2053374957",
        "name": "Agnes E. Wold"
      }
    ]
  },
  "13091007": {
    "paperId": "7a12502ba5b9686e37b0ec9d86a2dc7f4b7022ac",
    "externalIds": {
      "DBLP": "conf/icde/GulhaneMMRRSSTT11",
      "MAG": "2163072729",
      "DOI": "10.1109/ICDE.2011.5767842",
      "CorpusId": 13091007
    },
    "publicationVenue": {
      "id": "764e3630-ddac-4c21-af4b-9d32ffef082e",
      "name": "IEEE International Conference on Data Engineering",
      "type": "conference",
      "alternate_names": [
        "ICDE",
        "Int Conf Data Eng",
        "IEEE Int Conf Data Eng",
        "International Conference on Data Engineering"
      ],
      "url": "http://www.wikicfp.com/cfp/program?id=1331"
    },
    "title": "Web-scale information extraction with vertex",
    "abstract": "Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages. To operate at Web scale, Vertex employs a host of novel algorithms for (1) Grouping similar structured pages in a Web site, (2) Picking the appropriate sample pages for wrapper inference, (3) Learning XPath-based extraction rules that are robust to variations in site structure, (4) Detecting site changes by monitoring sample pages, and (5) Optimizing editorial costs by reusing rules, etc. The system is deployed in production and currently extracts more than 250 million records from more than 200 Web sites. To the best of our knowledge, Vertex is the first system to do high-precision information extraction at Web scale.",
    "venue": "IEEE International Conference on Data Engineering",
    "year": 2011,
    "referenceCount": 17,
    "citationCount": 90,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2627799",
        "name": "P. Gulhane"
      },
      {
        "authorId": "2136102",
        "name": "Amit Madaan"
      },
      {
        "authorId": "3259494",
        "name": "Rupesh R. Mehta"
      },
      {
        "authorId": "2311735",
        "name": "J. Ramamirtham"
      },
      {
        "authorId": "1696519",
        "name": "R. Rastogi"
      },
      {
        "authorId": "1837802",
        "name": "Sandeepkumar Satpal"
      },
      {
        "authorId": "1757518",
        "name": "Srinivasan H. Sengamedu"
      },
      {
        "authorId": "2990683",
        "name": "Ashwin Tengli"
      },
      {
        "authorId": "2081450365",
        "name": "Charu Tiwari"
      }
    ]
  },
  "51993171": {
    "paperId": "e49e6dbdfdb813b42fff716a8b11951de2d5cbf3",
    "externalIds": {
      "DBLP": "journals/pvldb/CafarellaHLMYWW18",
      "MAG": "3026586184",
      "DOI": "10.14778/3229863.3240492",
      "CorpusId": 51993171
    },
    "publicationVenue": {
      "id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e",
      "name": "Proceedings of the VLDB Endowment",
      "type": "journal",
      "alternate_names": [
        "Proceedings of The Vldb Endowment",
        "Proc VLDB Endow",
        "Proc Vldb Endow"
      ],
      "issn": "2150-8097",
      "url": "http://dl.acm.org/toc.cfm?id=J1174",
      "alternate_urls": [
        "http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"
      ]
    },
    "title": "Ten Years of WebTables",
    "abstract": "\n In 2008, we wrote about WebTables, an effort to exploit the large and diverse set of structured databases casually published online in the form of HTML tables. The past decade has seen a flurry of research and commercial activities around the WebTables project itself, as well as the broad topic of informal online structured data. In this paper, we\n 1\n will review the WebTables project, and try to place it in the broader context of the decade of work that followed. We will also show how the progress over the past ten years sets up an exciting agenda for the future, and will draw upon many corners of the data management community.\n",
    "venue": "Proceedings of the VLDB Endowment",
    "year": 2018,
    "referenceCount": 42,
    "citationCount": 62,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1725561",
        "name": "Michael J. Cafarella"
      },
      {
        "authorId": "1770962",
        "name": "A. Halevy"
      },
      {
        "authorId": "8386466",
        "name": "Hongrae Lee"
      },
      {
        "authorId": "2224716",
        "name": "Jayant Madhavan"
      },
      {
        "authorId": "40592227",
        "name": "Cong Yu"
      },
      {
        "authorId": "2111220343",
        "name": "D. Wang"
      },
      {
        "authorId": "48144872",
        "name": "Eugene Wu"
      }
    ]
  },
  "3627801": {
    "paperId": "0e46803ac8fc715b72d7f935a3f383ade945487f",
    "externalIds": {
      "ArXiv": "1703.05028",
      "DBLP": "conf/sigmod/0002HCHRLR18",
      "MAG": "2604259521",
      "DOI": "10.1145/3183713.3183729",
      "CorpusId": 3627801,
      "PubMed": "29937618"
    },
    "publicationVenue": null,
    "title": "Fonduer: Knowledge Base Construction from Richly Formatted Data",
    "abstract": "We focus on knowledge base construction (KBC) from richly formatted data. In contrast to KBC from text or tabular data, KBC from richly formatted data aims to extract relations conveyed jointly via textual, structural, tabular, and visual expressions. We introduce Fonduer, a machine-learning-based KBC system for richly formatted data. Fonduer presents a new data model that accounts for three challenging characteristics of richly formatted data: (1) prevalent document-level relations, (2) multimodality, and (3) data variety. Fonduer uses a new deep-learning model to automatically capture the representation (i.e., features) needed to learn how to extract relations from richly formatted data. Finally, Fonduer provides a new programming model that enables users to convert domain expertise, based on multiple modalities of information, to meaningful signals of supervision for training a KBC system. Fonduer-based KBC systems are in production for a range of use cases, including at a major online retailer. We compare Fonduer against state-of-the-art KBC approaches in four different domains. We show that Fonduer achieves an average improvement of 41 F1 points on the quality of the output knowledge base---and in some cases produces up to 1.87x the number of correct entries---compared to expert-curated public knowledge bases. We also conduct a user study to assess the usability of Fonduer's new programming model. We show that after using Fonduer for only 30 minutes, non-domain experts are able to design KBC systems that achieve on average 23 F1 points higher quality than traditional machine-learning-based KBC approaches.",
    "venue": "SIGMOD Conference",
    "year": 2017,
    "referenceCount": 52,
    "citationCount": 97,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144766615",
        "name": "Sen Wu"
      },
      {
        "authorId": "2065637845",
        "name": "Luke Hsiao"
      },
      {
        "authorId": "2149478197",
        "name": "Xiaoxia Cheng"
      },
      {
        "authorId": "34302368",
        "name": "Braden Hancock"
      },
      {
        "authorId": "145071799",
        "name": "Theodoros Rekatsinas"
      },
      {
        "authorId": "1721681",
        "name": "P. Levis"
      },
      {
        "authorId": "2114485554",
        "name": "C. R\u00e9"
      }
    ]
  },
  "102353905": {
    "paperId": "dcb28c8ba94434eb8a06e81eb55bfdbc343d2340",
    "externalIds": {
      "MAG": "2951384723",
      "DBLP": "conf/naacl/JiaWP19",
      "ArXiv": "1904.02347",
      "ACL": "N19-1370",
      "DOI": "10.18653/v1/N19-1370",
      "CorpusId": 102353905
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Document-Level N-ary Relation Extraction with Multiscale Representation Learning",
    "abstract": "Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, n-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level n-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system\u2019s purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous n-ary relation extraction methods.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 32,
    "citationCount": 135,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3422908",
        "name": "Robin Jia"
      },
      {
        "authorId": "2109566188",
        "name": "Cliff Wong"
      },
      {
        "authorId": "1759772",
        "name": "Hoifung Poon"
      }
    ]
  },
  "5774632": {
    "paperId": "131383aa1f91eb0e9578dcae80f4dfcfb0f11e3e",
    "externalIds": {
      "DBLP": "journals/pvldb/BronziCMP13",
      "MAG": "2097874932",
      "DOI": "10.14778/2536206.2536209",
      "CorpusId": 5774632
    },
    "publicationVenue": {
      "id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e",
      "name": "Proceedings of the VLDB Endowment",
      "type": "journal",
      "alternate_names": [
        "Proceedings of The Vldb Endowment",
        "Proc VLDB Endow",
        "Proc Vldb Endow"
      ],
      "issn": "2150-8097",
      "url": "http://dl.acm.org/toc.cfm?id=J1174",
      "alternate_urls": [
        "http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"
      ]
    },
    "title": "Extraction and Integration of Partially Overlapping Web Sources",
    "abstract": "We present an unsupervised approach for harvesting the data exposed by a set of structured and partially overlapping data-intensive web sources. Our proposal comes within a formal framework tackling two problems: the data extraction problem, to generate extraction rules based on the input websites, and the data integration problem, to integrate the extracted data in a unified schema. We introduce an original algorithm, WEIR, to solve the stated problems and formally prove its correctness. WEIR leverages the overlapping data among sources to make better decisions both in the data extraction (by pruning rules that do not lead to redundant information) and in the data integration (by reflecting local properties of a source over the mediated schema). Along the way, we characterize the amount of redundancy needed by our algorithm to produce a solution, and present experimental results to show the benefits of our approach with respect to existing solutions.",
    "venue": "Proceedings of the VLDB Endowment",
    "year": 2013,
    "referenceCount": 32,
    "citationCount": 60,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1760944",
        "name": "Mirko Bronzi"
      },
      {
        "authorId": "1791339",
        "name": "Valter Crescenzi"
      },
      {
        "authorId": "1796590",
        "name": "P. Merialdo"
      },
      {
        "authorId": "1802817",
        "name": "Paolo Papotti"
      }
    ]
  },
  "4557963": {
    "paperId": "cf5ea582bccc7cb21a2ebeb7a0987f79652bde8d",
    "externalIds": {
      "DBLP": "conf/kdd/0001GHHLMSSZ14",
      "MAG": "2016753842",
      "DOI": "10.1145/2623330.2623623",
      "CorpusId": 4557963
    },
    "publicationVenue": {
      "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
      "name": "Knowledge Discovery and Data Mining",
      "type": "conference",
      "alternate_names": [
        "KDD",
        "Knowl Discov Data Min"
      ],
      "url": "http://www.acm.org/sigkdd/"
    },
    "title": "Knowledge vault: a web-scale approach to probabilistic knowledge fusion",
    "abstract": "Recent years have witnessed a proliferation of large-scale knowledge bases, including Wikipedia, Freebase, YAGO, Microsoft's Satori, and Google's Knowledge Graph. To increase the scale even further, we need to explore automatic methods for constructing knowledge bases. Previous approaches have primarily focused on text-based extraction, which can be very noisy. Here we introduce Knowledge Vault, a Web-scale probabilistic knowledge base that combines extractions from Web content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived from existing knowledge repositories. We employ supervised machine learning methods for fusing these distinct information sources. The Knowledge Vault is substantially bigger than any previously published structured knowledge repository, and features a probabilistic inference system that computes calibrated probabilities of fact correctness. We report the results of multiple studies that explore the relative utility of the different information sources and extraction methods.",
    "venue": "Knowledge Discovery and Data Mining",
    "year": 2014,
    "referenceCount": 50,
    "citationCount": 1697,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145867172",
        "name": "X. Dong"
      },
      {
        "authorId": "1718798",
        "name": "E. Gabrilovich"
      },
      {
        "authorId": "1728179",
        "name": "Geremy Heitz"
      },
      {
        "authorId": "40428294",
        "name": "Wilko Horn"
      },
      {
        "authorId": "1914797",
        "name": "N. Lao"
      },
      {
        "authorId": "1702318",
        "name": "K. Murphy"
      },
      {
        "authorId": "2931575",
        "name": "Thomas Strohmann"
      },
      {
        "authorId": "2109375570",
        "name": "Shaohua Sun"
      },
      {
        "authorId": null,
        "name": "Wei Zhang"
      }
    ]
  },
  "102352698": {
    "paperId": "f262ef2f50dfcaf07dc6598f22fb9b2470b37cf1",
    "externalIds": {
      "MAG": "2952677853",
      "DBLP": "journals/corr/abs-1904-03296",
      "ACL": "N19-1308",
      "ArXiv": "1904.03296",
      "DOI": "10.18653/v1/N19-1308",
      "CorpusId": 102352698
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "A general framework for information extraction using dynamic span graphs",
    "abstract": "We introduce a general framework for several information extraction tasks that share span representations using dynamically constructed span graphs. The graphs are dynamically constructed by selecting the most confident entity spans and linking these nodes with confidence-weighted relation types and coreferences. The dynamic span graph allow coreference and relation type confidences to propagate through the graph to iteratively refine the span representations. This is unlike previous multi-task frameworks for information extraction in which the only interaction between tasks is in the shared first-layer LSTM. Our framework significantly outperforms state-of-the-art on multiple information extraction tasks across multiple datasets reflecting different domains. We further observe that the span enumeration approach is good at detecting nested span entities, with significant F1 score improvement on the ACE dataset.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 38,
    "citationCount": 306,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145081697",
        "name": "Yi Luan"
      },
      {
        "authorId": "30051202",
        "name": "David Wadden"
      },
      {
        "authorId": "2265599",
        "name": "Luheng He"
      },
      {
        "authorId": "2107663537",
        "name": "A. Shah"
      },
      {
        "authorId": "144339506",
        "name": "Mari Ostendorf"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ]
  },
  "174799980": {
    "paperId": "b31eef8d9263b02f7d0c1ab55b26012550a2e95a",
    "externalIds": {
      "DBLP": "conf/naacl/LockardSD19",
      "MAG": "2955858358",
      "ACL": "N19-1309",
      "DOI": "10.18653/v1/N19-1309",
      "CorpusId": 174799980
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "OpenCeres: When Open Information Extraction Meets the Semi-Structured Web",
    "abstract": "Open Information Extraction (OpenIE), the problem of harvesting triples from natural language text whose predicate relations are not aligned to any pre-defined ontology, has been a popular subject of research for the last decade. However, this research has largely ignored the vast quantity of facts available in semi-structured webpages. In this paper, we define the problem of OpenIE from semi-structured websites to extract such facts, and present an approach for solving it. We also introduce a labeled evaluation dataset to motivate research in this area. Given a semi-structured website and a set of seed facts for some relations existing on its pages, we employ a semi-supervised label propagation technique to automatically create training data for the relations present on the site. We then use this training data to learn a classifier for relation extraction. Experimental results of this method on our new benchmark dataset obtained a precision of over 70%. A larger scale extraction experiment on 31 websites in the movie vertical resulted in the extraction of over 2 million triples.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 29,
    "citationCount": 50,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144182018",
        "name": "Colin Lockard"
      },
      {
        "authorId": "3310534",
        "name": "Prashant Shiralkar"
      },
      {
        "authorId": "2143917898",
        "name": "Xin Dong"
      }
    ]
  },
  "53109320": {
    "paperId": "1da8e1ad1814d81f69433ac877ef70caa950e4e6",
    "externalIds": {
      "MAG": "2898700358",
      "DBLP": "journals/corr/abs-1810-13083",
      "ArXiv": "1810.13083",
      "ACL": "N19-1082",
      "DOI": "10.18653/v1/N19-1082",
      "CorpusId": 53109320
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "GraphIE: A Graph-Based Framework for Information Extraction",
    "abstract": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks \u2014 namely textual, social media and visual information extraction \u2014 shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 43,
    "citationCount": 105,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "5606742",
        "name": "Yujie Qian"
      },
      {
        "authorId": "2628786",
        "name": "Enrico Santus"
      },
      {
        "authorId": "8752221",
        "name": "Zhijing Jin"
      },
      {
        "authorId": "144084849",
        "name": "Jiang Guo"
      },
      {
        "authorId": "1741283",
        "name": "R. Barzilay"
      }
    ]
  },
  "91184338": {
    "paperId": "b1832b749528755dfcbe462717f4f5afc07243b8",
    "externalIds": {
      "DBLP": "journals/corr/abs-1904-01172",
      "MAG": "2928107702",
      "CorpusId": 91184338
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Commonsense Reasoning for Natural Language Understanding: A Survey of Benchmarks, Resources, and Approaches",
    "abstract": "Commonsense knowledge and commonsense reasoning are some of the main bottlenecks in machine intelligence. In the NLP community, many benchmark datasets and tasks have been created to address commonsense reasoning for language understanding. These tasks are designed to assess machines' ability to acquire and learn commonsense knowledge in order to reason and understand natural language text. As these tasks become instrumental and a driving force for commonsense research, this paper aims to provide an overview of existing tasks and benchmarks, knowledge resources, and learning and inference approaches toward commonsense reasoning for natural language understanding. Through this, our goal is to support a better understanding of the state of the art, its limitations, and future challenges.",
    "venue": "arXiv.org",
    "year": 2019,
    "referenceCount": 166,
    "citationCount": 71,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "89093987",
        "name": "Shane Storks"
      },
      {
        "authorId": "3193409",
        "name": "Qiaozi Gao"
      },
      {
        "authorId": "1707259",
        "name": "J. Chai"
      }
    ]
  },
  "15710851": {
    "paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f",
    "externalIds": {
      "DBLP": "conf/aaaiss/Levesque11",
      "MAG": "2267020232",
      "CorpusId": 15710851
    },
    "publicationVenue": null,
    "title": "The Winograd Schema Challenge",
    "abstract": "In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. Like the original, it involves responding to typed English sentences, and English-speaking adults will have no difficulty with it. Unlike the original, the subject is not required to engage in a conversation and fool an interrogator into believing she is dealing with a person. Moreover, the test is arranged in such a way that having full access to a large corpus of English text might not help much. Finally, the interrogator or a third party will be able to decide unambiguously after a few minutes whether or not a subject has passed the test.",
    "venue": "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning",
    "year": 2011,
    "referenceCount": 26,
    "citationCount": 1263,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143634377",
        "name": "H. Levesque"
      },
      {
        "authorId": "144883814",
        "name": "E. Davis"
      },
      {
        "authorId": "40429476",
        "name": "L. Morgenstern"
      }
    ]
  },
  "15206880": {
    "paperId": "26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
    "externalIds": {
      "DBLP": "journals/corr/SpeerCH16",
      "ArXiv": "1612.03975",
      "MAG": "2561529111",
      "DOI": "10.1609/aaai.v31i1.11164",
      "CorpusId": 15206880
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
    "abstract": "\n \n Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.\n \n",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2016,
    "referenceCount": 40,
    "citationCount": 2612,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145696762",
        "name": "R. Speer"
      },
      {
        "authorId": "2060230787",
        "name": "Joshua Chin"
      },
      {
        "authorId": "2232845",
        "name": "Catherine Havasi"
      }
    ]
  },
  "16567195": {
    "paperId": "cceb698cbbb828537f2f195fb70b6fdc586d3327",
    "externalIds": {
      "MAG": "2050482109",
      "DBLP": "conf/cikm/GordonD13",
      "DOI": "10.1145/2509558.2509563",
      "CorpusId": 16567195
    },
    "publicationVenue": {
      "id": "c2abee38-d372-4e2a-ac1e-2cd22999a564",
      "name": "Conference on Automated Knowledge Base Construction",
      "type": "conference",
      "alternate_names": [
        "AKBC",
        "Conf Autom Knowl Base Constr",
        "Automated Knowledge Base Construction",
        "Autom Knowl Base Constr"
      ],
      "url": "https://www.akbc.ws/"
    },
    "title": "Reporting bias and knowledge acquisition",
    "abstract": "Much work in knowledge extraction from text tacitly assumes that the frequency with which people write about actions, outcomes, or properties is a reflection of real-world frequencies or the degree to which a property is characteristic of a class of individuals. In this paper, we question this idea, examining the phenomenon of reporting bias and the challenge it poses for knowledge extraction. We conclude with discussion of approaches to learning commonsense knowledge from text despite this distortion.",
    "venue": "Conference on Automated Knowledge Base Construction",
    "year": 2013,
    "referenceCount": 38,
    "citationCount": 209,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145402198",
        "name": "Jonathan Gordon"
      },
      {
        "authorId": "7536576",
        "name": "Benjamin Van Durme"
      }
    ]
  },
  "1726501": {
    "paperId": "85b68477a6e031d88b963833e15a4b4fc6855264",
    "externalIds": {
      "MAG": "2891014615",
      "DBLP": "conf/naacl/MostafazadehCHP16",
      "ACL": "N16-1098",
      "ArXiv": "1604.01696",
      "DOI": "10.18653/v1/N16-1098",
      "CorpusId": 1726501
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories",
    "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 50,
    "citationCount": 665,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2400138",
        "name": "N. Mostafazadeh"
      },
      {
        "authorId": "1729918",
        "name": "Nathanael Chambers"
      },
      {
        "authorId": "144137069",
        "name": "Xiaodong He"
      },
      {
        "authorId": "153432684",
        "name": "Devi Parikh"
      },
      {
        "authorId": "1746610",
        "name": "Dhruv Batra"
      },
      {
        "authorId": "1909300",
        "name": "Lucy Vanderwende"
      },
      {
        "authorId": "143967473",
        "name": "Pushmeet Kohli"
      },
      {
        "authorId": "145844737",
        "name": "James F. Allen"
      }
    ]
  },
  "53296520": {
    "paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
    "externalIds": {
      "MAG": "2952331680",
      "ACL": "N19-1421",
      "DBLP": "journals/corr/abs-1811-00937",
      "ArXiv": "1811.00937",
      "DOI": "10.18653/v1/N19-1421",
      "CorpusId": 53296520
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
    "abstract": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 44,
    "citationCount": 1354,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "12371246",
        "name": "Alon Talmor"
      },
      {
        "authorId": "47426264",
        "name": "Jonathan Herzig"
      },
      {
        "authorId": "35219984",
        "name": "Nicholas Lourie"
      },
      {
        "authorId": "1750652",
        "name": "Jonathan Berant"
      }
    ]
  },
  "60742189": {
    "paperId": "54ffc8f1cb11ec21eb14a6706b8b6d9b192a1b32",
    "externalIds": {
      "MAG": "1489140743",
      "CorpusId": 60742189
    },
    "publicationVenue": null,
    "title": "A Semantic Approach to English Grammar",
    "abstract": "This book shows how grammar helps people communicate and looks at the ways grammar and meaning interrelate. The author starts from the notion that a speaker codes a meaning into grammatical forms which the listener is then able to recover: each word, he shows, has its own meaning and each bit of grammar its own function, their combinations creating and limiting the possibilities for different words. He uncovers a rationale for the varying grammatical properties of different words and in the process explains many facts about English - such as why we can say I wish to go, I wish that he would go, and I want to go but not I want that he would go. The first part of the book reviews the main points of English syntax and discusses English verbs in terms of their semantic types including those of Motion, Giving, Speaking, Liking, and Trying. In the second part Professor Dixon looks at eight grammatical topics, including complement clauses, transitivity and causatives, passives, and the promotion of a non-subject to subject, as in Dictionaries sell well. This is the updated and revised edition of A New Approach to English Grammar on Semantic Principles. It includes new chapters on tense and aspect, nominalizations and possession, and adverbs and negation, and contains a new discussion of comparative forms of adjectives. It also explains recent changes in English grammar, including how they has replaced the tabooed he as a pronoun referring to either gender, as in When a student reads this book, they will learn a lot about English grammar in a most enjoyable manner.",
    "venue": "",
    "year": 2005,
    "referenceCount": 0,
    "citationCount": 223,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34256957",
        "name": "R. Dixon"
      }
    ]
  },
  "1642392": {
    "paperId": "eec3a236ecd185712ce65fb336141f8656eea13d",
    "externalIds": {
      "DBLP": "journals/tacl/KiperwasserG16",
      "MAG": "2950886545",
      "ArXiv": "1603.04351",
      "ACL": "Q16-1023",
      "DOI": "10.1162/tacl_a_00101",
      "CorpusId": 1642392
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations",
    "abstract": "We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 52,
    "citationCount": 658,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2022679",
        "name": "E. Kiperwasser"
      },
      {
        "authorId": "2089067",
        "name": "Yoav Goldberg"
      }
    ]
  },
  "8233374": {
    "paperId": "4a715ea217dc5ecc2b16d6cf542bfb3f4a10f2b5",
    "externalIds": {
      "MAG": "3100095716",
      "DBLP": "journals/corr/HershcovichAR17",
      "ArXiv": "1704.00552",
      "ACL": "P17-1104",
      "DOI": "10.18653/v1/P17-1104",
      "CorpusId": 8233374
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Transition-Based Directed Acyclic Graph Parser for UCCA",
    "abstract": "We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 77,
    "citationCount": 93,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2086349",
        "name": "Daniel Hershcovich"
      },
      {
        "authorId": "2769805",
        "name": "Omri Abend"
      },
      {
        "authorId": "145009917",
        "name": "A. Rappoport"
      }
    ]
  },
  "15939234": {
    "paperId": "4908fc4d7f58383170c085fe8238a868e9a901f9",
    "externalIds": {
      "MAG": "2952816901",
      "DBLP": "conf/acl/PengTS17",
      "ArXiv": "1704.06855",
      "ACL": "P17-1186",
      "DOI": "10.18653/v1/P17-1186",
      "CorpusId": 15939234
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Deep Multitask Learning for Semantic Dependency Parsing",
    "abstract": "We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches\u2014one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at https://github.com/Noahs-ARK/NeurboParser.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 75,
    "citationCount": 144,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1818378366",
        "name": "Hao Peng"
      },
      {
        "authorId": "38094552",
        "name": "Sam Thomson"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      }
    ]
  },
  "19488885": {
    "paperId": "7ada8577807aefcad4f8120e8a031cceba065ec9",
    "externalIds": {
      "ACL": "P18-1035",
      "ArXiv": "1805.00287",
      "MAG": "2964233404",
      "DBLP": "conf/acl/AbendRH18",
      "DOI": "10.18653/v1/P18-1035",
      "CorpusId": 19488885
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Multitask Parsing Across Semantic Representations",
    "abstract": "The ability to consolidate information of different types is at the core of intelligence, and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others. In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 87,
    "citationCount": 67,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2086349",
        "name": "Daniel Hershcovich"
      },
      {
        "authorId": "2769805",
        "name": "Omri Abend"
      },
      {
        "authorId": "145009917",
        "name": "A. Rappoport"
      }
    ]
  },
  "7741748": {
    "paperId": "e32e3feb2225b427caa05eb26f241671196fc942",
    "externalIds": {
      "ACL": "P17-1008",
      "DBLP": "conf/acl/AbendR17",
      "MAG": "2741481273",
      "DOI": "10.18653/v1/P17-1008",
      "CorpusId": 7741748
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "The State of the Art in Semantic Representation",
    "abstract": "Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 119,
    "citationCount": 81,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2769805",
        "name": "Omri Abend"
      },
      {
        "authorId": "145009917",
        "name": "A. Rappoport"
      }
    ]
  },
  "11461990": {
    "paperId": "02258d796c3b52c2fd88bca8300465ba79f6199a",
    "externalIds": {
      "MAG": "2623186251",
      "DBLP": "journals/coling/DengX17",
      "ACL": "J17-3002",
      "DOI": "10.1162/COLI_a_00292",
      "CorpusId": 11461990
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Translation Divergences in Chinese\u2013English Machine Translation: An Empirical Investigation",
    "abstract": "In this article, we conduct an empirical investigation of translation divergences between Chinese and English relying on a parallel treebank. To do this, we first devise a hierarchical alignment scheme where Chinese and English parse trees are aligned in a way that eliminates conflicts and redundancies between word alignments and syntactic parses to prevent the generation of spurious translation divergences. Using this Hierarchically Aligned Chinese\u2013English Parallel Treebank (HACEPT), we are able to semi-automatically identify and categorize the translation divergences between the two languages and quantify each type of translation divergence. Our results show that the translation divergences are much broader than described in previous studies that are largely based on anecdotal evidence and linguistic knowledge. The distribution of the translation divergences also shows that some high-profile translation divergences that motivate previous research are actually very rare in our data, whereas other translation divergences that have previously received little attention actually exist in large quantities. We also show that HACEPT allows the extraction of syntax-based translation rules, most of which are expressive enough to capture the translation divergences, and point out that the syntactic annotation in existing treebanks is not optimal for extracting such translation rules. We also discuss the implications of our study for attempts to bridge translation divergences by devising shared semantic representations across languages. Our quantitative results lend further support to the observation that although it is possible to bridge some translation divergences with semantic representations, other translation divergences are open-ended, thus building a semantic representation that captures all possible translation divergences may be impractical.",
    "venue": "International Conference on Computational Logic",
    "year": 2017,
    "referenceCount": 89,
    "citationCount": 27,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "121137142",
        "name": "D. Deng"
      },
      {
        "authorId": "1702849",
        "name": "Nianwen Xue"
      }
    ]
  },
  "245635": {
    "paperId": "6c6fa1184cfc25b0e1b9e2c835b40be4d716bfe2",
    "externalIds": {
      "DBLP": "conf/tlt/CroftNLR17",
      "MAG": "2585756977",
      "CorpusId": 245635
    },
    "publicationVenue": {
      "id": "32e97278-1c8c-4e1f-a5b6-9a6359ea439d",
      "name": "International Workshop on Treebanks and Linguistic Theories",
      "type": "conference",
      "alternate_names": [
        "TLT",
        "Int Workshop Treebanks Linguistic Theor"
      ]
    },
    "title": "Linguistic Typology meets Universal Dependencies",
    "abstract": "Current work on universal dependency schemes in NLP does not make reference to the extensive typological research on language universals, but could bene\ufb01t since many principles are shared between the two enterprises. We propose a revision of the syntactic dependencies in the Universal Dependencies scheme (Nivre et al. [16, 17]) based on four principles derived from contemporary typological theory: dependencies should be based primarily on universal construction types over language-speci\ufb01c strategies; syntactic dependency labels should match lexical feature names for the same function; dependencies should be based on the information packaging function of constructions, not lexical semantic types; and dependencies should keep distinct the \u201cranks\u201d of the functional dependency tree.",
    "venue": "International Workshop on Treebanks and Linguistic Theories",
    "year": 2017,
    "referenceCount": 33,
    "citationCount": 41,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144456145",
        "name": "W. Bruce Croft"
      },
      {
        "authorId": "8613581",
        "name": "D. Nordquist"
      },
      {
        "authorId": "27963760",
        "name": "Katherine Looney"
      },
      {
        "authorId": "145666891",
        "name": "Michael Regan"
      }
    ]
  },
  "216056404": {
    "paperId": "b805693c17961af2cc7f859c1a54320b26036f46",
    "externalIds": {
      "DBLP": "journals/corr/abs-2004-10643",
      "ACL": "2020.lrec-1.497",
      "ArXiv": "2004.10643",
      "MAG": "3018647120",
      "CorpusId": 216056404
    },
    "publicationVenue": {
      "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
      "name": "International Conference on Language Resources and Evaluation",
      "type": "conference",
      "alternate_names": [
        "LREC",
        "Int Conf Lang Resour Evaluation"
      ],
      "url": "http://www.lrec-conf.org/"
    },
    "title": "Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection",
    "abstract": "Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. The annotation consists in a linguistically motivated word segmentation; a morphological layer comprising lemmas, universal part-of-speech tags, and standardized morphological features; and a syntactic layer focusing on syntactic relations between predicates, arguments and modifiers. In this paper, we describe version 2 of the universal guidelines (UD v2), discuss the major changes from UD v1 to UD v2, and give an overview of the currently available treebanks for 90 languages.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2020,
    "referenceCount": 22,
    "citationCount": 471,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1720988",
        "name": "Joakim Nivre"
      },
      {
        "authorId": "2241127",
        "name": "M. Marneffe"
      },
      {
        "authorId": "1694491",
        "name": "Filip Ginter"
      },
      {
        "authorId": "1602260260",
        "name": "Jan Hajivc"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      },
      {
        "authorId": "1708916",
        "name": "S. Pyysalo"
      },
      {
        "authorId": "145157639",
        "name": "Sebastian Schuster"
      },
      {
        "authorId": "3262036",
        "name": "Francis M. Tyers"
      },
      {
        "authorId": "1771298",
        "name": "Daniel Zeman"
      }
    ]
  },
  "15829786": {
    "paperId": "e569d99f3a0fcfa038631dda2b44c73a6e8e97b8",
    "externalIds": {
      "DBLP": "conf/sc/Schutze92",
      "MAG": "2149671658",
      "DOI": "10.1109/SUPERC.1992.236684",
      "CorpusId": 15829786
    },
    "publicationVenue": null,
    "title": "Dimensions of meaning",
    "abstract": "The representation of documents and queries as vectors in a high-dimensional space is well-established in information retrieval. The author proposes that the semantics of words and contexts in a text be represented as vectors. The dimensions of the space are words and the initial vectors are determined by the words occurring close to the entity to be represented, which implies that the space has several thousand dimensions (words). This makes the vector representations (which are dense) too cumbersome to use directly. Therefore, dimensionality reduction by means of a singular value decomposition is employed. The author analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction. >",
    "venue": "Supercomputing '92",
    "year": 1992,
    "referenceCount": 13,
    "citationCount": 467,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144418438",
        "name": "Hinrich Sch\u00fctze"
      }
    ]
  },
  "5959482": {
    "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
    "externalIds": {
      "MAG": "2950577311",
      "DBLP": "journals/corr/abs-1301-3781",
      "ArXiv": "1301.3781",
      "CorpusId": 5959482
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
    "venue": "International Conference on Learning Representations",
    "year": 2013,
    "referenceCount": 36,
    "citationCount": 29920,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2047446108",
        "name": "Tomas Mikolov"
      },
      {
        "authorId": "2118440152",
        "name": "Kai Chen"
      },
      {
        "authorId": "32131713",
        "name": "G. Corrado"
      },
      {
        "authorId": "49959210",
        "name": "J. Dean"
      }
    ]
  },
  "1957433": {
    "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
    "externalIds": {
      "DBLP": "conf/emnlp/PenningtonSM14",
      "ACL": "D14-1162",
      "MAG": "2250539671",
      "DOI": "10.3115/v1/D14-1162",
      "CorpusId": 1957433
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2014,
    "referenceCount": 32,
    "citationCount": 30669,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143845796",
        "name": "Jeffrey Pennington"
      },
      {
        "authorId": "2166511",
        "name": "R. Socher"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      }
    ]
  },
  "7890036": {
    "paperId": "59761abc736397539bdd01ad7f9d91c8607c0457",
    "externalIds": {
      "MAG": "2507974895",
      "DBLP": "conf/conll/MelamudGD16",
      "ACL": "K16-1006",
      "DOI": "10.18653/v1/K16-1006",
      "CorpusId": 7890036
    },
    "publicationVenue": {
      "id": "3779a5a7-9119-4f69-84fe-f7eef193eb49",
      "name": "Conference on Computational Natural Language Learning",
      "type": "conference",
      "alternate_names": [
        "CoNLL",
        "Conf Comput Nat Lang Learn"
      ]
    },
    "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM",
    "abstract": "Context representations are central to various NLP tasks, such as word sense disam-biguation, named entity recognition, co-reference resolution, and many more. In this work we present a neural model for ef\ufb01ciently learning a generic context embedding function from large corpora, us-ing bidirectional LSTM. With a very simple application of our context representations, we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pre-trained models, suggesting they could be useful in a wide variety of NLP tasks.",
    "venue": "Conference on Computational Natural Language Learning",
    "year": 2016,
    "referenceCount": 36,
    "citationCount": 484,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2298649",
        "name": "Oren Melamud"
      },
      {
        "authorId": "34508613",
        "name": "J. Goldberger"
      },
      {
        "authorId": "7465342",
        "name": "Ido Dagan"
      }
    ]
  },
  "3626819": {
    "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
    "externalIds": {
      "DBLP": "journals/corr/abs-1802-05365",
      "MAG": "2949856395",
      "ArXiv": "1802.05365",
      "ACL": "N18-1202",
      "DOI": "10.18653/v1/N18-1202",
      "CorpusId": 3626819
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Deep Contextualized Word Representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 64,
    "citationCount": 11113,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39139825",
        "name": "Matthew E. Peters"
      },
      {
        "authorId": "50043859",
        "name": "Mark Neumann"
      },
      {
        "authorId": "2136562",
        "name": "Mohit Iyyer"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      },
      {
        "authorId": "143997772",
        "name": "Christopher Clark"
      },
      {
        "authorId": "2544107",
        "name": "Kenton Lee"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ]
  },
  "13696533": {
    "paperId": "bf9db8ca2dce7386cbed1ae0fd6465148cdb2b98",
    "externalIds": {
      "MAG": "2799894091",
      "DBLP": "journals/jair/Camacho-Collados18",
      "ArXiv": "1805.04032",
      "DOI": "10.1613/JAIR.1.11259",
      "CorpusId": 13696533
    },
    "publicationVenue": {
      "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
      "name": "Journal of Artificial Intelligence Research",
      "type": "journal",
      "alternate_names": [
        "JAIR",
        "J Artif Intell Res",
        "The Journal of Artificial Intelligence Research"
      ],
      "issn": "1076-9757",
      "url": "http://www.jair.org/"
    },
    "title": "From Word to Sense Embeddings: A Survey on Vector Representations of Meaning",
    "abstract": "\n \n \nOver the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality. \n \n \n",
    "venue": "Journal of Artificial Intelligence Research",
    "year": 2018,
    "referenceCount": 238,
    "citationCount": 317,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1387447871",
        "name": "Jos\u00e9 Camacho-Collados"
      },
      {
        "authorId": "1717641",
        "name": "Mohammad Taher Pilehvar"
      }
    ]
  },
  "52098907": {
    "paperId": "ac11062f1f368d97f4c826c317bf50dcc13fdb59",
    "externalIds": {
      "DBLP": "conf/emnlp/PetersNZY18",
      "ArXiv": "1808.08949",
      "MAG": "2950405925",
      "ACL": "D18-1179",
      "DOI": "10.18653/v1/D18-1179",
      "CorpusId": 52098907
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Dissecting Contextual Word Embeddings: Architecture and Representation",
    "abstract": "Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 56,
    "citationCount": 406,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39139825",
        "name": "Matthew E. Peters"
      },
      {
        "authorId": "50043859",
        "name": "Mark Neumann"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      },
      {
        "authorId": "144105277",
        "name": "Wen-tau Yih"
      }
    ]
  },
  "52967399": {
    "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
    "externalIds": {
      "MAG": "2951055169",
      "ACL": "N19-1423",
      "DBLP": "journals/corr/abs-1810-04805",
      "ArXiv": "1810.04805",
      "DOI": "10.18653/v1/N19-1423",
      "CorpusId": 52967399
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 63,
    "citationCount": 83753,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39172707",
        "name": "Jacob Devlin"
      },
      {
        "authorId": "1744179",
        "name": "Ming-Wei Chang"
      },
      {
        "authorId": "2544107",
        "name": "Kenton Lee"
      },
      {
        "authorId": "3259253",
        "name": "Kristina Toutanova"
      }
    ]
  },
  "182952898": {
    "paperId": "a1f000b88e81f02b2a0d7a4097171428364af8c7",
    "externalIds": {
      "DBLP": "journals/corr/abs-1906-03824",
      "MAG": "2948714225",
      "ArXiv": "1906.03824",
      "CorpusId": 182952898
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Survey on Neural Machine Reading Comprehension",
    "abstract": "Enabling a machine to read and comprehend the natural language documents so that it can answer some questions remains an elusive challenge. In recent years, the popularity of deep learning and the establishment of large-scale datasets have both promoted the prosperity of Machine Reading Comprehension. This paper aims to present how to utilize the Neural Network to build a Reader and introduce some classic models, analyze what improvements they make. Further, we also point out the defects of existing models and future research directions",
    "venue": "arXiv.org",
    "year": 2019,
    "referenceCount": 33,
    "citationCount": 29,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2064466537",
        "name": "Boyu Qiu"
      },
      {
        "authorId": "2118183867",
        "name": "Xu Chen"
      },
      {
        "authorId": "2073589",
        "name": "Jungang Xu"
      },
      {
        "authorId": "46676156",
        "name": "Yingfei Sun"
      }
    ]
  },
  "213613608": {
    "paperId": "4043a936960de8e149dc208178fe1bcb157c7fa4",
    "externalIds": {
      "ArXiv": "1904.01172",
      "MAG": "2990752173",
      "CorpusId": 213613608
    },
    "publicationVenue": null,
    "title": "Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches",
    "abstract": "In the NLP community, recent years have seen a surge of research activities that address machines' ability to perform deep language understanding which goes beyond what is explicitly stated in text, rather relying on reasoning and knowledge of the world. Many benchmark tasks and datasets have been created to support the development and evaluation of such natural language inference ability. As these benchmarks become instrumental and a driving force for the NLP research community, this paper aims to provide an overview of recent benchmarks, relevant knowledge resources, and state-of-the-art learning and inference approaches in order to support a better understanding of this growing field.",
    "venue": "",
    "year": 2019,
    "referenceCount": 307,
    "citationCount": 115,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "89093987",
        "name": "Shane Storks"
      },
      {
        "authorId": "3193409",
        "name": "Qiaozi Gao"
      },
      {
        "authorId": "1707259",
        "name": "J. Chai"
      }
    ]
  },
  "219124082": {
    "paperId": "4311eefc03a3f391bae39ebf364cbd5f8b90a001",
    "externalIds": {
      "MAG": "3031912764",
      "DBLP": "journals/corr/abs-2005-14709",
      "ArXiv": "2005.14709",
      "CorpusId": 219124082
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Beyond Leaderboards: A survey of methods for revealing weaknesses in Natural Language Inference data and models",
    "abstract": "Recent years have seen a growing number of publications that analyse Natural Language Inference (NLI) datasets for superficial cues, whether they undermine the complexity of the tasks underlying those datasets and how they impact those models that are optimised and evaluated on this data. This structured survey provides an overview of the evolving research area by categorising reported weaknesses in models and datasets and the methods proposed to reveal and alleviate those weaknesses for the English language. We summarise and discuss the findings and conclude with a set of recommendations for possible future research directions. We hope it will be a useful resource for researchers who propose new datasets, to have a set of tools to assess the suitability and quality of their data to evaluate various phenomena of interest, as well as those who develop novel architectures, to further understand the implications of their improvements with respect to their model's acquired capabilities.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 136,
    "citationCount": 18,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "71034258",
        "name": "Viktor Schlegel"
      },
      {
        "authorId": "2144507",
        "name": "G. Nenadic"
      },
      {
        "authorId": "1400900759",
        "name": "R. Batista-Navarro"
      }
    ]
  },
  "219306476": {
    "paperId": "20499f3c6fe9f84a12c9def941e2e12846a00c77",
    "externalIds": {
      "DBLP": "conf/conll/NgWBHSB14",
      "MAG": "2098297786",
      "ACL": "W14-1701",
      "DOI": "10.3115/v1/W14-1701",
      "CorpusId": 219306476
    },
    "publicationVenue": null,
    "title": "The CoNLL-2014 Shared Task on Grammatical Error Correction",
    "abstract": "The CoNLL-2014 shared task was devoted to grammatical error correction of all error types. In this paper, we give the task definition, present the data sets, and describe the evaluation metric and scorer used in the shared task. We also give an overview of the various approaches adopted by the participating teams, and present the evaluation results. Compared to the CoNLL2013 shared task, we have introduced the following changes in CoNLL-2014: (1) A participating system is expected to detect and correct grammatical errors of all types, instead of just the five error types in CoNLL-2013; (2) The evaluation metric was changed from F1 to F0.5, to emphasize precision over recall; and (3) We have two human annotators who independently annotated the test essays, compared to just one human annotator in CoNLL-2013.",
    "venue": "CoNLL Shared Task",
    "year": 2014,
    "referenceCount": 31,
    "citationCount": 496,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34789794",
        "name": "H. Ng"
      },
      {
        "authorId": "2069266",
        "name": "S. Wu"
      },
      {
        "authorId": "145693410",
        "name": "Ted Briscoe"
      },
      {
        "authorId": "3271719",
        "name": "Christian Hadiwinoto"
      },
      {
        "authorId": "32406168",
        "name": "Raymond Hendy Susanto"
      },
      {
        "authorId": "145178009",
        "name": "Christopher Bryant"
      }
    ]
  },
  "18051414": {
    "paperId": "be08a1189ce88c4a6d6a98784377eb02e578d95a",
    "externalIds": {
      "DBLP": "journals/tacl/RozovskayaR14",
      "ACL": "Q14-1033",
      "MAG": "2238150896",
      "DOI": "10.1162/tacl_a_00193",
      "CorpusId": 18051414
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Building a State-of-the-Art Grammatical Error Correction System",
    "abstract": "This paper identifies and examines the key principles underlying building a state-of-the-art grammatical error correction system. We do this by analyzing the Illinois system that placed first among seventeen teams in the recent CoNLL-2013 shared task on grammatical error correction. The system focuses on five different types of errors common among non-native English writers. We describe four design principles that are relevant for correcting all of these errors, analyze the system along these dimensions, and show how each of these dimensions contributes to the performance.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2014,
    "referenceCount": 47,
    "citationCount": 33,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2271568",
        "name": "Alla Rozovskaya"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "6820419": {
    "paperId": "b19f365aab0bf8c6cf712c07313b919556bfacc0",
    "externalIds": {
      "ACL": "D16-1161",
      "MAG": "2964187553",
      "DBLP": "journals/corr/Junczys-Dowmunt16b",
      "ArXiv": "1605.06353",
      "DOI": "10.18653/v1/D16-1161",
      "CorpusId": 6820419
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction",
    "abstract": "In this work, we study parameter tuning towards the M^2 metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M^2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M^2 and offer partial solutions. We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M^2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M^2.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2016,
    "referenceCount": 36,
    "citationCount": 102,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1733933",
        "name": "Marcin Junczys-Dowmunt"
      },
      {
        "authorId": "3272639",
        "name": "Roman Grundkiewicz"
      }
    ]
  },
  "19236015": {
    "paperId": "6ed38b0cb510fa91434eb63ab464bee66c9323c6",
    "externalIds": {
      "DBLP": "conf/aaai/ChollampattN18",
      "MAG": "2951200639",
      "ArXiv": "1801.08831",
      "DOI": "10.1609/aaai.v32i1.12069",
      "CorpusId": 19236015
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction",
    "abstract": "\n \n We improve automatic correction of grammatical, orthographic, and collocation errors in text using a multilayer convolutional encoder-decoder neural network. The network is initialized with embeddings that make use of character N-gram information to better suit this task. When evaluated on common benchmark test data sets (CoNLL-2014 and JFLEG), our model substantially outperforms all prior neural approaches on this task as well as strong statistical machine translation-based systems with neural and task-specific features trained on the same data. Our analysis shows the superiority of convolutional neural networks over recurrent neural networks such as long short-term memory (LSTM) networks in capturing the local context via attention, and thereby improving the coverage in correcting grammatical errors. By ensembling multiple models, and incorporating an N-gram language model and edit features via rescoring, our novel method becomes the first neural approach to outperform the current state-of-the-art statistical machine translation-based approach, both in terms of grammaticality and fluency.\n \n",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2018,
    "referenceCount": 27,
    "citationCount": 208,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3422793",
        "name": "Shamil Chollampatt"
      },
      {
        "authorId": "34789794",
        "name": "H. Ng"
      }
    ]
  },
  "195504787": {
    "paperId": "7cc6f009feb5ad5ad0e1ff00c551fb318fc95016",
    "externalIds": {
      "ACL": "W19-4427",
      "MAG": "2948335087",
      "DBLP": "conf/bea/GrundkiewiczJH19",
      "DOI": "10.18653/v1/W19-4427",
      "CorpusId": 195504787
    },
    "publicationVenue": null,
    "title": "Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data",
    "abstract": "Considerable effort has been made to address the data sparsity problem in neural grammatical error correction. In this work, we propose a simple and surprisingly effective unsupervised synthetic error generation method based on confusion sets extracted from a spellchecker to increase the amount of training data. Synthetic data is used to pre-train a Transformer sequence-to-sequence model, which not only improves over a strong baseline trained on authentic error-annotated data, but also enables the development of a practical GEC system in a scenario where little genuine error-annotated data is available. The developed systems placed first in the BEA19 shared task, achieving 69.47 and 64.24 F_{0.5} in the restricted and low-resource tracks respectively, both on the W&I+LOCNESS test set. On the popular CoNLL 2014 test set, we report state-of-the-art results of 64.16 M\u00b2 for the submitted system, and 61.30 M\u00b2 for the constrained system trained on the NUCLE and Lang-8 data.",
    "venue": "BEA@ACL",
    "year": 2019,
    "referenceCount": 55,
    "citationCount": 166,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3272639",
        "name": "Roman Grundkiewicz"
      },
      {
        "authorId": "1733933",
        "name": "Marcin Junczys-Dowmunt"
      },
      {
        "authorId": "1702066",
        "name": "Kenneth Heafield"
      }
    ]
  },
  "202539354": {
    "paperId": "1a5ef51ae0c0ee1216e14aa390734cf7581c3b27",
    "externalIds": {
      "ACL": "D19-1119",
      "ArXiv": "1909.00502",
      "DBLP": "journals/corr/abs-1909-00502",
      "MAG": "2970521905",
      "DOI": "10.18653/v1/D19-1119",
      "CorpusId": 202539354
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction",
    "abstract": "The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models. However, consensus is lacking on experimental configurations, namely, choosing how the pseudo data should be generated or used. In this study, these choices are investigated through extensive experiments, and state-of-the-art performance is achieved on the CoNLL-2014 test set (F0.5=65.0) and the official test set of the BEA-2019 shared task (F0.5=70.2) without making any modifications to the model architecture.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 40,
    "citationCount": 142,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32140786",
        "name": "Shun Kiyono"
      },
      {
        "authorId": "144042991",
        "name": "Jun Suzuki"
      },
      {
        "authorId": "35643168",
        "name": "Masato Mita"
      },
      {
        "authorId": "3079116",
        "name": "Tomoya Mizumoto"
      },
      {
        "authorId": "3040648",
        "name": "Kentaro Inui"
      }
    ]
  },
  "48356442": {
    "paperId": "b4aa5354e88564b2e4eeee3019ed04e5388042f3",
    "externalIds": {
      "MAG": "2807710978",
      "DBLP": "conf/coling/MagerGSM18",
      "ArXiv": "1806.04291",
      "ACL": "C18-1006",
      "CorpusId": 48356442
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Challenges of language technologies for the indigenous languages of the Americas",
    "abstract": "Indigenous languages of the American continent are highly diverse. However, they have received little attention from the technological perspective. In this paper, we review the research, the digital resources and the available NLP systems that focus on these languages. We present the main challenges and research questions that arise when distant languages and low-resource scenarios are faced. We would like to encourage NLP research in linguistically rich and diverse areas like the Americas.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2018,
    "referenceCount": 97,
    "citationCount": 82,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "153151470",
        "name": "Manuel Mager"
      },
      {
        "authorId": "1409305289",
        "name": "Ximena Gutierrez-Vasques"
      },
      {
        "authorId": "32889164",
        "name": "Gerardo E Sierra"
      },
      {
        "authorId": "1403616824",
        "name": "Ivan Vladimir Meza Ruiz"
      }
    ]
  },
  "38196008": {
    "paperId": "842232e4c1f11239fb67cb0f1bc068149df64183",
    "externalIds": {
      "ACL": "J09-3007",
      "DOI": "10.1162/coli.35.3.469",
      "CorpusId": 38196008
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Last Words: Natural Language Processing and Linguistic Fieldwork",
    "abstract": null,
    "venue": "International Conference on Computational Logic",
    "year": null,
    "referenceCount": 0,
    "citationCount": 35,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "21308992",
        "name": "Steven Bird"
      }
    ]
  },
  "201666291": {
    "paperId": "21d4ffff54d3f71d028f9bda89c22a496e0fbb82",
    "externalIds": {
      "DBLP": "journals/corr/abs-1908-08971",
      "ArXiv": "1908.08971",
      "MAG": "2969530280",
      "CorpusId": 201666291
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Deploying Technology to Save Endangered Languages",
    "abstract": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages.",
    "venue": "arXiv.org",
    "year": 2019,
    "referenceCount": 5,
    "citationCount": 6,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Environmental Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "38878141",
        "name": "Hilaria Cruz"
      },
      {
        "authorId": "2060201926",
        "name": "Joseph Waring"
      }
    ]
  },
  "146056073": {
    "paperId": "55b9e8f6194e745190cce0dd6db7eca4ff53a260",
    "externalIds": {
      "MAG": "2937657581",
      "ACL": "W19-6003",
      "DOI": "10.33011/COMPUTEL.V1I.341",
      "CorpusId": 146056073
    },
    "publicationVenue": null,
    "title": "Future Directions in Technological Support for Language Documentation",
    "abstract": "To reduce the annotation burden placed on linguistic fieldworkers, freeing up time for deeper linguistic analysis and descriptive work, the language documentation community has been working with machine learning researchers to investigate what assistive role technology can play, with promising early results. This paper describes a number of potential follow-up technical projects that we believe would be worthwhile and straightforward to do. We provide examples of the annotation tasks for computer scientists; descriptions of the technological challenges involved and the estimated level of complexity; and pointers to relevant literature. We hope providing a clear overview of what the needs are and what annotation challenges exist will help facilitate the dialogue and collaboration between computer scientists and fieldwork linguists.",
    "venue": "Proceedings of the Workshop on Computational Methods for Endangered Languages",
    "year": 2019,
    "referenceCount": 55,
    "citationCount": 17,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8775666",
        "name": "D. Esch"
      },
      {
        "authorId": "92304991",
        "name": "Ben Foley"
      },
      {
        "authorId": "79396737",
        "name": "Nay San"
      }
    ]
  },
  "52011439": {
    "paperId": "243880fde63abfc287bd1356c2e1dbf68a1a0aac",
    "externalIds": {
      "MAG": "2823189734",
      "ACL": "C18-1222",
      "DBLP": "conf/coling/LittellKKPACJ18",
      "CorpusId": 52011439
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Indigenous language technologies in Canada: Assessment, challenges, and successes",
    "abstract": "In this article, we discuss which text, speech, and image technologies have been developed, and would be feasible to develop, for the approximately 60 Indigenous languages spoken in Canada. In particular, we concentrate on technologies that may be feasible to develop for most or all of these languages, not just those that may be feasible for the few most-resourced of these. We assess past achievements and consider future horizons for Indigenous language transliteration, text prediction, spell-checking, approximate search, machine translation, speech recognition, speaker diarization, speech synthesis, optical character recognition, and computer-aided language learning.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2018,
    "referenceCount": 47,
    "citationCount": 40,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3070462",
        "name": "Patrick Littell"
      },
      {
        "authorId": "145131025",
        "name": "Anna Kazantseva"
      },
      {
        "authorId": "143937779",
        "name": "R. Kuhn"
      },
      {
        "authorId": "51183422",
        "name": "Aidan Pine"
      },
      {
        "authorId": "2587266",
        "name": "Antti Arppe"
      },
      {
        "authorId": "2052347303",
        "name": "Christopher Cox"
      },
      {
        "authorId": "46230685",
        "name": "M. Junker"
      }
    ]
  },
  "11955283": {
    "paperId": "82e14d316f8e21b883a6a580f29c9953e6ce1886",
    "externalIds": {
      "DBLP": "journals/speech/BesacierBKS14a",
      "MAG": "2091746061",
      "DOI": "10.1016/J.SPECOM.2013.07.008",
      "CorpusId": 11955283
    },
    "publicationVenue": {
      "id": "e819bc36-457b-4018-82c7-cd1c01850557",
      "name": "Speech Communication",
      "type": "journal",
      "alternate_names": [
        "Speech Commun"
      ],
      "issn": "0167-6393",
      "url": "https://www.journals.elsevier.com/speech-communication",
      "alternate_urls": [
        "http://www.sciencedirect.com/science/journal/01676393"
      ]
    },
    "title": "Automatic speech recognition for under-resourced languages: A survey",
    "abstract": null,
    "venue": "Speech Communication",
    "year": 2014,
    "referenceCount": 163,
    "citationCount": 482,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143823463",
        "name": "L. Besacier"
      },
      {
        "authorId": "1790459",
        "name": "E. Barnard"
      },
      {
        "authorId": "145191867",
        "name": "Alexey Karpov"
      },
      {
        "authorId": "145618636",
        "name": "Tanja Schultz"
      }
    ]
  },
  "53065219": {
    "paperId": "c859416a8e5682bee3c35df29bc02e02a22de072",
    "externalIds": {
      "MAG": "2883972335",
      "CorpusId": 53065219
    },
    "publicationVenue": null,
    "title": "Integrating automatic transcription into the language documentation workflow: Experiments with Na data and the Persephone toolkit",
    "abstract": "Automatic speech recognition tools have potential for facilitating language documentation, but in practice these tools remain little-used by linguists for a variety of reasons, such as that the technology is still new (and evolving rapidly), user-friendly interfaces are still under development, and case studies demonstrating the practical usefulness of automatic recognition in a low-resource setting remain few. This article reports on a success story in integrating automatic transcription into the language documentation workflow, specifically for Yongning Na, a language of Southwest China. Using PERSEPHONE, an open-source toolkit, a single-speaker speech transcription tool was trained over five hours of manually transcribed speech. The experiments found that this method can achieve a remarkably low error rate (on the order of 17%), and that automatic transcriptions were useful as a canvas for the linguist. The present report is intended for linguists with little or no knowledge of speech processing. It aims to provide insights into (i) the way the tool operates and (ii) the process of collaborating with natural language processing specialists. Practical recommendations are offered on how to anticipate the requirements of this type of technology from the early stages of data collection in the field.",
    "venue": "",
    "year": 2018,
    "referenceCount": 96,
    "citationCount": 44,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39720748",
        "name": "Alexis Michaud"
      },
      {
        "authorId": "38535429",
        "name": "Oliver Adams"
      },
      {
        "authorId": "143620680",
        "name": "Trevor Cohn"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "46673007",
        "name": "Severine Guillaume"
      }
    ]
  },
  "53333371": {
    "paperId": "9476918d232768de4f2cbc13240c6626f49b4d04",
    "externalIds": {
      "CorpusId": 53333371
    },
    "publicationVenue": null,
    "title": "Building Speech Recognition Systems for Language Documentation: The CoEDL Endangered Language Pipeline and Inference System (Elpis)",
    "abstract": "Machine learning has revolutionised speech technologies for major world languages, but these technologies have generally not been available for the roughly 4,000 languages with populations of fewer than 10,000 speakers. This paper describes the development of Elpis, a pipeline which language documentation workers with minimal computational experience can use to build their own speech recognition models, resulting in models being built for 16 languages from the Asia-Pacific region. Elpis puts machine learning speech technologies within reach of people working with languages with scarce data, in a scalable way. This is impactful since it enables language communities to cross the digital divide, and speeds up language documentation. Complete automation of the process is not feasible for languages with small quantities of data and potentially large vocabularies. Hence our goal is not full automation, but rather to make a practical and effective workflow that integrates machine learning technologies.",
    "venue": "",
    "year": 2018,
    "referenceCount": 18,
    "citationCount": 47,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "92304991",
        "name": "Ben Foley"
      },
      {
        "authorId": "32286262",
        "name": "Joshua T. Arnold"
      },
      {
        "authorId": "1405433180",
        "name": "Rolando Coto-Solano"
      },
      {
        "authorId": "5963331",
        "name": "Gautier Durantin"
      },
      {
        "authorId": "144016062",
        "name": "T. M. Ellison"
      },
      {
        "authorId": "8775666",
        "name": "D. Esch"
      },
      {
        "authorId": "145936386",
        "name": "Scott Heath"
      },
      {
        "authorId": "4438531",
        "name": "Franti\u0161ek Kratochvil"
      },
      {
        "authorId": "1410034849",
        "name": "Zara Maxwell-Smith"
      },
      {
        "authorId": "2082556414",
        "name": "David Nash"
      },
      {
        "authorId": "26949393",
        "name": "Ola Olsson"
      },
      {
        "authorId": "2072102908",
        "name": "Mark Richards"
      },
      {
        "authorId": "79396737",
        "name": "Nay San"
      },
      {
        "authorId": "2343936",
        "name": "H. Stoakes"
      },
      {
        "authorId": "24739790",
        "name": "N. Thieberger"
      },
      {
        "authorId": "1716264",
        "name": "Janet Wiles"
      }
    ]
  },
  "201058388": {
    "paperId": "f249e3a7d4f7f964e9a4ca6e633ac31410a91dd8",
    "externalIds": {
      "ACL": "D19-1091",
      "ArXiv": "1908.05838",
      "MAG": "2968525655",
      "DBLP": "conf/emnlp/AnastasopoulosN19",
      "DOI": "10.18653/v1/D19-1091",
      "CorpusId": 201058388
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Pushing the Limits of Low-Resource Morphological Inflection",
    "abstract": "Recent years have seen exceptional strides in the task of automatic morphological inflection generation. However, for a long tail of languages the necessary resources are hard to come by, and state-of-the-art neural methods that work well under higher resource settings perform poorly in the face of a paucity of data. In response, we propose a battery of improvements that greatly improve performance under such low-resource conditions. First, we present a novel two-step attention architecture for the inflection decoder. In addition, we investigate the effects of cross-lingual transfer from single and multiple languages, as well as monolingual data hallucination. The macro-averaged accuracy of our models outperforms the state-of-the-art by 15 percentage points. Also, we identify the crucial factors for success with cross-lingual transfer for morphological inflection: typological similarity and a common representation across languages.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 39,
    "citationCount": 75,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49513989",
        "name": "Antonios Anastasopoulos"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  "63671278": {
    "paperId": "20394c89e24d9060ecc69b8a58bdab7833c5b5bd",
    "externalIds": {
      "MAG": "2540540486",
      "CorpusId": 63671278
    },
    "publicationVenue": null,
    "title": "Markov Logic: A Unifying Framework for Statistical Relational Learning",
    "abstract": "This chapter contains sections titled: The Need for a Unifying Framework, Markov Networks, First-order Logic, Markov Logic, SRL Approaches, SRL Tasks, Inference, Learning, Experiments, Conclusion, Acknowledgments, References",
    "venue": "",
    "year": 2007,
    "referenceCount": 15,
    "citationCount": 203,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1746034",
        "name": "L. Getoor"
      },
      {
        "authorId": "1685978",
        "name": "B. Taskar"
      }
    ]
  },
  "1067591": {
    "paperId": "ab4850b6151ca9a9337dbba94115bde342876d50",
    "externalIds": {
      "MAG": "2952952750",
      "ArXiv": "1102.1808",
      "DBLP": "journals/corr/abs-1102-1808",
      "DOI": "10.1007/s10994-013-5335-x",
      "CorpusId": 1067591
    },
    "publicationVenue": {
      "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
      "name": "Machine-mediated learning",
      "type": "journal",
      "alternate_names": [
        "Mach learn",
        "Machine Learning",
        "Mach Learn"
      ],
      "issn": "0732-6718",
      "alternate_issns": [
        "0885-6125"
      ],
      "url": "http://www.springer.com/computer/artificial/journal/10994",
      "alternate_urls": [
        "https://link.springer.com/journal/10994",
        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
      ]
    },
    "title": "From machine learning to machine reasoning",
    "abstract": null,
    "venue": "Machine-mediated learning",
    "year": 2011,
    "referenceCount": 64,
    "citationCount": 267,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "52184096",
        "name": "L. Bottou"
      }
    ]
  },
  "1755720": {
    "paperId": "9dbb506ded56ff4b7ab65aa92b363c0112987f10",
    "externalIds": {
      "MAG": "2767656849",
      "ArXiv": "1711.03902",
      "DBLP": "journals/corr/abs-1711-03902",
      "DOI": "10.3233/FAIA210348",
      "CorpusId": 1755720
    },
    "publicationVenue": null,
    "title": "Neural-Symbolic Learning and Reasoning: A Survey and Interpretation",
    "abstract": "The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.",
    "venue": "Neuro-Symbolic Artificial Intelligence",
    "year": 2017,
    "referenceCount": 176,
    "citationCount": 295,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143862012",
        "name": "Tarek R. Besold"
      },
      {
        "authorId": "2925941",
        "name": "A. Garcez"
      },
      {
        "authorId": "144349956",
        "name": "Sebastian Bader"
      },
      {
        "authorId": "145042515",
        "name": "H. Bowman"
      },
      {
        "authorId": "1740213",
        "name": "Pedro M. Domingos"
      },
      {
        "authorId": "1699771",
        "name": "P. Hitzler"
      },
      {
        "authorId": "1743582",
        "name": "Kai-Uwe K\u00fchnberger"
      },
      {
        "authorId": "2335532",
        "name": "L. Lamb"
      },
      {
        "authorId": "3021654",
        "name": "Daniel Lowd"
      },
      {
        "authorId": "144829981",
        "name": "P. Lima"
      },
      {
        "authorId": "2910868",
        "name": "L. Penning"
      },
      {
        "authorId": "2263909",
        "name": "Gadi Pinkas"
      },
      {
        "authorId": "1759772",
        "name": "Hoifung Poon"
      },
      {
        "authorId": "1753715",
        "name": "Gerson Zaverucha"
      }
    ]
  },
  "155092677": {
    "paperId": "833c4ac0599f4b8c5f1ee6ea948ec675fbe56b15",
    "externalIds": {
      "MAG": "2946752867",
      "ArXiv": "1905.06088",
      "DBLP": "journals/corr/abs-1905-06088",
      "CorpusId": 155092677
    },
    "publicationVenue": null,
    "title": "Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",
    "abstract": "Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.",
    "venue": "FLAP",
    "year": 2019,
    "referenceCount": 59,
    "citationCount": 261,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      },
      {
        "category": "Art",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2925941",
        "name": "A. Garcez"
      },
      {
        "authorId": "145467467",
        "name": "M. Gori"
      },
      {
        "authorId": "2335532",
        "name": "L. Lamb"
      },
      {
        "authorId": "144077615",
        "name": "L. Serafini"
      },
      {
        "authorId": "145570895",
        "name": "Michael Spranger"
      },
      {
        "authorId": "1930235",
        "name": "S. Tran"
      }
    ]
  },
  "51893222": {
    "paperId": "3df952d4a724655f7520ff95d4b2cef90fff0cae",
    "externalIds": {
      "MAG": "2886794383",
      "DBLP": "journals/corr/abs-1808-00033",
      "ArXiv": "1808.00033",
      "DOI": "10.1145/3359786",
      "CorpusId": 51893222
    },
    "publicationVenue": {
      "id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
      "name": "Communications of the ACM",
      "type": "journal",
      "alternate_names": [
        "Commun ACM",
        "Communications of The ACM"
      ],
      "issn": "0001-0782",
      "url": "http://www.acm.org/pubs/cacm/",
      "alternate_urls": [
        "http://portal.acm.org/cacm",
        "http://www.acm.org/pubs/contents/journals/cacm/",
        "https://cacm.acm.org/"
      ]
    },
    "title": "Techniques for interpretable machine learning",
    "abstract": "Uncovering the mysterious ways machine learning models make decisions.",
    "venue": "Communications of the ACM",
    "year": 2018,
    "referenceCount": 52,
    "citationCount": 975,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3432460",
        "name": "Mengnan Du"
      },
      {
        "authorId": "47717322",
        "name": "Ninghao Liu"
      },
      {
        "authorId": "48539382",
        "name": "Xia Hu"
      }
    ]
  },
  "220058074": {
    "paperId": "6efe7653b9a7928bc47b61dfeb84c0831a1d7a39",
    "externalIds": {
      "MAG": "3037722615",
      "DBLP": "conf/acl/ChenY20",
      "ACL": "2020.acl-tutorials.8",
      "DOI": "10.18653/v1/2020.acl-tutorials.8",
      "CorpusId": 220058074
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Open-Domain Question Answering",
    "abstract": "This tutorial provides a comprehensive and coherent overview of cutting-edge research in open-domain question answering (QA), the task of answering questions using a large collection of documents of diversified topics. We will start by first giving a brief historical background, discussing the basic setup and core technical challenges of the research problem, and then describe modern datasets with the common evaluation metrics and benchmarks. The focus will then shift to cutting-edge models proposed for open-domain QA, including two-stage retriever-reader approaches, dense retriever and end-to-end training, and retriever-free methods. Finally, we will cover some hybrid approaches using both text and large knowledge bases and conclude the tutorial with important open questions. We hope that the tutorial will not only help the audience to acquire up-to-date knowledge but also provide new perspectives to stimulate the advances of open-domain QA research in the next phase.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 36,
    "citationCount": 17,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "50536468",
        "name": "Danqi Chen"
      },
      {
        "authorId": "144105277",
        "name": "Wen-tau Yih"
      }
    ]
  },
  "220060314": {
    "paperId": "c24a3ba1f161df77bdf9374d787851d6ce7e366b",
    "externalIds": {
      "CorpusId": 220060314
    },
    "publicationVenue": null,
    "title": "Introductory Tutorial: Commonsense Reasoning for Natural Language Processing",
    "abstract": null,
    "venue": "",
    "year": null,
    "referenceCount": 74,
    "citationCount": 89,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2729164",
        "name": "Maarten Sap"
      },
      {
        "authorId": "3103343",
        "name": "Vered Shwartz"
      },
      {
        "authorId": "8536286",
        "name": "Antoine Bosselut"
      },
      {
        "authorId": "2257385140",
        "name": "Yejin Choi"
      },
      {
        "authorId": "2249759427",
        "name": "Dan Roth"
      }
    ]
  },
  "207718082": {
    "paperId": "cb40a5e6d4fc0290452345791bb91040aed76961",
    "externalIds": {
      "MAG": "2742330194",
      "DBLP": "journals/sigkdd/ShuSWTL17",
      "ArXiv": "1708.01967",
      "DOI": "10.1145/3137597.3137600",
      "CorpusId": 207718082
    },
    "publicationVenue": null,
    "title": "Fake News Detection on Social Media: A Data Mining Perspective",
    "abstract": "Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of \\fake news\", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ine ective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media.",
    "venue": "SKDD",
    "year": 2017,
    "referenceCount": 106,
    "citationCount": 2526,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145800151",
        "name": "Kai Shu"
      },
      {
        "authorId": "2880010",
        "name": "A. Sliva"
      },
      {
        "authorId": "2893721",
        "name": "Suhang Wang"
      },
      {
        "authorId": "1736632",
        "name": "Jiliang Tang"
      },
      {
        "authorId": "145896397",
        "name": "Huan Liu"
      }
    ]
  },
  "207743293": {
    "paperId": "290513795d653bd13a27c0688b12a459eb66c711",
    "externalIds": {
      "MAG": "2610676001",
      "ArXiv": "1704.00656",
      "DBLP": "journals/corr/ZubiagaABLP17",
      "DOI": "10.1145/3161603",
      "CorpusId": 207743293
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "Detection and Resolution of Rumours in Social Media",
    "abstract": "Despite the increasing use of social media platforms for information and news gathering, its unmoderated nature often leads to the emergence and spread of rumours, i.e., items of information that are unverified at the time of posting. At the same time, the openness of social media platforms provides opportunities to study how users share and discuss rumours, and to explore how to automatically assess their veracity, using natural language processing and data mining techniques. In this article, we introduce and discuss two types of rumours that circulate on social media: long-standing rumours that circulate for long periods of time, and newly emerging rumours spawned during fast-paced events such as breaking news, where reports are released piecemeal and often with an unverified status in their early stages. We provide an overview of research into social media rumours with the ultimate goal of developing a rumour classification system that consists of four components: rumour detection, rumour tracking, rumour stance classification, and rumour veracity classification. We delve into the approaches presented in the scientific literature for the development of each of these four components. We summarise the efforts and achievements so far toward the development of rumour classification systems and conclude with suggestions for avenues for future research in social media mining for the detection and resolution of rumours.",
    "venue": "ACM Computing Surveys",
    "year": 2017,
    "referenceCount": 196,
    "citationCount": 754,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2805349",
        "name": "A. Zubiaga"
      },
      {
        "authorId": "145970060",
        "name": "Ahmet Aker"
      },
      {
        "authorId": "1723649",
        "name": "Kalina Bontcheva"
      },
      {
        "authorId": "1991548",
        "name": "Maria Liakata"
      },
      {
        "authorId": "144723416",
        "name": "R. Procter"
      }
    ]
  },
  "49320819": {
    "paperId": "22616702da06431668022c649a017af9b333c530",
    "externalIds": {
      "MAG": "2963567867",
      "DBLP": "journals/corr/abs-1806-07687",
      "ArXiv": "1806.07687",
      "ACL": "C18-1283",
      "CorpusId": 49320819
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Automated Fact Checking: Task Formulations, Methods and Future Directions",
    "abstract": "The recently increased focus on misinformation has stimulated research in fact checking, the task of assessing the truthfulness of a claim. Research in automating this task has been conducted in a variety of disciplines including natural language processing, machine learning, knowledge representation, databases, and journalism. While there has been substantial progress, relevant papers and articles have been published in research communities that are often unaware of each other and use inconsistent terminology, thus impeding understanding and further progress. In this paper we survey automated fact checking research stemming from natural language processing and related disciplines, unifying the task formulations and methodologies across papers and authors. Furthermore, we highlight the use of evidence as an important distinguishing factor among them cutting across task formulations and methods. We conclude with proposing avenues for future NLP research on automated fact checking.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2018,
    "referenceCount": 75,
    "citationCount": 258,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Law",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144603330",
        "name": "James Thorne"
      },
      {
        "authorId": "2064056928",
        "name": "Andreas Vlachos"
      }
    ]
  },
  "9060471": {
    "paperId": "6447bfcda1dfb2fa8484683711af92b7cbaeca2b",
    "externalIds": {
      "ArXiv": "1505.02463",
      "DBLP": "journals/sigkdd/LiGMLSZFH15",
      "MAG": "2951361729",
      "DOI": "10.1145/2897350.2897352",
      "CorpusId": 9060471
    },
    "publicationVenue": null,
    "title": "A Survey on Truth Discovery",
    "abstract": "Thanks to information explosion, data for the objects of interest can be collected from increasingly more sources. However, for the same object, there usually exist conflicts among the collected multi-source information. To tackle this challenge, truth discovery, which integrates multi-source noisy information by estimating the reliability of each source, has emerged as a hot topic. Several truth discovery methods have been proposed for various scenarios, and they have been successfully applied in diverse application domains. In this survey, we focus on providing a comprehensive overview of truth discovery methods, and summarizing them from different aspects. We also discuss some future directions of truth discovery research. We hope that this survey will promote a better understanding of the current progress on truth discovery, and offer some guidelines on how to apply these approaches in application domains.",
    "venue": "SKDD",
    "year": 2015,
    "referenceCount": 85,
    "citationCount": 401,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2110479359",
        "name": "Yaliang Li"
      },
      {
        "authorId": "144407304",
        "name": "Jing Gao"
      },
      {
        "authorId": "2598592",
        "name": "Chuishi Meng"
      },
      {
        "authorId": "37696683",
        "name": "Qi Li"
      },
      {
        "authorId": "143843304",
        "name": "Lu Su"
      },
      {
        "authorId": "2112525352",
        "name": "Bo Zhao"
      },
      {
        "authorId": "3228071",
        "name": "Wei Fan"
      },
      {
        "authorId": "145325584",
        "name": "Jiawei Han"
      }
    ]
  },
  "4410672": {
    "paperId": "73bfad11b96a69cb882028ead115751adb55252d",
    "externalIds": {
      "MAG": "3114725063",
      "ArXiv": "2307.07903",
      "DBLP": "journals/corr/abs-2307-07903",
      "DOI": "10.1126/science.aao2998",
      "CorpusId": 4410672,
      "PubMed": "29590025"
    },
    "publicationVenue": {
      "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
      "name": "Science",
      "type": "journal",
      "issn": "0193-4511",
      "alternate_issns": [
        "0036-8075"
      ],
      "url": "https://www.jstor.org/journal/science",
      "alternate_urls": [
        "https://www.sciencemag.org/",
        "http://www.sciencemag.org/",
        "http://www.jstor.org/journals/00368075.html",
        "http://www.sciencemag.org/archive/"
      ]
    },
    "title": "The science of fake news",
    "abstract": "Addressing fake news requires a multidisciplinary effort The rise of fake news highlights the erosion of long-standing institutional bulwarks against misinformation in the internet age. Concern over the problem is global. However, much remains unknown regarding the vulnerabilities of individuals, institutions, and society to manipulations by malicious actors. A new system of safeguards is needed. Below, we discuss extant social and computer science research regarding belief in fake news and the mechanisms by which it spreads. Fake news has a long history, but we focus on unanswered scientific questions raised by the proliferation of its most recent, politically oriented incarnation. Beyond selected references in the text, suggested further reading can be found in the supplementary materials.",
    "venue": "Science",
    "year": 2018,
    "referenceCount": 19,
    "citationCount": 2889,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3185333",
        "name": "D. Lazer"
      },
      {
        "authorId": "40508064",
        "name": "M. Baum"
      },
      {
        "authorId": "2237559",
        "name": "Y. Benkler"
      },
      {
        "authorId": "4859855",
        "name": "Adam J. Berinsky"
      },
      {
        "authorId": "40828798",
        "name": "Kelly M. Greenhill"
      },
      {
        "authorId": "143653472",
        "name": "F. Menczer"
      },
      {
        "authorId": "1976593",
        "name": "Miriam J. Metzger"
      },
      {
        "authorId": "2064358",
        "name": "B. Nyhan"
      },
      {
        "authorId": "2998138",
        "name": "Gordon Pennycook"
      },
      {
        "authorId": "145792941",
        "name": "David M. Rothschild"
      },
      {
        "authorId": "50156656",
        "name": "M. Schudson"
      },
      {
        "authorId": "2404363",
        "name": "S. Sloman"
      },
      {
        "authorId": "3171769",
        "name": "C. Sunstein"
      },
      {
        "authorId": "26668235",
        "name": "Emily A. Thorson"
      },
      {
        "authorId": "1783914",
        "name": "D. Watts"
      },
      {
        "authorId": "46714697",
        "name": "Jonathan Zittrain"
      }
    ]
  },
  "4549072": {
    "paperId": "ef07defaf08123d5e1a8bd41ad6e2db5e5b225e3",
    "externalIds": {
      "MAG": "2790166049",
      "DOI": "10.1126/science.aap9559",
      "CorpusId": 4549072,
      "PubMed": "29590045"
    },
    "publicationVenue": {
      "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
      "name": "Science",
      "type": "journal",
      "issn": "0193-4511",
      "alternate_issns": [
        "0036-8075"
      ],
      "url": "https://www.jstor.org/journal/science",
      "alternate_urls": [
        "https://www.sciencemag.org/",
        "http://www.sciencemag.org/",
        "http://www.jstor.org/journals/00368075.html",
        "http://www.sciencemag.org/archive/"
      ]
    },
    "title": "The spread of true and false news online",
    "abstract": "Lies spread faster than the truth There is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. To understand how false news spreads, Vosoughi et al. used a data set of rumor cascades on Twitter from 2006 to 2017. About 126,000 rumors were spread by \u223c3 million people. False news reached more people than the truth; the top 1% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. Falsehood also diffused faster than the truth. The degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. Science, this issue p. 1146 A large-scale analysis of tweets reveals that false rumors spread further and faster than the truth. We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98% agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.",
    "venue": "Science",
    "year": 2018,
    "referenceCount": 73,
    "citationCount": 5276,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1918441",
        "name": "Soroush Vosoughi"
      },
      {
        "authorId": "145364504",
        "name": "D. Roy"
      },
      {
        "authorId": "2413779",
        "name": "Sinan Aral"
      }
    ]
  },
  "67748733": {
    "paperId": "8114cf0628c29e8309d6f1e2ef61030f64a7b28c",
    "externalIds": {
      "MAG": "3004975108",
      "DBLP": "journals/csur/KucukC20",
      "DOI": "10.1145/3369026",
      "CorpusId": 67748733
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "Stance Detection",
    "abstract": "Automatic elicitation of semantic information from natural language texts is an important research problem with many practical application areas. Especially after the recent proliferation of online content through channels such as social media sites, news portals, and forums; solutions to problems such as sentiment analysis, sarcasm/controversy/veracity/rumour/fake news detection, and argument mining gained increasing impact and significance, revealed with large volumes of related scientific publications. In this article, we tackle an important problem from the same family and present a survey of stance detection in social media posts and (online) regular texts. Although stance detection is defined in different ways in different application settings, the most common definition is \u201cautomatic classification of the stance of the producer of a piece of text, towards a target, into one of these three classes: {Favor, Against, Neither}.\u201d Our survey includes definitions of related problems and concepts, classifications of the proposed approaches so far, descriptions of the relevant datasets and tools, and related outstanding issues. Stance detection is a recent natural language processing topic with diverse application areas, and our survey article on this newly emerging topic will act as a significant resource for interested researchers and practitioners.",
    "venue": "ACM Computing Surveys",
    "year": 2020,
    "referenceCount": 193,
    "citationCount": 134,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1910084",
        "name": "D. K\u00fc\u00e7\u00fck"
      },
      {
        "authorId": "2083563",
        "name": "F. Can"
      }
    ]
  },
  "220483038": {
    "paperId": "d3833e446e536f7627ae01c45cf265d6e736e78c",
    "externalIds": {
      "MAG": "3041412367",
      "DBLP": "journals/corr/abs-2007-08024",
      "ArXiv": "2007.08024",
      "DOI": "10.24963/ijcai.2020/672",
      "CorpusId": 220483038
    },
    "publicationVenue": {
      "id": "67f7f831-711a-43c8-8785-1e09005359b5",
      "name": "International Joint Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Int Jt Conf Artif Intell",
        "IJCAI"
      ],
      "url": "http://www.ijcai.org/"
    },
    "title": "A Survey on Computational Propaganda Detection",
    "abstract": "Propaganda campaigns aim at influencing people's mindset with the purpose of advancing a specific agenda. They exploit the anonymity of the Internet, the micro-profiling ability of social networks, and the ease of automatically creating and managing coordinated networks of accounts, to reach millions of social network users with persuasive messages, specifically targeted to topics each individual user is sensitive to, and ultimately influencing the outcome on a targeted issue. \n\nIn this survey, we review the state of the art on computational propaganda detection from the perspective of Natural Language Processing and Network Analysis, arguing about the need for combined efforts between these communities. We further discuss current challenges and future research directions.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2020,
    "referenceCount": 54,
    "citationCount": 171,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34086979",
        "name": "Giovanni Da San Martino"
      },
      {
        "authorId": "40598011",
        "name": "S. Cresci"
      },
      {
        "authorId": "1397442049",
        "name": "Alberto Barr\u00f3n-Cede\u00f1o"
      },
      {
        "authorId": "1885974",
        "name": "Seunghak Yu"
      },
      {
        "authorId": "1728076",
        "name": "R. D. Pietro"
      },
      {
        "authorId": "1683562",
        "name": "Preslav Nakov"
      }
    ]
  },
  "1914124": {
    "paperId": "6f90ad2553c2a73948f614d19c763ec3d5e58542",
    "externalIds": {
      "ArXiv": "1407.5225",
      "MAG": "1837843568",
      "DBLP": "journals/corr/FerraraVDMF14",
      "DOI": "10.1145/2818717",
      "CorpusId": 1914124
    },
    "publicationVenue": {
      "id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
      "name": "Communications of the ACM",
      "type": "journal",
      "alternate_names": [
        "Commun ACM",
        "Communications of The ACM"
      ],
      "issn": "0001-0782",
      "url": "http://www.acm.org/pubs/cacm/",
      "alternate_urls": [
        "http://portal.acm.org/cacm",
        "http://www.acm.org/pubs/contents/journals/cacm/",
        "https://cacm.acm.org/"
      ]
    },
    "title": "The rise of social bots",
    "abstract": "Today's social bots are sophisticated and sometimes menacing. Indeed, their presence can endanger online ecosystems as well as our society.",
    "venue": "Communications of the ACM",
    "year": 2014,
    "referenceCount": 52,
    "citationCount": 1748,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Business",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48898287",
        "name": "Emilio Ferrara"
      },
      {
        "authorId": "2307347",
        "name": "Onur Varol"
      },
      {
        "authorId": "2057124",
        "name": "Clayton A. Davis"
      },
      {
        "authorId": "143653472",
        "name": "F. Menczer"
      },
      {
        "authorId": "1769960",
        "name": "A. Flammini"
      }
    ]
  },
  "252277994": {
    "paperId": "8ce2cb70d5a98ebe3bc6cb10d830dd2282a3e766",
    "externalIds": {
      "MAG": "2797652359",
      "DBLP": "journals/corr/abs-1804-03461",
      "ArXiv": "1804.03461",
      "DOI": "10.1145/3309699",
      "CorpusId": 252277994
    },
    "publicationVenue": {
      "id": "146869d7-bc08-4ccf-89c8-2aba81d32bde",
      "name": "ACM Journal of Data and Information Quality",
      "type": "journal",
      "alternate_names": [
        "Journal of Data and Information Quality",
        "J Data Inf Qual",
        "ACM J Data Inf Qual"
      ],
      "issn": "1936-1963",
      "url": "https://dl.acm.org/citation.cfm?id=J1191&picked=prox",
      "alternate_urls": [
        "https://dl.acm.org/loi/jdiq",
        "https://jdiq.acm.org/",
        "http://portal.acm.org/jdiq"
      ]
    },
    "title": "The Web of False Information",
    "abstract": "A new era of Information Warfare has arrived. Various actors, including state-sponsored ones, are weaponizing information on Online Social Networks to run false-information campaigns with targeted manipulation of public opinion on specific topics. These false-information campaigns can have dire consequences to the public: mutating their opinions and actions, especially with respect to critical world events like major elections. Evidently, the problem of false information on the Web is a crucial one and needs increased public awareness as well as immediate attention from law enforcement agencies, public institutions, and in particular, the research community. In this article, we make a step in this direction by providing a typology of the Web\u2019s false-information ecosystem, composed of various types of false-information, actors, and their motives. We report a comprehensive overview of existing research on the false-information ecosystem by identifying several lines of work: (1) how the public perceives false information; (2) understanding the propagation of false information; (3) detecting and containing false information on the Web; and (4) false information on the political stage. In this work, we pay particular attention to political false information as: (1) it can have dire consequences to the community (e.g., when election results are mutated) and (2) previous work shows that this type of false information propagates faster and further when compared to other types of false information. Finally, for each of these lines of work, we report several future research directions that can help us better understand and mitigate the emerging problem of false-information dissemination on the Web.",
    "venue": "ACM Journal of Data and Information Quality",
    "year": 2018,
    "referenceCount": 214,
    "citationCount": 146,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Political Science",
        "source": "external"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Law",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3447293",
        "name": "Savvas Zannettou"
      },
      {
        "authorId": "2698864",
        "name": "Michael Sirivianos"
      },
      {
        "authorId": "144728530",
        "name": "Jeremy Blackburn"
      },
      {
        "authorId": "1946641",
        "name": "N. Kourtellis"
      }
    ]
  },
  "44111303": {
    "paperId": "2ed166a3301209ccd9838e26ec4648a4d2f07bd9",
    "externalIds": {
      "MAG": "2804927761",
      "DBLP": "journals/cacm/Baeza-Yates18",
      "DOI": "10.1145/3209581",
      "CorpusId": 44111303
    },
    "publicationVenue": {
      "id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
      "name": "Communications of the ACM",
      "type": "journal",
      "alternate_names": [
        "Commun ACM",
        "Communications of The ACM"
      ],
      "issn": "0001-0782",
      "url": "http://www.acm.org/pubs/cacm/",
      "alternate_urls": [
        "http://portal.acm.org/cacm",
        "http://www.acm.org/pubs/contents/journals/cacm/",
        "https://cacm.acm.org/"
      ]
    },
    "title": "Bias on the web",
    "abstract": "Bias in Web data and use taints the algorithms behind Web-based applications, delivering equally biased results.",
    "venue": "Communications of the ACM",
    "year": 2018,
    "referenceCount": 37,
    "citationCount": 212,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1389957009",
        "name": "R. Baeza-Yates"
      }
    ]
  },
  "11319376": {
    "paperId": "5c39e37022661f81f79e481240ed9b175dec6513",
    "externalIds": {
      "MAG": "2594475271",
      "ArXiv": "1702.08608",
      "CorpusId": 11319376
    },
    "publicationVenue": null,
    "title": "Towards A Rigorous Science of Interpretable Machine Learning",
    "abstract": "As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",
    "venue": "",
    "year": 2017,
    "referenceCount": 57,
    "citationCount": 3298,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1388372395",
        "name": "F. Doshi-Velez"
      },
      {
        "authorId": "3351164",
        "name": "Been Kim"
      }
    ]
  },
  "5981909": {
    "paperId": "d516daff247f7157fccde6649ace91d969cd1973",
    "externalIds": {
      "MAG": "3083430015",
      "DBLP": "journals/queue/Lipton18",
      "ArXiv": "1606.03490",
      "DOI": "10.1145/3233231",
      "CorpusId": 5981909
    },
    "publicationVenue": {
      "id": "4bd27ad1-f6fa-4340-9d9d-c62ac5be6fc0",
      "name": "Queue",
      "type": "journal",
      "alternate_names": [
        "ACM Queue"
      ],
      "issn": "1542-7730",
      "url": "http://portal.acm.org/queue",
      "alternate_urls": [
        "http://www.acmqueue.org/"
      ]
    },
    "title": "The mythos of model interpretability",
    "abstract": "In machine learning, the concept of interpretability is both important and slippery.",
    "venue": "Queue",
    "year": 2016,
    "referenceCount": 40,
    "citationCount": 3357,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32219137",
        "name": "Zachary Chase Lipton"
      }
    ]
  },
  "67855860": {
    "paperId": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f",
    "externalIds": {
      "MAG": "2934842096",
      "DBLP": "journals/corr/abs-1902-10186",
      "ACL": "N19-1357",
      "ArXiv": "1902.10186",
      "DOI": "10.18653/v1/N19-1357",
      "CorpusId": 67855860
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Attention is not Explanation",
    "abstract": "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful \u201cexplanations\u201d for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 33,
    "citationCount": 1200,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49837811",
        "name": "Sarthak Jain"
      },
      {
        "authorId": "1912476",
        "name": "Byron C. Wallace"
      }
    ]
  },
  "7228830": {
    "paperId": "ffb949d3493c3b2f3c9acf9c75cb03938933ddf0",
    "externalIds": {
      "MAG": "2738015883",
      "DBLP": "conf/emnlp/JiaL17",
      "ArXiv": "1707.07328",
      "ACL": "D17-1215",
      "DOI": "10.18653/v1/D17-1215",
      "CorpusId": 7228830
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
    "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2017,
    "referenceCount": 39,
    "citationCount": 1534,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3422908",
        "name": "Robin Jia"
      },
      {
        "authorId": "145419642",
        "name": "Percy Liang"
      }
    ]
  },
  "13029170": {
    "paperId": "c0883f5930a232a9c1ad601c978caede29155979",
    "externalIds": {
      "ACL": "N16-3020",
      "MAG": "2951501516",
      "DBLP": "conf/naacl/Ribeiro0G16",
      "ArXiv": "1602.04938",
      "DOI": "10.1145/2939672.2939778",
      "CorpusId": 13029170
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "\u201cWhy Should I Trust You?\u201d: Explaining the Predictions of Any Classifier",
    "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 41,
    "citationCount": 14837,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "78846919",
        "name": "Marco Tulio Ribeiro"
      },
      {
        "authorId": "34650964",
        "name": "Sameer Singh"
      },
      {
        "authorId": "1730156",
        "name": "Carlos Guestrin"
      }
    ]
  },
  "1450294": {
    "paperId": "dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71",
    "externalIds": {
      "MAG": "2962851944",
      "ArXiv": "1312.6034",
      "DBLP": "journals/corr/SimonyanVZ13",
      "CorpusId": 1450294
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
    "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].",
    "venue": "International Conference on Learning Representations",
    "year": 2013,
    "referenceCount": 13,
    "citationCount": 6801,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34838386",
        "name": "K. Simonyan"
      },
      {
        "authorId": "1687524",
        "name": "A. Vedaldi"
      },
      {
        "authorId": "1688869",
        "name": "Andrew Zisserman"
      }
    ]
  },
  "202712654": {
    "paperId": "ddd27dba038d0ed14c48cd027812df58a902ece2",
    "externalIds": {
      "MAG": "2974284957",
      "DBLP": "journals/corr/abs-1909-09251",
      "ArXiv": "1909.09251",
      "ACL": "D19-3002",
      "DOI": "10.18653/v1/D19-3002",
      "CorpusId": 202712654
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models",
    "abstract": "Neural NLP models are increasingly accurate but are imperfect and opaque\u2014they break in counterintuitive ways and leave end users puzzled at their behavior. Model interpretation methods ameliorate this opacity by providing explanations for specific model predictions. Unfortunately, existing interpretation codebases make it difficult to apply these methods to new models and tasks, which hinders adoption for practitioners and burdens interpretability researchers. We introduce AllenNLP Interpret, a flexible framework for interpreting NLP models. The toolkit provides interpretation primitives (e.g., input gradients) for any AllenNLP model and task, a suite of built-in interpretation methods, and a library of front-end visualization components. We demonstrate the toolkit\u2019s flexibility and utility by implementing live demos for five interpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension using BiDAF). These demos, alongside our code and tutorials, are available at https://allennlp.org/interpret.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 25,
    "citationCount": 133,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145217343",
        "name": "Eric Wallace"
      },
      {
        "authorId": "1388109456",
        "name": "Jens Tuyls"
      },
      {
        "authorId": "49606614",
        "name": "Junlin Wang"
      },
      {
        "authorId": "17097887",
        "name": "Sanjay Subramanian"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      },
      {
        "authorId": "34650964",
        "name": "Sameer Singh"
      }
    ]
  },
  "13756489": {
    "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
    "externalIds": {
      "ArXiv": "1706.03762",
      "MAG": "2963403868",
      "DBLP": "conf/nips/VaswaniSPUJGKP17",
      "CorpusId": 13756489
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Attention is All you Need",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "venue": "Neural Information Processing Systems",
    "year": 2017,
    "referenceCount": 41,
    "citationCount": 109003,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40348417",
        "name": "Ashish Vaswani"
      },
      {
        "authorId": "1846258",
        "name": "Noam M. Shazeer"
      },
      {
        "authorId": "3877127",
        "name": "Niki Parmar"
      },
      {
        "authorId": "39328010",
        "name": "Jakob Uszkoreit"
      },
      {
        "authorId": "145024664",
        "name": "Llion Jones"
      },
      {
        "authorId": "19177000",
        "name": "Aidan N. Gomez"
      },
      {
        "authorId": "40527594",
        "name": "Lukasz Kaiser"
      },
      {
        "authorId": "3443442",
        "name": "Illia Polosukhin"
      }
    ]
  },
  "218971783": {
    "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
    "externalIds": {
      "ArXiv": "2005.14165",
      "DBLP": "conf/nips/BrownMRSKDNSSAA20",
      "MAG": "3030163527",
      "CorpusId": 218971783
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Language Models are Few-Shot Learners",
    "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 146,
    "citationCount": 32805,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "31035595",
        "name": "Tom B. Brown"
      },
      {
        "authorId": "2056658938",
        "name": "Benjamin Mann"
      },
      {
        "authorId": "39849748",
        "name": "Nick Ryder"
      },
      {
        "authorId": "2065894334",
        "name": "Melanie Subbiah"
      },
      {
        "authorId": "152724169",
        "name": "J. Kaplan"
      },
      {
        "authorId": "6515819",
        "name": "Prafulla Dhariwal"
      },
      {
        "authorId": "2072676",
        "name": "Arvind Neelakantan"
      },
      {
        "authorId": "67311962",
        "name": "Pranav Shyam"
      },
      {
        "authorId": "144864359",
        "name": "Girish Sastry"
      },
      {
        "authorId": "119609682",
        "name": "Amanda Askell"
      },
      {
        "authorId": "144517868",
        "name": "Sandhini Agarwal"
      },
      {
        "authorId": "1404060687",
        "name": "Ariel Herbert-Voss"
      },
      {
        "authorId": "2064404342",
        "name": "Gretchen Krueger"
      },
      {
        "authorId": "103143311",
        "name": "T. Henighan"
      },
      {
        "authorId": "48422824",
        "name": "R. Child"
      },
      {
        "authorId": "1992922591",
        "name": "A. Ramesh"
      },
      {
        "authorId": "2052152920",
        "name": "Daniel M. Ziegler"
      },
      {
        "authorId": "49387725",
        "name": "Jeff Wu"
      },
      {
        "authorId": "2059411355",
        "name": "Clemens Winter"
      },
      {
        "authorId": "144239765",
        "name": "Christopher Hesse"
      },
      {
        "authorId": "2108828435",
        "name": "Mark Chen"
      },
      {
        "authorId": "2064673055",
        "name": "Eric Sigler"
      },
      {
        "authorId": "1380985420",
        "name": "Ma-teusz Litwin"
      },
      {
        "authorId": "145565184",
        "name": "Scott Gray"
      },
      {
        "authorId": "1490681878",
        "name": "B. Chess"
      },
      {
        "authorId": "2115193883",
        "name": "Jack Clark"
      },
      {
        "authorId": "133740015",
        "name": "Christopher Berner"
      },
      {
        "authorId": "52238703",
        "name": "Sam McCandlish"
      },
      {
        "authorId": "38909097",
        "name": "Alec Radford"
      },
      {
        "authorId": "1701686",
        "name": "I. Sutskever"
      },
      {
        "authorId": "2698777",
        "name": "Dario Amodei"
      }
    ]
  },
  "220265858": {
    "paperId": "1882f194cb43828852cc052887671e55a80f945a",
    "externalIds": {
      "MAG": "3040573126",
      "DBLP": "conf/iclr/LepikhinLXCFHKS21",
      "ArXiv": "2006.16668",
      "CorpusId": 220265858
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
    "abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "referenceCount": 99,
    "citationCount": 891,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "150077954",
        "name": "Dmitry Lepikhin"
      },
      {
        "authorId": "34946720",
        "name": "HyoukJoong Lee"
      },
      {
        "authorId": "2145139570",
        "name": "Yuanzhong Xu"
      },
      {
        "authorId": "7167328",
        "name": "Dehao Chen"
      },
      {
        "authorId": "2345617",
        "name": "Orhan Firat"
      },
      {
        "authorId": "2145438541",
        "name": "Yanping Huang"
      },
      {
        "authorId": "2048712",
        "name": "M. Krikun"
      },
      {
        "authorId": "1846258",
        "name": "Noam M. Shazeer"
      },
      {
        "authorId": "2545358",
        "name": "Z. Chen"
      }
    ]
  },
  "207808916": {
    "paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97",
    "externalIds": {
      "DBLP": "journals/corr/abs-1912-02292",
      "ArXiv": "1912.02292",
      "MAG": "2994081359",
      "DOI": "10.1088/1742-5468/ac3a74",
      "CorpusId": 207808916
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Deep double descent: where bigger models and more data hurt",
    "abstract": "We show that a variety of modern deep learning tasks exhibit a \u2018double-descent\u2019 phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "referenceCount": 50,
    "citationCount": 839,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2181918",
        "name": "Preetum Nakkiran"
      },
      {
        "authorId": "79952988",
        "name": "Gal Kaplun"
      },
      {
        "authorId": "51041277",
        "name": "Yamini Bansal"
      },
      {
        "authorId": "121270172",
        "name": "Tristan Yang"
      },
      {
        "authorId": "1697211",
        "name": "B. Barak"
      },
      {
        "authorId": "1701686",
        "name": "I. Sutskever"
      }
    ]
  },
  "7200347": {
    "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
    "externalIds": {
      "ArXiv": "1503.02531",
      "MAG": "1821462560",
      "DBLP": "journals/corr/HintonVD15",
      "CorpusId": 7200347
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Distilling the Knowledge in a Neural Network",
    "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
    "venue": "arXiv.org",
    "year": 2015,
    "referenceCount": 9,
    "citationCount": 17494,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1695689",
        "name": "Geoffrey E. Hinton"
      },
      {
        "authorId": "1689108",
        "name": "O. Vinyals"
      },
      {
        "authorId": "49959210",
        "name": "J. Dean"
      }
    ]
  },
  "202889175": {
    "paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97",
    "externalIds": {
      "MAG": "2975429091",
      "CorpusId": 202889175
    },
    "publicationVenue": null,
    "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models",
    "abstract": "Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly, the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.",
    "venue": "",
    "year": 2019,
    "referenceCount": 36,
    "citationCount": 609,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1388156275",
        "name": "Iulia Turc"
      },
      {
        "authorId": "1744179",
        "name": "Ming-Wei Chang"
      },
      {
        "authorId": "2544107",
        "name": "Kenton Lee"
      },
      {
        "authorId": "3259253",
        "name": "Kristina Toutanova"
      }
    ]
  },
  "202719327": {
    "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
    "externalIds": {
      "MAG": "3105966348",
      "DBLP": "conf/emnlp/JiaoYSJCL0L20",
      "ArXiv": "1909.10351",
      "ACL": "2020.findings-emnlp.372",
      "DOI": "10.18653/v1/2020.findings-emnlp.372",
      "CorpusId": 202719327
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "TinyBERT: Distilling BERT for Natural Language Understanding",
    "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be effectively transferred to a small \u201cstudent\u201d TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
    "venue": "Findings",
    "year": 2019,
    "referenceCount": 57,
    "citationCount": 1641,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39706649",
        "name": "Xiaoqi Jiao"
      },
      {
        "authorId": "1384668226",
        "name": "Yichun Yin"
      },
      {
        "authorId": "50812138",
        "name": "Lifeng Shang"
      },
      {
        "authorId": "145820291",
        "name": "Xin Jiang"
      },
      {
        "authorId": "2117025507",
        "name": "Xiao Chen"
      },
      {
        "authorId": "2111818678",
        "name": "Linlin Li"
      },
      {
        "authorId": "49451193",
        "name": "F. Wang"
      },
      {
        "authorId": "1688015",
        "name": "Qun Liu"
      }
    ]
  },
  "202565587": {
    "paperId": "745e4b36a1759177871288cae51fbae0b873b5e5",
    "externalIds": {
      "MAG": "2973061659",
      "DBLP": "conf/aaai/ShenDYMYGMK20",
      "ArXiv": "1909.05840",
      "DOI": "10.1609/AAAI.V34I05.6409",
      "CorpusId": 202565587
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT",
    "abstract": "Transformer based architectures have become de-facto models used for a range of Natural Language Processing tasks. In particular, the BERT based models achieved significant accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However, BERT based models have a prohibitive memory footprint and latency. As a result, deploying BERT based models in resource constrained environments has become a challenging task. In this work, we perform an extensive analysis of fine-tuned BERT models using second order Hessian information, and we use our results to propose a novel method for quantizing BERT models to ultra low precision. In particular, we propose a new group-wise quantization scheme, and we use Hessian-based mix-precision method to compress the model further. We extensively test our proposed method on BERT downstream tasks of SST-2, MNLI, CoNLL-03, and SQuAD. We can achieve comparable performance to baseline with at most 2.3% performance degradation, even with ultra-low precision quantization down to 2 bits, corresponding up to 13\u00d7 compression of the model parameters, and up to 4\u00d7 compression of the embedding table as well as activations. Among all tasks, we observed the highest performance loss for BERT fine-tuned on SQuAD. By probing into the Hessian based analysis as well as visualization, we show that this is related to the fact that current training/fine-tuning strategy of BERT does not converge for SQuAD.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "referenceCount": 46,
    "citationCount": 525,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2191455",
        "name": "Sheng Shen"
      },
      {
        "authorId": "143879884",
        "name": "Zhen Dong"
      },
      {
        "authorId": "119870944",
        "name": "Jiayu Ye"
      },
      {
        "authorId": "2115503326",
        "name": "Linjian Ma"
      },
      {
        "authorId": "9088433",
        "name": "Z. Yao"
      },
      {
        "authorId": "10419477",
        "name": "A. Gholami"
      },
      {
        "authorId": "143884206",
        "name": "Michael W. Mahoney"
      },
      {
        "authorId": "1732330",
        "name": "K. Keutzer"
      }
    ]
  },
  "204509218": {
    "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
    "externalIds": {
      "ArXiv": "1910.06188",
      "DBLP": "journals/corr/abs-1910-06188",
      "MAG": "2979314664",
      "DOI": "10.1109/EMC2-NIPS53020.2019.00016",
      "CorpusId": 204509218
    },
    "publicationVenue": null,
    "title": "Q8BERT: Quantized 8Bit BERT",
    "abstract": "Recently, pre-trained Transformer [1] based language models such as BERT [2] and GPT [3], have shown great improvement in many Natural Language Processing (NLP) tasks. However, these models contain a large amount of parameters. The emergence of even larger and more accurate models such as GPT2 [4] and Megatron11https://github.com/NVIDIA/Megatron-LM, suggest a trend of large pre-trained Transformer models. However, using these large models in production environments is a complex task requiring a large amount of compute, memory and power resources. In this work we show how to perform quantization-aware training during the fine-tuning phase of BERT in order to compress BERT by 4x with minimal accuracy loss. Furthermore, the produced quantized model can accelerate inference speed if it is optimized for 8bit Integer supporting hardware.",
    "venue": "2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)",
    "year": 2019,
    "referenceCount": 18,
    "citationCount": 471,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1387202086",
        "name": "Ofir Zafrir"
      },
      {
        "authorId": "3150063",
        "name": "Guy Boudoukh"
      },
      {
        "authorId": "2477428",
        "name": "Peter Izsak"
      },
      {
        "authorId": "2134755",
        "name": "Moshe Wasserblat"
      }
    ]
  },
  "53388625": {
    "paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
    "externalIds": {
      "MAG": "2951099858",
      "DBLP": "conf/iclr/FrankleC19",
      "ArXiv": "1803.03635",
      "CorpusId": 53388625
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
    "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. \nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. \nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "referenceCount": 68,
    "citationCount": 3099,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "25581960",
        "name": "Jonathan Frankle"
      },
      {
        "authorId": "1701041",
        "name": "Michael Carbin"
      }
    ]
  },
  "218538350": {
    "paperId": "143b0bd73e7e765945e0b8620f84f52878617560",
    "externalIds": {
      "MAG": "3023182682",
      "ACL": "2020.acl-main.360",
      "ArXiv": "2005.03454",
      "DBLP": "conf/acl/BrixBN20",
      "DOI": "10.18653/v1/2020.acl-main.360",
      "CorpusId": 218538350
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture",
    "abstract": "Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs. This is relevant both for time-critical and on-device computations using neural networks. The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model. On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity. Furthermore, we confirm that the parameter\u2019s initial sign and not its specific value is the primary factor for successful training, and show that magnitude pruning cannot be used to find winning lottery tickets.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 28,
    "citationCount": 37,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51518834",
        "name": "Christopher Brix"
      },
      {
        "authorId": "1872287",
        "name": "Parnia Bahar"
      },
      {
        "authorId": "145322333",
        "name": "H. Ney"
      }
    ]
  },
  "218665313": {
    "paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e",
    "externalIds": {
      "MAG": "3099715410",
      "DBLP": "conf/nips/Sanh0R20",
      "ArXiv": "2005.07683",
      "CorpusId": 218665313
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning",
    "abstract": "Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 60,
    "citationCount": 416,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51918868",
        "name": "Victor Sanh"
      },
      {
        "authorId": "50335211",
        "name": "Thomas Wolf"
      },
      {
        "authorId": "2531268",
        "name": "Alexander M. Rush"
      }
    ]
  },
  "215737171": {
    "paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2",
    "externalIds": {
      "DBLP": "journals/corr/abs-2004-05150",
      "MAG": "3015468748",
      "ArXiv": "2004.05150",
      "CorpusId": 215737171
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Longformer: The Long-Document Transformer",
    "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 59,
    "citationCount": 3389,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46181066",
        "name": "Iz Beltagy"
      },
      {
        "authorId": "39139825",
        "name": "Matthew E. Peters"
      },
      {
        "authorId": "2527954",
        "name": "Arman Cohan"
      }
    ]
  },
  "209315300": {
    "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
    "externalIds": {
      "DBLP": "journals/corr/abs-2001-04451",
      "MAG": "2994673210",
      "ArXiv": "2001.04451",
      "CorpusId": 209315300
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Reformer: The Efficient Transformer",
    "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.",
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "referenceCount": 26,
    "citationCount": 2029,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143808231",
        "name": "Nikita Kitaev"
      },
      {
        "authorId": "40527594",
        "name": "Lukasz Kaiser"
      },
      {
        "authorId": "6639036",
        "name": "Anselm Levskaya"
      }
    ]
  },
  "219530577": {
    "paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
    "externalIds": {
      "ArXiv": "2006.04768",
      "MAG": "3033529678",
      "DBLP": "journals/corr/abs-2006-04768",
      "CorpusId": 219530577
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Linformer: Self-Attention with Linear Complexity",
    "abstract": "Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 33,
    "citationCount": 1429,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2096387",
        "name": "Sinong Wang"
      },
      {
        "authorId": "46708422",
        "name": "Belinda Z. Li"
      },
      {
        "authorId": "2072010",
        "name": "Madian Khabsa"
      },
      {
        "authorId": "2087117615",
        "name": "Han Fang"
      },
      {
        "authorId": "2110815489",
        "name": "Hao Ma"
      }
    ]
  },
  "202538495": {
    "paperId": "f6390beca54411b06f3bde424fb983a451789733",
    "externalIds": {
      "MAG": "2970777192",
      "ArXiv": "1909.00015",
      "DBLP": "journals/corr/abs-1909-00015",
      "ACL": "D19-1223",
      "DOI": "10.18653/v1/D19-1223",
      "CorpusId": 202538495
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Adaptively Sparse Transformers",
    "abstract": "Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter \u2013 which controls the shape and sparsity of alpha-entmax \u2013 allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 40,
    "citationCount": 222,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "146783606",
        "name": "Gon\u00e7alo M. Correia"
      },
      {
        "authorId": "2114966",
        "name": "Vlad Niculae"
      },
      {
        "authorId": "145644643",
        "name": "Andr\u00e9 F. T. Martins"
      }
    ]
  },
  "220424511": {
    "paperId": "cd4ffe5e014601a3d6b64121355d29a730591490",
    "externalIds": {
      "MAG": "3041753721",
      "DBLP": "conf/nips/VyasKF20",
      "ArXiv": "2007.04825",
      "CorpusId": 220424511
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Fast Transformers with Clustered Attention",
    "abstract": "Transformers have been proven a successful model for a variety of tasks in sequence modeling. However, computing the attention matrix, which is their key component, has quadratic complexity with respect to the sequence length, thus making them prohibitively expensive for large sequences. To address this, we propose clustered attention, which instead of computing the attention for every query, groups queries into clusters and computes attention just for the centroids. To further improve this approximation, we use the computed clusters to identify the keys with the highest attention per query and compute the exact key/query dot products. This results in a model with linear complexity with respect to the sequence length for a fixed number of clusters. We evaluate our approach on two automatic speech recognition datasets and show that our model consistently outperforms vanilla transformers for a given computational budget. Finally, we demonstrate that our model can approximate arbitrarily complex attention distributions with a minimal number of clusters by approximating a pretrained BERT model on GLUE and SQuAD benchmarks with only 25 clusters and no loss in performance.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 34,
    "citationCount": 139,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2992087",
        "name": "Apoorv Vyas"
      },
      {
        "authorId": "3493855",
        "name": "Angelos Katharopoulos"
      },
      {
        "authorId": "116272138",
        "name": "Franccois Fleuret"
      }
    ]
  },
  "220250819": {
    "paperId": "6f68e1bb253925d8431588555d3010419f322e04",
    "externalIds": {
      "DBLP": "conf/icml/KatharopoulosV020",
      "MAG": "3037798801",
      "ArXiv": "2006.16236",
      "CorpusId": 220250819
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
    "abstract": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.",
    "venue": "International Conference on Machine Learning",
    "year": 2020,
    "referenceCount": 43,
    "citationCount": 1355,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3493855",
        "name": "Angelos Katharopoulos"
      },
      {
        "authorId": "2992087",
        "name": "Apoorv Vyas"
      },
      {
        "authorId": "143958923",
        "name": "Nikolaos Pappas"
      },
      {
        "authorId": "116272138",
        "name": "Franccois Fleuret"
      }
    ]
  },
  "220831004": {
    "paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3",
    "externalIds": {
      "ArXiv": "2007.14062",
      "DBLP": "journals/corr/abs-2007-14062",
      "MAG": "3045733172",
      "CorpusId": 220831004
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Big Bird: Transformers for Longer Sequences",
    "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 118,
    "citationCount": 1775,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Geography",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1771307",
        "name": "M. Zaheer"
      },
      {
        "authorId": "1947314",
        "name": "Guru Guruganesh"
      },
      {
        "authorId": "89890133",
        "name": "Kumar Avinava Dubey"
      },
      {
        "authorId": "1643737606",
        "name": "J. Ainslie"
      },
      {
        "authorId": "114577307",
        "name": "Chris Alberti"
      },
      {
        "authorId": "1722671",
        "name": "Santiago Onta\u00f1\u00f3n"
      },
      {
        "authorId": "38552691",
        "name": "Philip Pham"
      },
      {
        "authorId": "101210026",
        "name": "Anirudh Ravula"
      },
      {
        "authorId": "145196279",
        "name": "Qifan Wang"
      },
      {
        "authorId": "113906155",
        "name": "Li Yang"
      },
      {
        "authorId": "143629707",
        "name": "Amr Ahmed"
      }
    ]
  },
  "12857568": {
    "paperId": "3e27529f55fa966201613b2d8b56b1934ce6af54",
    "externalIds": {
      "MAG": "2963424553",
      "DBLP": "journals/corr/abs-1708-00214",
      "ACL": "D17-1309",
      "ArXiv": "1708.00214",
      "DOI": "10.18653/v1/D17-1309",
      "CorpusId": 12857568
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Natural Language Processing with Small Feed-Forward Networks",
    "abstract": "We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2017,
    "referenceCount": 44,
    "citationCount": 36,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35025872",
        "name": "Jan A. Botha"
      },
      {
        "authorId": "2585932",
        "name": "Emily Pitler"
      },
      {
        "authorId": "1698523",
        "name": "Ji Ma"
      },
      {
        "authorId": "3058597",
        "name": "A. Bakalov"
      },
      {
        "authorId": "3251354",
        "name": "Alexandru Salcianu"
      },
      {
        "authorId": "145045509",
        "name": "David Weiss"
      },
      {
        "authorId": "143957226",
        "name": "Ryan T. McDonald"
      },
      {
        "authorId": "1754497",
        "name": "Slav Petrov"
      }
    ]
  },
  "59523610": {
    "paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3",
    "externalIds": {
      "MAG": "2952355681",
      "DBLP": "conf/icml/SoLL19",
      "ArXiv": "1901.11117",
      "CorpusId": 59523610
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "The Evolved Transformer",
    "abstract": "Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments -- the Evolved Transformer -- demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original \"big\" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.",
    "venue": "International Conference on Machine Learning",
    "year": 2019,
    "referenceCount": 49,
    "citationCount": 446,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48165870",
        "name": "David R. So"
      },
      {
        "authorId": "145246869",
        "name": "Chen Liang"
      },
      {
        "authorId": "2827616",
        "name": "Quoc V. Le"
      }
    ]
  },
  "215238853": {
    "paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
    "externalIds": {
      "MAG": "3034457371",
      "DBLP": "journals/corr/abs-2004-02984",
      "ArXiv": "2004.02984",
      "ACL": "2020.acl-main.195",
      "DOI": "10.18653/v1/2020.acl-main.195",
      "CorpusId": 215238853
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices",
    "abstract": "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 67,
    "citationCount": 708,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48064856",
        "name": "Zhiqing Sun"
      },
      {
        "authorId": "40244451",
        "name": "Hongkun Yu"
      },
      {
        "authorId": "1718192",
        "name": "Xiaodan Song"
      },
      {
        "authorId": "2112354219",
        "name": "Renjie Liu"
      },
      {
        "authorId": "35729970",
        "name": "Yiming Yang"
      },
      {
        "authorId": "65855107",
        "name": "Denny Zhou"
      }
    ]
  },
  "218674536": {
    "paperId": "66ceae37af0e7ace4755a5e8f41162d0f6cf7677",
    "externalIds": {
      "DBLP": "journals/corr/abs-2005-07877",
      "MAG": "3025572566",
      "ArXiv": "2005.07877",
      "CorpusId": 218674536
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "MicroNet for Efficient Language Modeling",
    "abstract": "It is important to design compact language models for efficient deployment. We improve upon recent advances in both the language modeling domain and the model-compression domain to construct parameter and computation efficient language models. We use an efficient transformer-based architecture with adaptive embedding and softmax, differentiable non-parametric cache, Hebbian softmax, knowledge distillation, network pruning, and low-bit quantization. In this paper, we provide the winning solution to the NeurIPS 2019 MicroNet Challenge in the language modeling track. Compared to the baseline language model provided by the MicroNet Challenge, our model is 90 times more parameter-efficient and 36 times more computation-efficient while achieving the required test perplexity of 35 on the Wikitext-103 dataset. We hope that this work will aid future research into efficient language models, and we have released our full source code at this https URL.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 36,
    "citationCount": 8,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Engineering",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1390567306",
        "name": "Zhongxia Yan"
      },
      {
        "authorId": "1477757714",
        "name": "Hanrui Wang"
      },
      {
        "authorId": "35578711",
        "name": "Demi Guo"
      },
      {
        "authorId": "143840275",
        "name": "Song Han"
      }
    ]
  },
  "218971877": {
    "paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
    "externalIds": {
      "ACL": "2020.acl-main.686",
      "DBLP": "journals/corr/abs-2005-14187",
      "MAG": "3029385331",
      "ArXiv": "2005.14187",
      "DOI": "10.18653/v1/2020.acl-main.686",
      "CorpusId": 218971877
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing",
    "abstract": "Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT\u201914 translation task on Raspberry Pi-4, HAT can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline Transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over Evolved Transformer with 12,041\u00d7 less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 52,
    "citationCount": 243,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35446689",
        "name": "Hanrui Wang"
      },
      {
        "authorId": "1390573666",
        "name": "Zhanghao Wu"
      },
      {
        "authorId": "47781592",
        "name": "Zhijian Liu"
      },
      {
        "authorId": "2114069742",
        "name": "Han Cai"
      },
      {
        "authorId": "20515689",
        "name": "Ligeng Zhu"
      },
      {
        "authorId": "144158271",
        "name": "Chuang Gan"
      },
      {
        "authorId": "143840275",
        "name": "Song Han"
      }
    ]
  },
  "219966938": {
    "paperId": "eb1602ecba96beadeb7d2f05e1b57fa6b339fc69",
    "externalIds": {
      "DBLP": "journals/corr/abs-2006-11316",
      "ArXiv": "2006.11316",
      "ACL": "2020.sustainlp-1.17",
      "MAG": "3036463250",
      "DOI": "10.18653/v1/2020.sustainlp-1.17",
      "CorpusId": 219966938
    },
    "publicationVenue": null,
    "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?",
    "abstract": "Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. Toward this end, we consider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today\u2019s highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. To begin to address this problem, we draw inspiration from the computer vision community, where work such as MobileNet has demonstrated that grouped convolutions (e.g. depthwise convolutions) can enable speedups without sacrificing accuracy. We demonstrate how to replace several operations in self-attention layers with grouped convolutions, and we use this technique in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. A PyTorch-based implementation of SqueezeBERT is available as part of the Hugging Face Transformers library: https://huggingface.co/squeezebert",
    "venue": "SUSTAINLP",
    "year": 2020,
    "referenceCount": 104,
    "citationCount": 111,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3346186",
        "name": "F. Iandola"
      },
      {
        "authorId": "47291134",
        "name": "Albert Eaton Shaw"
      },
      {
        "authorId": "143781417",
        "name": "Ravi Krishna"
      },
      {
        "authorId": "1732330",
        "name": "K. Keutzer"
      }
    ]
  },
  "235613336": {
    "paperId": "b08c360ddf899923aebf25913706b4f03e54eccd",
    "externalIds": {
      "DBLP": "conf/iclr/MehtaGIZH21",
      "CorpusId": 235613336
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "DeLighT: Deep and Light-weight Transformer",
    "abstract": null,
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "referenceCount": 0,
    "citationCount": 77,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "s2-fos-model"
      },
      {
        "category": "Materials Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144839857",
        "name": "Sachin Mehta"
      },
      {
        "authorId": "2320509",
        "name": "Marjan Ghazvininejad"
      },
      {
        "authorId": "1900163",
        "name": "Srini Iyer"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ]
  },
  "201646309": {
    "paperId": "93d63ec754f29fa22572615320afe0521f7ec66d",
    "externalIds": {
      "DBLP": "journals/corr/abs-1908-10084",
      "ACL": "D19-1410",
      "ArXiv": "1908.10084",
      "MAG": "2971193649",
      "DOI": "10.18653/v1/D19-1410",
      "CorpusId": 201646309
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 38,
    "citationCount": 9826,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2959414",
        "name": "Nils Reimers"
      },
      {
        "authorId": "1730400",
        "name": "Iryna Gurevych"
      }
    ]
  },
  "207870430": {
    "paperId": "7be8c119dbe065c52125ee7716601751f3116844",
    "externalIds": {
      "MAG": "2988841832",
      "ArXiv": "1911.00172",
      "DBLP": "journals/corr/abs-1911-00172",
      "CorpusId": 207870430
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Generalization through Memorization: Nearest Neighbor Language Models",
    "abstract": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "referenceCount": 32,
    "citationCount": 722,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3030219",
        "name": "Urvashi Khandelwal"
      },
      {
        "authorId": "39455775",
        "name": "Omer Levy"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      }
    ]
  },
  "211204736": {
    "paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56",
    "externalIds": {
      "ArXiv": "2002.08909",
      "MAG": "3034671305",
      "DBLP": "conf/icml/GuuLTPC20",
      "CorpusId": 211204736
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
    "abstract": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
    "venue": "International Conference on Machine Learning",
    "year": 2020,
    "referenceCount": 43,
    "citationCount": 1680,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2091768",
        "name": "Kelvin Guu"
      },
      {
        "authorId": "2544107",
        "name": "Kenton Lee"
      },
      {
        "authorId": "9941702",
        "name": "Zora Tung"
      },
      {
        "authorId": "2616463",
        "name": "Panupong Pasupat"
      },
      {
        "authorId": "1744179",
        "name": "Ming-Wei Chang"
      }
    ]
  },
  "5556470": {
    "paperId": "80d800dfadbe2e6c7b2367d9229cc82912d55889",
    "externalIds": {
      "ArXiv": "1404.5997",
      "MAG": "1598866093",
      "DBLP": "journals/corr/Krizhevsky14",
      "CorpusId": 5556470
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "One weird trick for parallelizing convolutional neural networks",
    "abstract": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.",
    "venue": "arXiv.org",
    "year": 2014,
    "referenceCount": 8,
    "citationCount": 1234,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2064160",
        "name": "A. Krizhevsky"
      }
    ]
  },
  "129945164": {
    "paperId": "0b5d7a79205b44952e24025ce5d46e9f3aa401a1",
    "externalIds": {
      "ArXiv": "1904.10631",
      "MAG": "2941620283",
      "DBLP": "journals/corr/abs-1904-10631",
      "CorpusId": 129945164
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Low-Memory Neural Network Training: A Technical Report",
    "abstract": "Memory is increasingly often the bottleneck when training neural network models. Despite this, techniques to lower the overall memory requirements of training have been less widely studied compared to the extensive literature on reducing the memory requirements of inference. In this paper we study a fundamental question: How much memory is actually needed to train a neural network? To answer this question, we profile the overall memory usage of training on two representative deep learning benchmarks -- the WideResNet model for image classification and the DynamicConv Transformer model for machine translation -- and comprehensively evaluate four standard techniques for reducing the training memory requirements: (1) imposing sparsity on the model, (2) using low precision, (3) microbatching, and (4) gradient checkpointing. We explore how each of these techniques in isolation affects both the peak memory usage of training and the quality of the end model, and explore the memory, accuracy, and computation tradeoffs incurred when combining these techniques. Using appropriate combinations of these techniques, we show that it is possible to the reduce the memory required to train a WideResNet-28-2 on CIFAR-10 by up to 60.7x with a 0.4% loss in accuracy, and reduce the memory required to train a DynamicConv model on IWSLT'14 German to English translation by up to 8.7x with a BLEU score drop of 0.15.",
    "venue": "arXiv.org",
    "year": 2019,
    "referenceCount": 95,
    "citationCount": 89,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145193121",
        "name": "N. Sohoni"
      },
      {
        "authorId": "145284500",
        "name": "Christopher R. Aberger"
      },
      {
        "authorId": "37866790",
        "name": "Megan Leszczynski"
      },
      {
        "authorId": "1679327",
        "name": "Jian Zhang"
      },
      {
        "authorId": "2114485554",
        "name": "C. R\u00e9"
      }
    ]
  },
  "210861095": {
    "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
    "externalIds": {
      "MAG": "3001279689",
      "ArXiv": "2001.08361",
      "DBLP": "journals/corr/abs-2001-08361",
      "CorpusId": 210861095
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Scaling Laws for Neural Language Models",
    "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 59,
    "citationCount": 3363,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "152724169",
        "name": "J. Kaplan"
      },
      {
        "authorId": "52238703",
        "name": "Sam McCandlish"
      },
      {
        "authorId": "103143311",
        "name": "T. Henighan"
      },
      {
        "authorId": "31035595",
        "name": "Tom B. Brown"
      },
      {
        "authorId": "1490681878",
        "name": "B. Chess"
      },
      {
        "authorId": "48422824",
        "name": "R. Child"
      },
      {
        "authorId": "145565184",
        "name": "Scott Gray"
      },
      {
        "authorId": "38909097",
        "name": "Alec Radford"
      },
      {
        "authorId": "49387725",
        "name": "Jeff Wu"
      },
      {
        "authorId": "2698777",
        "name": "Dario Amodei"
      }
    ]
  },
  "5705211": {
    "paperId": "946dabbc13f06070f7618cd4ca6733a95b4b03c3",
    "externalIds": {
      "MAG": "2089967673",
      "DBLP": "journals/ai/BatemanHRT10",
      "DOI": "10.1016/j.artint.2010.05.008",
      "CorpusId": 5705211
    },
    "publicationVenue": {
      "id": "96018464-22dc-4b5c-a172-c2f4a30ce131",
      "name": "Artificial Intelligence",
      "type": "journal",
      "alternate_names": [
        "Artif Intell"
      ],
      "issn": "0004-3702",
      "alternate_issns": [
        "2633-1403",
        "2710-1673",
        "2710-1681"
      ],
      "url": "http://www.elsevier.com/locate/artint",
      "alternate_urls": [
        "http://www.sciencedirect.com/science/journal/00043702",
        "https://www.journals.elsevier.com/artificial-intelligence"
      ]
    },
    "title": "A linguistic ontology of space for natural language processing",
    "abstract": null,
    "venue": "Artificial Intelligence",
    "year": 2010,
    "referenceCount": 194,
    "citationCount": 233,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2242089300",
        "name": "John A. Bateman"
      },
      {
        "authorId": "3176893",
        "name": "J. Hois"
      },
      {
        "authorId": "2323115674",
        "name": "Robert J. Ross"
      },
      {
        "authorId": "2089708",
        "name": "T. Tenbrink"
      }
    ]
  },
  "2655805": {
    "paperId": "0ee8427e27e193c23dbfdc0aed8c5123528e6579",
    "externalIds": {
      "DBLP": "conf/lrec/KordJamshidiOM10",
      "MAG": "23606365",
      "ACL": "L10-1584",
      "CorpusId": 2655805
    },
    "publicationVenue": {
      "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
      "name": "International Conference on Language Resources and Evaluation",
      "type": "conference",
      "alternate_names": [
        "LREC",
        "Int Conf Lang Resour Evaluation"
      ],
      "url": "http://www.lrec-conf.org/"
    },
    "title": "Spatial Role Labeling: Task Definition and Annotation Scheme",
    "abstract": "One of the essential functions of natural language is to talk about spatial relationships between objects. Linguistic constructs can express highly complex, relational structures of objects, spatial relations between them, and patterns of motion through spaces relative to some reference point. Learning how to map this information onto a formal representation from a text is a challenging problem. At present no well-defined framework for automatic spatial information extraction exists that can handle all of these issues. In this paper we introduce the task of spatial role labeling and propose an annotation scheme that is language-independent and facilitates the application of machine learning techniques. Our framework consists of a set of spatial roles based on the theory of holistic spatial semantics with the intent of covering all aspects of spatial concepts, including both static and dynamic spatial relations. We illustrate our annotation scheme with many examples throughout the paper, and in addition we highlight how to connect to spatial calculi such as region connection calculus and also how our approach fits into related work.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2010,
    "referenceCount": 31,
    "citationCount": 78,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2190934",
        "name": "Parisa Kordjamshidi"
      },
      {
        "authorId": "2541098",
        "name": "M. V. Otterlo"
      },
      {
        "authorId": "145446752",
        "name": "Marie-Francine Moens"
      }
    ]
  },
  "205811696": {
    "paperId": "63471e3ed74385b14cd74b7abcfb52f61b00086f",
    "externalIds": {
      "MAG": "1994281017",
      "DBLP": "journals/scc/PustejovskyM11",
      "DOI": "10.1080/13875868.2010.543497",
      "CorpusId": 205811696
    },
    "publicationVenue": null,
    "title": "The Qualitative Spatial Dynamics of Motion in Language",
    "abstract": "Abstract In this paper, we discuss the strategies that languages employ to express motion, focusing on the distinction between path predicates, such as enter, arrive, and leave and manner-of-motion predicates, such as walk, bike, and roll. We present an overview of some qualitative spatiotemporal models of movement, and discuss their adequacy for capturing motion constructions in natural languages. Building on many aspects of these qualitative models, we introduce a framework within dynamic logic for the characterization of spatial change. This model, called Dynamic Interval Temporal Logic (DITL), is developed to analyze both classes of motion predicates, as well as complex compositional constructions involving spatial and manner Prepositional Phrases. Further, DITL serves as a semantics for a linguistically expressive markup language for annotating spatiotemporal information in text, called Spatiotemporal Markup Language (STML). We outline the syntax of this language, and discuss how DITL provides for a natural interpretation of the annotation specification for use in a variety of applications.",
    "venue": "Spatial Cogn. Comput.",
    "year": 2011,
    "referenceCount": 69,
    "citationCount": 68,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1707726",
        "name": "J. Pustejovsky"
      },
      {
        "authorId": "1723806",
        "name": "Jessica L. Moszkowicz"
      }
    ]
  },
  "263861428": {
    "paperId": "2c5d4e99bd86411305e42c52009af75758272471",
    "externalIds": {
      "DBLP": "books/daglib/0029214",
      "DOI": "10.1093/acprof:oso/9780199601240.001.0001",
      "CorpusId": 263861428
    },
    "publicationVenue": null,
    "title": "Interpreting Motion - Grounded Representations for Spatial Language",
    "abstract": null,
    "venue": "Explorations in language and space",
    "year": 2012,
    "referenceCount": 0,
    "citationCount": 38,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1729172",
        "name": "I. Mani"
      },
      {
        "authorId": "2265111544",
        "name": "James Pustejovsky"
      }
    ]
  },
  "221508017": {
    "paperId": "258eb3cbc20b350cb4c183c09d3029d850da6c8e",
    "externalIds": {
      "CorpusId": 221508017
    },
    "publicationVenue": null,
    "title": "Changing perspective: Local alignment of reference frames in dialogue",
    "abstract": "In this paper we examine how people negotiate, interpret and repair the frame of reference (FoR) in free dialogues discussing spatial scenes. We describe a pilot study in which participants are given different perspectives of the same scene and asked to locate several objects that are only shown on one of their pictures. This task requires participants to coordinate on FoR in order to identify the missing objects. Preliminary results indicate that conversational participants align locally on FoR but do not converge on a global frame of reference. Misunderstandings lead to clarification sequences in which participants shift the FoR. These findings have implications for situated dialogue systems.",
    "venue": "",
    "year": 2015,
    "referenceCount": 22,
    "citationCount": 14,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2995275",
        "name": "Simon Dobnik"
      },
      {
        "authorId": "1812874",
        "name": "C. Howes"
      }
    ]
  },
  "15921971": {
    "paperId": "5b12f6ff72b42659138b2ab4c25cc7052edf72d0",
    "externalIds": {
      "MAG": "2017262716",
      "DBLP": "journals/ws/KordjamshidiM15",
      "DOI": "10.1016/j.websem.2014.06.001",
      "CorpusId": 15921971
    },
    "publicationVenue": {
      "id": "78d61825-c1d6-4e7f-ab9d-1525db71105c",
      "name": "Journal of Web Semantics",
      "type": "journal",
      "alternate_names": [
        "J Web Semant"
      ],
      "issn": "1570-8268",
      "url": "http://www.elsevier.com/locate/websem",
      "alternate_urls": [
        "http://www.sciencedirect.com/science/journal/15708268",
        "http://www.elsevier.com/wps/find/journaldescription.cws_home/671322/description"
      ]
    },
    "title": "Global machine learning for spatial ontology population",
    "abstract": null,
    "venue": "Journal of Web Semantics",
    "year": 2015,
    "referenceCount": 61,
    "citationCount": 57,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Geography",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2190934",
        "name": "Parisa Kordjamshidi"
      },
      {
        "authorId": "145446752",
        "name": "Marie-Francine Moens"
      }
    ]
  },
  "11947761": {
    "paperId": "c3b2e90aba82756e0725e23867a87fec91d9bcd9",
    "externalIds": {
      "DBLP": "conf/lrec/PustejovskyK16",
      "MAG": "2952127362",
      "ACL": "L16-1730",
      "ArXiv": "1610.01508",
      "CorpusId": 11947761
    },
    "publicationVenue": {
      "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
      "name": "International Conference on Language Resources and Evaluation",
      "type": "conference",
      "alternate_names": [
        "LREC",
        "Int Conf Lang Resour Evaluation"
      ],
      "url": "http://www.lrec-conf.org/"
    },
    "title": "VoxML: A Visualization Modeling Language",
    "abstract": "We present the specification for a modeling language, VoxML, which encodes semantic knowledge of real-world objects represented as three-dimensional models, and of events and attributes related to and enacted over these objects.VoxML is intended to overcome the limitations of existing 3D visual markup languages by allowing for the encoding of a broad range of semantic knowledge that can be exploited by a variety of systems and platforms, leading to multimodal simulations of real-world scenarios using conceptual objects that represent their semantic values",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2016,
    "referenceCount": 38,
    "citationCount": 73,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1707726",
        "name": "J. Pustejovsky"
      },
      {
        "authorId": "34079649",
        "name": "Nikhil Krishnaswamy"
      }
    ]
  },
  "20973461": {
    "paperId": "b8d358cbe798c0cd25af8fd78785d575ead3daa3",
    "externalIds": {
      "CorpusId": 20973461
    },
    "publicationVenue": null,
    "title": "Do You See What I See? Effects of POV on Spatial Relation Speci\ufb01cations",
    "abstract": "In this paper, we examine a set of object interactions generated with a 3D natural language simulation and visualization platform, VoxSim (Krishnaswamy and Pustejovsky 2016b). These simulations all realize the natural language relations \u201ctouching\u201d and \u201cnear\u201d over a test set of various objects within a 3-dimensional world that interprets descriptions of motion events and renders their visual instantiations from the perspective of an embodied virtual agent. These object interactions were evaluated by human judges using Amazon Mechanical Turk and we examine some of the qualitative interpretations provided by humans over these computer-generated interpretations of underspeci\ufb01ed relations, conditioned on the frame of reference (agent\u2019s point of view) and object position relative to that point of view (POV). Through analysis of the human evaluations, we \ufb01nd that average eval-uator satisfaction with many speci\ufb01cations for these relations appears to strongly depend on the relationship between the two objects and between the objects and the POV.",
    "venue": "",
    "year": 2017,
    "referenceCount": 52,
    "citationCount": 1,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34079649",
        "name": "Nikhil Krishnaswamy"
      },
      {
        "authorId": "1707726",
        "name": "J. Pustejovsky"
      }
    ]
  },
  "56798299": {
    "paperId": "b29e13444e3da7c7e2fa605742211435f2c615ae",
    "externalIds": {
      "MAG": "2626283244",
      "DOI": "10.1007/978-94-024-0881-2_37",
      "CorpusId": 56798299
    },
    "publicationVenue": null,
    "title": "ISO-Space: Annotating Static and Dynamic Spatial Information",
    "abstract": null,
    "venue": "",
    "year": 2017,
    "referenceCount": 41,
    "citationCount": 27,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1707726",
        "name": "J. Pustejovsky"
      }
    ]
  },
  "12628986": {
    "paperId": "1295f871f2532274fb32b7815a605b3b3e6c7b6f",
    "externalIds": {
      "MAG": "2519891957",
      "DOI": "10.1007/978-94-024-0881-2_38",
      "CorpusId": 12628986
    },
    "publicationVenue": null,
    "title": "Spatial Role Labeling Annotation Scheme",
    "abstract": null,
    "venue": "",
    "year": 2017,
    "referenceCount": 55,
    "citationCount": 13,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2190934",
        "name": "Parisa Kordjamshidi"
      },
      {
        "authorId": "2541098",
        "name": "M. V. Otterlo"
      },
      {
        "authorId": "145446752",
        "name": "Marie-Francine Moens"
      }
    ]
  },
  "5911617": {
    "paperId": "a5dbb7039fe593990186f4bc0dca0a6d14ff5b06",
    "externalIds": {
      "MAG": "2950075089",
      "DBLP": "journals/corr/TanB17",
      "ArXiv": "1707.03804",
      "DOI": "10.1609/aaai.v32i1.12012",
      "CorpusId": 5911617
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Source-Target Inference Models for Spatial Instruction Understanding",
    "abstract": "\n \n Models that can execute natural language instructions for situated robotic tasks such as assembly and navigation have several useful applications in homes, offices, and remote scenarios.We study the semantics of spatially-referred configuration and arrangement instructions, based on the challenging Bisk-2016 blank-labeled block dataset. This task involves finding a source block and moving it to the target position (mentioned via a reference block and offset), where the blocks have no names or colors and are just referred to via spatial location features.We present novel models for the subtasks of source block classification and target position regression, based on joint-loss language and spatial-world representation learning, as well as CNN-based and dual attention models to compute the alignment between the world blocks and the instruction phrases. For target position prediction, we compare two inference approaches: annealed sampling via policy gradient versus expectation inference via supervised regression. Our models achieve the new state-of-the-art on this task, with an improvement of 47% on source block accuracy and 22% on target position distance.\n \n",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2017,
    "referenceCount": 27,
    "citationCount": 14,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47300698",
        "name": "Hao Tan"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      }
    ]
  },
  "19152379": {
    "paperId": "45f0115613b9c79572af9365daafc58bede9851e",
    "externalIds": {
      "MAG": "2769921607",
      "DBLP": "conf/aaai/CollellGM18",
      "ArXiv": "1711.06821",
      "DOI": "10.1609/aaai.v32i1.12239",
      "CorpusId": 19152379
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates",
    "abstract": "\n \n Spatial understanding is a fundamental problem with wide-reaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on,\" \"below,\" etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g., \"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.\n \n",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2017,
    "referenceCount": 22,
    "citationCount": 39,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144481186",
        "name": "Guillem Collell"
      },
      {
        "authorId": "1681236",
        "name": "L. Gool"
      },
      {
        "authorId": "145446752",
        "name": "Marie-Francine Moens"
      }
    ]
  },
  "195063887": {
    "paperId": "3630c7b73f08b942f58acbce2179ef03442e1ad4",
    "externalIds": {
      "DBLP": "conf/iwcs/KrishnaswamyP19",
      "ACL": "W19-0507",
      "MAG": "2955718891",
      "DOI": "10.18653/v1/W19-0507",
      "CorpusId": 195063887
    },
    "publicationVenue": {
      "id": "be46d9bc-380b-4b57-8140-bbd0da45ce9c",
      "name": "International Conference on Computational Semantics",
      "type": "conference",
      "alternate_names": [
        "IWCS",
        "Int Conf Comput Semant"
      ],
      "url": "http://www.sigsem.org/wiki/index.php?title=IWCS"
    },
    "title": "Generating a Novel Dataset of Multimodal Referring Expressions",
    "abstract": "Referring expressions and definite descriptions of objects in space exploit information both about object characteristics and locations. To resolve potential ambiguity, referencing strategies in language can rely on increasingly abstract concepts to distinguish an object in a given location from similar ones elsewhere, yet the description of the intended location may still be imprecise or difficult to interpret. Meanwhile, modalities such as gesture may communicate spatial information such as locations in a more concise manner. In real peer-to-peer communication, humans use language and gesture together to reference entities, with a capacity for mixing and changing modalities where needed. While recent progress in AI and human-computer interaction has created systems where a human can interact with a computer multimodally, computers often lack the capacity to intelligently mix modalities when generating referring expressions. We present a novel dataset of referring expressions combining natural language and gesture, describe its creation and evaluation, and its uses to train computational models for generating and interpreting multimodal referring expressions.",
    "venue": "International Conference on Computational Semantics",
    "year": 2019,
    "referenceCount": 45,
    "citationCount": 17,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34079649",
        "name": "Nikhil Krishnaswamy"
      },
      {
        "authorId": "1707726",
        "name": "J. Pustejovsky"
      }
    ]
  },
  "216638592": {
    "paperId": "b7adef89d7a0e7b743ab098d583a90b1cbfc6de7",
    "externalIds": {
      "ACL": "D14-1140",
      "DBLP": "conf/emnlp/GrissomHBMD14",
      "MAG": "2251955814",
      "DOI": "10.3115/v1/D14-1140",
      "CorpusId": 216638592
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Don't Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation",
    "abstract": "We introduce a reinforcement learningbased approach to simultaneous machine translation\u2014producing a translation while receiving input words\u2014 between languages with drastically different word orders: from verb-final languages (e.g., German) to verb-medial languages (English). In traditional machine translation, a translator must \u201cwait\u201d for source material to appear before translation begins. We remove this bottleneck by predicting the final verb in advance. We use reinforcement learning to learn when to trust predictions about unseen, future portions of the sentence. We also introduce an evaluation metric to measure expeditiousness and quality. We show that our new translation model outperforms batch and monotone translation strategies.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2014,
    "referenceCount": 35,
    "citationCount": 119,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2778913",
        "name": "Alvin Grissom II"
      },
      {
        "authorId": "144533687",
        "name": "He He"
      },
      {
        "authorId": "1389036863",
        "name": "Jordan L. Boyd-Graber"
      },
      {
        "authorId": "2113564796",
        "name": "John Morgan"
      },
      {
        "authorId": "1722360",
        "name": "Hal Daum\u00e9"
      }
    ]
  },
  "216804792": {
    "paperId": "942eb04b5fe958fc07d5a00df1d27edcada4f05e",
    "externalIds": {
      "MAG": "2251530174",
      "ACL": "D15-1006",
      "DBLP": "conf/emnlp/HeGMBD15",
      "DOI": "10.18653/v1/D15-1006",
      "CorpusId": 216804792
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Syntax-based Rewriting for Simultaneous Machine Translation",
    "abstract": "Divergent word order between languages causes delay in simultaneous machine translation. We present a sentence rewriting method that generates more monotonic translations to improve the speedaccuracy tradeoff. We design grammaticality and meaning-preserving syntactic transformation rules that operate on constituent parse trees. We apply the rules to reference translations to make their word order closer to the source language word order. On Japanese-English translation (two languages with substantially different structure), incorporating the rewritten, more monotonic reference translation into a phrase-based machine translation system enables better translations faster than a baseline system that only uses gold reference translations.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2015,
    "referenceCount": 35,
    "citationCount": 35,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144466851",
        "name": "He He"
      },
      {
        "authorId": "2778913",
        "name": "Alvin Grissom II"
      },
      {
        "authorId": "2113564796",
        "name": "John Morgan"
      },
      {
        "authorId": "1389036863",
        "name": "Jordan L. Boyd-Graber"
      },
      {
        "authorId": "1722360",
        "name": "Hal Daum\u00e9"
      }
    ]
  },
  "14003387": {
    "paperId": "db2a669b6b7a7c8bcab31e973a9e010cd6dcc7f5",
    "externalIds": {
      "ArXiv": "1606.02012",
      "DBLP": "journals/corr/ChoE16",
      "MAG": "2419292002",
      "CorpusId": 14003387
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Can neural machine translation do simultaneous translation?",
    "abstract": "We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.",
    "venue": "arXiv.org",
    "year": 2016,
    "referenceCount": 22,
    "citationCount": 149,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1979489",
        "name": "Kyunghyun Cho"
      },
      {
        "authorId": "3422698",
        "name": "Masha Esipova"
      }
    ]
  },
  "2782776": {
    "paperId": "b13e9d23983273c0c67b91ae70c55d4c3f745b8b",
    "externalIds": {
      "DBLP": "conf/eacl/NeubigCGL17",
      "MAG": "2950853958",
      "ACL": "E17-1099",
      "ArXiv": "1610.00388",
      "DOI": "10.18653/V1/E17-1099",
      "CorpusId": 2782776
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "Learning to Translate in Real-time with Neural Machine Translation",
    "abstract": "Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 24,
    "citationCount": 208,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "1979489",
        "name": "Kyunghyun Cho"
      },
      {
        "authorId": "3016273",
        "name": "Jiatao Gu"
      },
      {
        "authorId": "2052674293",
        "name": "V. Li"
      }
    ]
  },
  "189762487": {
    "paperId": "9d3480e46cc506b73d5291387c6452998690fdd3",
    "externalIds": {
      "MAG": "2951562371",
      "DBLP": "conf/acl/MaHXZLZZHLLWW19",
      "ACL": "P19-1289",
      "DOI": "10.18653/v1/P19-1289",
      "CorpusId": 189762487
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework",
    "abstract": "Simultaneous translation, which translates sentences before they are finished, is use- ful in many scenarios but is notoriously dif- ficult due to word-order differences. While the conventional seq-to-seq framework is only suitable for full-sentence translation, we pro- pose a novel prefix-to-prefix framework for si- multaneous translation that implicitly learns to anticipate in a single translation model. Within this framework, we present a very sim- ple yet surprisingly effective \u201cwait-k\u201d policy trained to generate the target sentence concur- rently with the source sentence, but always k words behind. Experiments show our strat- egy achieves low latency and reasonable qual- ity (compared to full-sentence translation) on 4 directions: zh\u2194en and de\u2194en.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 22,
    "citationCount": 256,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1847848",
        "name": "Mingbo Ma"
      },
      {
        "authorId": "48545084",
        "name": "Liang Huang"
      },
      {
        "authorId": "2054473960",
        "name": "Hao Xiong"
      },
      {
        "authorId": "40223399",
        "name": "Renjie Zheng"
      },
      {
        "authorId": "66057453",
        "name": "Kaibo Liu"
      },
      {
        "authorId": "20712300",
        "name": "Baigong Zheng"
      },
      {
        "authorId": "30750818",
        "name": "Chuanqiang Zhang"
      },
      {
        "authorId": "37985966",
        "name": "Zhongjun He"
      },
      {
        "authorId": "2110117273",
        "name": "Hairong Liu"
      },
      {
        "authorId": "2155448199",
        "name": "Xing Li"
      },
      {
        "authorId": "40354707",
        "name": "Hua Wu"
      },
      {
        "authorId": "144270731",
        "name": "Haifeng Wang"
      }
    ]
  },
  "186206508": {
    "paperId": "05b3a6acc8be299cc2a2678e5d81712b71c748e5",
    "externalIds": {
      "DBLP": "conf/acl/ArivazhaganCMCY19",
      "ArXiv": "1906.05218",
      "MAG": "2951997403",
      "ACL": "P19-1126",
      "DOI": "10.18653/v1/P19-1126",
      "CorpusId": 186206508
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Monotonic Infinite Lookback Attention for Simultaneous Machine Translation",
    "abstract": "Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking, with applications to live and streaming scenarios. Simultaneous systems must carefully schedule their reading of the source sentence to balance quality against latency. We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation (NMT) model that attends over all source tokens read thus far. We do so by introducing Monotonic Infinite Lookback (MILk) attention, which maintains both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk\u2019s adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 26,
    "citationCount": 186,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3365231",
        "name": "N. Arivazhagan"
      },
      {
        "authorId": "144507724",
        "name": "Colin Cherry"
      },
      {
        "authorId": "3153147",
        "name": "Wolfgang Macherey"
      },
      {
        "authorId": "145039780",
        "name": "Chung-Cheng Chiu"
      },
      {
        "authorId": "3014143",
        "name": "Semih Yavuz"
      },
      {
        "authorId": "34320634",
        "name": "Ruoming Pang"
      },
      {
        "authorId": "40400230",
        "name": "Wei Li"
      },
      {
        "authorId": "2402716",
        "name": "Colin Raffel"
      }
    ]
  },
  "16946362": {
    "paperId": "d13bb317e87f3f6da10da11059ebf4350b754814",
    "externalIds": {
      "DBLP": "journals/jair/GattK18",
      "MAG": "2949199278",
      "ArXiv": "1703.09902",
      "DOI": "10.1613/jair.5477",
      "CorpusId": 16946362
    },
    "publicationVenue": {
      "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
      "name": "Journal of Artificial Intelligence Research",
      "type": "journal",
      "alternate_names": [
        "JAIR",
        "J Artif Intell Res",
        "The Journal of Artificial Intelligence Research"
      ],
      "issn": "1076-9757",
      "url": "http://www.jair.org/"
    },
    "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation",
    "abstract": "This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past two decades, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artifical intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of nlp, with an emphasis on different evaluation methods and the relationships between them.",
    "venue": "Journal of Artificial Intelligence Research",
    "year": 2017,
    "referenceCount": 549,
    "citationCount": 771,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1700894",
        "name": "Albert Gatt"
      },
      {
        "authorId": "2297195264",
        "name": "E. Krahmer"
      }
    ]
  },
  "160025533": {
    "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
    "externalIds": {
      "MAG": "2955855238",
      "CorpusId": 160025533
    },
    "publicationVenue": null,
    "title": "Language Models are Unsupervised Multitask Learners",
    "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
    "venue": "",
    "year": 2019,
    "referenceCount": 75,
    "citationCount": 19298,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "38909097",
        "name": "Alec Radford"
      },
      {
        "authorId": "49387725",
        "name": "Jeff Wu"
      },
      {
        "authorId": "48422824",
        "name": "R. Child"
      },
      {
        "authorId": "150970919",
        "name": "D. Luan"
      },
      {
        "authorId": "2698777",
        "name": "Dario Amodei"
      },
      {
        "authorId": "1701686",
        "name": "I. Sutskever"
      }
    ]
  },
  "162169061": {
    "paperId": "eb011ccdf9ea739ea86be85b268a4d958266b624",
    "externalIds": {
      "MAG": "2945886944",
      "DBLP": "journals/corr/abs-1905-08836",
      "ArXiv": "1905.08836",
      "CorpusId": 162169061
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Sample Efficient Text Summarization Using a Single Pre-Trained Transformer",
    "abstract": "Language model (LM) pre-training has resulted in impressive performance and sample efficiency on a variety of language understanding tasks. However, it remains unclear how to best use pre-trained LMs for generation tasks such as abstractive summarization, particularly to enhance sample efficiency. In these sequence-to-sequence settings, prior work has experimented with loading pre-trained weights into the encoder and/or decoder networks, but used non-pre-trained encoder-decoder attention weights. We instead use a pre-trained decoder-only network, where the same Transformer LM both encodes the source and generates the summary. This ensures that all parameters in the network, including those governing attention over source states, have been pre-trained before the fine-tuning step. Experiments on the CNN/Daily Mail dataset show that our pre-trained Transformer LM substantially improves over pre-trained Transformer encoder-decoder networks in limited-data settings. For instance, it achieves 13.1 ROUGE-2 using only 1% of the training data (~3000 examples), while pre-trained encoder-decoder models score 2.3 ROUGE-2.",
    "venue": "arXiv.org",
    "year": 2019,
    "referenceCount": 28,
    "citationCount": 74,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3030219",
        "name": "Urvashi Khandelwal"
      },
      {
        "authorId": "144358401",
        "name": "Kevin Clark"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      },
      {
        "authorId": "40527594",
        "name": "Lukasz Kaiser"
      }
    ]
  },
  "127986954": {
    "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
    "externalIds": {
      "DBLP": "journals/corr/abs-1904-09751",
      "MAG": "2938704169",
      "ArXiv": "1904.09751",
      "CorpusId": 127986954
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "The Curious Case of Neural Text Degeneration",
    "abstract": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. \nIn this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "referenceCount": 44,
    "citationCount": 2742,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "14487640",
        "name": "Ari Holtzman"
      },
      {
        "authorId": "144685020",
        "name": "Jan Buys"
      },
      {
        "authorId": "2152141637",
        "name": "Li Du"
      },
      {
        "authorId": "39191185",
        "name": "Maxwell Forbes"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      }
    ]
  },
  "14674248": {
    "paperId": "66021a920001bc3e6258bffe7076d647614147b7",
    "externalIds": {
      "MAG": "658020064",
      "DBLP": "conf/icml/KusnerSKW15",
      "CorpusId": 14674248
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "From Word Embeddings To Document Distances",
    "abstract": "We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to \"travel\" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.",
    "venue": "International Conference on Machine Learning",
    "year": 2015,
    "referenceCount": 49,
    "citationCount": 2017,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1940272",
        "name": "Matt J. Kusner"
      },
      {
        "authorId": "2117103358",
        "name": "Yu Sun"
      },
      {
        "authorId": "1971973",
        "name": "Nicholas I. Kolkin"
      },
      {
        "authorId": "7446832",
        "name": "Kilian Q. Weinberger"
      }
    ]
  },
  "7147309": {
    "paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
    "externalIds": {
      "DBLP": "journals/corr/RanzatoCAZ15",
      "MAG": "2950469587",
      "ArXiv": "1511.06732",
      "CorpusId": 7147309
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Sequence Level Training with Recurrent Neural Networks",
    "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.",
    "venue": "International Conference on Learning Representations",
    "year": 2015,
    "referenceCount": 36,
    "citationCount": 1538,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1706809",
        "name": "Marc'Aurelio Ranzato"
      },
      {
        "authorId": "3295092",
        "name": "S. Chopra"
      },
      {
        "authorId": "2325985",
        "name": "Michael Auli"
      },
      {
        "authorId": "2563432",
        "name": "Wojciech Zaremba"
      }
    ]
  },
  "748227": {
    "paperId": "d82b55c35c8673774a708353838918346f6c006f",
    "externalIds": {
      "DBLP": "conf/conll/BowmanVVDJB16",
      "MAG": "2210838531",
      "ACL": "K16-1002",
      "ArXiv": "1511.06349",
      "DOI": "10.18653/v1/K16-1002",
      "CorpusId": 748227
    },
    "publicationVenue": {
      "id": "3779a5a7-9119-4f69-84fe-f7eef193eb49",
      "name": "Conference on Computational Natural Language Learning",
      "type": "conference",
      "alternate_names": [
        "CoNLL",
        "Conf Comput Nat Lang Learn"
      ]
    },
    "title": "Generating Sentences from a Continuous Space",
    "abstract": "The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.",
    "venue": "Conference on Computational Natural Language Learning",
    "year": 2015,
    "referenceCount": 49,
    "citationCount": 2260,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3644767",
        "name": "Samuel R. Bowman"
      },
      {
        "authorId": "2546951",
        "name": "L. Vilnis"
      },
      {
        "authorId": "1689108",
        "name": "O. Vinyals"
      },
      {
        "authorId": "2555924",
        "name": "Andrew M. Dai"
      },
      {
        "authorId": "1944541",
        "name": "R. J\u00f3zefowicz"
      },
      {
        "authorId": "1751569",
        "name": "Samy Bengio"
      }
    ]
  },
  "21731209": {
    "paperId": "6db2b93a2d4007371030644173f1001c959214d2",
    "externalIds": {
      "MAG": "2963283805",
      "ArXiv": "1805.06087",
      "DBLP": "journals/corr/abs-1805-06087",
      "ACL": "P18-1152",
      "DOI": "10.18653/v1/P18-1152",
      "CorpusId": 21731209
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Learning to Write with Cooperative Discriminators",
    "abstract": "Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as Grice\u2019s maxims, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 57,
    "citationCount": 221,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "14487640",
        "name": "Ari Holtzman"
      },
      {
        "authorId": "144685020",
        "name": "Jan Buys"
      },
      {
        "authorId": "39191185",
        "name": "Maxwell Forbes"
      },
      {
        "authorId": "2691021",
        "name": "Antoine Bosselut"
      },
      {
        "authorId": "145798491",
        "name": "David Golub"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      }
    ]
  },
  "203912051": {
    "paperId": "a2ae7155d94686fe83f26f6d6ca2dfacd16c5e5c",
    "externalIds": {
      "MAG": "2979666134",
      "DBLP": "journals/coling/LawrenceR19",
      "DOI": "10.1162/coli_a_00364",
      "CorpusId": 203912051
    },
    "publicationVenue": {
      "id": "ee37a78c-f3d8-407a-bd24-bb97fe6dbab9",
      "name": "Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Comput Linguistics"
      ],
      "issn": "0891-2017",
      "alternate_issns": [
        "1530-9312",
        "0362-613x",
        "0362-613X"
      ],
      "url": "http://aclanthology.info/venues/cl",
      "alternate_urls": [
        "http://mitpress.mit.edu/catalog/item/default.asp?ttype=4&tid=10",
        "https://www.mitpressjournals.org/loi/coli"
      ]
    },
    "title": "Argument Mining: A Survey",
    "abstract": "Argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. Understanding argumentative structure makes it possible to determine not only what positions people are adopting, but also why they hold the opinions they do, providing valuable insights in domains as diverse as financial market prediction and public relations. This survey explores the techniques that establish the foundations for argument mining, provides a review of recent advances in argument mining techniques, and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.",
    "venue": "Computational Linguistics",
    "year": 2020,
    "referenceCount": 249,
    "citationCount": 400,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Business",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2055083035",
        "name": "J. Lawrence"
      },
      {
        "authorId": "145989424",
        "name": "C. Reed"
      }
    ]
  },
  "232305184": {
    "paperId": "8f7697835d88d1e61cf36cf005f3b46558f05a49",
    "externalIds": {
      "DBLP": "journals/nature/SlonimBABBBCCDE21",
      "DOI": "10.1038/s41586-021-03215-w",
      "CorpusId": 232305184,
      "PubMed": "33731946"
    },
    "publicationVenue": {
      "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
      "name": "Nature",
      "type": "journal",
      "issn": "0028-0836",
      "url": "https://www.nature.com/",
      "alternate_urls": [
        "http://www.nature.com/nature/",
        "https://www.nature.com/nature/",
        "http://www.nature.com/nature/archive/index.html"
      ]
    },
    "title": "An autonomous debating system",
    "abstract": null,
    "venue": "Nature",
    "year": 2021,
    "referenceCount": 49,
    "citationCount": 124,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1766595",
        "name": "N. Slonim"
      },
      {
        "authorId": "2911299",
        "name": "Yonatan Bilu"
      },
      {
        "authorId": "2059199916",
        "name": "Carlos Alzate"
      },
      {
        "authorId": "1693525",
        "name": "Roy Bar-Haim"
      },
      {
        "authorId": "50757607",
        "name": "Ben Bogin"
      },
      {
        "authorId": "2610023",
        "name": "Francesca Bonin"
      },
      {
        "authorId": "41019330",
        "name": "Leshem Choshen"
      },
      {
        "authorId": "1405434669",
        "name": "Edo Cohen-Karlik"
      },
      {
        "authorId": "2839128",
        "name": "Lena Dankin"
      },
      {
        "authorId": "39068807",
        "name": "Lilach Edelstein"
      },
      {
        "authorId": "1402680837",
        "name": "L. Ein-Dor"
      },
      {
        "authorId": "2056556257",
        "name": "Roni Friedman-Melamed"
      },
      {
        "authorId": "71873369",
        "name": "A. Gavron"
      },
      {
        "authorId": "48835746",
        "name": "Ariel Gera"
      },
      {
        "authorId": "2975469",
        "name": "Martin Gleize"
      },
      {
        "authorId": "3291191",
        "name": "Shai Gretz"
      },
      {
        "authorId": "1891570",
        "name": "Dan Gutfreund"
      },
      {
        "authorId": "41127252",
        "name": "Alon Halfon"
      },
      {
        "authorId": "2086349",
        "name": "Daniel Hershcovich"
      },
      {
        "authorId": "1678934",
        "name": "R. Hoory"
      },
      {
        "authorId": "150348519",
        "name": "Yufang Hou"
      },
      {
        "authorId": "31732092",
        "name": "S. Hummel"
      },
      {
        "authorId": "2697312",
        "name": "Michal Jacovi"
      },
      {
        "authorId": "2078553",
        "name": "Charles Jochim"
      },
      {
        "authorId": "2965962",
        "name": "Yoav Kantor"
      },
      {
        "authorId": "1722434",
        "name": "Yoav Katz"
      },
      {
        "authorId": "1775524",
        "name": "D. Konopnicki"
      },
      {
        "authorId": "2981455",
        "name": "Zvi Kons"
      },
      {
        "authorId": "2020379",
        "name": "Lili Kotlerman"
      },
      {
        "authorId": "2058740289",
        "name": "Dalia Krieger"
      },
      {
        "authorId": "1396093833",
        "name": "Dan Lahav"
      },
      {
        "authorId": "1847650",
        "name": "Tamar Lavee"
      },
      {
        "authorId": "48496836",
        "name": "Ran Levy"
      },
      {
        "authorId": "2089779821",
        "name": "Naftali Liberman"
      },
      {
        "authorId": "1727535",
        "name": "Y. Mass"
      },
      {
        "authorId": "48499250",
        "name": "Amir Menczel"
      },
      {
        "authorId": "8963527",
        "name": "Shachar Mirkin"
      },
      {
        "authorId": "81154108",
        "name": "Guy Moshkowich"
      },
      {
        "authorId": "1405604910",
        "name": "Shila Ofek-Koifman"
      },
      {
        "authorId": "80108223",
        "name": "Matan Orbach"
      },
      {
        "authorId": "2653682",
        "name": "Ella Rabinovich"
      },
      {
        "authorId": "1905713",
        "name": "Ruty Rinott"
      },
      {
        "authorId": "1764141",
        "name": "Slava Shechtman"
      },
      {
        "authorId": "2035252",
        "name": "D. Sheinwald"
      },
      {
        "authorId": "1734246",
        "name": "Eyal Shnarch"
      },
      {
        "authorId": "2627091",
        "name": "Ilya Shnayderman"
      },
      {
        "authorId": "1696998",
        "name": "A. Soffer"
      },
      {
        "authorId": "51451979",
        "name": "Artem Spector"
      },
      {
        "authorId": "2464133",
        "name": "B. Sznajder"
      },
      {
        "authorId": "35874066",
        "name": "Assaf Toledo"
      },
      {
        "authorId": "1403181290",
        "name": "Orith Toledo-Ronen"
      },
      {
        "authorId": "5598623",
        "name": "Elad Venezian"
      },
      {
        "authorId": "48361424",
        "name": "R. Aharonov"
      }
    ]
  },
  "18847466": {
    "paperId": "1f09e8a4c897543253065cbcdbd6f4f0053d4ebf",
    "externalIds": {
      "ACL": "C14-1141",
      "MAG": "2251647857",
      "DBLP": "conf/coling/LevyBHAS14",
      "CorpusId": 18847466
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Context Dependent Claim Detection",
    "abstract": "While discussing a concrete controversial topic, most humans will find it challenging to swiftly raise a diverse set of convincing and relevant claims that should set the basis of their arguments. Here, we formally define the challenging task of automatic claim detection in a given context and discuss its associated unique difficulties. Further, we outline a preliminary solution to this task, and assess its performance over annotated real world data, collected specifically for that purpose over hundreds of Wikipedia articles. We report promising results of a supervised learning approach, which is based on a cascade of classifiers designed to properly handle the skewed data which is inherent to the defined task. These results demonstrate the viability of the introduced task.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2014,
    "referenceCount": 19,
    "citationCount": 222,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48496836",
        "name": "Ran Levy"
      },
      {
        "authorId": "2911299",
        "name": "Yonatan Bilu"
      },
      {
        "authorId": "2086349",
        "name": "Daniel Hershcovich"
      },
      {
        "authorId": "1697314",
        "name": "E. Aharoni"
      },
      {
        "authorId": "1766595",
        "name": "N. Slonim"
      }
    ]
  },
  "1804771": {
    "paperId": "3c95247788654b7b60deca996bd749413d21e781",
    "externalIds": {
      "DBLP": "conf/emnlp/RinottDPKAS15",
      "MAG": "2250762536",
      "ACL": "D15-1050",
      "DOI": "10.18653/v1/D15-1050",
      "CorpusId": 1804771
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Show Me Your Evidence - an Automatic Method for Context Dependent Evidence Detection",
    "abstract": "Engaging in a debate with oneself or others to take decisions is an integral part of our day-today life. A debate on a topic (say, use of performance enhancing drugs) typically proceeds by one party making an assertion/claim (say, PEDs are bad for health) and then providing an evidence to support the claim (say, a 2006 study shows that PEDs have psychiatric side effects). In this work, we propose the task of automatically detecting such evidences from unstructured text that support a given claim. This task has many practical applications in decision support and persuasion enhancement in a wide range of domains. We first introduce an extensive benchmark data set tailored for this task, which allows training statistical models and assessing their performance. Then, we suggest a system architecture based on supervised learning to address the evidence detection task. Finally, promising experimental results are reported.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2015,
    "referenceCount": 42,
    "citationCount": 216,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1905713",
        "name": "Ruty Rinott"
      },
      {
        "authorId": "2839128",
        "name": "Lena Dankin"
      },
      {
        "authorId": "2069526063",
        "name": "C. A. Perez"
      },
      {
        "authorId": "2361078",
        "name": "Mitesh M. Khapra"
      },
      {
        "authorId": "1697314",
        "name": "E. Aharoni"
      },
      {
        "authorId": "1766595",
        "name": "N. Slonim"
      }
    ]
  },
  "2938060": {
    "paperId": "0f4b0cef1e0305dc4e5139387e67a13a664668b2",
    "externalIds": {
      "MAG": "2266769560",
      "DBLP": "conf/ijcai/LippiT15",
      "CorpusId": 2938060
    },
    "publicationVenue": {
      "id": "67f7f831-711a-43c8-8785-1e09005359b5",
      "name": "International Joint Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Int Jt Conf Artif Intell",
        "IJCAI"
      ],
      "url": "http://www.ijcai.org/"
    },
    "title": "Context-Independent Claim Detection for Argument Mining",
    "abstract": "Argumentation mining aims to automatically identify structured argument data from unstructured natural language text. This challenging, multi-faceted task is recently gaining a growing attention, especially due to its many potential applications. One particularly important aspect of argumentation mining is claim identification. Most of the current approaches are engineered to address specific domains. However, argumentative sentences are often characterized by common rhetorical structures, independently of the domain. We thus propose a method that exploits structured parsing information to detect claims without resorting to contextual information, and yet achieve a performance comparable to that of state-of-the-art methods that heavily rely on the context.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2015,
    "referenceCount": 32,
    "citationCount": 141,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3428634",
        "name": "Marco Lippi"
      },
      {
        "authorId": "2896208",
        "name": "Paolo Torroni"
      }
    ]
  },
  "53083232": {
    "paperId": "a1e91322798c7ba97ab115c58817adb06005c0c1",
    "externalIds": {
      "DBLP": "journals/corr/abs-1802-05758",
      "ArXiv": "1802.05758",
      "ACL": "D18-1402",
      "MAG": "2787235839",
      "DOI": "10.18653/v1/D18-1402",
      "CorpusId": 53083232
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Cross-topic Argument Mining from Heterogeneous Sources",
    "abstract": "Argument mining is a core technology for automating argument search in large document collections. Despite its usefulness for this task, most current approaches are designed for use only with specific text types and fall short when applied to heterogeneous texts. In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts. We source annotations for over 25,000 instances covering eight controversial topics. We show that integrating topic information into bidirectional long short-term memory networks outperforms vanilla BiLSTMs by more than 3 percentage points in F1 in two- and three-label cross-topic settings. We also show that these results can be further improved by leveraging additional data for topic relevance using multi-task learning.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 45,
    "citationCount": 191,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3067663",
        "name": "Christian Stab"
      },
      {
        "authorId": "1818919",
        "name": "Tristan Miller"
      },
      {
        "authorId": "36294183",
        "name": "Benjamin Schiller"
      },
      {
        "authorId": "32832559",
        "name": "Pranav Rai"
      },
      {
        "authorId": "1730400",
        "name": "Iryna Gurevych"
      }
    ]
  },
  "208267859": {
    "paperId": "a774a4cda9523cdc618830538861cd47311cfd27",
    "externalIds": {
      "DBLP": "conf/aaai/Ein-DorSDHSGAGC20",
      "MAG": "2997020034",
      "ArXiv": "1911.10763",
      "DOI": "10.1609/AAAI.V34I05.6270",
      "CorpusId": 208267859
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Corpus Wide Argument Mining - a Working Solution",
    "abstract": "One of the main tasks in argument mining is the retrieval of argumentative content pertaining to a given topic. Most previous work addressed this task by retrieving a relatively small number of relevant documents as the initial source for such content. This line of research yielded moderate success, which is of limited use in a real-world system. Furthermore, for such a system to yield a comprehensive set of relevant arguments, over a wide range of topics, it requires leveraging a large and diverse corpus in an appropriate manner. Here we present a first end-to-end high-precision, corpus-wide argument mining system. This is made possible by combining sentence-level queries over an appropriate indexing of a very large corpus of newspaper articles, with an iterative annotation scheme. This scheme addresses the inherent label bias in the data and pinpoints the regions of the sample space whose manual labeling is required to obtain high-precision among top-ranked candidates.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "referenceCount": 38,
    "citationCount": 61,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1402680837",
        "name": "L. Ein-Dor"
      },
      {
        "authorId": "1734246",
        "name": "Eyal Shnarch"
      },
      {
        "authorId": "2839128",
        "name": "Lena Dankin"
      },
      {
        "authorId": "41127252",
        "name": "Alon Halfon"
      },
      {
        "authorId": "2464133",
        "name": "B. Sznajder"
      },
      {
        "authorId": "48835746",
        "name": "Ariel Gera"
      },
      {
        "authorId": "2059199916",
        "name": "Carlos Alzate"
      },
      {
        "authorId": "2975469",
        "name": "Martin Gleize"
      },
      {
        "authorId": "41019330",
        "name": "Leshem Choshen"
      },
      {
        "authorId": "39517968",
        "name": "Yufang Hou"
      },
      {
        "authorId": "2911299",
        "name": "Yonatan Bilu"
      },
      {
        "authorId": "48361424",
        "name": "R. Aharonov"
      },
      {
        "authorId": "1766595",
        "name": "N. Slonim"
      }
    ]
  },
  "141282": {
    "paperId": "3f020157c741f869da2a5daa2971b90d37fa9581",
    "externalIds": {
      "DBLP": "conf/eacl/WachsmuthSHPBHN17",
      "MAG": "2741686183",
      "ACL": "E17-1017",
      "DOI": "10.18653/V1/E17-1017",
      "CorpusId": 141282
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "Computational Argumentation Quality Assessment in Natural Language",
    "abstract": "Research on computational argumentation faces the problem of how to automatically assess the quality of an argument or argumentation. While different quality dimensions have been approached in natural language processing, a common understanding of argumentation quality is still missing. This paper presents the first holistic work on computational argumentation quality in natural language. We comprehensively survey the diverse existing theories and approaches to assess logical, rhetorical, and dialectical quality dimensions, and we derive a systematic taxonomy from these. In addition, we provide a corpus with 320 arguments, annotated for all 15 dimensions in the taxonomy. Our results establish a common ground for research on computational argumentation quality assessment.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 69,
    "citationCount": 196,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2626599",
        "name": "Henning Wachsmuth"
      },
      {
        "authorId": "33494034",
        "name": "Nona Naderi"
      },
      {
        "authorId": "39517968",
        "name": "Yufang Hou"
      },
      {
        "authorId": "2911299",
        "name": "Yonatan Bilu"
      },
      {
        "authorId": "3331141",
        "name": "Vinodkumar Prabhakaran"
      },
      {
        "authorId": "2007871156",
        "name": "Tim Alberdingk Thijm"
      },
      {
        "authorId": "145036961",
        "name": "Graeme Hirst"
      },
      {
        "authorId": "144146081",
        "name": "Benno Stein"
      }
    ]
  },
  "3083231": {
    "paperId": "25ae911c13da7ef9def56ee30170920ebd48a668",
    "externalIds": {
      "MAG": "2518510348",
      "DBLP": "conf/acl/HabernalG16",
      "ACL": "P16-1150",
      "DOI": "10.18653/v1/P16-1150",
      "CorpusId": 3083231
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM",
    "abstract": "We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments, namely their convincingness. We cast the problem as relation classification, where a pair of arguments having the same stance to the same prompt is judged. We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation \u201cA is more convincing than B\u201d exhibits properties of total ordering; these findings are used as global constraints for cleaning the crowdsourced data. We propose two tasks: (1) predicting which argument from an argument pair is more convincing and (2) ranking all arguments to the topic based on their convincingness. We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman\u2019s correlation in a cross-topic evaluation. We release the newly created corpus UKPConvArg1 and the experimental software under open licenses.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 46,
    "citationCount": 213,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2572366",
        "name": "Ivan Habernal"
      },
      {
        "authorId": "1730400",
        "name": "Iryna Gurevych"
      }
    ]
  },
  "10432955": {
    "paperId": "68b81fe3662c30bc0e27a5e23a69e7091ae22f53",
    "externalIds": {
      "ACL": "E17-1024",
      "MAG": "2739836857",
      "DBLP": "conf/eacl/SlonimBSBD17",
      "DOI": "10.18653/V1/E17-1024",
      "CorpusId": 10432955
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "Stance Classification of Context-Dependent Claims",
    "abstract": "Recent work has addressed the problem of detecting relevant claims for a given controversial topic. We introduce the complementary task of Claim Stance Classification, along with the first benchmark dataset for this task. We decompose this problem into: (a) open-domain target identification for topic and claim (b) sentiment classification for each target, and (c) open-domain contrast detection between the topic and the claim targets. Manual annotation of the dataset confirms the applicability and validity of our model. We describe an implementation of our model, focusing on a novel algorithm for contrast detection. Our approach achieves promising results, and is shown to outperform several baselines, which represent the common practice of applying a single, monolithic classifier for stance classification.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 40,
    "citationCount": 149,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1693525",
        "name": "Roy Bar-Haim"
      },
      {
        "authorId": "145963427",
        "name": "Indrajit Bhattacharya"
      },
      {
        "authorId": "3125402",
        "name": "Francesco Dinuzzo"
      },
      {
        "authorId": "2909575",
        "name": "Amrita Saha"
      },
      {
        "authorId": "1766595",
        "name": "N. Slonim"
      }
    ]
  },
  "196199072": {
    "paperId": "2819c908cd3fb0a4135eccc9ff86dc952851a5c8",
    "externalIds": {
      "DBLP": "conf/acl/BiluGHSLMMGS19",
      "ArXiv": "1908.08336",
      "ACL": "P19-1097",
      "MAG": "2952495911",
      "DOI": "10.18653/v1/P19-1097",
      "CorpusId": 196199072
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Argument Invention from First Principles",
    "abstract": "Competitive debaters often find themselves facing a challenging task \u2013 how to debate a topic they know very little about, with only minutes to prepare, and without access to books or the Internet? What they often do is rely on \u201dfirst principles\u201d, commonplace arguments which are relevant to many topics, and which they have refined in past debates. In this work we aim to explicitly define a taxonomy of such principled recurring arguments, and, given a controversial topic, to automatically identify which of these arguments are relevant to the topic. As far as we know, this is the first time that this approach to argument invention is formalized and made explicit in the context of NLP. The main goal of this work is to show that it is possible to define such a taxonomy. While the taxonomy suggested here should be thought of as a \u201dfirst attempt\u201d it is nonetheless coherent, covers well the relevant topics and coincides with what professional debaters actually argue in their speeches, and facilitates automatic argument invention for new topics.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 42,
    "citationCount": 19,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2911299",
        "name": "Yonatan Bilu"
      },
      {
        "authorId": "48835746",
        "name": "Ariel Gera"
      },
      {
        "authorId": "2086349",
        "name": "Daniel Hershcovich"
      },
      {
        "authorId": "2464133",
        "name": "B. Sznajder"
      },
      {
        "authorId": "1396093833",
        "name": "Dan Lahav"
      },
      {
        "authorId": "81154108",
        "name": "Guy Moshkowich"
      },
      {
        "authorId": "150020801",
        "name": "Anael Malet"
      },
      {
        "authorId": "71873369",
        "name": "A. Gavron"
      },
      {
        "authorId": "1766595",
        "name": "N. Slonim"
      }
    ]
  },
  "53080821": {
    "paperId": "89f256abf0e0187fcf0a56a4df6f447a2c0b17bb",
    "externalIds": {
      "ACL": "D18-1078",
      "DBLP": "conf/emnlp/MirkinMOKKLJBAS18",
      "MAG": "2890194160",
      "DOI": "10.18653/v1/D18-1078",
      "CorpusId": 53080821
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Listening Comprehension over Argumentative Content",
    "abstract": "This paper presents a task for machine listening comprehension in the argumentation domain and a corresponding dataset in English. We recorded 200 spontaneous speeches arguing for or against 50 controversial topics. For each speech, we formulated a question, aimed at confirming or rejecting the occurrence of potential arguments in the speech. Labels were collected by listening to the speech and marking which arguments were mentioned by the speaker. We applied baseline methods addressing the task, to be used as a benchmark for future work over this dataset. All data used in this work is freely available for research.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 27,
    "citationCount": 24,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8963527",
        "name": "Shachar Mirkin"
      },
      {
        "authorId": "81154108",
        "name": "Guy Moshkowich"
      },
      {
        "authorId": "80108223",
        "name": "Matan Orbach"
      },
      {
        "authorId": "2020379",
        "name": "Lili Kotlerman"
      },
      {
        "authorId": "2965962",
        "name": "Yoav Kantor"
      },
      {
        "authorId": "1847650",
        "name": "Tamar Lavee"
      },
      {
        "authorId": "2697312",
        "name": "Michal Jacovi"
      },
      {
        "authorId": "2911299",
        "name": "Yonatan Bilu"
      },
      {
        "authorId": "48361424",
        "name": "R. Aharonov"
      },
      {
        "authorId": "1766595",
        "name": "N. Slonim"
      }
    ]
  },
  "60498119": {
    "paperId": "e8eb363f3d87aaec3bb7d1f74917e21724123b93",
    "externalIds": {
      "DBLP": "books/ox/05/Bach05",
      "MAG": "3030555352",
      "DOI": "10.1002/9780470758335.CH13",
      "CorpusId": 60498119
    },
    "publicationVenue": null,
    "title": "The algebra of events",
    "abstract": "A number of writers have commented on the close parallels between the mass-count distinction in nominal systems and the aspectual classification of verbal expressions (Allen, 1966; Mourelatos, 1978; L. Carlson, 1981; Hoepelman and Rohrer, 1980) that has been the subject of much attention in recent years in linguistics and philosophy. To take just one class of examples for now, there is a parallel between the two sets of distinctions in their cooccurrence patterns with expressions denoting numbers or amounts, as in Examples (1a)\u2013(4b):",
    "venue": "The Language of Time - A Reader",
    "year": 1986,
    "referenceCount": 22,
    "citationCount": 796,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144282503",
        "name": "Emmon Bach"
      }
    ]
  },
  "6341459": {
    "paperId": "b1cdc9884113bb055505272b9c15772cb558d221",
    "externalIds": {
      "ACL": "D13-1185",
      "MAG": "2250836735",
      "DBLP": "conf/emnlp/Chambers13",
      "CorpusId": 6341459
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Event Schema Induction with a Probabilistic Entity-Driven Model",
    "abstract": "Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline\u2019s performance, and outperforms the HMM by 7 F1 points (20%).",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2013,
    "referenceCount": 28,
    "citationCount": 122,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1729918",
        "name": "Nathanael Chambers"
      }
    ]
  },
  "202539612": {
    "paperId": "ec9446482c8448911cbc7c98a676fcd156ab20af",
    "externalIds": {
      "DBLP": "conf/emnlp/LiGMS19",
      "ACL": "D19-1405",
      "MAG": "2971921724",
      "ArXiv": "1909.00126",
      "DOI": "10.18653/v1/D19-1405",
      "CorpusId": 202539612
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "A Logic-Driven Framework for Consistency of Neural Models",
    "abstract": "While neural models show remarkable accuracy on individual predictions, their internal beliefs can be inconsistent across examples. In this paper, we formalize such inconsistency as a generalization of prediction error. We propose a learning framework for constraining models using logic rules to regularize them away from inconsistency. Our framework can leverage both labeled and unlabeled examples and is directly compatible with off-the-shelf learning schemes without model redesign. We instantiate our framework on natural language inference, where experiments show that enforcing invariants stated in logic can help make the predictions of neural models both accurate and consistent.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 45,
    "citationCount": 90,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47387745",
        "name": "Tao Li"
      },
      {
        "authorId": "46346053",
        "name": "Vivek Gupta"
      },
      {
        "authorId": "41016174",
        "name": "Maitrey Mehta"
      },
      {
        "authorId": "3052879",
        "name": "Vivek Srikumar"
      }
    ]
  },
  "220048375": {
    "paperId": "222dcbf5ee19fdfc9cfbd9c75af168a5c2122a4a",
    "externalIds": {
      "ACL": "2020.acl-main.713",
      "DBLP": "conf/acl/LinJHW20",
      "MAG": "3035229828",
      "DOI": "10.18653/v1/2020.acl-main.713",
      "CorpusId": 220048375
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Joint Neural Model for Information Extraction with Global Features",
    "abstract": "Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 28,
    "citationCount": 370,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2117032681",
        "name": "Ying Lin"
      },
      {
        "authorId": "2113323573",
        "name": "Heng Ji"
      },
      {
        "authorId": "143857288",
        "name": "Fei Huang"
      },
      {
        "authorId": "3008832",
        "name": "Lingfei Wu"
      }
    ]
  },
  "222306079": {
    "paperId": "518a0d1669369511c6b2f0687b68d65da3938e12",
    "externalIds": {
      "DBLP": "conf/emnlp/WangCZR20",
      "MAG": "3092898576",
      "ArXiv": "2010.06727",
      "ACL": "2020.emnlp-main.51",
      "DOI": "10.18653/v1/2020.emnlp-main.51",
      "CorpusId": 222306079
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Joint Constrained Learning for Event-Event Relation Extraction",
    "abstract": "Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them. Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations. Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations by converting these constraints into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process. We also present a promising case study showing the effectiveness of our approach in inducing event complexes on an external corpus.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 40,
    "citationCount": 104,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34269118",
        "name": "Haoyu Wang"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      },
      {
        "authorId": "49723569",
        "name": "Hongming Zhang"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "529375": {
    "paperId": "74f61af390292fc197659ae698429df4a2de62df",
    "externalIds": {
      "ACL": "P08-1090",
      "DBLP": "conf/acl/ChambersJ08",
      "MAG": "2151295812",
      "CorpusId": 529375
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Unsupervised Learning of Narrative Event Chains",
    "abstract": "Hand-coded scripts were used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge. We propose unsupervised induction of similar schemata called narrative event chains from raw newswire text. A narrative event chain is a partially ordered set of events related by a common protagonist. We describe a three step process to learning narrative event chains. The first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments. The second applies a temporal classifier to partially order the connected events. Finally, the third prunes and clusters self-contained chains from the space of events. We introduce two evaluations: the narrative cloze to evaluate event relatedness, and an order coherence task to evaluate narrative order. We show a 36% improvement over baseline for narrative prediction and 25% for temporal coherence.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2008,
    "referenceCount": 22,
    "citationCount": 641,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1729918",
        "name": "Nathanael Chambers"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      }
    ]
  },
  "222305618": {
    "paperId": "88119224c18e891c9dd550b2ced8ff5049be3849",
    "externalIds": {
      "MAG": "3093318556",
      "ACL": "2020.emnlp-main.119",
      "ArXiv": "2010.08525",
      "DBLP": "conf/emnlp/ZhangCWSR20",
      "DOI": "10.18653/v1/2020.emnlp-main.119",
      "CorpusId": 222305618
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Analogous Process Structure Induction for Sub-event Sequence Prediction",
    "abstract": "Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories. Thus, knowledge about a known process such as \"buying a car\" can be used in the context of a new but analogous process such as \"buying a house\". Nevertheless, most event understanding work in NLP is still at the ground level and does not consider abstraction. In this paper, we propose an Analogous Process Structure Induction APSI framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes. As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 28,
    "citationCount": 41,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49723569",
        "name": "Hongming Zhang"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      },
      {
        "authorId": "34269118",
        "name": "Haoyu Wang"
      },
      {
        "authorId": "1809614",
        "name": "Yangqiu Song"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "218581125": {
    "paperId": "b9485d1e2c66c3ae452ec4903c2a157caef4d2ed",
    "externalIds": {
      "MAG": "3023293091",
      "ACL": "2020.acl-main.678",
      "DBLP": "journals/corr/abs-2005-04304",
      "ArXiv": "2005.04304",
      "DOI": "10.18653/v1/2020.acl-main.678",
      "CorpusId": 218581125
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Temporal Common Sense Acquisition with Minimal Supervision",
    "abstract": "Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly. This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TacoLM, a temporal common sense language model. Our method is shown to give quality predictions of various dimensions of temporal common sense (on UDST and a newly collected dataset from RealNews). It also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the standard BERT. Thus, it will be an important component of temporal NLP.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 45,
    "citationCount": 88,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145360756",
        "name": "Ben Zhou"
      },
      {
        "authorId": "3333257",
        "name": "Qiang Ning"
      },
      {
        "authorId": "1783281",
        "name": "Daniel Khashabi"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "6719686": {
    "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
    "externalIds": {
      "MAG": "2604763608",
      "DBLP": "journals/corr/FinnAL17",
      "ArXiv": "1703.03400",
      "CorpusId": 6719686
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
    "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",
    "venue": "International Conference on Machine Learning",
    "year": 2017,
    "referenceCount": 52,
    "citationCount": 10690,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46881670",
        "name": "Chelsea Finn"
      },
      {
        "authorId": "1689992",
        "name": "P. Abbeel"
      },
      {
        "authorId": "1736651",
        "name": "S. Levine"
      }
    ]
  },
  "309759": {
    "paperId": "c269858a7bb34e8350f2442ccf37797856ae9bca",
    "externalIds": {
      "DBLP": "conf/nips/SnellSZ17",
      "ArXiv": "1703.05175",
      "MAG": "2950537964",
      "CorpusId": 309759
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Prototypical Networks for Few-shot Learning",
    "abstract": "We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.",
    "venue": "Neural Information Processing Systems",
    "year": 2017,
    "referenceCount": 38,
    "citationCount": 7297,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39770136",
        "name": "Jake Snell"
      },
      {
        "authorId": "1754860",
        "name": "Kevin Swersky"
      },
      {
        "authorId": "1804104",
        "name": "R. Zemel"
      }
    ]
  },
  "8909022": {
    "paperId": "be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6",
    "externalIds": {
      "MAG": "2432717477",
      "DBLP": "journals/corr/VinyalsBLKW16",
      "ArXiv": "1606.04080",
      "CorpusId": 8909022
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Matching Networks for One Shot Learning",
    "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.",
    "venue": "Neural Information Processing Systems",
    "year": 2016,
    "referenceCount": 30,
    "citationCount": 6778,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1689108",
        "name": "O. Vinyals"
      },
      {
        "authorId": "1723876",
        "name": "C. Blundell"
      },
      {
        "authorId": "2542999",
        "name": "T. Lillicrap"
      },
      {
        "authorId": "2645384",
        "name": "K. Kavukcuoglu"
      },
      {
        "authorId": "1688276",
        "name": "Daan Wierstra"
      }
    ]
  },
  "67413369": {
    "paperId": "29c887794eed2ca9462638ff853e6fe1ab91d5d8",
    "externalIds": {
      "MAG": "2753160622",
      "DBLP": "conf/iclr/RaviL17",
      "CorpusId": 67413369
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Optimization as a Model for Few-Shot Learning",
    "abstract": null,
    "venue": "International Conference on Learning Representations",
    "year": 2016,
    "referenceCount": 18,
    "citationCount": 3260,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49517463",
        "name": "S. Ravi"
      },
      {
        "authorId": "1777528",
        "name": "H. Larochelle"
      }
    ]
  },
  "2928017": {
    "paperId": "71683e224ab91617950956b5005ed0439a733a71",
    "externalIds": {
      "MAG": "2427497464",
      "DBLP": "conf/nips/AndrychowiczDCH16",
      "ArXiv": "1606.04474",
      "CorpusId": 2928017
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Learning to learn by gradient descent by gradient descent",
    "abstract": "The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.",
    "venue": "Neural Information Processing Systems",
    "year": 2016,
    "referenceCount": 44,
    "citationCount": 1905,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2206490",
        "name": "Marcin Andrychowicz"
      },
      {
        "authorId": "1715051",
        "name": "Misha Denil"
      },
      {
        "authorId": "2016840",
        "name": "Sergio Gomez Colmenarejo"
      },
      {
        "authorId": "3243579",
        "name": "Matthew W. Hoffman"
      },
      {
        "authorId": "144846367",
        "name": "David Pfau"
      },
      {
        "authorId": "1725157",
        "name": "T. Schaul"
      },
      {
        "authorId": "1737568",
        "name": "Nando de Freitas"
      }
    ]
  },
  "260464809": {
    "paperId": "a486e2839291111bb44fa1f07731ada123539f75",
    "externalIds": {
      "MAG": "2550821151",
      "DBLP": "journals/corr/JohnsonSLKWCTVW16",
      "ACL": "Q17-1024",
      "ArXiv": "1611.04558",
      "DOI": "10.1162/tacl_a_00065",
      "CorpusId": 260464809
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Google\u2019s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
    "abstract": "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT\u201914 benchmarks, a single multilingual model achieves comparable performance for English\u2192French and surpasses state-of-theart results for English\u2192German. Similarly, a single multilingual model surpasses state-of-the-art results for French\u2192English and German\u2192English on WMT\u201914 and WMT\u201915 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 35,
    "citationCount": 2001,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2109675545",
        "name": "Melvin Johnson"
      },
      {
        "authorId": "144927151",
        "name": "M. Schuster"
      },
      {
        "authorId": "2827616",
        "name": "Quoc V. Le"
      },
      {
        "authorId": "2048712",
        "name": "M. Krikun"
      },
      {
        "authorId": "48607963",
        "name": "Yonghui Wu"
      },
      {
        "authorId": "2545358",
        "name": "Z. Chen"
      },
      {
        "authorId": "144203200",
        "name": "Nikhil Thorat"
      },
      {
        "authorId": "1765169",
        "name": "F. Vi\u00e9gas"
      },
      {
        "authorId": "145233583",
        "name": "M. Wattenberg"
      },
      {
        "authorId": "2227182886",
        "name": "Gregory S. Corrado"
      },
      {
        "authorId": "48342565",
        "name": "Macduff Hughes"
      },
      {
        "authorId": "2056946837",
        "name": "Jeffrey Dean"
      }
    ]
  },
  "266840788": {
    "paperId": "4954c898d898197663d46e91ae7d83c3df7ac11f",
    "externalIds": {
      "MAG": "2939710050",
      "ArXiv": "1904.05862",
      "DBLP": "conf/interspeech/SchneiderBCA19",
      "DOI": "10.21437/interspeech.2019-1873",
      "CorpusId": 266840788
    },
    "publicationVenue": {
      "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
      "name": "Interspeech",
      "type": "conference",
      "alternate_names": [
        "Conf Int Speech Commun Assoc",
        "INTERSPEECH",
        "Conference of the International Speech Communication Association"
      ],
      "issn": "2308-457X",
      "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
      "alternate_urls": [
        "http://www.isca-speech.org/"
      ]
    },
    "title": "wav2vec: Unsupervised Pre-training for Speech Recognition",
    "abstract": "We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36% when only a few hours of transcribed data is available. Our approach achieves 2.43% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.",
    "venue": "Interspeech",
    "year": 2019,
    "referenceCount": 45,
    "citationCount": 484,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2278301828",
        "name": "Steffen Schneider"
      },
      {
        "authorId": "51428394",
        "name": "Alexei Baevski"
      },
      {
        "authorId": "2939803",
        "name": "R. Collobert"
      },
      {
        "authorId": "2325985",
        "name": "Michael Auli"
      }
    ]
  },
  "219966759": {
    "paperId": "49a049dc85e2380dde80501a984878341dd8efdf",
    "externalIds": {
      "ArXiv": "2006.11477",
      "DBLP": "conf/nips/BaevskiZMA20",
      "MAG": "3036601975",
      "CorpusId": 219966759
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
    "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 62,
    "citationCount": 4647,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51428394",
        "name": "Alexei Baevski"
      },
      {
        "authorId": "2110147709",
        "name": "Henry Zhou"
      },
      {
        "authorId": "40360972",
        "name": "Abdel-rahman Mohamed"
      },
      {
        "authorId": "2325985",
        "name": "Michael Auli"
      }
    ]
  },
  "33433997": {
    "paperId": "38be2a1a9aa093a3d1de074f04fab47147d01418",
    "externalIds": {
      "DBLP": "journals/phonetica/Ogden11",
      "MAG": "2058175044",
      "DOI": "10.1159/000328775",
      "CorpusId": 33433997
    },
    "publicationVenue": {
      "id": "0af01024-a480-491e-bf1a-5302e5dd138f",
      "name": "Phonetica: International Journal of Phonetic Science",
      "type": "journal",
      "alternate_names": [
        "Phonetica",
        "Phon Int J Phon Sci"
      ],
      "issn": "0031-8388",
      "url": "https://www.karger.com/Journal/Home/224275",
      "alternate_urls": [
        "http://www.karger.com/Journal/Home/224275"
      ]
    },
    "title": "An Introduction to English Phonetics",
    "abstract": "1. Introduction 2. Overview of the human speech mechanism 3. Representing speech 4. Voicing 5. Vowels 6. Approximants 7. Plosives 8. Nasals 9. Fricatives 10. Airstreams 11. Sounds and structures Glossary",
    "venue": "Phonetica: International Journal of Phonetic Science",
    "year": 2009,
    "referenceCount": 37,
    "citationCount": 140,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144059102",
        "name": "Richard Ogden"
      }
    ]
  },
  "60158090": {
    "paperId": "1f2585c4a0d9ef823d28f8e3a55cbdfb3466b5e9",
    "externalIds": {
      "MAG": "644670744",
      "CorpusId": 60158090
    },
    "publicationVenue": null,
    "title": "Analysing Conversation: An Introduction to Prosody",
    "abstract": "List of Tables and Figures Acknowledgements Preliminaries Pitch: Introduction Pitch: Intonation Pitch: Range and Register Time: Sound and Syllable Duration Speech Rate Speech Rhythm Pauses Loudness Voice Quality Outlook: Future Issues in Research on Prosody in Conversation Answers to Exercises Appendix: Transcription Conventions Glossary Notes Bibliography Index",
    "venue": "",
    "year": 2010,
    "referenceCount": 0,
    "citationCount": 85,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "12507724",
        "name": "B. S. Reed"
      }
    ]
  },
  "14486649": {
    "paperId": "10f992779c2601af8a33f53c813cb6342791873f",
    "externalIds": {
      "MAG": "2239141610",
      "DBLP": "journals/taffco/EybenSSSABDELNT16",
      "DOI": "10.1109/TAFFC.2015.2457417",
      "CorpusId": 14486649
    },
    "publicationVenue": {
      "id": "a88e2a6c-903f-42e1-8dfc-f2547e32020a",
      "name": "IEEE Transactions on Affective Computing",
      "type": "journal",
      "alternate_names": [
        "IEEE Trans Affect Comput"
      ],
      "issn": "1949-3045",
      "url": "https://ieeexplore.ieee.org/document/7160715/",
      "alternate_urls": [
        "http://ieeexplore.ieee.org/servlet/opac?punumber=5165369"
      ]
    },
    "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
    "abstract": "Work on voice sciences over recent decades has led to a proliferation of acoustic parameters that are used quite selectively and are not always extracted in a similar fashion. With many independent teams working in different research areas, shared standards become an essential safeguard to ensure compliance with state-of-the-art methods allowing appropriate comparison of results across studies and potential integration and combination of extraction and recognition systems. In this paper we propose a basic standard acoustic parameter set for various areas of automatic voice analysis, such as paralinguistic or clinical speech analysis. In contrast to a large brute-force parameter set, we present a minimalistic set of voice parameters here. These were selected based on a) their potential to index affective physiological changes in voice production, b) their proven value in former studies as well as their automatic extractability, and c) their theoretical significance. The set is intended to provide a common baseline for evaluation of future research and eliminate differences caused by varying parameter sets or even different implementations of the same parameters. Our implementation is publicly available with the openSMILE toolkit. Comparative evaluations of the proposed feature set and large baseline feature sets of INTERSPEECH challenges show a high performance of the proposed set in relation to its size.",
    "venue": "IEEE Transactions on Affective Computing",
    "year": 2016,
    "referenceCount": 73,
    "citationCount": 1363,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1751126",
        "name": "F. Eyben"
      },
      {
        "authorId": "2462740",
        "name": "K. Scherer"
      },
      {
        "authorId": "145411696",
        "name": "Bj\u00f6rn Schuller"
      },
      {
        "authorId": "144000012",
        "name": "J. Sundberg"
      },
      {
        "authorId": "1742930",
        "name": "E. Andr\u00e9"
      },
      {
        "authorId": "2106794",
        "name": "C. Busso"
      },
      {
        "authorId": "1713369",
        "name": "L. Devillers"
      },
      {
        "authorId": "145815422",
        "name": "J. Epps"
      },
      {
        "authorId": "2576655",
        "name": "Petri Laukka"
      },
      {
        "authorId": "145254843",
        "name": "Shrikanth S. Narayanan"
      },
      {
        "authorId": "1776021",
        "name": "K. Truong"
      }
    ]
  },
  "7389778": {
    "paperId": "f03df59311ecff3ab229f59058d091c123dfacb6",
    "externalIds": {
      "MAG": "2000633092",
      "DOI": "10.1080/23273798.2014.963130",
      "CorpusId": 7389778
    },
    "publicationVenue": null,
    "title": "Prosody in context: a review",
    "abstract": "Prosody conveys information about the linguistic context of an utterance at every level of linguistic organisation, from the word up to the discourse context. Acoustic correlates of prosody cue this rich contextual information, but interpreting prosodic cues in terms of the lexical, syntactic and discourse information they encode also requires recognising prosodic variation due to speaker, language variety, speech style and other properties of the situational context. This review reveals the complex interaction among contextual factors that influence the phonological form and phonetic expression of prosody. Empirical challenges in prosodic transcription are discussed along with production evidence that reveals striking variability in the phonological encoding of prosody and in its phonetic expression. The review points to the need for a model of prosody that is robust to contextually driven variation affecting the production and perception of prosodic form.",
    "venue": "",
    "year": 2015,
    "referenceCount": 263,
    "citationCount": 147,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145914754",
        "name": "Jennifer Cole"
      }
    ]
  },
  "2067922": {
    "paperId": "2be0e1198aabd6c36e8f0c52d3f8540c704a03ab",
    "externalIds": {
      "CorpusId": 2067922
    },
    "publicationVenue": null,
    "title": "SPEECH PROSODY \u2014 THEORIES , MODELS AND ANALYSIS",
    "abstract": "Modern study of speech prosody started almost as early as modern study of segmental aspect of speech (Cruttendon, 1997). Over the decades, many theories and models are proposed. While the diversity of approaches is a sign of creativity of the field, the situation could be confusing for readers who are new to the area. Even for seasoned researchers, if they have not given much thought to methodological issues, the key differences between the many approaches may not be immediately clear. This chapter offers an overview of the state of the art in prosody research mainly from a methodological perspective. I will first try to highlight the critical differences between the theories and models of prosody by outlining a way of classifying them along a number of dividing lines. I will then discuss a number of key issues in prosody analysis, with focus also mainly on methodological differences.",
    "venue": "",
    "year": 2014,
    "referenceCount": 108,
    "citationCount": 7,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2110289371",
        "name": "Yi Xu"
      }
    ]
  },
  "150384188": {
    "paperId": "14dac7d55fed0b20c43660a573c73702391bdf77",
    "externalIds": {
      "MAG": "2921495256",
      "DOI": "10.1017/9781316848265",
      "CorpusId": 150384188
    },
    "publicationVenue": null,
    "title": "Prosodic Patterns in English Conversation",
    "abstract": "Beyond words, spoken language involves prosody: intonation, loudness, timing, and the like. In conversation prosody is vital: it enables people to mark things as important or incidental, show interest in something or change the topic, be sympathetic or business-like, and so on. Without prosody, conversations would be just alternating short speeches: the human element would be lost. This book explains how speakers of American English use prosody to accomplish things in conversation. While native speakers do this without conscious awareness, that does not mean it is simple. Attempts to pin down the details have faced many challenges, but now, in a remarkable convergence, researchers in diverse traditions \u2013 experimental phonetics, formal phonology, conversation analysis, and signal processing \u2013 have independently begun using compatible styles of description. The shared core is the notion of prosodic construction. Prosodic constructions are recurring temporal patterns of prosodic features that express specific meanings and functions. These typically involve not only intonation but also energy, speaking rate, timing, and articulation properties, often with synchronized contributions by two participants. For example, consider one that is common in active listening. A listener can show interest and engagement by periodically nodding or saying uh-huh or the like, but this is not done at random. Example 1 illustrates.",
    "venue": "",
    "year": 2019,
    "referenceCount": 1,
    "citationCount": 68,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32987878",
        "name": "Nigel G. Ward"
      }
    ]
  },
  "13401254": {
    "paperId": "93ed6511a0ae5b13ccf445081ab829d415ca47df",
    "externalIds": {
      "ArXiv": "1703.04818",
      "MAG": "2599207826",
      "DBLP": "journals/corr/BuiRR17",
      "CorpusId": 13401254
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Neural Graph Machines: Learning Neural Networks Using Graphs",
    "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).",
    "venue": "arXiv.org",
    "year": 2017,
    "referenceCount": 25,
    "citationCount": 28,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "23519191",
        "name": "T. Bui"
      },
      {
        "authorId": "35014893",
        "name": "Sujith Ravi"
      },
      {
        "authorId": "2525389",
        "name": "Vivek Ramavajjala"
      }
    ]
  },
  "202888986": {
    "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
    "externalIds": {
      "MAG": "2996428491",
      "DBLP": "journals/corr/abs-1909-11942",
      "ArXiv": "1909.11942",
      "CorpusId": 202888986
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
    "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "referenceCount": 73,
    "citationCount": 5908,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2362534",
        "name": "Zhenzhong Lan"
      },
      {
        "authorId": "46221498",
        "name": "Mingda Chen"
      },
      {
        "authorId": "7685850",
        "name": "Sebastian Goodman"
      },
      {
        "authorId": "1700980",
        "name": "Kevin Gimpel"
      },
      {
        "authorId": "48267618",
        "name": "Piyush Sharma"
      },
      {
        "authorId": "1737285",
        "name": "Radu Soricut"
      }
    ]
  },
  "204838007": {
    "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
    "externalIds": {
      "MAG": "2981852735",
      "DBLP": "journals/corr/abs-1910-10683",
      "ArXiv": "1910.10683",
      "CorpusId": 204838007
    },
    "publicationVenue": {
      "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
      "name": "Journal of machine learning research",
      "type": "journal",
      "alternate_names": [
        "Journal of Machine Learning Research",
        "J mach learn res",
        "J Mach Learn Res"
      ],
      "issn": "1532-4435",
      "alternate_issns": [
        "1533-7928"
      ],
      "url": "http://www.ai.mit.edu/projects/jmlr/",
      "alternate_urls": [
        "http://jmlr.csail.mit.edu/",
        "http://www.jmlr.org/",
        "http://portal.acm.org/affiliated/jmlr"
      ]
    },
    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
    "venue": "Journal of machine learning research",
    "year": 2019,
    "referenceCount": 134,
    "citationCount": 16754,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2402716",
        "name": "Colin Raffel"
      },
      {
        "authorId": "1846258",
        "name": "Noam M. Shazeer"
      },
      {
        "authorId": "145625142",
        "name": "Adam Roberts"
      },
      {
        "authorId": "3844009",
        "name": "Katherine Lee"
      },
      {
        "authorId": "46617804",
        "name": "Sharan Narang"
      },
      {
        "authorId": "1380243217",
        "name": "Michael Matena"
      },
      {
        "authorId": "2389316",
        "name": "Yanqi Zhou"
      },
      {
        "authorId": "2157338362",
        "name": "Wei Li"
      },
      {
        "authorId": "35025299",
        "name": "Peter J. Liu"
      }
    ]
  },
  "352650": {
    "paperId": "a78273144520d57e150744cf75206e881e11cc5b",
    "externalIds": {
      "DBLP": "conf/icml/NgiamKKNLN11",
      "ArXiv": "2301.04856",
      "MAG": "2184188583",
      "DOI": "10.48550/arXiv.2301.04856",
      "CorpusId": 352650
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Multimodal Deep Learning",
    "abstract": "Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.",
    "venue": "International Conference on Machine Learning",
    "year": 2011,
    "referenceCount": 31,
    "citationCount": 3024,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2020608",
        "name": "Jiquan Ngiam"
      },
      {
        "authorId": "2556428",
        "name": "A. Khosla"
      },
      {
        "authorId": "1390603950",
        "name": "Mingyu Kim"
      },
      {
        "authorId": "145578392",
        "name": "Juhan Nam"
      },
      {
        "authorId": "1697141",
        "name": "Honglak Lee"
      },
      {
        "authorId": "34699434",
        "name": "A. Ng"
      }
    ]
  },
  "199453025": {
    "paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287",
    "externalIds": {
      "MAG": "2966715458",
      "DBLP": "journals/corr/abs-1908-02265",
      "ArXiv": "1908.02265",
      "CorpusId": 199453025
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
    "abstract": "We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.",
    "venue": "Neural Information Processing Systems",
    "year": 2019,
    "referenceCount": 51,
    "citationCount": 3280,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8553015",
        "name": "Jiasen Lu"
      },
      {
        "authorId": "1746610",
        "name": "Dhruv Batra"
      },
      {
        "authorId": "153432684",
        "name": "Devi Parikh"
      },
      {
        "authorId": "2297229",
        "name": "Stefan Lee"
      }
    ]
  },
  "208637516": {
    "paperId": "9915315f5cae822e98c94382ce3b0a6f9a7f8e5e",
    "externalIds": {
      "MAG": "2993313557",
      "DBLP": "conf/cvpr/LuGRPL20",
      "ArXiv": "1912.02315",
      "DOI": "10.1109/cvpr42600.2020.01045",
      "CorpusId": 208637516
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "12-in-1: Multi-Task Vision and Language Representation Learning",
    "abstract": "Much of vision-and-language research focuses on a small but diverse set of independent tasks and supporting datasets often studied in isolation; however, the visually-grounded language understanding skills required for success at these tasks overlap significantly. In this work, we investigate these relationships between vision-and-language tasks by developing a large-scale, multi-task model. Our approach culminates in a single model on 12 datasets from four broad categories of task including visual question answering, caption-based image retrieval, grounding referring expressions, and multimodal verification. Compared to independently trained single-task models, this represents a reduction from approximately 3 billion parameters to 270 million while simultaneously improving performance by 2.05 points on average across tasks. We use our multi-task framework to perform in-depth analysis of the effect of joint training diverse tasks. Further, we show that finetuning task-specific models from our single multi-task model can lead to further improvements, achieving performance at or above the state-of-the-art.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2019,
    "referenceCount": 66,
    "citationCount": 457,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8553015",
        "name": "Jiasen Lu"
      },
      {
        "authorId": "28554843",
        "name": "Vedanuj Goswami"
      },
      {
        "authorId": "34849128",
        "name": "Marcus Rohrbach"
      },
      {
        "authorId": "153432684",
        "name": "Devi Parikh"
      },
      {
        "authorId": "121944615",
        "name": "Stefan Lee"
      }
    ]
  },
  "201103729": {
    "paperId": "79c93274429d6355959f1e4374c2147bb81ea649",
    "externalIds": {
      "MAG": "2969862959",
      "DBLP": "conf/emnlp/TanB19",
      "ACL": "D19-1514",
      "ArXiv": "1908.07490",
      "DOI": "10.18653/v1/D19-1514",
      "CorpusId": 201103729
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
    "abstract": "Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 47,
    "citationCount": 2257,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3218666",
        "name": "Hao Hao Tan"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      }
    ]
  },
  "201317624": {
    "paperId": "4aa6298b606941a282d735fa3143da293199d2ca",
    "externalIds": {
      "ArXiv": "1908.08530",
      "MAG": "2995460200",
      "DBLP": "conf/iclr/SuZCLLWD20",
      "CorpusId": 201317624
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations",
    "abstract": "We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at \\url{this https URL}.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "referenceCount": 52,
    "citationCount": 1555,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145499378",
        "name": "Weijie Su"
      },
      {
        "authorId": "2578924",
        "name": "Xizhou Zhu"
      },
      {
        "authorId": "2112823372",
        "name": "Yue Cao"
      },
      {
        "authorId": "2156072370",
        "name": "Bin Li"
      },
      {
        "authorId": "152309485",
        "name": "Lewei Lu"
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei"
      },
      {
        "authorId": "3304536",
        "name": "Jifeng Dai"
      }
    ]
  },
  "102483628": {
    "paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a",
    "externalIds": {
      "DBLP": "journals/corr/abs-1904-01766",
      "ArXiv": "1904.01766",
      "MAG": "2981851019",
      "DOI": "10.1109/ICCV.2019.00756",
      "CorpusId": 102483628
    },
    "publicationVenue": {
      "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision",
      "type": "conference",
      "alternate_names": [
        "ICCV",
        "IEEE Int Conf Comput Vis",
        "ICCV Workshops",
        "ICCV Work"
      ],
      "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
    },
    "title": "VideoBERT: A Joint Model for Video and Language Representation Learning",
    "abstract": "Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2019,
    "referenceCount": 40,
    "citationCount": 1154,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1491624845",
        "name": "Chen Sun"
      },
      {
        "authorId": "49588480",
        "name": "Austin Myers"
      },
      {
        "authorId": "1856025",
        "name": "Carl Vondrick"
      },
      {
        "authorId": "1702318",
        "name": "K. Murphy"
      },
      {
        "authorId": "2462253",
        "name": "C. Schmid"
      }
    ]
  },
  "203594078": {
    "paperId": "025a0dc4a2a98742f1b410b6318a46de2c854b22",
    "externalIds": {
      "MAG": "2975357369",
      "CorpusId": 203594078
    },
    "publicationVenue": null,
    "title": "Learning Video Representations using Contrastive Bidirectional Transformer",
    "abstract": "This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. Our method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). We also show how to learn representations from sequences of visual features and sequences of words derived from ASR (automatic speech recognition), and show that such cross-modal training (when possible) helps even more.",
    "venue": "",
    "year": 2019,
    "referenceCount": 65,
    "citationCount": 262,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1491624845",
        "name": "Chen Sun"
      },
      {
        "authorId": "9943923",
        "name": "Fabien Baradel"
      },
      {
        "authorId": "1702318",
        "name": "K. Murphy"
      },
      {
        "authorId": "2462253",
        "name": "C. Schmid"
      }
    ]
  },
  "220249786": {
    "paperId": "10d11f0045dc7f217c7f01bc6cbb47929e9b8808",
    "externalIds": {
      "DBLP": "journals/corr/abs-2006-16228",
      "ArXiv": "2006.16228",
      "MAG": "3100177202",
      "CorpusId": 220249786
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Self-Supervised MultiModal Versatile Networks",
    "abstract": "Videos are a rich source of multi-modal supervision. In this work, we learn representations using self-supervision by leveraging three modalities naturally present in videos: vision, audio and language. To this end, we introduce the notion of a multimodal versatile network -- a network that can ingest multiple modalities and whose representations enable downstream tasks in multiple modalities. In particular, we explore how best to combine the modalities, such that fine-grained representations of audio and vision can be maintained, whilst also integrating text into a common embedding. Driven by versatility, we also introduce a novel process of deflation, so that the networks can be effortlessly applied to the visual data in the form of video or a static image. We demonstrate how such networks trained on large collections of unlabelled video data can be applied on video, video-text, image and audio tasks. Equipped with these representations, we obtain state-of-the-art performance on multiple challenging benchmarks including UCF101, HMDB51 and ESC-50 when compared to previous self-supervised work.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 101,
    "citationCount": 353,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2285263",
        "name": "Jean-Baptiste Alayrac"
      },
      {
        "authorId": "39257069",
        "name": "Adri\u00e0 Recasens"
      },
      {
        "authorId": "145721402",
        "name": "R. Schneider"
      },
      {
        "authorId": "2065840140",
        "name": "Relja Arandjelovi'c"
      },
      {
        "authorId": "16092809",
        "name": "Jason Ramapuram"
      },
      {
        "authorId": "3364908",
        "name": "J. Fauw"
      },
      {
        "authorId": "1466466597",
        "name": "Lucas Smaira"
      },
      {
        "authorId": "48373216",
        "name": "S. Dieleman"
      },
      {
        "authorId": "1688869",
        "name": "Andrew Zisserman"
      }
    ]
  },
  "1364249": {
    "paperId": "ca2858b2040724ae9f29ba601df12aae2e539596",
    "externalIds": {
      "MAG": "2153568660",
      "DBLP": "conf/acl/KleinM04",
      "ACL": "P04-1061",
      "DOI": "10.3115/1218955.1219016",
      "CorpusId": 1364249
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency",
    "abstract": "We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2004,
    "referenceCount": 31,
    "citationCount": 572,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "38666915",
        "name": "D. Klein"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      }
    ]
  },
  "12885015": {
    "paperId": "b360859eb746963767e554ae32cee1d1f3bcbc22",
    "externalIds": {
      "MAG": "2566475580",
      "ACL": "D16-1073",
      "DBLP": "conf/emnlp/JiangHT16",
      "DOI": "10.18653/v1/D16-1073",
      "CorpusId": 12885015
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Unsupervised Neural Dependency Parsing",
    "abstract": "Unsupervised dependency parsing aims to learn a dependency grammar from text annotated with only POS tags. Various features and inductive biases are often used to incorporate prior knowledge into learning. One useful type of prior information is that there exist correlations between the parameters of grammar rules involving different POS tags. Previous work employed manually designed features or special prior distributions to encode such information. In this paper, we propose a novel approach to unsupervised dependency parsing that uses a neural model to predict grammar rule probabilities based on distributed representation of POS tags. The distributed representation is automatically learned from data and captures the correlations between POS tags. Our experiments show that our approach outperforms previous approaches utilizing POS correlations and is competitive with recent state-of-the-art approaches on nine different languages. \u00a9 2016 Association for Computational Linguistics",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2016,
    "referenceCount": 21,
    "citationCount": 57,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "50262192",
        "name": "Yong Jiang"
      },
      {
        "authorId": "144836032",
        "name": "Wenjuan Han"
      },
      {
        "authorId": "40341553",
        "name": "Kewei Tu"
      }
    ]
  },
  "7324510": {
    "paperId": "0bf69a49c2baed67fa9a044daa24b9e199e73093",
    "externalIds": {
      "MAG": "2949549191",
      "ArXiv": "cmp-lg/9409010",
      "DBLP": "journals/corr/StolckeO94a",
      "DOI": "10.1007/3-540-58473-0_141",
      "CorpusId": 7324510
    },
    "publicationVenue": {
      "id": "ccb36bb0-2502-400e-b912-bc274eefc49b",
      "name": "International Conference on Graphics and Interaction",
      "type": "conference",
      "alternate_names": [
        "International Conference on Grammatical Inference",
        "Int Colloq Gramm Inference",
        "International Colloquium on Grammatical Inference",
        "Int Conf Gramm Inference",
        "ICGI",
        "Int Conf Graph Interact"
      ],
      "url": "http://eurise.univ-st-etienne.fr/gi/"
    },
    "title": "Inducing Probabilistic Grammars by Bayesian Model Merging",
    "abstract": null,
    "venue": "International Conference on Graphics and Interaction",
    "year": 1994,
    "referenceCount": 26,
    "citationCount": 281,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1762744",
        "name": "A. Stolcke"
      },
      {
        "authorId": "1808760",
        "name": "S. Omohundro"
      }
    ]
  },
  "6397366": {
    "paperId": "e55e5df3dd48e913d9c2a1704a5c1bf6d8e5ba1d",
    "externalIds": {
      "ACL": "D12-1121",
      "MAG": "2164910554",
      "DBLP": "conf/emnlp/TuH12",
      "CorpusId": 6397366
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars",
    "abstract": "We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2012,
    "referenceCount": 32,
    "citationCount": 37,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40341553",
        "name": "Kewei Tu"
      },
      {
        "authorId": "145513516",
        "name": "Vasant G Honavar"
      }
    ]
  },
  "28556787": {
    "paperId": "a1733d564c3283199aa23cf68fdf9e944f0c5359",
    "externalIds": {
      "DBLP": "conf/emnlp/CaiJT17",
      "ArXiv": "1708.01018",
      "MAG": "2744831323",
      "ACL": "D17-1171",
      "DOI": "10.18653/v1/D17-1171",
      "CorpusId": 28556787
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "CRF Autoencoder for Unsupervised Dependency Parsing",
    "abstract": "Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning generative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2017,
    "referenceCount": 19,
    "citationCount": 34,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "4442130",
        "name": "Jiong Cai"
      },
      {
        "authorId": "50262192",
        "name": "Yong Jiang"
      },
      {
        "authorId": "40341553",
        "name": "Kewei Tu"
      }
    ]
  },
  "102350997": {
    "paperId": "d7dc79050f17154e7cf57501cf6cab1b9c18f232",
    "externalIds": {
      "ACL": "N19-1114",
      "DBLP": "journals/corr/abs-1904-03746",
      "ArXiv": "1904.03746",
      "MAG": "2952203969",
      "DOI": "10.18653/v1/N19-1114",
      "CorpusId": 102350997
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Unsupervised Recurrent Neural Network Grammars",
    "abstract": "Recurrent neural network grammars (RNNG) are generative models of language which jointly model syntax and surface structure by incrementally generating a syntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs achieve strong language modeling and parsing performance, but require an annotated corpus of parse trees. In this work, we experiment with unsupervised learning of RNNGs. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser. On language modeling, unsupervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 95,
    "citationCount": 113,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "38367242",
        "name": "Yoon Kim"
      },
      {
        "authorId": "2531268",
        "name": "Alexander M. Rush"
      },
      {
        "authorId": "2109352263",
        "name": "Lei Yu"
      },
      {
        "authorId": "3376845",
        "name": "A. Kuncoro"
      },
      {
        "authorId": "1745899",
        "name": "Chris Dyer"
      },
      {
        "authorId": "94303026",
        "name": "G\u00e1bor Melis"
      }
    ]
  },
  "219302730": {
    "paperId": "919aa58480ac34f6b7ea433d5fa6368745aa572b",
    "externalIds": {
      "MAG": "2295951612",
      "DBLP": "journals/tacl/PassonneauC14",
      "ACL": "Q14-1025",
      "DOI": "10.1162/tacl_a_00185",
      "CorpusId": 219302730
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "The Benefits of a Model of Annotation",
    "abstract": "Standard agreement measures for interannotator reliability are neither necessary nor sufficient to ensure a high quality corpus. In a case study of word sense annotation, conventional methods for evaluating labels from trained annotators are contrasted with a probabilistic annotation model applied to crowdsourced data. The annotation model provides far more information, including a certainty measure for each gold standard label; the crowdsourced data was collected at less than half the cost of the conventional approach.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2014,
    "referenceCount": 36,
    "citationCount": 40,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1703046",
        "name": "R. Passonneau"
      },
      {
        "authorId": "2579894",
        "name": "Bob Carpenter"
      }
    ]
  },
  "58535743": {
    "paperId": "6806f6aba9170a4d6e8a6ebeb25c539fe756aebb",
    "externalIds": {
      "ACL": "Q18-1040",
      "DBLP": "journals/tacl/PaunCCHKP18",
      "MAG": "2899689163",
      "DOI": "10.1162/tacl_a_00040",
      "CorpusId": 58535743
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Comparing Bayesian Models of Annotation",
    "abstract": "The analysis of crowdsourced annotations in natural language processing is concerned with identifying (1) gold standard labels, (2) annotator accuracies and biases, and (3) item difficulties and error patterns. Traditionally, majority voting was used for 1, and coefficients of agreement for 2 and 3. Lately, model-based analysis of corpus annotations have proven better at all three tasks. But there has been relatively little work comparing them on the same datasets. This paper aims to fill this gap by analyzing six models of annotation, covering different approaches to annotator ability, item difficulty, and parameter pooling (tying) across annotators and items. We evaluate these models along four aspects: comparison to gold labels, predictive accuracy for new annotations, annotator characterization, and item difficulty, using four datasets with varying degrees of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 52,
    "citationCount": 85,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "11545402",
        "name": "Silviu Paun"
      },
      {
        "authorId": "2579894",
        "name": "Bob Carpenter"
      },
      {
        "authorId": "144010750",
        "name": "Jon Chamberlain"
      },
      {
        "authorId": "2022288",
        "name": "Dirk Hovy"
      },
      {
        "authorId": "2993548",
        "name": "Udo Kruschwitz"
      },
      {
        "authorId": "1678591",
        "name": "Massimo Poesio"
      }
    ]
  },
  "202538925": {
    "paperId": "8905e237469322e921d6a1bc7e8e0269a99b4fc1",
    "externalIds": {
      "MAG": "2971962903",
      "ACL": "D19-1101",
      "DBLP": "conf/emnlp/SimpsonG19",
      "DOI": "10.18653/v1/D19-1101",
      "CorpusId": 202538925
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "A Bayesian Approach for Sequence Tagging with Crowds",
    "abstract": "Current methods for sequence tagging, a core task in NLP, are data hungry, which motivates the use of crowdsourcing as a cheap way to obtain labelled data. However, annotators are often unreliable and current aggregation methods cannot capture common types of span annotation error. To address this, we propose a Bayesian method for aggregating sequence tags that reduces errors by modelling sequential dependencies between the annotations as well as the ground-truth labels. By taking a Bayesian approach, we account for uncertainty in the model due to both annotator errors and the lack of data for modelling annotators who complete few tasks. We evaluate our model on crowdsourced data for named entity recognition, information extraction and argument mining, showing that our sequential model outperforms the previous state of the art, and that Bayesian approaches outperform non-Bayesian alternatives. We also find that our approach can reduce crowdsourcing costs through more effective active learning, as it better captures uncertainty in the sequence labels when there are few annotations.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 36,
    "citationCount": 29,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145795026",
        "name": "Edwin Simpson"
      },
      {
        "authorId": "1730400",
        "name": "Iryna Gurevych"
      }
    ]
  },
  "23142740": {
    "paperId": "83d322146d7558124bf6052a16adff4fe22a9412",
    "externalIds": {
      "DBLP": "conf/ijcai/YinHZY17",
      "MAG": "2739753637",
      "DOI": "10.24963/ijcai.2017/184",
      "CorpusId": 23142740
    },
    "publicationVenue": {
      "id": "67f7f831-711a-43c8-8785-1e09005359b5",
      "name": "International Joint Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Int Jt Conf Artif Intell",
        "IJCAI"
      ],
      "url": "http://www.ijcai.org/"
    },
    "title": "Aggregating Crowd Wisdoms with Label-aware Autoencoders",
    "abstract": "Aggregating crowd wisdoms takes multiple labels from various sources and infers true labels for objects. Recent research work makes progress by learning source credibility from data and roughly form three kinds of modeling frameworks: weighted majority voting, trust propagation, and generative models. In this paper, we propose a novel framework named Label-Aware Autoencoders (LAA) to aggregate crowd wisdoms. LAA integrates a classifier and a reconstructor into a unified model to infer labels in an unsupervised manner. Analogizing classical autoencoders, we can regard the classifier as an encoder, the reconstructor as a decoder, and inferred labels as latent features. To the best of our knowledge, it is the first trial to combine label aggregation with autoencoders. We adopt networks to implement the classifier and the reconstructor which have the potential to automatically learn underlying patterns of source credibility. To further improve inference accuracy, we introduce object ambiguity and latent aspects into LAA. Experiments on three real-world datasets show that proposed models achieve impressive inference accuracy improvement over state-of-the-art models.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2017,
    "referenceCount": 35,
    "citationCount": 50,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2705335",
        "name": "Li'ang Yin"
      },
      {
        "authorId": "47180442",
        "name": "Jianhua Han"
      },
      {
        "authorId": "2108309275",
        "name": "Weinan Zhang"
      },
      {
        "authorId": "1811427",
        "name": "Yong Yu"
      }
    ]
  },
  "201103726": {
    "paperId": "d2a2be6ce932a0f1939f31cfff4d64ea3d76723d",
    "externalIds": {
      "ArXiv": "1908.07086",
      "MAG": "3005295611",
      "DBLP": "journals/corr/abs-1908-07086",
      "DOI": "10.1109/ICCV.2019.00971",
      "CorpusId": 201103726
    },
    "publicationVenue": {
      "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision",
      "type": "conference",
      "alternate_names": [
        "ICCV",
        "IEEE Int Conf Comput Vis",
        "ICCV Workshops",
        "ICCV Work"
      ],
      "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
    },
    "title": "Human Uncertainty Makes Classification More Robust",
    "abstract": "The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2019,
    "referenceCount": 55,
    "citationCount": 267,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "6672056",
        "name": "Joshua C. Peterson"
      },
      {
        "authorId": "4348165",
        "name": "Ruairidh M. Battleday"
      },
      {
        "authorId": "1799860",
        "name": "T. Griffiths"
      },
      {
        "authorId": "2192178",
        "name": "Olga Russakovsky"
      }
    ]
  },
  "19179988": {
    "paperId": "a464e45d17da3e60bffb87290fab46f89607b7be",
    "externalIds": {
      "DBLP": "journals/corr/abs-1709-01779",
      "ArXiv": "1709.01779",
      "MAG": "2953287512",
      "DOI": "10.1609/aaai.v32i1.11506",
      "CorpusId": 19179988
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Deep learning from crowds",
    "abstract": "\n \n Over the last few years, deep learning has revolutionized the field of machine learning by dramatically improving the state-of-the-art in various domains. However, as the size of supervised artificial neural networks grows, typically so does the need for larger labeled datasets. Recently, crowdsourcing has established itself as an efficient and cost-effective solution for labeling large sets of data in a scalable manner, but it often requires aggregating labels from multiple noisy contributors with different levels of expertise. In this paper, we address the problem of learning deep neural networks from crowds. We begin by describing an EM algorithm for jointly learning the parameters of the network and the reliabilities of the annotators. Then, a novel general-purpose crowd layer is proposed, which allows us to train deep neural networks end-to-end, directly from the noisy labels of multiple annotators, using only backpropagation. We empirically show that the proposed approach is able to internally capture the reliability and biases of different annotators and achieve new state-of-the-art results for various crowdsourced datasets across different settings, namely classification, regression and sequence labeling.\n \n",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2017,
    "referenceCount": 32,
    "citationCount": 238,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143791184",
        "name": "Filipe Rodrigues"
      },
      {
        "authorId": "143915473",
        "name": "Francisco C\u00e2mara Pereira"
      }
    ]
  },
  "215754220": {
    "paperId": "b57a537ae33092b7acf83dbd0470c6c03752fc79",
    "externalIds": {
      "DBLP": "conf/acl/SperberP20",
      "ACL": "2020.acl-main.661",
      "ArXiv": "2004.06358",
      "MAG": "3034625919",
      "DOI": "10.18653/v1/2020.acl-main.661",
      "CorpusId": 215754220
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Speech Translation and the End-to-End Promise: Taking Stock of Where We Are",
    "abstract": "Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives. Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 95,
    "citationCount": 97,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3011998",
        "name": "Matthias Sperber"
      },
      {
        "authorId": "1775245",
        "name": "M. Paulik"
      }
    ]
  },
  "266923": {
    "paperId": "c95a9010bb05d77e334e280fb6dd987aaf053098",
    "externalIds": {
      "MAG": "2582956876",
      "ArXiv": "1612.01744",
      "DBLP": "journals/corr/BerardPSB16",
      "CorpusId": 266923
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation",
    "abstract": "This paper proposes a first attempt to build an end-to-end speech-to-text translation \nsystem, which does not use source language text during learning or decoding. Relaxing the need \nfor source language transcription would drastically change the data collection methodology in \nspeech translation, especially in under-resourced scenarios.",
    "venue": "Neural Information Processing Systems",
    "year": 2016,
    "referenceCount": 16,
    "citationCount": 300,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3449364",
        "name": "Alexandre Berard"
      },
      {
        "authorId": "1721354",
        "name": "O. Pietquin"
      },
      {
        "authorId": "1890220",
        "name": "Christophe Servan"
      },
      {
        "authorId": "143823463",
        "name": "L. Besacier"
      }
    ]
  },
  "7857444": {
    "paperId": "dda047fd87610911c82778243f72f60d1c063383",
    "externalIds": {
      "MAG": "2605131327",
      "ArXiv": "1703.08581",
      "DBLP": "conf/interspeech/WeissCJWC17",
      "DOI": "10.21437/Interspeech.2017-503",
      "CorpusId": 7857444
    },
    "publicationVenue": {
      "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
      "name": "Interspeech",
      "type": "conference",
      "alternate_names": [
        "Conf Int Speech Commun Assoc",
        "INTERSPEECH",
        "Conference of the International Speech Communication Association"
      ],
      "issn": "2308-457X",
      "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
      "alternate_urls": [
        "http://www.isca-speech.org/"
      ]
    },
    "title": "Sequence-to-Sequence Models Can Directly Translate Foreign Speech",
    "abstract": "We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one language into text in another. The model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the ground truth source language transcription during training. We apply a slightly modified sequence-to-sequence with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex task, illustrating the power of attention-based models. A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation task, outperforming a cascade of independently trained sequence-to-sequence speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set. In addition, we find that making use of the training data in both languages by multi-task training sequence-to-sequence speech translation and recognition models with a shared encoder network can improve performance by a further 1.4 BLEU points.",
    "venue": "Interspeech",
    "year": 2017,
    "referenceCount": 34,
    "citationCount": 325,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39571582",
        "name": "Ron J. Weiss"
      },
      {
        "authorId": "2292403",
        "name": "J. Chorowski"
      },
      {
        "authorId": "3111912",
        "name": "N. Jaitly"
      },
      {
        "authorId": "48607963",
        "name": "Yonghui Wu"
      },
      {
        "authorId": "2545358",
        "name": "Z. Chen"
      }
    ]
  },
  "174800963": {
    "paperId": "999b4b988180b9168d4fd4bdceaf421cd7f17096",
    "externalIds": {
      "ACL": "N19-1202",
      "MAG": "2945700568",
      "DBLP": "conf/naacl/GangiCBNT19",
      "DOI": "10.18653/v1/N19-1202",
      "CorpusId": 174800963
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "MuST-C: a Multilingual Speech Translation Corpus",
    "abstract": "Current research on spoken language translation (SLT) has to confront with the scarcity of sizeable and publicly available training corpora. This problem hinders the adoption of neural end-to-end approaches, which represent the state of the art in the two parent tasks of SLT: automatic speech recognition and machine translation. To fill this gap, we created MuST-C, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for SLT from English into 8 languages. For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations. Together with a description of the corpus creation methodology (scalable to add new data and cover new languages), we provide an empirical verification of its quality and SLT results computed with a state-of-the-art approach on each language direction.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 35,
    "citationCount": 392,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39640268",
        "name": "Mattia Antonino Di Gangi"
      },
      {
        "authorId": "27086451",
        "name": "R. Cattoni"
      },
      {
        "authorId": "2486762",
        "name": "L. Bentivogli"
      },
      {
        "authorId": "2138026",
        "name": "Matteo Negri"
      },
      {
        "authorId": "145862931",
        "name": "Marco Turchi"
      }
    ]
  },
  "52160439": {
    "paperId": "19f66dd83abef074b04169ed448251b55429e6d9",
    "externalIds": {
      "DBLP": "journals/corr/abs-1809-01431",
      "ACL": "N19-1006",
      "MAG": "2950528074",
      "ArXiv": "1809.01431",
      "DOI": "10.18653/v1/N19-1006",
      "CorpusId": 52160439
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Pre-training on high-resource speech recognition improves low-resource speech-to-text translation",
    "abstract": "We present a simple approach to improve direct speech-to-text translation (ST) when the source language is low-resource: we pre-train the model on a high-resource automatic speech recognition (ASR) task, and then fine-tune its parameters for ST. We demonstrate that our approach is effective by pre-training on 300 hours of English ASR data to improve Spanish English ST from 10.8 to 20.2 BLEU when only 20 hours of Spanish-English ST training data are available. Through an ablation study, we find that the pre-trained encoder (acoustic model) accounts for most of the improvement, despite the fact that the shared language in these tasks is the target language text, not the source language audio. Applying this insight, we show that pre-training on ASR helps ST even when the ASR language differs from both source and target ST languages: pre-training on French ASR also improves Spanish-English ST. Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 52,
    "citationCount": 183,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3469333",
        "name": "Sameer Bansal"
      },
      {
        "authorId": "2308553",
        "name": "H. Kamper"
      },
      {
        "authorId": "2924113",
        "name": "Karen Livescu"
      },
      {
        "authorId": "144871732",
        "name": "Adam Lopez"
      },
      {
        "authorId": "1991315",
        "name": "S. Goldwater"
      }
    ]
  },
  "53222964": {
    "paperId": "b6222ad8acdf327368b45fb7fa5f4cf374d6da80",
    "externalIds": {
      "DBLP": "conf/icassp/JiaJMWCCALW19",
      "MAG": "2899716505",
      "ArXiv": "1811.02050",
      "DOI": "10.1109/ICASSP.2019.8683343",
      "CorpusId": 53222964
    },
    "publicationVenue": {
      "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
      "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "type": "conference",
      "alternate_names": [
        "Int Conf Acoust Speech Signal Process",
        "IEEE Int Conf Acoust Speech Signal Process",
        "ICASSP",
        "International Conference on Acoustics, Speech, and Signal Processing"
      ],
      "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
    },
    "title": "Leveraging Weakly Supervised Data to Improve End-to-end Speech-to-text Translation",
    "abstract": "End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2018,
    "referenceCount": 33,
    "citationCount": 155,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1691944",
        "name": "Ye Jia"
      },
      {
        "authorId": "145657834",
        "name": "Melvin Johnson"
      },
      {
        "authorId": "3153147",
        "name": "Wolfgang Macherey"
      },
      {
        "authorId": "39571582",
        "name": "Ron J. Weiss"
      },
      {
        "authorId": "145144022",
        "name": "Yuan Cao"
      },
      {
        "authorId": "145039780",
        "name": "Chung-Cheng Chiu"
      },
      {
        "authorId": "51893005",
        "name": "Naveen Ari"
      },
      {
        "authorId": "51923161",
        "name": "Stella Laurenzo"
      },
      {
        "authorId": "48607963",
        "name": "Yonghui Wu"
      }
    ]
  },
  "119111907": {
    "paperId": "5dab371fecc43904c0b785a50136d20cee43a99a",
    "externalIds": {
      "ACL": "Q19-1020",
      "MAG": "2949788262",
      "ArXiv": "1904.07209",
      "DBLP": "journals/tacl/SperberNNW19",
      "DOI": "10.1162/tacl_a_00270",
      "CorpusId": 119111907
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation",
    "abstract": "Speech translation has traditionally been approached through cascaded models consisting of a speech recognizer trained on a corpus of transcribed speech, and a machine translation system trained on parallel texts. Several recent works have shown the feasibility of collapsing the cascade into a single, direct model that can be trained in an end-to-end fashion on a corpus of translated speech. However, experiments are inconclusive on whether the cascade or the direct model is stronger, and have only been conducted under the unrealistic assumption that both are trained on equal amounts of data, ignoring other available speech recognition and machine translation corpora. In this paper, we demonstrate that direct speech translation models require more data to perform well than cascaded models, and although they allow including auxiliary data through multi-task training, they are poor at exploiting such data, putting them at a severe disadvantage. As a remedy, we propose the use of end- to-end trainable models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment. We show that such models naturally decompose into multi-task\u2013trainable recognition and translation tasks and propose an attention-passing technique that alleviates error propagation issues in a previous formulation of a model with two attention stages. Our proposed model outperforms all examined baselines and is able to exploit auxiliary training data much more effectively than direct attentional models.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 30,
    "citationCount": 97,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3011998",
        "name": "Matthias Sperber"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "2920247",
        "name": "J. Niehues"
      },
      {
        "authorId": "1724972",
        "name": "A. Waibel"
      }
    ]
  },
  "218971763": {
    "paperId": "15fb586993d1b269a72e61cfcebb69a56de6a3f1",
    "externalIds": {
      "MAG": "3028712640",
      "DBLP": "conf/acl/SaleskyB20",
      "ACL": "2020.acl-main.217",
      "ArXiv": "2005.13681",
      "DOI": "10.18653/v1/2020.acl-main.217",
      "CorpusId": 218971763
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Phone Features Improve Speech Translation",
    "abstract": "End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation. Their performance is often assumed to be superior, though in many conditions this is not yet the case. We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. Further, we introduce two methods to incorporate phone features into ST models. We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work \u2013 by up to 9 BLEU on our low-resource setting.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 29,
    "citationCount": 27,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3448427",
        "name": "Elizabeth Salesky"
      },
      {
        "authorId": "1690706",
        "name": "A. Black"
      }
    ]
  },
  "202714219": {
    "paperId": "d0a313a557bd43a7cacb3e5479cd7c491f7faa5c",
    "externalIds": {
      "MAG": "2973048981",
      "DBLP": "conf/interspeech/GangiNT19",
      "DOI": "10.21437/interspeech.2019-3045",
      "CorpusId": 202714219
    },
    "publicationVenue": {
      "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
      "name": "Interspeech",
      "type": "conference",
      "alternate_names": [
        "Conf Int Speech Commun Assoc",
        "INTERSPEECH",
        "Conference of the International Speech Communication Association"
      ],
      "issn": "2308-457X",
      "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
      "alternate_urls": [
        "http://www.isca-speech.org/"
      ]
    },
    "title": "Adapting Transformer to End-to-End Spoken Language Translation",
    "abstract": "Neural end-to-end architectures for sequence-to-sequence learning represent the state of the art in machine translation (MT) and speech recognition (ASR). Their use is also promising for end-to-end spoken language translation (SLT), which combines the main challenges of ASR and MT. Exploiting existing neural architectures, however, requires task-speci\ufb01c adaptations. A network that has obtained state-of-the-art results in MT with reduced training time is Transformer. However, its direct application to speech input is hindered by two limitations of the self-attention network on which it is based: quadratic memory complexity and no explicit modeling of short-range dependencies between input features. High memory complexity poses constraints to the size of models trainable with a GPU, while the inadequate modeling of local dependencies harms \ufb01nal translation quality. This paper presents an adaptation of Transformer to end-to-end SLT that consists in: i) downsampling the input with convolutional neural networks to make the training process feasible on GPUs, ii) modeling the bidimensional nature of a spectrogram, and iii) adding a distance penalty to the attention, so to bias it towards local context. SLT experiments on 8 language directions show that, with our adaptation, Transformer outperforms a strong RNN-based baseline with a signi\ufb01cant reduction in training time.",
    "venue": "Interspeech",
    "year": 2019,
    "referenceCount": 38,
    "citationCount": 121,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39640268",
        "name": "Mattia Antonino Di Gangi"
      },
      {
        "authorId": "2138026",
        "name": "Matteo Negri"
      },
      {
        "authorId": "145862931",
        "name": "Marco Turchi"
      }
    ]
  },
  "203610481": {
    "paperId": "8b231737e0048a400527d89aa56c712e8b9bc690",
    "externalIds": {
      "MAG": "2978606418",
      "DBLP": "conf/asru/InagumaDKW19",
      "ArXiv": "1910.00254",
      "DOI": "10.1109/ASRU46091.2019.9003832",
      "CorpusId": 203610481
    },
    "publicationVenue": {
      "id": "29014a7c-861f-43bd-b4d6-63edf4cd57ef",
      "name": "Automatic Speech Recognition & Understanding",
      "type": "conference",
      "alternate_names": [
        "IEEE Automatic Speech Recognition and Understanding Workshop",
        "Autom Speech Recognit  Underst",
        "ASRU",
        "IEEE Autom Speech Recognit Underst Workshop"
      ]
    },
    "title": "Multilingual End-to-End Speech Translation",
    "abstract": "In this paper, we propose a simple yet effective framework for multilingual end-to-end speech translation (ST), in which speech utterances in source languages are directly translated to the desired target languages with a universal sequence-to-sequence architecture. While multilingual models have shown to be useful for automatic speech recognition (ASR) and machine translation (MT), this is the first time they are applied to the end-to-end ST problem. We show the effectiveness of multilingual end-to-end ST in two scenarios: one-to-many and many-to-many translations with publicly available data. We experimentally confirm that multilingual end-to-end ST models significantly outperform bilingual ones in both scenarios. The generalization of multilingual training is also evaluated in a transfer learning scenario to a very low-resource language pair. All of our codes and the database are publicly available to encourage further research in this emergent multilingual ST topic11Available at https://github.com/espnet/espnet..",
    "venue": "Automatic Speech Recognition & Understanding",
    "year": 2019,
    "referenceCount": 61,
    "citationCount": 82,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49276525",
        "name": "H. Inaguma"
      },
      {
        "authorId": "1800354",
        "name": "Kevin Duh"
      },
      {
        "authorId": "1717105",
        "name": "Tatsuya Kawahara"
      },
      {
        "authorId": "1746678",
        "name": "Shinji Watanabe"
      }
    ]
  },
  "29476961": {
    "paperId": "0c40a8815f6e977d713cf253a636219b32c17559",
    "externalIds": {
      "ACL": "J08-1008",
      "DOI": "10.1162/coli.2008.34.1.137",
      "CorpusId": 29476961
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Last Words: On Becoming a Discipline",
    "abstract": null,
    "venue": "International Conference on Computational Logic",
    "year": null,
    "referenceCount": 0,
    "citationCount": 10,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145332819",
        "name": "Mark Steedman"
      }
    ]
  },
  "3074096": {
    "paperId": "2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb",
    "externalIds": {
      "CorpusId": 3074096
    },
    "publicationVenue": null,
    "title": "Deep Learning",
    "abstract": "Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.",
    "venue": "",
    "year": 2015,
    "referenceCount": 1,
    "citationCount": 40809,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1688882",
        "name": "Yann LeCun"
      },
      {
        "authorId": "1751762",
        "name": "Yoshua Bengio"
      },
      {
        "authorId": "1695689",
        "name": "Geoffrey E. Hinton"
      }
    ]
  },
  "3515219": {
    "paperId": "c2a7afbb5609a723f8eea91bfde4b02579b048d6",
    "externalIds": {
      "MAG": "2962824887",
      "DBLP": "conf/iclr/ArtetxeLAC18",
      "ArXiv": "1710.11041",
      "CorpusId": 3515219
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Unsupervised Neural Machine Translation",
    "abstract": "In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely remove the need of parallel data and propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation. Despite the simplicity of the approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation. The model can also profit from small parallel corpora, and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences, respectively. Our implementation is released as an open source project.",
    "venue": "International Conference on Learning Representations",
    "year": 2017,
    "referenceCount": 38,
    "citationCount": 755,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2347956",
        "name": "Mikel Artetxe"
      },
      {
        "authorId": "3255091",
        "name": "Gorka Labaka"
      },
      {
        "authorId": "1733049",
        "name": "Eneko Agirre"
      },
      {
        "authorId": "1979489",
        "name": "Kyunghyun Cho"
      }
    ]
  },
  "3518190": {
    "paperId": "e3d772986d176057aca2f5e3eb783da53b559134",
    "externalIds": {
      "MAG": "2949520888",
      "ArXiv": "1711.00043",
      "DBLP": "conf/iclr/LampleCDR18",
      "CorpusId": 3518190
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
    "abstract": "Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.",
    "venue": "International Conference on Learning Representations",
    "year": 2017,
    "referenceCount": 43,
    "citationCount": 1054,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1830914",
        "name": "Guillaume Lample"
      },
      {
        "authorId": "8905591",
        "name": "Ludovic Denoyer"
      },
      {
        "authorId": "1706809",
        "name": "Marc'Aurelio Ranzato"
      }
    ]
  },
  "201634541": {
    "paperId": "d802623e75b44b227acf33aec26a1607da2898b6",
    "externalIds": {
      "MAG": "2971254483",
      "ACL": "W19-5330",
      "DBLP": "conf/wmt/MarieSWCFUS19",
      "DOI": "10.18653/v1/W19-5330",
      "CorpusId": 201634541
    },
    "publicationVenue": {
      "id": "9aacb914-3edf-4e02-b8fe-5abf21c4d2ba",
      "name": "Conference on Machine Translation",
      "type": "conference",
      "alternate_names": [
        "WMT",
        "Conf Mach Transl"
      ]
    },
    "title": "NICT\u2019s Unsupervised Neural and Statistical Machine Translation Systems for the WMT19 News Translation Task",
    "abstract": "This paper presents the NICT\u2019s participation in the WMT19 unsupervised news translation task. We participated in the unsupervised translation direction: German-Czech. Our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (\u201cconstraint\u2019\u201d), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and English-Kazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions.",
    "venue": "Conference on Machine Translation",
    "year": 2019,
    "referenceCount": 28,
    "citationCount": 21,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2064068087",
        "name": "Benjamin Marie"
      },
      {
        "authorId": "122309052",
        "name": "Haipeng Sun"
      },
      {
        "authorId": "108085542",
        "name": "Rui Wang"
      },
      {
        "authorId": "2849740",
        "name": "Kehai Chen"
      },
      {
        "authorId": "46566611",
        "name": "Atsushi Fujita"
      },
      {
        "authorId": "1802277",
        "name": "M. Utiyama"
      },
      {
        "authorId": "1698363",
        "name": "E. Sumita"
      }
    ]
  },
  "222291274": {
    "paperId": "067906c924810e8ffc595ff8c9c4b0b2906cca85",
    "externalIds": {
      "DBLP": "journals/corr/abs-2010-05122",
      "ACL": "2020.wmt-1.22",
      "MAG": "3092280534",
      "ArXiv": "2010.05122",
      "CorpusId": 222291274
    },
    "publicationVenue": {
      "id": "9aacb914-3edf-4e02-b8fe-5abf21c4d2ba",
      "name": "Conference on Machine Translation",
      "type": "conference",
      "alternate_names": [
        "WMT",
        "Conf Mach Transl"
      ]
    },
    "title": "SJTU-NICT\u2019s Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task",
    "abstract": "In this paper, we introduced our joint team SJTU-NICT \u2018s participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions.",
    "venue": "Conference on Machine Translation",
    "year": 2020,
    "referenceCount": 70,
    "citationCount": 14,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "30658665",
        "name": "Z. Li"
      },
      {
        "authorId": "47941144",
        "name": "Hai Zhao"
      },
      {
        "authorId": "108085542",
        "name": "Rui Wang"
      },
      {
        "authorId": "2849740",
        "name": "Kehai Chen"
      },
      {
        "authorId": "1802277",
        "name": "M. Utiyama"
      },
      {
        "authorId": "1698363",
        "name": "E. Sumita"
      }
    ]
  },
  "3432876": {
    "paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
    "externalIds": {
      "DBLP": "journals/corr/WilliamsNB17",
      "MAG": "2963846996",
      "ArXiv": "1704.05426",
      "ACL": "N18-1101",
      "DOI": "10.18653/v1/N18-1101",
      "CorpusId": 3432876
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 56,
    "citationCount": 4141,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "81840293",
        "name": "Adina Williams"
      },
      {
        "authorId": "10666396",
        "name": "Nikita Nangia"
      },
      {
        "authorId": "3644767",
        "name": "Samuel R. Bowman"
      }
    ]
  },
  "19435386": {
    "paperId": "a9e28863c7fb963b40a379c5a4e0da00eb031933",
    "externalIds": {
      "DBLP": "conf/acl/SuhrLYA17",
      "ACL": "P17-2034",
      "MAG": "2741631785",
      "DOI": "10.18653/v1/P17-2034",
      "CorpusId": 19435386
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Corpus of Natural Language for Visual Reasoning",
    "abstract": "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 29,
    "citationCount": 225,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32849969",
        "name": "Alane Suhr"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "2053174592",
        "name": "James Yeh"
      },
      {
        "authorId": "3167681",
        "name": "Yoav Artzi"
      }
    ]
  },
  "53178856": {
    "paperId": "cf336d272a30d6ad6141db67faa64deb8791cd61",
    "externalIds": {
      "MAG": "2963530300",
      "DBLP": "conf/acl/SuhrZZZBA19",
      "ACL": "P19-1644",
      "ArXiv": "1811.00491",
      "DOI": "10.18653/v1/P19-1644",
      "CorpusId": 53178856
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Corpus for Reasoning about Natural Language Grounded in Photographs",
    "abstract": "We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 78,
    "citationCount": 538,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32849969",
        "name": "Alane Suhr"
      },
      {
        "authorId": "49219517",
        "name": "Stephanie Zhou"
      },
      {
        "authorId": "83384205",
        "name": "Iris Zhang"
      },
      {
        "authorId": "1471618793",
        "name": "Huajun Bai"
      },
      {
        "authorId": "3167681",
        "name": "Yoav Artzi"
      }
    ]
  },
  "202764530": {
    "paperId": "be2ce82730600d9b2eb2df9f2762f9d4beb6222d",
    "externalIds": {
      "DBLP": "conf/emnlp/SuhrYSYKMZA19",
      "ACL": "D19-1218",
      "ArXiv": "1910.03655",
      "MAG": "2979931343",
      "DOI": "10.18653/v1/D19-1218",
      "CorpusId": 202764530
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Executing Instructions in Situated Collaborative Interactions",
    "abstract": "We study a collaborative scenario where a user not only instructs a system to complete tasks, but also acts alongside it. This allows the user to adapt to the system abilities by changing their language or deciding to simply accomplish some tasks themselves, and requires the system to effectively recover from errors as the user strategically assigns it new goals. We build a game environment to study this scenario, and learn to map user instructions to system actions. We introduce a learning approach focused on recovery from cascading errors between instructions, and modeling methods to explicitly reason about instructions with multiple goals. We evaluate with a new evaluation protocol using recorded interactions and online games with human users, and observe how users adapt to the system abilities.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 60,
    "citationCount": 84,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32849969",
        "name": "Alane Suhr"
      },
      {
        "authorId": "12693261",
        "name": "Claudia Yan"
      },
      {
        "authorId": "1379733243",
        "name": "Jack Schluger"
      },
      {
        "authorId": "2112268019",
        "name": "Stanley Yu"
      },
      {
        "authorId": "2138322941",
        "name": "Hadi Khader"
      },
      {
        "authorId": "1379733217",
        "name": "Marwa Mouallem"
      },
      {
        "authorId": "83384205",
        "name": "Iris Zhang"
      },
      {
        "authorId": "3167681",
        "name": "Yoav Artzi"
      }
    ]
  },
  "52057510": {
    "paperId": "39e734da43eb8c72e9549b42e96760545036f8e5",
    "externalIds": {
      "DBLP": "journals/corr/abs-1808-07036",
      "ACL": "D18-1241",
      "MAG": "2951831170",
      "ArXiv": "1808.07036",
      "DOI": "10.18653/v1/D18-1241",
      "CorpusId": 52057510
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "QuAC: Question Answering in Context",
    "abstract": "We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 31,
    "citationCount": 773,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2890423",
        "name": "Eunsol Choi"
      },
      {
        "authorId": "144533687",
        "name": "He He"
      },
      {
        "authorId": "2136562",
        "name": "Mohit Iyyer"
      },
      {
        "authorId": "2064210",
        "name": "Mark Yatskar"
      },
      {
        "authorId": "144105277",
        "name": "Wen-tau Yih"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      },
      {
        "authorId": "145419642",
        "name": "Percy Liang"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ]
  },
  "55268987": {
    "paperId": "349b576d6919ebf68617c81378bab9a90c7389b5",
    "externalIds": {
      "MAG": "2398112150",
      "DOI": "10.1111/J.1540-6261.2010.01625.X",
      "CorpusId": 55268987
    },
    "publicationVenue": null,
    "title": "When is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks",
    "abstract": "Previous research uses negative word counts to measure the tone of a text. We show that word lists developed for other disciplines misclassify common words in financial text. In a large sample of 10 Ks during 1994 to 2008, almost three-fourths of the words identified as negative by the widely used Harvard Dictionary are words typically not considered negative in financial contexts. We develop an alternative negative word list, along with five other word lists, that better reflect tone in financial text. We link the word lists to 10 K filing returns, trading volume, return volatility, fraud, material weakness, and unexpected earnings.",
    "venue": "",
    "year": 2010,
    "referenceCount": 56,
    "citationCount": 4067,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Business",
        "source": "external"
      },
      {
        "category": "Economics",
        "source": "s2-fos-model"
      },
      {
        "category": "Business",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46173917",
        "name": "Tim Loughran"
      },
      {
        "authorId": "35005086",
        "name": "B. Mcdonald"
      }
    ]
  },
  "14727513": {
    "paperId": "e498784edf2c02fe0b228479f88120f08b381cb6",
    "externalIds": {
      "MAG": "2171468534",
      "DBLP": "journals/corr/abs-1010-3003",
      "ArXiv": "1010.3003",
      "DOI": "10.1016/j.jocs.2010.12.007",
      "CorpusId": 14727513
    },
    "publicationVenue": {
      "id": "449b5e8b-cda2-4a8e-b27b-9b7917ded9eb",
      "name": "Journal of Computer Science",
      "type": "journal",
      "alternate_names": [
        "Journal of Computational Science",
        "J Comput Sci"
      ],
      "issn": "1549-3636",
      "alternate_issns": [
        "1877-7503",
        "2362-0110"
      ],
      "url": "https://thescipub.com/journals/jcs/",
      "alternate_urls": [
        "http://ansinet.org/sciencepub/c4p.php?j_id=jcs",
        "https://www.journals.elsevier.com/journal-of-computational-science",
        "http://journals.sjp.ac.lk/index.php/jcs",
        "http://www.sciencedirect.com/science/journal/18777503"
      ]
    },
    "title": "Twitter mood predicts the stock market",
    "abstract": null,
    "venue": "Journal of Computer Science",
    "year": 2010,
    "referenceCount": 75,
    "citationCount": 4923,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Economics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1756468",
        "name": "J. Bollen"
      },
      {
        "authorId": "2053590600",
        "name": "Huina Mao"
      },
      {
        "authorId": "145681778",
        "name": "Xiao-Jun Zeng"
      }
    ]
  },
  "264221114": {
    "paperId": "cabb0a468af8184e0e930841435b65679b580521",
    "externalIds": {
      "MAG": "3098201315",
      "ArXiv": "1809.05356",
      "DBLP": "journals/corr/abs-1809-05356",
      "DOI": "10.1109/WI.2018.00-97",
      "CorpusId": 264221114
    },
    "publicationVenue": {
      "id": "801703f5-a310-442a-a58a-41bb05ae1f68",
      "name": "International Conference on Wirtschaftsinformatik",
      "type": "conference",
      "alternate_names": [
        "WI",
        "Int Conf Wirtsch",
        "Web Intell",
        "IEEE/WIC/ACM Int Conf Web Intell",
        "IEEE/WIC/ACM International Conference on Web Intelligence",
        "Web Intelligence"
      ],
      "url": "https://aisel.aisnet.org/wi/",
      "alternate_urls": [
        "http://www.wikicfp.com/cfp/program?id=3061"
      ]
    },
    "title": "Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting",
    "abstract": "Numerals that contain much information in financial documents are crucial for financial decision making. They play different roles in financial analysis processes. This paper is aimed at understanding the meanings of numerals in financial tweets for fine-grained crowd-based forecasting. We propose a taxonomy that classifies the numerals in financial tweets into 7 categories, and further extend some of these categories into several subcategories. Neural network-based models with word and character-level encoders are proposed for 7-way classification and 17-way classification. We perform backtest to confirm the effectiveness of the numeric opinions made by the crowd. This work is the first attempt to understand numerals in financial social media data, and we provide the first comparison of fine-grained opinion of individual investors and analysts based on their forecast price. The numeral corpus used in our experiments, called FinNum 1.0, is available for research purposes.",
    "venue": "International Conference on Wirtschaftsinformatik",
    "year": 2018,
    "referenceCount": 22,
    "citationCount": 42,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      },
      {
        "category": "Business",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2109523457",
        "name": "Chung-Chi Chen"
      },
      {
        "authorId": "152354730",
        "name": "Hen-Hsen Huang"
      },
      {
        "authorId": "3448762",
        "name": "Yow-Ting Shiue"
      },
      {
        "authorId": "2237850759",
        "name": "Hsin-Hsi Chen"
      }
    ]
  },
  "174801540": {
    "paperId": "f0c3de5686d859ad60bb872e150cf8b598f92c9a",
    "externalIds": {
      "ArXiv": "1906.02868",
      "ACL": "P19-1047",
      "DBLP": "journals/corr/abs-1906-02868",
      "MAG": "2948415302",
      "DOI": "10.18653/v1/P19-1047",
      "CorpusId": 174801540
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Modeling Financial Analysts\u2019 Decision Making via the Pragmatics and Semantics of Earnings Calls",
    "abstract": "Every fiscal quarter, companies hold earnings calls in which company executives respond to questions from analysts. After these calls, analysts often change their price target recommendations, which are used in equity re- search reports to help investors make deci- sions. In this paper, we examine analysts\u2019 decision making behavior as it pertains to the language content of earnings calls. We identify a set of 20 pragmatic features of analysts\u2019 questions which we correlate with analysts\u2019 pre-call investor recommendations. We also analyze the degree to which semantic and pragmatic features from an earnings call complement market data in predicting analysts\u2019 post-call changes in price targets. Our results show that earnings calls are moderately predictive of analysts\u2019 decisions even though these decisions are influenced by a number of other factors including private communication with company executives and market conditions. A breakdown of model errors indicates disparate performance on calls from different market sectors.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 55,
    "citationCount": 49,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Business",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Economics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145137850",
        "name": "Katherine A. Keith"
      },
      {
        "authorId": "1690152",
        "name": "Amanda Stent"
      }
    ]
  },
  "234787011": {
    "paperId": "b68bb98aa5075934d92b1d34387be31e2449faf8",
    "externalIds": {
      "DBLP": "series/sbcs/ChenHC21",
      "DOI": "10.1007/978-981-16-2881-8",
      "CorpusId": 234787011
    },
    "publicationVenue": null,
    "title": "From Opinion Mining to Financial Argument Mining",
    "abstract": null,
    "venue": "Springer Briefs in Computer Science",
    "year": 2021,
    "referenceCount": 0,
    "citationCount": 28,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Business",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Economics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2109523457",
        "name": "Chung-Chi Chen"
      },
      {
        "authorId": "152354730",
        "name": "Hen-Hsen Huang"
      },
      {
        "authorId": "153924342",
        "name": "Hsin-Hsi Chen"
      }
    ]
  },
  "222272210": {
    "paperId": "c845494445f3bfa01d8245a4759b144e27aa3788",
    "externalIds": {
      "DBLP": "journals/corr/abs-2010-04389",
      "ArXiv": "2010.04389",
      "MAG": "3092288641",
      "DOI": "10.1145/3512467",
      "CorpusId": 222272210
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "A Survey of Knowledge-enhanced Text Generation",
    "abstract": "The goal of text-to-text generation is to make machines express like a human in many applications such as conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural language processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and knowledge graph into the text generation system. This research topic is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on this topic over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.",
    "venue": "ACM Computing Surveys",
    "year": 2020,
    "referenceCount": 237,
    "citationCount": 231,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "38767143",
        "name": "W. Yu"
      },
      {
        "authorId": "70461341",
        "name": "Wenhao Yu"
      },
      {
        "authorId": "8652308",
        "name": "Chenguang Zhu"
      },
      {
        "authorId": "1993150474",
        "name": "Zaitang Li"
      },
      {
        "authorId": "2749311",
        "name": "Zhiting Hu"
      },
      {
        "authorId": "1786863",
        "name": "Qingyun Wang"
      },
      {
        "authorId": "2113323573",
        "name": "Heng Ji"
      },
      {
        "authorId": "1470716407",
        "name": "Meng Jiang"
      }
    ]
  },
  "8174613": {
    "paperId": "02534853626c18c9a097c2712f1ddf3613257d35",
    "externalIds": {
      "MAG": "2304113845",
      "ArXiv": "1603.06393",
      "DBLP": "conf/acl/GuLLL16",
      "ACL": "P16-1154",
      "DOI": "10.18653/v1/P16-1154",
      "CorpusId": 8174613
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
    "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 21,
    "citationCount": 1505,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3016273",
        "name": "Jiatao Gu"
      },
      {
        "authorId": "11955007",
        "name": "Zhengdong Lu"
      },
      {
        "authorId": "49404233",
        "name": "Hang Li"
      },
      {
        "authorId": "2052674293",
        "name": "V. Li"
      }
    ]
  },
  "9514751": {
    "paperId": "d95069ee71bc2c3e171832872f437caa2e53432f",
    "externalIds": {
      "MAG": "2521114121",
      "DBLP": "conf/aaai/XingWWLHZM17",
      "DOI": "10.1609/aaai.v31i1.10981",
      "CorpusId": 9514751
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Topic Aware Neural Response Generation",
    "abstract": "\n \n We consider incorporating topic information into a sequence-to-sequence framework to generate informative and interesting responses for chatbots. To this end, we propose a topic aware sequence-to-sequence (TA-Seq2Seq) model. The model utilizes topics to simulate prior human knowledge that guides them to form informative and interesting responses in conversation, and leverages topic information in generation by a joint attention mechanism and a biased generation probability. The joint attention mechanism summarizes the hidden vectors of an input message as context vectors by message attention and synthesizes topic vectors by topic attention from the topic words of the message obtained from a pre-trained LDA model, with these vectors jointly affecting the generation of words in decoding. To increase the possibility of topic words appearing in responses, the model modifies the generation probability of topic words by adding an extra probability item to bias the overall distribution. Empirical studies on both automatic evaluation metrics and human annotations show that TA-Seq2Seq can generate more informative and interesting responses, significantly outperforming state-of-the-art response generation models.\n \n",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2016,
    "referenceCount": 32,
    "citationCount": 472,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1399291043",
        "name": "Chen Xing"
      },
      {
        "authorId": "145717888",
        "name": "Wei Wu"
      },
      {
        "authorId": "49176273",
        "name": "Yu Wu"
      },
      {
        "authorId": "2146651412",
        "name": "Jie Liu"
      },
      {
        "authorId": "9221107",
        "name": "Yalou Huang"
      },
      {
        "authorId": "143849609",
        "name": "M. Zhou"
      },
      {
        "authorId": "1712167",
        "name": "Wei-Ying Ma"
      }
    ]
  },
  "20981275": {
    "paperId": "2a215755d7548ffc82079ce734c4ac60b62f6f56",
    "externalIds": {
      "ArXiv": "1703.00955",
      "DBLP": "conf/icml/HuYLSX17",
      "MAG": "2953029759",
      "CorpusId": 20981275
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Toward Controlled Generation of Text",
    "abstract": "Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.",
    "venue": "International Conference on Machine Learning",
    "year": 2017,
    "referenceCount": 41,
    "citationCount": 955,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2749311",
        "name": "Zhiting Hu"
      },
      {
        "authorId": "8387085",
        "name": "Zichao Yang"
      },
      {
        "authorId": "40250403",
        "name": "Xiaodan Liang"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      },
      {
        "authorId": "143977260",
        "name": "E. Xing"
      }
    ]
  },
  "2024574": {
    "paperId": "7b221cce8fdbc1105956b27c938730fce8c1fc10",
    "externalIds": {
      "DBLP": "conf/aaai/ZhouHZZL18",
      "MAG": "2605133118",
      "ArXiv": "1704.01074",
      "DOI": "10.1609/aaai.v32i1.11325",
      "CorpusId": 2024574
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory",
    "abstract": "\n \n Perception and expression of emotion are key factors to the success of dialogue systems or conversational agents. However, this problem has not been studied in large-scale conversation generation so far. In this paper, we propose Emotional Chatting Machine (ECM) that can generate appropriate responses not only in content (relevant and grammatical) but also in emotion (emotionally consistent). To the best of our knowledge, this is the first work that addresses the emotion factor in large-scale conversation generation. ECM addresses the factor using three new mechanisms that respectively (1) models the high-level abstraction of emotion expressions by embedding emotion categories, (2) captures the change of implicit internal emotion states, and (3) uses explicit emotion expressions with an external emotion vocabulary. Experiments show that the proposed model can generate responses appropriate not only in content but also in emotion.\n \n",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2017,
    "referenceCount": 61,
    "citationCount": 698,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144751955",
        "name": "Hao Zhou"
      },
      {
        "authorId": "1730108",
        "name": "Minlie Huang"
      },
      {
        "authorId": "50615630",
        "name": "Tianyang Zhang"
      },
      {
        "authorId": "145213540",
        "name": "Xiaoyan Zhu"
      },
      {
        "authorId": "2149124481",
        "name": "Bing-Qian Liu"
      }
    ]
  },
  "7672408": {
    "paperId": "3580d8a5e7584e98d547ebfed900749d347f6714",
    "externalIds": {
      "DBLP": "conf/aaai/LiuWSCS18",
      "ArXiv": "1711.09724",
      "MAG": "2769637628",
      "DOI": "10.1609/aaai.v32i1.11925",
      "CorpusId": 7672408
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Table-to-text Generation by Structure-aware Seq2seq Learning",
    "abstract": "\n \n Table-to-text generation aims to generate a description for a factual table which can be viewed as a set of field-value records. To encode both the content and the structure of a table, we propose a novel structure-aware seq2seq architecture which consists of field-gating encoder and description generator with dual attention. In the encoding phase, we update the cell memory of the LSTM unit by a field gate and its corresponding field value in order to incorporate field information into table representation. In the decoding phase, dual attention mechanism which contains word level attention and field level attention is proposed to model the semantic relevance between the generated description and the table. We conduct experiments on the WIKIBIO dataset which contains over 700k biographies and corresponding infoboxes from Wikipedia. The attention visualizations and case studies show that our model is capable of generating coherent and informative descriptions based on the comprehensive understanding of both the content and the structure of a table. Automatic evaluations also show our model outperforms the baselines by a great margin. Code for this work is available on https://github.com/tyliupku/wiki2bio.\n \n",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2017,
    "referenceCount": 28,
    "citationCount": 257,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1500520681",
        "name": "Tianyu Liu"
      },
      {
        "authorId": "94053409",
        "name": "Kexiang Wang"
      },
      {
        "authorId": "39058310",
        "name": "Lei Sha"
      },
      {
        "authorId": "39488576",
        "name": "Baobao Chang"
      },
      {
        "authorId": "3335836",
        "name": "Zhifang Sui"
      }
    ]
  },
  "23892230": {
    "paperId": "13395213d47f78672ab4e81573f2b0fa0cfc8c6d",
    "externalIds": {
      "ACL": "D17-1239",
      "ArXiv": "1707.08052",
      "DBLP": "journals/corr/WisemanSR17",
      "MAG": "2739046565",
      "DOI": "10.18653/v1/D17-1239",
      "CorpusId": 23892230
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Challenges in Data-to-Document Generation",
    "abstract": "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2017,
    "referenceCount": 56,
    "citationCount": 564,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2844243",
        "name": "Sam Wiseman"
      },
      {
        "authorId": "1692491",
        "name": "Stuart M. Shieber"
      },
      {
        "authorId": "2531268",
        "name": "Alexander M. Rush"
      }
    ]
  },
  "51608183": {
    "paperId": "05cf65bea06b26d11a6324113bb4d6219e495a7b",
    "externalIds": {
      "DBLP": "conf/ijcai/ZhouYHZXZ18",
      "MAG": "2807873315",
      "DOI": "10.24963/ijcai.2018/643",
      "CorpusId": 51608183
    },
    "publicationVenue": {
      "id": "67f7f831-711a-43c8-8785-1e09005359b5",
      "name": "International Joint Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Int Jt Conf Artif Intell",
        "IJCAI"
      ],
      "url": "http://www.ijcai.org/"
    },
    "title": "Commonsense Knowledge Aware Conversation Generation with Graph Attention",
    "abstract": "Commonsense knowledge is vital to many natural language processing tasks. In this paper, we present a novel open-domain conversation generation model to demonstrate how large-scale commonsense knowledge can facilitate language understanding and generation. Given a user post, the model retrieves relevant knowledge graphs from a knowledge base and then encodes the graphs with a static graph attention mechanism, which augments the semantic information of the post and thus supports better understanding of the post. Then, during word generation, the model attentively reads the retrieved knowledge graphs and the knowledge triples within each graph to facilitate better generation through a dynamic graph attention mechanism. This is the first attempt that uses large-scale commonsense knowledge in conversation generation. Furthermore, unlike existing models that use knowledge triples (entities) separately and independently, our model treats each knowledge graph as a whole, which encodes more structured, connected semantic information in the graphs.\u00a0Experiments show that the proposed model can generate more appropriate and informative responses than state-of-the-art baselines.\u00a0",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2018,
    "referenceCount": 31,
    "citationCount": 487,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144751955",
        "name": "Hao Zhou"
      },
      {
        "authorId": "2061649994",
        "name": "Tom Young"
      },
      {
        "authorId": "1730108",
        "name": "Minlie Huang"
      },
      {
        "authorId": "2664328",
        "name": "Haizhou Zhao"
      },
      {
        "authorId": "2774294",
        "name": "Jingfang Xu"
      },
      {
        "authorId": "145213540",
        "name": "Xiaoyan Zhu"
      }
    ]
  },
  "102354588": {
    "paperId": "cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022",
    "externalIds": {
      "MAG": "2954922414",
      "DBLP": "conf/naacl/Koncel-Kedziorski19",
      "ArXiv": "1904.02342",
      "ACL": "N19-1238",
      "DOI": "10.18653/v1/N19-1238",
      "CorpusId": 102354588
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Text Generation from Knowledge Graphs with Graph Transformers",
    "abstract": "Generating texts which express complex ideas spanning multiple sentences requires a structured representation of their content (document plan), but these representations are prohibitively expensive to manually produce. In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph. Graphical knowledge representations are ubiquitous in computing, but pose a significant challenge for text generation techniques due to their non-hierarchical nature, collapsing of long-distance dependencies, and structural variety. We introduce a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints. Incorporated into an encoder-decoder setup, we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientific text. Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 35,
    "citationCount": 307,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1403698986",
        "name": "Rik Koncel-Kedziorski"
      },
      {
        "authorId": "93836311",
        "name": "Dhanush Bekal"
      },
      {
        "authorId": "145081697",
        "name": "Yi Luan"
      },
      {
        "authorId": "1747893",
        "name": "Mirella Lapata"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ]
  },
  "5076191": {
    "paperId": "c68fbc1f4aa72d30974f8a3071054e3b227137fd",
    "externalIds": {
      "MAG": "2798966449",
      "ArXiv": "1804.07998",
      "ACL": "D18-1316",
      "DBLP": "journals/corr/abs-1804-07998",
      "DOI": "10.18653/v1/D18-1316",
      "CorpusId": 5076191
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Generating Natural Language Adversarial Examples",
    "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97% and 70%, respectively. We additionally demonstrate that 92.3% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 26,
    "citationCount": 877,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3030212",
        "name": "M. Alzantot"
      },
      {
        "authorId": "49738125",
        "name": "Yash Sharma"
      },
      {
        "authorId": "143718836",
        "name": "Ahmed Elgohary"
      },
      {
        "authorId": "33386728",
        "name": "Bo-Jhang Ho"
      },
      {
        "authorId": "1702254",
        "name": "M. Srivastava"
      },
      {
        "authorId": "2782886",
        "name": "Kai-Wei Chang"
      }
    ]
  },
  "211133221": {
    "paperId": "6189bf5f4c851ad0217a782509f8818aca4c7ff4",
    "externalIds": {
      "ArXiv": "2002.06622",
      "DBLP": "journals/corr/abs-2002-06622",
      "MAG": "2995368830",
      "CorpusId": 211133221
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Robustness Verification for Transformers",
    "abstract": "Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding the behavior of a given model and for obtaining safety guarantees. However, previous methods are usually limited to relatively simple neural networks. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous work. We resolve these challenges and develop the first verification algorithm for Transformers. The certified robustness bounds computed by our method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of words in sentiment analysis.",
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "referenceCount": 55,
    "citationCount": 99,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2987927",
        "name": "Zhouxing Shi"
      },
      {
        "authorId": "49723481",
        "name": "Huan Zhang"
      },
      {
        "authorId": "2782886",
        "name": "Kai-Wei Chang"
      },
      {
        "authorId": "1730108",
        "name": "Minlie Huang"
      },
      {
        "authorId": "1793529",
        "name": "Cho-Jui Hsieh"
      }
    ]
  },
  "86813509": {
    "paperId": "534f4e7eca88a6909c6374b430f76ac6aab3ce25",
    "externalIds": {
      "MAG": "3092741845",
      "DBLP": "conf/naacl/PezeshkpourT019",
      "ACL": "N19-1337",
      "ArXiv": "1905.00563",
      "DOI": "10.18653/v1/N19-1337",
      "CorpusId": 86813509
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications",
    "abstract": "Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving accuracy and overlook other aspects such as robustness and interpretability. In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained. Using these single modifications of the graph, we identify the most influential fact for a predicted link and evaluate the sensitivity of the model to the addition of fake facts. We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes. To avoid the combinatorial search over all possible facts, we train a network to decode embeddings to their corresponding graph components, allowing the use of gradient-based optimization to identify the adversarial modification. We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 46,
    "citationCount": 69,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1713436",
        "name": "Pouya Pezeshkpour"
      },
      {
        "authorId": "2143726165",
        "name": "Yifan Tian"
      },
      {
        "authorId": "34650964",
        "name": "Sameer Singh"
      }
    ]
  },
  "218551201": {
    "paperId": "33ec7eb2168e37e3007d1059aa96b9a63254b4da",
    "externalIds": {
      "DBLP": "journals/corr/abs-2005-04118",
      "MAG": "3035507081",
      "ArXiv": "2005.04118",
      "ACL": "2020.acl-main.442",
      "DOI": "10.18653/v1/2020.acl-main.442",
      "CorpusId": 218551201
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
    "abstract": "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 33,
    "citationCount": 985,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "78846919",
        "name": "Marco Tulio Ribeiro"
      },
      {
        "authorId": "35232494",
        "name": "Tongshuang Sherry Wu"
      },
      {
        "authorId": "1730156",
        "name": "Carlos Guestrin"
      },
      {
        "authorId": "34650964",
        "name": "Sameer Singh"
      }
    ]
  },
  "21740766": {
    "paperId": "472644c5f4155635cf9e9e37540bfa53c20e7610",
    "externalIds": {
      "DBLP": "conf/acl/SinghGR18",
      "MAG": "2799007037",
      "ACL": "P18-1079",
      "DOI": "10.18653/v1/P18-1079",
      "CorpusId": 21740766
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Semantically Equivalent Adversarial Rules for Debugging NLP models",
    "abstract": "Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) \u2013 semantic-preserving perturbations that induce changes in the model\u2019s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) \u2013 simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 27,
    "citationCount": 472,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "78846919",
        "name": "Marco Tulio Ribeiro"
      },
      {
        "authorId": "34650964",
        "name": "Sameer Singh"
      },
      {
        "authorId": "1730156",
        "name": "Carlos Guestrin"
      }
    ]
  },
  "202538141": {
    "paperId": "4690190d6c110f7525f7250e1acf4a4eab42519f",
    "externalIds": {
      "ACL": "D19-1423",
      "DBLP": "journals/corr/abs-1909-00986",
      "MAG": "2971789279",
      "ArXiv": "1909.00986",
      "DOI": "10.18653/v1/D19-1423",
      "CorpusId": 202538141
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Certified Robustness to Adversarial Word Substitutions",
    "abstract": "State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models\u2019 robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain 75% adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI; in comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only 12% and 41%, respectively.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 33,
    "citationCount": 281,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3422908",
        "name": "Robin Jia"
      },
      {
        "authorId": "2655157",
        "name": "Aditi Raghunathan"
      },
      {
        "authorId": "1388052305",
        "name": "Kerem G\u00f6ksel"
      },
      {
        "authorId": "145419642",
        "name": "Percy Liang"
      }
    ]
  },
  "218487582": {
    "paperId": "32bc789f96acb37361ac55f36940bb52b759c229",
    "externalIds": {
      "DBLP": "conf/acl/JonesJRL20",
      "MAG": "3035441470",
      "ArXiv": "2005.01229",
      "ACL": "2020.acl-main.245",
      "DOI": "10.18653/v1/2020.acl-main.245",
      "CorpusId": 218487582
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Robust Encodings: A Framework for Combating Adversarial Typos",
    "abstract": "Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs. Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT. In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture. The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings. Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks. We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity). We instantiate RobEn to defend against a large family of adversarial typos. Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 41,
    "citationCount": 98,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145790135",
        "name": "Erik Jones"
      },
      {
        "authorId": "3422908",
        "name": "Robin Jia"
      },
      {
        "authorId": "2655157",
        "name": "Aditi Raghunathan"
      },
      {
        "authorId": "145419642",
        "name": "Percy Liang"
      }
    ]
  },
  "201657818": {
    "paperId": "4e14cf96c60e3d35b05e3a740c7c6bbe52f14677",
    "externalIds": {
      "DBLP": "conf/acl-deeplo/HeZW19",
      "MAG": "2970730986",
      "ACL": "D19-6115",
      "ArXiv": "1908.10763",
      "DOI": "10.18653/v1/D19-6115",
      "CorpusId": 201657818
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual",
    "abstract": "Statistical natural language inference (NLI) models are susceptible to learning dataset bias: superficial cues that happen to associate with the label on a particular dataset, but are not useful in general, e.g., negation words indicate contradiction. As exposed by several recent challenge datasets, these models perform poorly when such association is absent, e.g., predicting that \u201cI love dogs.\u201d contradicts \u201cI don\u2019t love cats.\u201d. Our goal is to design learning algorithms that guard against known dataset bias. We formalize the concept of dataset bias under the framework of distribution shift and present a simple debiasing algorithm based on residual fitting, which we call DRiFt. We first learn a biased model that only uses features that are known to relate to dataset bias. Then, we train a debiased model that fits to the residual of the biased model, focusing on examples that cannot be predicted well by biased features only. We use DRiFt to train three high-performing NLI models on two benchmark datasets, SNLI and MNLI. Our debiased models achieve significant gains over baseline models on two challenge test sets, while maintaining reasonable performance on the original test sets.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 46,
    "citationCount": 186,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2140062900",
        "name": "He He"
      },
      {
        "authorId": "40881843",
        "name": "Sheng Zha"
      },
      {
        "authorId": "3669925",
        "name": "Haohan Wang"
      }
    ]
  },
  "220514568": {
    "paperId": "04422085a52050516b9741e0fd1fda964b73dd53",
    "externalIds": {
      "DBLP": "journals/tacl/TuLGH20",
      "ArXiv": "2007.06778",
      "MAG": "3043024363",
      "DOI": "10.1162/tacl_a_00335",
      "CorpusId": 220514568
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models",
    "abstract": "Abstract Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples where the spurious correlations do not hold. When such minority examples are scarce, pre-trained models perform as poorly as models trained from scratch. In the case of extreme minority, we propose to use multi-task learning (MTL) to improve generalization. Our experiments on natural language inference and paraphrase identification show that MTL with the right auxiliary tasks significantly improves performance on challenging examples without hurting the in-distribution performance. Further, we show that the gain from MTL mainly comes from improved generalization from the minority examples. Our results highlight the importance of data diversity for overcoming spurious correlations.1",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 50,
    "citationCount": 171,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3376969",
        "name": "Lifu Tu"
      },
      {
        "authorId": "1387985266",
        "name": "Garima Lalwani"
      },
      {
        "authorId": "2921001",
        "name": "Spandana Gella"
      },
      {
        "authorId": "144533687",
        "name": "He He"
      }
    ]
  },
  "201698258": {
    "paperId": "3caf34532597683c980134579b156cd0d7db2f40",
    "externalIds": {
      "MAG": "2970290563",
      "ArXiv": "1908.07125",
      "DBLP": "conf/emnlp/WallaceFKGS19",
      "ACL": "D19-1221",
      "DOI": "10.18653/v1/D19-1221",
      "CorpusId": 201698258
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP",
    "abstract": "Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of \u201cwhy\u201d questions in SQuAD to be answered \u201cto kill american people\u201d, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 38,
    "citationCount": 746,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145217343",
        "name": "Eric Wallace"
      },
      {
        "authorId": "2113511266",
        "name": "Shi Feng"
      },
      {
        "authorId": "1380266797",
        "name": "Nikhil Kandpal"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      },
      {
        "authorId": "34650964",
        "name": "Sameer Singh"
      }
    ]
  },
  "202541036": {
    "paperId": "07398e448180ad75c44d30f23a65289d40ff6f52",
    "externalIds": {
      "MAG": "2970449623",
      "DBLP": "conf/emnlp/HuangSWDYGDK19",
      "ArXiv": "1909.01492",
      "ACL": "D19-1419",
      "DOI": "10.18653/v1/D19-1419",
      "CorpusId": 202541036
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation",
    "abstract": "Neural networks are part of many contemporary NLP systems, yet their empirical successes come at the price of vulnerability to adversarial attacks. Previous work has used adversarial training and data augmentation to partially mitigate such brittleness, but these are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations. In this work, we approach the problem from the opposite direction: to formally verify a system\u2019s robustness against a predefined class of adversarial attacks. We study text classification under synonym replacements or character flip perturbations. We propose modeling these input perturbations as a simplex and then using Interval Bound Propagation \u2013 a formal model verification method. We modify the conventional log-likelihood training objective to train models that can be efficiently verified, which would otherwise come with exponential search complexity. The resulting models show only little difference in terms of nominal accuracy, but have much improved verified accuracy under perturbations and come with an efficiently computable formal guarantee on worst case adversaries.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 47,
    "citationCount": 159,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2421691",
        "name": "Po-Sen Huang"
      },
      {
        "authorId": "49860489",
        "name": "Robert Stanforth"
      },
      {
        "authorId": "1851564",
        "name": "Johannes Welbl"
      },
      {
        "authorId": "1745899",
        "name": "Chris Dyer"
      },
      {
        "authorId": "1755465",
        "name": "Dani Yogatama"
      },
      {
        "authorId": "2071666",
        "name": "Sven Gowal"
      },
      {
        "authorId": "1729912",
        "name": "Krishnamurthy Dvijotham"
      },
      {
        "authorId": "143967473",
        "name": "Pushmeet Kohli"
      }
    ]
  },
  "219124328": {
    "paperId": "0a9c0e729dd95f5559e05f8bb4b7408f9409388e",
    "externalIds": {
      "DBLP": "conf/acl/YeGL20",
      "MAG": "3035164976",
      "ACL": "2020.acl-main.317",
      "ArXiv": "2005.14424",
      "DOI": "10.18653/v1/2020.acl-main.317",
      "CorpusId": 219124328
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions",
    "abstract": "State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution. For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution. In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness. Our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as BERT) and any types of models (world-level or subword-level). Our method significantly outperforms recent state-of-the-art methods for certified robustness on both IMDB and Amazon text classification tasks. To the best of our knowledge, we are the first work to achieve certified robustness on large systems such as BERT with practically meaningful certified accuracy.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 24,
    "citationCount": 89,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144785541",
        "name": "Mao Ye"
      },
      {
        "authorId": "29777869",
        "name": "Chengyue Gong"
      },
      {
        "authorId": "47362268",
        "name": "Qiang Liu"
      }
    ]
  },
  "207756753": {
    "paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d",
    "externalIds": {
      "MAG": "3034850762",
      "ACL": "2020.acl-main.441",
      "DBLP": "conf/acl/NieWDBWK20",
      "ArXiv": "1910.14599",
      "DOI": "10.18653/v1/2020.acl-main.441",
      "CorpusId": 207756753
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
    "abstract": "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 69,
    "citationCount": 902,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40383658",
        "name": "Yixin Nie"
      },
      {
        "authorId": "81840293",
        "name": "Adina Williams"
      },
      {
        "authorId": "31461304",
        "name": "Emily Dinan"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      },
      {
        "authorId": "145183709",
        "name": "J. Weston"
      },
      {
        "authorId": "1743722",
        "name": "Douwe Kiela"
      }
    ]
  },
  "155100063": {
    "paperId": "634c083444e11c89f30c93a2986cb43db35ca304",
    "externalIds": {
      "ACL": "Q19-1029",
      "MAG": "2966491090",
      "DOI": "10.1162/tacl_a_00279",
      "CorpusId": 155100063
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Trick Me If You Can: Human-in-the-Loop Generation of Adversarial Examples for Question Answering",
    "abstract": "Adversarial evaluation stress-tests a model\u2019s understanding of natural language. Because past approaches expose superficial patterns, the resulting adversarial examples are limited in complexity and diversity. We propose human- in-the-loop adversarial generation, where human authors are guided to break models. We aid the authors with interpretations of model predictions through an interactive user interface. We apply this generation framework to a question answering task called Quizbowl, where trivia enthusiasts craft adversarial questions. The resulting questions are validated via live human\u2013computer matches: Although the questions appear ordinary to humans, they systematically stump neural and information retrieval models. The adversarial questions cover diverse phenomena from multi-hop reasoning to entity type distractors, exposing open challenges in robust question answering.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 56,
    "citationCount": 130,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145217343",
        "name": "Eric Wallace"
      },
      {
        "authorId": "145009056",
        "name": "Pedro Rodriguez"
      },
      {
        "authorId": "2113511266",
        "name": "Shi Feng"
      },
      {
        "authorId": "2303128",
        "name": "Ikuya Yamada"
      },
      {
        "authorId": "1389036863",
        "name": "Jordan L. Boyd-Graber"
      }
    ]
  },
  "166228669": {
    "paperId": "162515d87256f13888d9d7ba95275ac4b6c35396",
    "externalIds": {
      "MAG": "2952872637",
      "DBLP": "conf/acl/PruthiDL19",
      "ACL": "P19-1561",
      "ArXiv": "1905.11268",
      "DOI": "10.18653/v1/P19-1561",
      "CorpusId": 166228669
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Combating Adversarial Misspellings with Robust Word Recognition",
    "abstract": "To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. Trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our method achieves 32% relative (and 3.3% absolute) error reduction over the vanilla semi-character model. Notably, our pipeline confers robustness on the downstream classifier, outperforming both adversarial training and off-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment analysis, a single adversarially-chosen character attack lowers accuracy from 90.3% to 45.8%. Our defense restores accuracy to 75%. Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 30,
    "citationCount": 288,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "7880098",
        "name": "Danish Pruthi"
      },
      {
        "authorId": "34994191",
        "name": "Bhuwan Dhingra"
      },
      {
        "authorId": "32219137",
        "name": "Zachary Chase Lipton"
      }
    ]
  },
  "52019251": {
    "paperId": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027",
    "externalIds": {
      "DBLP": "journals/corr/abs-1808-05326",
      "ArXiv": "1808.05326",
      "ACL": "D18-1009",
      "MAG": "2952007233",
      "DOI": "10.18653/v1/D18-1009",
      "CorpusId": 52019251
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference",
    "abstract": "Given a partial description like \u201cshe opened the hood of the car,\u201d humans can reason about the situation and anticipate what might come next (\u201dthen, she examined the engine\u201d). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 72,
    "citationCount": 680,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2545335",
        "name": "Rowan Zellers"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "4671928",
        "name": "Roy Schwartz"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      }
    ]
  },
  "196202909": {
    "paperId": "1adfa30bf112de20cb959014e44626d760aa8e4e",
    "externalIds": {
      "ACL": "P19-1103",
      "DBLP": "conf/acl/RenDHC19",
      "MAG": "2949128310",
      "DOI": "10.18653/v1/P19-1103",
      "CorpusId": 196202909
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency",
    "abstract": "We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 25,
    "citationCount": 633,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1906099",
        "name": "Shuhuai Ren"
      },
      {
        "authorId": "66357895",
        "name": "Yihe Deng"
      },
      {
        "authorId": "1702188",
        "name": "Kun He"
      },
      {
        "authorId": "2256319",
        "name": "Wanxiang Che"
      }
    ]
  },
  "54084368": {
    "paperId": "2815d46dc0092c5a89b043aef3a6a7805db753a7",
    "externalIds": {
      "MAG": "2954769794",
      "DBLP": "journals/corr/abs-1904-05440",
      "ArXiv": "1904.05440",
      "ACL": "S19-1032",
      "DOI": "10.18653/v1/S19-1032",
      "CorpusId": 54084368
    },
    "publicationVenue": {
      "id": "70713d09-6e4b-4554-9d3f-94d08aba320c",
      "name": "International Workshop on Semantic Evaluation",
      "type": "conference",
      "alternate_names": [
        "SemEval ",
        "Int Workshop Semantic Evaluation"
      ]
    },
    "title": "Generating Animations from Screenplays",
    "abstract": "Automatically generating animation from natural language text finds application in a number of areas e.g. movie script writing, instructional videos, and public safety. However, translating natural language text into animation is a challenging task. Existing text-to-animation systems can handle only very simple sentences, which limits their applications. In this paper, we develop a text-to-animation system which is capable of handling complex sentences. We achieve this by introducing a text simplification step into the process. Building on an existing animation generation system for screenwriting, we create a robust NLP pipeline to extract information from screenplays and map them to the system\u2019s knowledge base. We develop a set of linguistic transformation rules that simplify complex sentences. Information extracted from the simplified sentences is used to generate a rough storyboard and video depicting the text. Our sentence simplification module outperforms existing systems in terms of BLEU and SARI metrics.We further evaluated our system via a user study: 68% participants believe that our system generates reasonable animation from input screenplays.",
    "venue": "International Workshop on Semantic Evaluation",
    "year": 2019,
    "referenceCount": 42,
    "citationCount": 17,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "102684078",
        "name": "Yeyao Zhang"
      },
      {
        "authorId": "102649445",
        "name": "Eleftheria Tsipidi"
      },
      {
        "authorId": "34644616",
        "name": "Sasha Schriber"
      },
      {
        "authorId": "143980996",
        "name": "Mubbasir Kapadia"
      },
      {
        "authorId": "2280075525",
        "name": "M. Gross"
      },
      {
        "authorId": "2477939",
        "name": "Ashutosh Modi"
      }
    ]
  },
  "131763622": {
    "paperId": "000ea515050fba000709ccfeeac9efb0c7fabd05",
    "externalIds": {
      "MAG": "2955503152",
      "DBLP": "conf/starsem/BelinkovPSDR19",
      "ACL": "S19-1028",
      "ArXiv": "1907.04389",
      "DOI": "10.18653/v1/S19-1028",
      "CorpusId": 131763622
    },
    "publicationVenue": {
      "id": "70713d09-6e4b-4554-9d3f-94d08aba320c",
      "name": "International Workshop on Semantic Evaluation",
      "type": "conference",
      "alternate_names": [
        "SemEval ",
        "Int Workshop Semantic Evaluation"
      ]
    },
    "title": "On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference",
    "abstract": "Popular Natural Language Inference (NLI) datasets have been shown to be tainted by hypothesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy.",
    "venue": "International Workshop on Semantic Evaluation",
    "year": 2019,
    "referenceCount": 24,
    "citationCount": 68,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "48926630",
        "name": "Adam Poliak"
      },
      {
        "authorId": "1692491",
        "name": "Stuart M. Shieber"
      },
      {
        "authorId": "7536576",
        "name": "Benjamin Van Durme"
      },
      {
        "authorId": "2531268",
        "name": "Alexander M. Rush"
      }
    ]
  },
  "44220219": {
    "paperId": "96e53b9c4e7ce52b32f090ceb3b069786559a2b7",
    "externalIds": {
      "MAG": "2951280437",
      "ACL": "P18-1241",
      "DBLP": "conf/acl/HsiehYCZC18",
      "DOI": "10.18653/v1/P18-1241",
      "CorpusId": 44220219
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning",
    "abstract": "Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check if we can mislead neural image captioning systems to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 45,
    "citationCount": 126,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2108270137",
        "name": "Hongge Chen"
      },
      {
        "authorId": "49723481",
        "name": "Huan Zhang"
      },
      {
        "authorId": "153191489",
        "name": "Pin-Yu Chen"
      },
      {
        "authorId": "2882166",
        "name": "Jinfeng Yi"
      },
      {
        "authorId": "1793529",
        "name": "Cho-Jui Hsieh"
      }
    ]
  },
  "220047695": {
    "paperId": "aa2cd7b5a202e995adddd8bfe3fc0536faee70ed",
    "externalIds": {
      "MAG": "3035422387",
      "DBLP": "conf/acl/ZhengZZHCH20",
      "ACL": "2020.acl-main.590",
      "DOI": "10.18653/v1/2020.acl-main.590",
      "CorpusId": 220047695
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples",
    "abstract": "Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension. In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings. Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 42,
    "citationCount": 39,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2152196565",
        "name": "Xiaoqing Zheng"
      },
      {
        "authorId": "1634814790",
        "name": "Jiehang Zeng"
      },
      {
        "authorId": "2118765269",
        "name": "Yi Zhou"
      },
      {
        "authorId": "1793529",
        "name": "Cho-Jui Hsieh"
      },
      {
        "authorId": "2424698",
        "name": "Minhao Cheng"
      },
      {
        "authorId": "1790227",
        "name": "Xuanjing Huang"
      }
    ]
  },
  "133468229": {
    "paperId": "b73d5765d3b0c49b536e84b85608cfeed9bdba2e",
    "externalIds": {
      "MAG": "2926587947",
      "ACL": "N19-1336",
      "DBLP": "conf/naacl/ChengWH19",
      "DOI": "10.18653/v1/N19-1336",
      "CorpusId": 133468229
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Evaluating and Enhancing the Robustness of Dialogue Systems: A Case Study on a Negotiation Agent",
    "abstract": "Recent research has demonstrated that goal-oriented dialogue agents trained on large datasets can achieve striking performance when interacting with human users. In real world applications, however, it is important to ensure that the agent performs smoothly interacting with not only regular users but also those malicious ones who would attack the system through interactions in order to achieve goals for their own advantage. In this paper, we develop algorithms to evaluate the robustness of a dialogue agent by carefully designed attacks using adversarial agents. Those attacks are performed in both black-box and white-box settings. Furthermore, we demonstrate that adversarial training using our attacks can significantly improve the robustness of a goal-oriented dialogue system. On a case-study of the negotiation agent developed by (Lewis et al., 2017), our attacks reduced the average advantage of rewards between the attacker and the trained RL-based agent from 2.68 to -5.76 on a scale from -10 to 10 for randomized goals. Moreover, we show that with the adversarial training, we are able to improve the robustness of negotiation agents by 1.5 points on average against all our attacks.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 40,
    "citationCount": 45,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2424698",
        "name": "Minhao Cheng"
      },
      {
        "authorId": "2149192010",
        "name": "Wei Wei"
      },
      {
        "authorId": "1793529",
        "name": "Cho-Jui Hsieh"
      }
    ]
  },
  "192546007": {
    "paperId": "b66a943dd745c0868d03144d60b7cd2aeb3c2ba7",
    "externalIds": {
      "ACL": "P19-1147",
      "MAG": "2947469743",
      "DBLP": "conf/acl/HsiehCJWHH19",
      "DOI": "10.18653/v1/P19-1147",
      "CorpusId": 192546007
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "On the Robustness of Self-Attentive Models",
    "abstract": "This work examines the robustness of self-attentive neural networks against adversarial input perturbations. Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. Experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. In addition, we provide theoretical explanations for their superior robustness to support our claims.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 25,
    "citationCount": 91,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3338169",
        "name": "Yu-Lun Hsieh"
      },
      {
        "authorId": "2424698",
        "name": "Minhao Cheng"
      },
      {
        "authorId": "144854012",
        "name": "Da-Cheng Juan"
      },
      {
        "authorId": "2149192010",
        "name": "Wei Wei"
      },
      {
        "authorId": "144505734",
        "name": "W. Hsu"
      },
      {
        "authorId": "1793529",
        "name": "Cho-Jui Hsieh"
      }
    ]
  },
  "218486778": {
    "paperId": "2de50c0f27cdcc6cb000f3b67825d95f7d1ed2c5",
    "externalIds": {
      "DBLP": "journals/corr/abs-2005-01348",
      "ArXiv": "2005.01348",
      "ACL": "2020.acl-main.679",
      "MAG": "3021643468",
      "DOI": "10.18653/v1/2020.acl-main.679",
      "CorpusId": 218486778
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "The Sensitivity of Language Models and Humans to Winograd Schema Perturbations",
    "abstract": "Large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 52,
    "citationCount": 33,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "30671790",
        "name": "Mostafa Abdou"
      },
      {
        "authorId": "24881798",
        "name": "Vinit Ravishankar"
      },
      {
        "authorId": "46904636",
        "name": "Maria Barrett"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "50369944",
        "name": "Desmond Elliott"
      },
      {
        "authorId": "1700187",
        "name": "Anders S\u00f8gaard"
      }
    ]
  },
  "215191351": {
    "paperId": "72a1d0256b38dea6c3e7d10a63eacc51abdc96da",
    "externalIds": {
      "MAG": "3018505466",
      "ArXiv": "1909.06321",
      "ACL": "2020.acl-main.769",
      "DBLP": "conf/acl/MahabadiBH20",
      "DOI": "10.18653/v1/2020.acl-main.769",
      "CorpusId": 215191351
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "End-to-End Bias Mitigation by Modelling Biases in Corpora",
    "abstract": "Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models\u2019 predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. Our code and data are publicly available in https://github.com/rabeehk/robust-nli.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 40,
    "citationCount": 163,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2363901",
        "name": "Rabeeh Karimi Mahabadi"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "46712043",
        "name": "J. Henderson"
      }
    ]
  },
  "59604474": {
    "paperId": "0b9ac1035918823ffca1c6f55ec316b42d4e033f",
    "externalIds": {
      "DBLP": "conf/aclnut/KarpukhinLEG19",
      "MAG": "2914056350",
      "ArXiv": "1902.01509",
      "ACL": "D19-5506",
      "DOI": "10.18653/v1/D19-5506",
      "CorpusId": 59604474
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Training on Synthetic Noise Improves Robustness to Natural Noise in Machine Translation",
    "abstract": "Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 25,
    "citationCount": 111,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2067091563",
        "name": "Vladimir Karpukhin"
      },
      {
        "authorId": "39455775",
        "name": "Omer Levy"
      },
      {
        "authorId": "144154709",
        "name": "Jacob Eisenstein"
      },
      {
        "authorId": "2320509",
        "name": "Marjan Ghazvininejad"
      }
    ]
  },
  "52132833": {
    "paperId": "fc097d528fd62fe76d73fafbf0c57473b58d1e84",
    "externalIds": {
      "DBLP": "conf/wmt/MurrayC18",
      "ACL": "W18-6322",
      "ArXiv": "1808.10006",
      "MAG": "2951062070",
      "DOI": "10.18653/v1/W18-6322",
      "CorpusId": 52132833
    },
    "publicationVenue": {
      "id": "9aacb914-3edf-4e02-b8fe-5abf21c4d2ba",
      "name": "Conference on Machine Translation",
      "type": "conference",
      "alternate_names": [
        "WMT",
        "Conf Mach Transl"
      ]
    },
    "title": "Correcting Length Bias in Neural Machine Translation",
    "abstract": "We study two problems in neural machine translation (NMT). First, in beam search, whereas a wider beam should in principle help translation, it often hurts NMT. Second, NMT has a tendency to produce translations that are too short. Here, we argue that these problems are closely related and both rooted in label bias. We show that correcting the brevity problem almost eliminates the beam problem; we compare some commonly-used methods for doing this, finding that a simple per-word reward works well; and we introduce a simple and quick way to tune this reward using the perceptron algorithm.",
    "venue": "Conference on Machine Translation",
    "year": 2018,
    "referenceCount": 27,
    "citationCount": 139,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "38730896",
        "name": "Kenton Murray"
      },
      {
        "authorId": "145287425",
        "name": "David Chiang"
      }
    ]
  },
  "4956100": {
    "paperId": "2b110fce160468eb179b6c43ea27e098757a56dd",
    "externalIds": {
      "MAG": "2963126845",
      "DBLP": "journals/corr/abs-1804-06059",
      "ACL": "N18-1170",
      "ArXiv": "1804.06059",
      "DOI": "10.18653/v1/N18-1170",
      "CorpusId": 4956100
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Adversarial Example Generation with Syntactically Controlled Paraphrase Networks",
    "abstract": "We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) \u201cfool\u201d pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 53,
    "citationCount": 676,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2136562",
        "name": "Mohit Iyyer"
      },
      {
        "authorId": "1771118",
        "name": "J. Wieting"
      },
      {
        "authorId": "1700980",
        "name": "Kevin Gimpel"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ]
  },
  "49413369": {
    "paperId": "f3f4689e4346ce497a2c3e367f6ce365da9c8966",
    "externalIds": {
      "ACL": "C18-1055",
      "ArXiv": "1806.09030",
      "MAG": "2951467133",
      "DBLP": "conf/coling/EbrahimiLD18",
      "CorpusId": 49413369
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "On Adversarial Examples for Character-Level Neural Machine Translation",
    "abstract": "Evaluating on adversarial examples has become a standard procedure to measure robustness of deep learning models. Due to the difficulty of creating white-box adversarial examples for discrete text input, most analyses of the robustness of NLP models have been done through black-box adversarial examples. We investigate adversarial examples for character-level neural machine translation (NMT), and contrast black-box adversaries with a novel white-box adversary, which employs differentiable string-edit operations to rank adversarial changes. We propose two novel types of attacks which aim to remove or change a word in a translation, rather than simply break the NMT. We demonstrate that white-box adversarial examples are significantly stronger than their black-box counterparts in different attack scenarios, which show more serious vulnerabilities than previously known. In addition, after performing adversarial training, which takes only 3 times longer than regular training, we can improve the model\u2019s robustness significantly.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2018,
    "referenceCount": 22,
    "citationCount": 206,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39043512",
        "name": "J. Ebrahimi"
      },
      {
        "authorId": "3021654",
        "name": "Daniel Lowd"
      },
      {
        "authorId": "1721158",
        "name": "D. Dou"
      }
    ]
  },
  "7942973": {
    "paperId": "8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2",
    "externalIds": {
      "DBLP": "journals/corr/DozatM16",
      "MAG": "2963571341",
      "ArXiv": "1611.01734",
      "CorpusId": 7942973
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Deep Biaffine Attention for Neural Dependency Parsing",
    "abstract": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and 2.2%---and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.",
    "venue": "International Conference on Learning Representations",
    "year": 2016,
    "referenceCount": 33,
    "citationCount": 1174,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2277385",
        "name": "Timothy Dozat"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      }
    ]
  },
  "19206893": {
    "paperId": "928f9dccb806a3278d20d82cc53781c5f44e2bb1",
    "externalIds": {
      "MAG": "2950268825",
      "ArXiv": "1805.01052",
      "DBLP": "conf/acl/KleinK18",
      "ACL": "P18-1249",
      "DOI": "10.18653/v1/P18-1249",
      "CorpusId": 19206893
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Constituency Parsing with a Self-Attentive Encoder",
    "abstract": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 21,
    "citationCount": 510,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143808231",
        "name": "Nikita Kitaev"
      },
      {
        "authorId": "38666915",
        "name": "D. Klein"
      }
    ]
  },
  "51879191": {
    "paperId": "74b3f93ee47fe36ff1862ec7d52745f30ec7be49",
    "externalIds": {
      "DBLP": "conf/acl/ZhaoHLB18",
      "MAG": "2798304389",
      "ACL": "P18-1192",
      "DOI": "10.18653/v1/P18-1192",
      "CorpusId": 51879191
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Syntax for Semantic Role Labeling, To Be, Or Not To Be",
    "abstract": "Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence. Previous studies have shown syntactic information has a remarkable contribution to SRL performance. However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone. This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework. We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information. Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 36,
    "citationCount": 136,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51129953",
        "name": "Shexia He"
      },
      {
        "authorId": "30658665",
        "name": "Z. Li"
      },
      {
        "authorId": "36225434",
        "name": "Zhao Hai"
      },
      {
        "authorId": "51133532",
        "name": "Hongxiao Bai"
      }
    ]
  },
  "33626727": {
    "paperId": "a4dd3beea286a20c4e4f66436875932d597190bc",
    "externalIds": {
      "MAG": "2740765036",
      "DBLP": "conf/acl/HeLLZ17a",
      "ACL": "P17-1044",
      "DOI": "10.18653/v1/P17-1044",
      "CorpusId": 33626727
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Deep Semantic Role Labeling: What Works and What\u2019s Next",
    "abstract": "We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10% relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 32,
    "citationCount": 429,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2265599",
        "name": "Luheng He"
      },
      {
        "authorId": "2544107",
        "name": "Kenton Lee"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ]
  },
  "267846283": {
    "paperId": "247a912fbd3165ffb1463bb3c9bc17e2096ab9b5",
    "externalIds": {
      "MAG": "2135681771",
      "DBLP": "journals/coling/Cherry10",
      "ACL": "J10-4010",
      "DOI": "10.1162/coli_r_00023",
      "CorpusId": 267846283
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Book Review: Statistical Machine Translation by Philipp Koehn",
    "abstract": "Statistical Machine Translation provides a comprehensive and clear introduction to the most prominent techniques employed in the field of the same name (SMT). This textbook is aimed at students or researchers interested in a thorough entry-point to the field, and it does an excellent job of providing basic understanding for each of the many pieces of a statistical translation system. I consider this book to be an essential addition to any advanced undergraduate course or graduate course on SMT. The book is divided into three parts: Foundations, Core Methods, and Advanced Topics. Foundations (75 pages) covers an introduction to translation, working with text, and probability theory. Core Methods (170 pages) covers the main components of a standard phrase-based SMT system. Advanced Topics (125 pages) covers discriminative training and linguistics in SMT, including an in-depth discussion of syntactic SMT. The text as a whole assumes a certain familiarity with natural language processing; though the Foundations section provides an effort to fill in the gaps, the book\u2019s focus is decidedly translation. As such, students unfamiliar with NLP may sometimes need to consult a general NLP text. The book aims to provide a thorough introduction to each component of a statistical translation system, and it definitely succeeds in doing so. Supplementing this core material for each chapter is a highly inclusive Further Reading section. These sections provide brief narratives highlighting many relevant papers and alternative techniques for each topic addressed in the chapter. I suspect many readers will find these literature pointers to be quite valuable, from students wishing to dive deeper, to experienced SMT researchers wishing to get started in a new sub-field. Each chapter also closes with a short list of exercises. Many of these are very challenging (accurately indicated by a star-rating system), and involve getting your hands dirty with tools downloaded from the Web. The usefulness of these exercises will depend largely on the instructor\u2019s tastes; I view them as a bonus rather than a core feature of the book.",
    "venue": "International Conference on Computational Logic",
    "year": 2010,
    "referenceCount": 5,
    "citationCount": 0,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Art",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2285655408",
        "name": "Colin Cherry"
      }
    ]
  },
  "222125099": {
    "paperId": "4b322cf280f459deb6d9e2eb2430d1a28141934c",
    "externalIds": {
      "MAG": "3090395639",
      "ArXiv": "2010.00711",
      "DBLP": "journals/corr/abs-2010-00711",
      "ACL": "2020.aacl-main.46",
      "CorpusId": 222125099
    },
    "publicationVenue": null,
    "title": "A Survey of the State of Explainable AI for Natural Language Processing",
    "abstract": "Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.",
    "venue": "AACL",
    "year": 2020,
    "referenceCount": 82,
    "citationCount": 324,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1994333",
        "name": "Marina Danilevsky"
      },
      {
        "authorId": "143857309",
        "name": "Kun Qian"
      },
      {
        "authorId": "48361424",
        "name": "R. Aharonov"
      },
      {
        "authorId": "2208580",
        "name": "Yannis Katsis"
      },
      {
        "authorId": "1814905",
        "name": "B. Kawas"
      },
      {
        "authorId": "40655309",
        "name": "Prithviraj Sen"
      }
    ]
  },
  "53215110": {
    "paperId": "c5489d244bfc1e9b0d8c94bf6dd774ee1aca2def",
    "externalIds": {
      "DBLP": "conf/iclr/BauBSDDG19",
      "MAG": "2899032424",
      "ArXiv": "1811.01157",
      "CorpusId": 53215110
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Identifying and Controlling Important Neurons in Neural Machine Translation",
    "abstract": "Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.",
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "referenceCount": 36,
    "citationCount": 168,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2063937616",
        "name": "A. Bau"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "145775792",
        "name": "Hassan Sajjad"
      },
      {
        "authorId": "145938140",
        "name": "Nadir Durrani"
      },
      {
        "authorId": "6415321",
        "name": "Fahim Dalvi"
      },
      {
        "authorId": "145898106",
        "name": "James R. Glass"
      }
    ]
  },
  "56895415": {
    "paperId": "9c2156bc35c6f8e68aa21d4b2f339134a4d28708",
    "externalIds": {
      "DBLP": "journals/corr/abs-1812-09355",
      "MAG": "2963400886",
      "ArXiv": "1812.09355",
      "DOI": "10.1609/aaai.v33i01.33016309",
      "CorpusId": 56895415
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models",
    "abstract": "Despite the remarkable evolution of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. Previous work largely focused on what these models learn at the representation level. We break this analysis down further and study individual dimensions (neurons) in the vector representation learned by end-to-end neural models in NLP tasks. We propose two methods: Linguistic Correlation Analysis, based on a supervised method to extract the most relevant neurons with respect to an extrinsic task, and Cross-model Correlation Analysis, an unsupervised method to extract salient neurons w.r.t. the model itself. We evaluate the effectiveness of our techniques by ablating the identified neurons and reevaluating the network\u2019s performance for two tasks: neural machine translation (NMT) and neural language modeling (NLM). We further present a comprehensive analysis of neurons with the aim to address the following questions: i) how localized or distributed are different linguistic properties in the models? ii) are certain neurons exclusive to some properties and not others? iii) is the information more or less distributed in NMT vs. NLM? and iv) how important are the neurons identified through the linguistic correlation method to the overall task? Our code is publicly available as part of the NeuroX toolkit (Dalvi et al. 2019a). This paper is a non-archived version of the paper published at AAAI (Dalvi et al. 2019b).",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2018,
    "referenceCount": 34,
    "citationCount": 171,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "6415321",
        "name": "Fahim Dalvi"
      },
      {
        "authorId": "145938140",
        "name": "Nadir Durrani"
      },
      {
        "authorId": "145775792",
        "name": "Hassan Sajjad"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "2063937616",
        "name": "A. Bau"
      },
      {
        "authorId": "145898106",
        "name": "James R. Glass"
      }
    ]
  },
  "220055965": {
    "paperId": "56d1003fd02346e93354ab55cd204485c268512a",
    "externalIds": {
      "ArXiv": "2006.14032",
      "DBLP": "conf/nips/MuA20",
      "MAG": "3098680936",
      "CorpusId": 220055965
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Compositional Explanations of Neurons",
    "abstract": "We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple \"copy-paste\" adversarial examples that change model behavior in predictable ways.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 43,
    "citationCount": 152,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "24835910",
        "name": "Jesse Mu"
      },
      {
        "authorId": "2112400",
        "name": "Jacob Andreas"
      }
    ]
  },
  "218665291": {
    "paperId": "bf30db07357427cda6cc2b64fbcea783eb048f05",
    "externalIds": {
      "ArXiv": "2005.07647",
      "MAG": "3024936740",
      "DBLP": "journals/corr/abs-2005-07647",
      "CorpusId": 218665291
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Finding Experts in Transformer Models",
    "abstract": "In this work we study the presence of expert units in pre-trained Transformer Models (TM), and how they impact a model's performance. We define expert units to be neurons that are able to classify a concept with a given average precision, where a concept is represented by a binary set of sentences containing the concept (or not). Leveraging the OneSec dataset (Scarlini et al., 2019), we compile a dataset of 1641 concepts that allows diverse expert units in TM to be discovered. We show that expert units are important in several ways: (1) The presence of expert units is correlated ($r^2=0.833$) with the generalization power of TM, which allows ranking TM without requiring fine-tuning on suites of downstream tasks. We further propose an empirical method to decide how accurate such experts should be to evaluate generalization. (2) The overlap of top experts between concepts provides a sensible way to quantify concept co-learning, which can be used for explainability of unknown concepts. (3) We show how to self-condition off-the-shelf pre-trained language models to generate text with a given concept by forcing the top experts to be active, without requiring re-training the model or using additional parameters.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 37,
    "citationCount": 31,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2270464",
        "name": "Xavier Suau"
      },
      {
        "authorId": "1753336",
        "name": "L. Zappella"
      },
      {
        "authorId": "3301859",
        "name": "N. Apostoloff"
      }
    ]
  },
  "21889700": {
    "paperId": "442e10a3c6640ded9408622005e3c2a8906ce4c2",
    "externalIds": {
      "MAG": "2618851150",
      "DBLP": "journals/corr/LundbergL17",
      "ArXiv": "1705.07874",
      "CorpusId": 21889700
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "A Unified Approach to Interpreting Model Predictions",
    "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.",
    "venue": "Neural Information Processing Systems",
    "year": 2017,
    "referenceCount": 10,
    "citationCount": 17300,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "23451726",
        "name": "Scott M. Lundberg"
      },
      {
        "authorId": "2180463",
        "name": "Su-In Lee"
      }
    ]
  },
  "224818197": {
    "paperId": "cf592385909a1e3e9a428d8d6d8f427ab70b60a9",
    "externalIds": {
      "DBLP": "conf/acl/VoitaST20",
      "ArXiv": "2010.10907",
      "ACL": "2021.acl-long.91",
      "MAG": "3094519420",
      "DOI": "10.18653/v1/2021.acl-long.91",
      "CorpusId": 224818197
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation",
    "abstract": "In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying \u2018conservation principle\u2019 makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token\u2019s influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 46,
    "citationCount": 80,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46235299",
        "name": "Elena Voita"
      },
      {
        "authorId": "2082372",
        "name": "Rico Sennrich"
      },
      {
        "authorId": "144889265",
        "name": "Ivan Titov"
      }
    ]
  },
  "16747630": {
    "paperId": "f302e136c41db5de1d624412f68c9174cf7ae8be",
    "externalIds": {
      "DBLP": "journals/corr/SundararajanTY17",
      "MAG": "2949197630",
      "ArXiv": "1703.01365",
      "CorpusId": 16747630
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Axiomatic Attribution for Deep Networks",
    "abstract": "We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms\u2014 Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.",
    "venue": "International Conference on Machine Learning",
    "year": 2017,
    "referenceCount": 35,
    "citationCount": 5234,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "30740726",
        "name": "Mukund Sundararajan"
      },
      {
        "authorId": "40511120",
        "name": "Ankur Taly"
      },
      {
        "authorId": "34789908",
        "name": "Qiqi Yan"
      }
    ]
  },
  "44167055": {
    "paperId": "eb322f6f798fd1b381896d0b79f5498d89585b1f",
    "externalIds": {
      "MAG": "2950132931",
      "DBLP": "journals/corr/abs-1805-12233",
      "ArXiv": "1805.12233",
      "CorpusId": 44167055
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "How Important Is a Neuron?",
    "abstract": "The problem of attributing a deep network's prediction to its \\emph{input/base} features is well-studied. We introduce the notion of \\emph{conductance} to extend the notion of attribution to the understanding the importance of \\emph{hidden} units. \nInformally, the conductance of a hidden unit of a deep network is the \\emph{flow} of attribution via this hidden unit. We use conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs. We evaluate the effectiveness of conductance in multiple ways, including theoretical properties, ablation studies, and a feature selection task. The empirical evaluations are done using the Inception network over ImageNet data, and a sentiment analysis network over reviews. In both cases, we demonstrate the effectiveness of conductance in identifying interesting insights about the internal workings of these networks.",
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "referenceCount": 24,
    "citationCount": 117,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1696833",
        "name": "Kedar Dhamdhere"
      },
      {
        "authorId": "30740726",
        "name": "Mukund Sundararajan"
      },
      {
        "authorId": "34789908",
        "name": "Qiqi Yan"
      }
    ]
  },
  "211075890": {
    "paperId": "798ea191aad9401462b405fde1a6cefb4fe53fd5",
    "externalIds": {
      "DBLP": "journals/corr/abs-2002-04138",
      "MAG": "3005535506",
      "ArXiv": "2002.04138",
      "CorpusId": 211075890
    },
    "publicationVenue": {
      "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
      "name": "Journal of machine learning research",
      "type": "journal",
      "alternate_names": [
        "Journal of Machine Learning Research",
        "J mach learn res",
        "J Mach Learn Res"
      ],
      "issn": "1532-4435",
      "alternate_issns": [
        "1533-7928"
      ],
      "url": "http://www.ai.mit.edu/projects/jmlr/",
      "alternate_urls": [
        "http://jmlr.csail.mit.edu/",
        "http://www.jmlr.org/",
        "http://portal.acm.org/affiliated/jmlr"
      ]
    },
    "title": "Explaining Explanations: Axiomatic Feature Interactions for Deep Networks",
    "abstract": "Recent work has shown great promise in explaining neural network behavior. In particular, feature attribution methods explain which features were most important to a model's prediction on a given input. However, for many tasks, simply knowing which features were important to a model's prediction may not provide enough insight to understand model behavior. The interactions between features within the model may better help us understand not only the model, but also why certain features are more important than others. In this work, we present Integrated Hessians, an extension of Integrated Gradients that explains pairwise feature interactions in neural networks. Integrated Hessians overcomes several theoretical limitations of previous methods to explain interactions, and unlike such previous methods is not limited to a specific architecture or class of neural network. Additionally, we find that our method is faster than existing methods when the number of features is large, and outperforms previous methods on existing quantitative benchmarks. Code available at this https URL",
    "venue": "Journal of machine learning research",
    "year": 2020,
    "referenceCount": 124,
    "citationCount": 130,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51290559",
        "name": "Joseph D. Janizek"
      },
      {
        "authorId": "8575816",
        "name": "Pascal Sturmfels"
      },
      {
        "authorId": "2180463",
        "name": "Su-In Lee"
      }
    ]
  },
  "3144218": {
    "paperId": "36eff562f65125511b5dfab68ce7f7a943c27478",
    "externalIds": {
      "ArXiv": "1609.02907",
      "MAG": "2519887557",
      "DBLP": "journals/corr/KipfW16",
      "CorpusId": 3144218
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Semi-Supervised Classification with Graph Convolutional Networks",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "venue": "International Conference on Learning Representations",
    "year": 2016,
    "referenceCount": 38,
    "citationCount": 25886,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "41016725",
        "name": "Thomas Kipf"
      },
      {
        "authorId": "1678311",
        "name": "M. Welling"
      }
    ]
  },
  "8393918": {
    "paperId": "492f57ee9ceb61fb5a47ad7aebfec1121887a175",
    "externalIds": {
      "MAG": "2244807774",
      "ArXiv": "1511.05493",
      "DBLP": "journals/corr/LiTBZ15",
      "CorpusId": 8393918
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Gated Graph Sequence Neural Networks",
    "abstract": "Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
    "venue": "International Conference on Learning Representations",
    "year": 2015,
    "referenceCount": 39,
    "citationCount": 3116,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47002813",
        "name": "Yujia Li"
      },
      {
        "authorId": "1725299",
        "name": "Daniel Tarlow"
      },
      {
        "authorId": "2107692",
        "name": "Marc Brockschmidt"
      },
      {
        "authorId": "1804104",
        "name": "R. Zemel"
      }
    ]
  },
  "4755450": {
    "paperId": "6b7d6e6416343b2a122f8416e69059ce919026ef",
    "externalIds": {
      "DBLP": "conf/nips/HamiltonYL17",
      "MAG": "2952779545",
      "ArXiv": "1706.02216",
      "CorpusId": 4755450
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Inductive Representation Learning on Large Graphs",
    "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
    "venue": "Neural Information Processing Systems",
    "year": 2017,
    "referenceCount": 42,
    "citationCount": 13153,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49437682",
        "name": "William L. Hamilton"
      },
      {
        "authorId": "4058003",
        "name": "Z. Ying"
      },
      {
        "authorId": "1702139",
        "name": "J. Leskovec"
      }
    ]
  },
  "6206777": {
    "paperId": "2784000e1a3554374662f4d18cb5ad52f59c8de6",
    "externalIds": {
      "MAG": "2963653811",
      "ACL": "D17-1209",
      "ArXiv": "1704.04675",
      "DBLP": "journals/corr/BastingsTAMS17",
      "DOI": "10.18653/v1/D17-1209",
      "CorpusId": 6206777
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
    "abstract": "We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2017,
    "referenceCount": 50,
    "citationCount": 482,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3000862",
        "name": "Jasmijn Bastings"
      },
      {
        "authorId": "144889265",
        "name": "Ivan Titov"
      },
      {
        "authorId": "2782694",
        "name": "Wilker Aziz"
      },
      {
        "authorId": "2022957",
        "name": "Diego Marcheggiani"
      },
      {
        "authorId": "3540477",
        "name": "K. Sima'an"
      }
    ]
  },
  "199577786": {
    "paperId": "e47e6c814d2742527fdd352db13a5fd95b7ce24b",
    "externalIds": {
      "DBLP": "journals/corr/abs-1908-04942",
      "ArXiv": "1908.04942",
      "MAG": "2994900646",
      "CorpusId": 199577786
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation",
    "abstract": "Natural question generation (QG) aims to generate questions from a passage and an answer. Previous works on QG either (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information. To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq generator with a novel Bidirectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective combining both cross-entropy and RL losses to ensure the generation of syntactically and semantically valid text. We also introduce an effective Deep Alignment Network for incorporating the answer information into the passage at both the word and contextual levels. Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the standard SQuAD benchmark.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "referenceCount": 84,
    "citationCount": 143,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2144836395",
        "name": "Yu Chen"
      },
      {
        "authorId": "3008832",
        "name": "Lingfei Wu"
      },
      {
        "authorId": "1693515",
        "name": "Mohammed J. Zaki"
      }
    ]
  },
  "214003631": {
    "paperId": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
    "externalIds": {
      "ArXiv": "2006.13009",
      "MAG": "3101979678",
      "DBLP": "conf/nips/0022WZ20",
      "CorpusId": 214003631
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings",
    "abstract": "In this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph approaches close enough to the graph optimized for the prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 68,
    "citationCount": 355,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2144836395",
        "name": "Yu Chen"
      },
      {
        "authorId": "3008832",
        "name": "Lingfei Wu"
      },
      {
        "authorId": "1693515",
        "name": "Mohammed J. Zaki"
      }
    ]
  },
  "218486837": {
    "paperId": "9e979667aa81c294062c02ab3a48e87e47c54987",
    "externalIds": {
      "DBLP": "journals/corr/abs-2005-00646",
      "MAG": "3097986428",
      "ACL": "2020.emnlp-main.99",
      "ArXiv": "2005.00646",
      "DOI": "10.18653/v1/2020.emnlp-main.99",
      "CorpusId": 218486837
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering",
    "abstract": "Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model's prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 42,
    "citationCount": 219,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2115389088",
        "name": "Yanlin Feng"
      },
      {
        "authorId": "121022966",
        "name": "Xinyue Chen"
      },
      {
        "authorId": "51583409",
        "name": "Bill Yuchen Lin"
      },
      {
        "authorId": "2784644",
        "name": "Peifeng Wang"
      },
      {
        "authorId": "49781448",
        "name": "Jun Yan"
      },
      {
        "authorId": "1384550891",
        "name": "Xiang Ren"
      }
    ]
  },
  "4590511": {
    "paperId": "94eb48c1878efbe2ccff121bd600dd0fd8a75650",
    "externalIds": {
      "ArXiv": "1804.00823",
      "MAG": "2796167946",
      "DBLP": "journals/corr/abs-1804-00823",
      "CorpusId": 4590511
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Graph2Seq: Graph to Sequence Learning with Attention-based Neural Networks",
    "abstract": "The celebrated Sequence to Sequence learning (Seq2Seq) technique and its numerous variants achieve excellent performance on many tasks. However, many machine learning tasks have inputs naturally represented as graphs; existing Seq2Seq models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence. To address this challenge, we introduce a novel general end-to-end graph-to-sequence neural encoder-decoder model that maps an input graph to a sequence of vectors and uses an attention-based LSTM method to decode the target sequence from these vectors. Our method first generates the node and graph embeddings using an improved graph-based neural network with a novel aggregation strategy to incorporate edge direction information in the node embeddings. We further introduce an attention mechanism that aligns node embeddings and the decoding sequence to better cope with large graphs. Experimental results on bAbI, Shortest Path, and Natural Language Generation tasks demonstrate that our model achieves state-of-the-art performance and significantly outperforms existing graph neural networks, Seq2Seq, and Tree2Seq models; using the proposed bi-directional node embedding aggregation strategy, the model can converge rapidly to the optimal performance.",
    "venue": "arXiv.org",
    "year": 2018,
    "referenceCount": 70,
    "citationCount": 161,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "151485141",
        "name": "Kun Xu"
      },
      {
        "authorId": "3008832",
        "name": "Lingfei Wu"
      },
      {
        "authorId": "40296541",
        "name": "Zhiguo Wang"
      },
      {
        "authorId": "1717629",
        "name": "Yansong Feng"
      },
      {
        "authorId": "1757683",
        "name": "V. Sheinin"
      }
    ]
  },
  "215745291": {
    "paperId": "d365f9c805d59788f9ae5ad36fee69f9abd8d3c7",
    "externalIds": {
      "DBLP": "journals/tnn/ChenWZ24",
      "ArXiv": "2004.06015",
      "MAG": "3015927585",
      "DOI": "10.1109/TNNLS.2023.3264519",
      "CorpusId": 215745291,
      "PubMed": "37093721"
    },
    "publicationVenue": {
      "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
      "name": "IEEE Transactions on Neural Networks and Learning Systems",
      "alternate_names": [
        "IEEE Trans Neural Netw Learn Syst"
      ],
      "issn": "2162-237X",
      "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
      "alternate_urls": [
        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
      ]
    },
    "title": "Toward Subgraph-Guided Knowledge Graph Question Generation With Graph Neural Networks",
    "abstract": "Knowledge graph (KG) question generation (QG) aims to generate natural language questions from KGs and target answers. Previous works mostly focus on a simple setting that is to generate questions from a single KG triple. In this work, we focus on a more realistic setting where we aim to generate questions from a KG subgraph and target answers. In addition, most previous works built on either RNN- or Transformer-based models to encode a linearized KG subgraph, which totally discards the explicit structure information of a KG subgraph. To address this issue, we propose to apply a bidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we enhance our RNN decoder with a node-level copying mechanism to allow direct copying of node attributes from the KG subgraph to the output question. Both automatic and human evaluation results demonstrate that our model achieves new state-of-the-art scores, outperforming existing methods by a significant margin on two QG benchmarks. Experimental results also show that our QG model can consistently benefit the question-answering (QA) task as a means of data augmentation.",
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "year": 2020,
    "referenceCount": 126,
    "citationCount": 36,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2144836395",
        "name": "Yu Chen"
      },
      {
        "authorId": "3008832",
        "name": "Lingfei Wu"
      },
      {
        "authorId": "1693515",
        "name": "Mohammed J. Zaki"
      }
    ]
  },
  "216642054": {
    "paperId": "f1aef5403012d2a70344bc70d58d720aef85834c",
    "externalIds": {
      "ArXiv": "2004.13781",
      "MAG": "3021288808",
      "DBLP": "conf/emnlp/LiWFXXZ20",
      "ACL": "2020.findings-emnlp.255",
      "DOI": "10.18653/v1/2020.findings-emnlp.255",
      "CorpusId": 216642054
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem",
    "abstract": "The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as sequences while ignoring the important structural information for encoding, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our model for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.",
    "venue": "Findings",
    "year": 2020,
    "referenceCount": 56,
    "citationCount": 68,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2109104122",
        "name": "Shucheng Li"
      },
      {
        "authorId": "3008832",
        "name": "Lingfei Wu"
      },
      {
        "authorId": "2188773882",
        "name": "Shiwei Feng"
      },
      {
        "authorId": "2392383",
        "name": "Fangli Xu"
      },
      {
        "authorId": "1741521",
        "name": "Fengyuan Xu"
      },
      {
        "authorId": "2053863706",
        "name": "Sheng Zhong"
      }
    ]
  },
  "6857205": {
    "paperId": "455afd748e8834ef521e4b67c7c056d3c33429e2",
    "externalIds": {
      "DBLP": "conf/naacl/YangYDHSH16",
      "ACL": "N16-1174",
      "MAG": "2470673105",
      "DOI": "10.18653/v1/N16-1174",
      "CorpusId": 6857205
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Hierarchical Attention Networks for Document Classification",
    "abstract": "We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 42,
    "citationCount": 4302,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8387085",
        "name": "Zichao Yang"
      },
      {
        "authorId": "2022168",
        "name": "Diyi Yang"
      },
      {
        "authorId": "1745899",
        "name": "Chris Dyer"
      },
      {
        "authorId": "144137069",
        "name": "Xiaodong He"
      },
      {
        "authorId": "46234526",
        "name": "Alex Smola"
      },
      {
        "authorId": "144547315",
        "name": "E. Hovy"
      }
    ]
  },
  "207853300": {
    "paperId": "2c88d7486f9871cb741ba3c7076b8adbb7fd5b68",
    "externalIds": {
      "ArXiv": "1911.03631",
      "DBLP": "journals/corr/abs-1911-03631",
      "ACL": "2020.emnlp-main.710",
      "MAG": "2989536007",
      "DOI": "10.18653/v1/2020.emnlp-main.710",
      "CorpusId": 207853300
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Hierarchical Graph Network for Multi-hop Question Answering",
    "abstract": "In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes from different levels of granularity (questions, paragraphs, sentences, and entities), the representations of which are initialized with RoBERTa-based context encoders. Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction). By weaving heterogeneous nodes into an integral unified graph, this characteristic hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art in both the Distractor and Fullwiki settings.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 48,
    "citationCount": 164,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51444591",
        "name": "Yuwei Fang"
      },
      {
        "authorId": "2419809",
        "name": "S. Sun"
      },
      {
        "authorId": "144702900",
        "name": "Zhe Gan"
      },
      {
        "authorId": "2128387353",
        "name": "R. Pillai"
      },
      {
        "authorId": "2992833",
        "name": "Shuohang Wang"
      },
      {
        "authorId": "46700348",
        "name": "Jingjing Liu"
      }
    ]
  },
  "221702858": {
    "paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
    "externalIds": {
      "ArXiv": "2009.06732",
      "DBLP": "journals/csur/TayDBM23",
      "MAG": "3085139254",
      "DOI": "10.1145/3530811",
      "CorpusId": 221702858
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "Efficient Transformers: A Survey",
    "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \u201cX-former\u201d models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains.",
    "venue": "ACM Computing Surveys",
    "year": 2020,
    "referenceCount": 105,
    "citationCount": 976,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144447820",
        "name": "Yi Tay"
      },
      {
        "authorId": "3226635",
        "name": "Mostafa Dehghani"
      },
      {
        "authorId": "11774695",
        "name": "Dara Bahri"
      },
      {
        "authorId": "1680617",
        "name": "Donald Metzler"
      }
    ]
  },
  "202541012": {
    "paperId": "5665805becad6c87b194b260f2270d86d560bd3f",
    "externalIds": {
      "ArXiv": "1909.03186",
      "ACL": "2020.emnlp-main.748",
      "MAG": "2972114612",
      "DBLP": "conf/emnlp/PilaultLSP20",
      "DOI": "10.18653/v1/2020.emnlp-main.748",
      "CorpusId": 202541012
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models",
    "abstract": "We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work. Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 49,
    "citationCount": 189,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "104354626",
        "name": "Jonathan Pilault"
      },
      {
        "authorId": "144235909",
        "name": "Raymond Li"
      },
      {
        "authorId": "50324141",
        "name": "Sandeep Subramanian"
      },
      {
        "authorId": "1972076",
        "name": "C. Pal"
      }
    ]
  },
  "45813168": {
    "paperId": "c80c7ab615b2fad5148a7848dbdd26a2dc50dd3d",
    "externalIds": {
      "MAG": "9014458",
      "DOI": "10.2307/2346806",
      "CorpusId": 45813168
    },
    "publicationVenue": null,
    "title": "Maximum Likelihood Estimation of Observer Error\u2010Rates Using the EM Algorithm",
    "abstract": "In compiling a patient record many facets are subject to errors of measurement. A model is presented which allows individual error-rates to be estimated for polytomous facets even when the patient's \"true\" response is not available. The EM algorithm is shown to provide a slow but sure way of obtaining maximum likelihood estimates of the parameters of interest. Some preliminary experience is reported and the limitations of the method are described.",
    "venue": "",
    "year": 1979,
    "referenceCount": 7,
    "citationCount": 1773,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144845491",
        "name": "A. Dawid"
      },
      {
        "authorId": "1909853",
        "name": "A. Skene"
      }
    ]
  },
  "207908258": {
    "paperId": "12584bd527408145b1a1d6b9489c49710ff3d737",
    "externalIds": {
      "ACL": "D19-5904",
      "MAG": "2983464073",
      "DOI": "10.18653/v1/D19-5904",
      "CorpusId": 207908258
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "A Dataset of Crowdsourced Word Sequences: Collections and Answer Aggregation for Ground Truth Creation",
    "abstract": "The target outputs of many NLP tasks are word sequences. To collect the data for training and evaluating models, the crowd is a cheaper and easier to access than the oracle. To ensure the quality of the crowdsourced data, people can assign multiple workers to one question and then aggregate the multiple answers with diverse quality into a golden one. How to aggregate multiple crowdsourced word sequences with diverse quality is a curious and challenging problem. People need a dataset for addressing this problem. We thus create a dataset (CrowdWSA2019) which contains the translated sentences generated from multiple workers. We provide three approaches as the baselines on the task of extractive word sequence aggregation. Specially, one of them is an original one we propose which models the reliability of workers. We also discuss some issues on ground truth creation of word sequences which can be addressed based on this dataset.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 20,
    "citationCount": 13,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40340606",
        "name": "Jiyi Li"
      },
      {
        "authorId": "3029852",
        "name": "Fumiyo Fukumoto"
      }
    ]
  },
  "53228157": {
    "paperId": "50f34fed4cd6cfa761efc0a9ca12bf75d799cc8e",
    "externalIds": {
      "DBLP": "conf/uist/BernsteinLMHAKCP10",
      "MAG": "2234829506",
      "DOI": "10.1145/1866029.1866078",
      "CorpusId": 53228157
    },
    "publicationVenue": {
      "id": "c62b1316-0733-4b4c-8017-c07e18afa954",
      "name": "ACM Symposium on User Interface Software and Technology",
      "type": "conference",
      "alternate_names": [
        "User Interface Software and Technology",
        "ACM Symp User Interface Softw Technol",
        "User Interface Softw Technol",
        "UIST"
      ],
      "url": "http://www.acm.org/uist/"
    },
    "title": "Soylent: a word processor with a crowd inside",
    "abstract": "This paper introduces architectural and interaction patterns for integrating crowdsourced human contributions directly into user interfaces. We focus on writing and editing, complex endeavors that span many levels of conceptual and pragmatic activity. Authoring tools offer help with pragmatics, but for higher-level help, writers commonly turn to other people. We thus present Soylent, a word processing interface that enables writers to call on Mechanical Turk workers to shorten, proofread, and otherwise edit parts of their documents on demand. To improve worker quality, we introduce the Find-Fix-Verify crowd programming pattern, which splits tasks into a series of generation and review stages. Evaluation studies demonstrate the feasibility of crowdsourced editing and investigate questions of reliability, cost, wait time, and work time for edits.",
    "venue": "ACM Symposium on User Interface Software and Technology",
    "year": 2010,
    "referenceCount": 38,
    "citationCount": 926,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145879842",
        "name": "Michael S. Bernstein"
      },
      {
        "authorId": "48155668",
        "name": "Greg Little"
      },
      {
        "authorId": "152160465",
        "name": "Rob Miller"
      },
      {
        "authorId": "28226629",
        "name": "Bjoern Hartmann"
      },
      {
        "authorId": "1797833",
        "name": "M. Ackerman"
      },
      {
        "authorId": "1743286",
        "name": "David R Karger"
      },
      {
        "authorId": "144141670",
        "name": "David Crowell"
      },
      {
        "authorId": "1814699",
        "name": "Katrina Panovich"
      }
    ]
  },
  "263898183": {
    "paperId": "92f24748ecb81af9220e6305dd23f16e4570e303",
    "externalIds": {
      "ACL": "W10-0701",
      "DBLP": "conf/naacl/Callison-BurchD10",
      "MAG": "2127849236",
      "CorpusId": 263898183
    },
    "publicationVenue": null,
    "title": "Creating Speech and Language Data With Amazon\u2019s Mechanical Turk",
    "abstract": "In this paper we give an introduction to using Amazon's Mechanical Turk crowdsourcing platform for the purpose of collecting data for human language technologies. We survey the papers published in the NAACL-2010 Workshop. 24 researchers participated in the workshop's shared task to create data for speech and language applications with $100.",
    "venue": "Mturk@HLT-NAACL",
    "year": 2010,
    "referenceCount": 65,
    "citationCount": 202,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1763608",
        "name": "Chris Callison-Burch"
      },
      {
        "authorId": "1478928280",
        "name": "Mark Dredze"
      }
    ]
  },
  "6837877": {
    "paperId": "bc556572a30553cffb4f80263573e6c2d7c2e3d7",
    "externalIds": {
      "DBLP": "journals/lre/Biemann13",
      "MAG": "2078277979",
      "DOI": "10.1007/s10579-012-9180-5",
      "CorpusId": 6837877
    },
    "publicationVenue": {
      "id": "7dda5bd1-752f-45e5-bc7b-09633096916e",
      "name": "Language Resources and Evaluation",
      "type": "journal",
      "alternate_names": [
        "Lang Resour Evaluation"
      ],
      "issn": "1574-020X",
      "url": "https://link.springer.com/journal/10579",
      "alternate_urls": [
        "http://www.jstor.org/action/showPublication?journalCode=langresoeval"
      ]
    },
    "title": "Creating a system for lexical substitutions from scratch using crowdsourcing",
    "abstract": null,
    "venue": "Language Resources and Evaluation",
    "year": 2012,
    "referenceCount": 40,
    "citationCount": 69,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "31565315",
        "name": "Chris Biemann"
      }
    ]
  },
  "7008675": {
    "paperId": "0165568bcc1a819c18564567f2ec15d859be2519",
    "externalIds": {
      "ACL": "D08-1027",
      "DBLP": "conf/emnlp/SnowOJN08",
      "MAG": "1970381522",
      "DOI": "10.3115/1613715.1613751",
      "CorpusId": 7008675
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Cheap and Fast \u2013 But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks",
    "abstract": "Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2008,
    "referenceCount": 35,
    "citationCount": 2323,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144621026",
        "name": "R. Snow"
      },
      {
        "authorId": "1401020033",
        "name": "Brendan T. O'Connor"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      },
      {
        "authorId": "34699434",
        "name": "A. Ng"
      }
    ]
  },
  "8924783": {
    "paperId": "9005cc8e33e2cf875588e5d1225c8b9e3f300a57",
    "externalIds": {
      "MAG": "2260880659",
      "CorpusId": 8924783
    },
    "publicationVenue": null,
    "title": "Quality-Based Pricing for Crowdsourced Workers",
    "abstract": "The emergence of online paid crowdsourcing platforms, such as Amazon Mechanical Turk (AMT), presents us huge opportunities to distribute tasks to human workers around the world, on-demand and at scale. In such settings, online workers can come and complete tasks posted by a company, and work for as long or as little as they wish. Given the absolute freedom of choice, crowdsourcing eliminates the overhead of the hiring (and dismissal) process. However, this exibility introduces a di erent set of ine ciencies: verifying the quality of every submitted piece of work is an expensive operation, which often requires the same level of e ort as performing the task itself. There are many research challenges that emerge in this paid-crowdsourcing setting. How can we ensure that the submitted work is accurate? How can we estimate the quality of the workers, and the quality of the submitted results? How should we pay online workers that have imperfect quality? We present a comprehensive scheme for managing quality of crowdsourcing processes: First, we present an algorithm for estimating the quality of the participating workers and, by extension, of the generated data. We show how we can separate systematic worker biases from unrecoverable errors and how to generate an unbiased \"worker quality\" measurement that can be used to objectively rank workers according to their performance. Next, we describe a pricing scheme that identi es the fair payment level for a worker, adjusting the payment level according to the contributed information by each worker. Our pricing policy, which pays workers based on their expected quality, reservation wage, and expected lifetime, estimates not only 1 the payment level but also accommodates measurement uncertainties and allows the workers to receive a fair wage, even in the presence of temporary incorrect estimations of quality. Our experimental results demonstrate that the proposed pricing strategy performs better than the commonly adopted uniform-pricing strategy. We conclude the paper by describing strategies that build on our quality control and pricing framework, to build crowdsourced tasks of increasingly higher complexity, while still maintaining a tight quality control of the process, even if we allow participants of unknown quality to join the process.",
    "venue": "",
    "year": 2013,
    "referenceCount": 68,
    "citationCount": 47,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Engineering",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Economics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2152453484",
        "name": "Jing Wang"
      },
      {
        "authorId": "2942126",
        "name": "Panagiotis G. Ipeirotis"
      },
      {
        "authorId": "1752722",
        "name": "F. Provost"
      }
    ]
  },
  "225062337": {
    "paperId": "455cdafd55a5b5ddefa029bf97801327e142646d",
    "externalIds": {
      "DBLP": "conf/naacl/HedderichLASK21",
      "MAG": "3094140582",
      "ACL": "2021.naacl-main.201",
      "ArXiv": "2010.12309",
      "DOI": "10.18653/V1/2021.NAACL-MAIN.201",
      "CorpusId": 225062337
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios",
    "abstract": "Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 254,
    "citationCount": 246,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51133383",
        "name": "Michael A. Hedderich"
      },
      {
        "authorId": "47665464",
        "name": "Lukas Lange"
      },
      {
        "authorId": "145793834",
        "name": "Heike Adel"
      },
      {
        "authorId": "2013656",
        "name": "Jannik Strotgen"
      },
      {
        "authorId": "2561225",
        "name": "D. Klakow"
      }
    ]
  },
  "243865588": {
    "paperId": "84aec29de31b56b3324c00667dfac62850f8dadf",
    "externalIds": {
      "DBLP": "conf/emnlp/HuangLSJBCPG021",
      "ACL": "2021.emnlp-main.813",
      "DOI": "10.18653/v1/2021.emnlp-main.813",
      "CorpusId": 243865588
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Few-Shot Named Entity Recognition: An Empirical Baseline Study",
    "abstract": "This paper presents an empirical study to efficiently build named entity recognition (NER) systems when a small amount of in-domain labeled data is available. Based upon recent Transformer-based self-supervised pre-trained language models (PLMs), we investigate three orthogonal schemes to improve model generalization ability in few-shot settings: (1) meta-learning to construct prototypes for different entity types, (2) task-specific supervised pre-training on noisy web data to extract entity-related representations and (3) self-training to leverage unlabeled in-domain data. On 10 public NER datasets, we perform extensive empirical comparisons over the proposed schemes and their combinations with various proportions of labeled data, our experiments show that (i)in the few-shot learning setting, the proposed NER schemes significantly improve or outperform the commonly used baseline, a PLM-based linear classifier fine-tuned using domain labels. (ii) We create new state-of-the-art results on both few-shot and training-free settings compared with existing methods.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 73,
    "citationCount": 85,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3488341",
        "name": "Jiaxin Huang"
      },
      {
        "authorId": "2109737569",
        "name": "Chunyuan Li"
      },
      {
        "authorId": "2043231778",
        "name": "K. Subudhi"
      },
      {
        "authorId": "144430856",
        "name": "Damien Jose"
      },
      {
        "authorId": "2071648958",
        "name": "S. Balakrishnan"
      },
      {
        "authorId": "2109136147",
        "name": "Weizhu Chen"
      },
      {
        "authorId": "1780690",
        "name": "Baolin Peng"
      },
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "153034701",
        "name": "Jiawei Han"
      }
    ]
  },
  "246867025": {
    "paperId": "aeb8344608e4f89bca8f508831ef24b64ec01e9a",
    "externalIds": {
      "ArXiv": "2202.08063",
      "CorpusId": 246867025
    },
    "publicationVenue": null,
    "title": "Information Extraction in Low-Resource Scenarios: Survey and Perspective",
    "abstract": "Information Extraction (IE) seeks to derive structured information from unstructured texts, often facing challenges in low-resource scenarios due to data scarcity and unseen classes. This paper presents a review of neural approaches to low-resource IE from \\emph{traditional} and \\emph{LLM-based} perspectives, systematically categorizing them into a fine-grained taxonomy. Then we conduct empirical study on LLM-based methods compared with previous state-of-the-art models, and discover that (1) well-tuned LMs are still predominant; (2) tuning open-resource LLMs and ICL with GPT family is promising in general; (3) the optimal LLM-based technical solution for low-resource IE can be task-dependent. In addition, we discuss low-resource IE with LLMs, highlight promising applications, and outline potential research directions. This survey aims to foster understanding of this field, inspire new ideas, and encourage widespread applications in both academia and industry.",
    "venue": "",
    "year": 2022,
    "referenceCount": 257,
    "citationCount": 4,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "152931849",
        "name": "Shumin Deng"
      },
      {
        "authorId": "2153010067",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "2155551120",
        "name": "Hui Chen"
      },
      {
        "authorId": "2068169902",
        "name": "Feiyu Xiong"
      },
      {
        "authorId": "9416872",
        "name": "Jeff Z. Pan"
      },
      {
        "authorId": "49178307",
        "name": "Huajun Chen"
      }
    ]
  },
  "226262304": {
    "paperId": "7db87539bcaed817c820c4e0c0855ec5fb24344c",
    "externalIds": {
      "DBLP": "conf/emnlp/GuiYZLFGH20",
      "MAG": "3098383912",
      "ACL": "2020.emnlp-main.181",
      "ArXiv": "2012.10608",
      "DOI": "10.18653/v1/2020.emnlp-main.181",
      "CorpusId": 226262304
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Uncertainty-Aware Label Refinement for Sequence Labeling",
    "abstract": "Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks. However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved. In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies, while being much more computationally efficient. A base model first predicts draft labels, and then a novel two-stream self-attention model makes refinements on these draft predictions based on long-range label dependencies, which can achieve parallel decoding for a faster prediction. In addition, in order to mitigate the side effects of incorrect draft labels, Bayesian neural networks are used to indicate the labels with a high probability of being wrong, which can greatly assist in preventing error propagation. The experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also greatly accelerated the inference process.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 40,
    "citationCount": 25,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2067331064",
        "name": "Tao Gui"
      },
      {
        "authorId": "65846898",
        "name": "Jiacheng Ye"
      },
      {
        "authorId": "1409702669",
        "name": "Qi Zhang"
      },
      {
        "authorId": "2145371018",
        "name": "Zhengyan Li"
      },
      {
        "authorId": "1411255713",
        "name": "Zichu Fei"
      },
      {
        "authorId": "2171182",
        "name": "Yeyun Gong"
      },
      {
        "authorId": "1790227",
        "name": "Xuanjing Huang"
      }
    ]
  },
  "218613850": {
    "paperId": "013faec0400d315935e71a2bdfeb22cc83752b3e",
    "externalIds": {
      "MAG": "3035053871",
      "DBLP": "journals/corr/abs-2005-06312",
      "ArXiv": "2005.06312",
      "ACL": "2020.acl-main.141",
      "DOI": "10.18653/v1/2020.acl-main.141",
      "CorpusId": 218613850
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Reasoning with Latent Structure Refinement for Document-Level Relation Extraction",
    "abstract": "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 65,
    "citationCount": 251,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2056582888",
        "name": "Guoshun Nan"
      },
      {
        "authorId": "2681038",
        "name": "Zhijiang Guo"
      },
      {
        "authorId": "3305422",
        "name": "Ivan Sekulic"
      },
      {
        "authorId": "2153424287",
        "name": "Wei Lu"
      }
    ]
  },
  "237295186": {
    "paperId": "1a2e90dff605dad7dbefeed121e6d295c7a77d62",
    "externalIds": {
      "DBLP": "conf/www/ChenZXDYTHSC22",
      "ArXiv": "2104.07650",
      "DOI": "10.1145/3485447.3511998",
      "CorpusId": 237295186
    },
    "publicationVenue": {
      "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
      "name": "The Web Conference",
      "type": "conference",
      "alternate_names": [
        "Web Conf",
        "WWW"
      ],
      "url": "http://www.iw3c2.org/"
    },
    "title": "KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction",
    "abstract": "Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in GitHub1 for reproducibility.",
    "venue": "The Web Conference",
    "year": 2021,
    "referenceCount": 92,
    "citationCount": 350,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2143735911",
        "name": "Xiang Chen"
      },
      {
        "authorId": "2608639",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "2153010067",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "2110972563",
        "name": "Xin Xie"
      },
      {
        "authorId": "152931849",
        "name": "Shumin Deng"
      },
      {
        "authorId": "4841460",
        "name": "Yunzhi Yao"
      },
      {
        "authorId": "2111727840",
        "name": "Chuanqi Tan"
      },
      {
        "authorId": "2087380523",
        "name": "Fei Huang"
      },
      {
        "authorId": "2059080424",
        "name": "Luo Si"
      },
      {
        "authorId": "49178307",
        "name": "Huajun Chen"
      }
    ]
  },
  "49313245": {
    "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
    "externalIds": {
      "MAG": "2965425874",
      "CorpusId": 49313245
    },
    "publicationVenue": null,
    "title": "Improving Language Understanding by Generative Pre-Training",
    "abstract": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classi\ufb01cation. Although large unlabeled text corpora are abundant, labeled data for learning these speci\ufb01c tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative \ufb01ne-tuning on each speci\ufb01c task. In contrast to previous approaches, we make use of task-aware input transformations during \ufb01ne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speci\ufb01cally crafted for each task, signi\ufb01cantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).",
    "venue": "",
    "year": 2018,
    "referenceCount": 73,
    "citationCount": 10115,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "38909097",
        "name": "Alec Radford"
      },
      {
        "authorId": "144958935",
        "name": "Karthik Narasimhan"
      }
    ]
  },
  "160543569": {
    "paperId": "ef8d3369f0d5d78f1c0989e2dd59d5ca8f045441",
    "externalIds": {
      "MAG": "656085213",
      "CorpusId": 160543569
    },
    "publicationVenue": null,
    "title": "Cantonese as Written Language: The Growth of a Written Chinese Vernacular",
    "abstract": "List of Illustrations and TablesPreface 1. Why Study the Development of Written Cantonese? 2. From Spoken Vernacular to Written Language 3. Spoken and Written Cantonese 4. Written Cantonese in Pre-modern Guangdong 5. The Hong Kong Dialect Literature Movement 6. Written Cantonese in Modern Hong Kong 7. Why Has Use of Written Cantonese Increased? 8. Epilogue: The Future of Written Cantonese Appendix 1: Cantonese Texts Appendix 2: Interviews and Public Lectures Appendix 3: Titles of Publications and Published Works Appendix 4: Characters for Chinese Terms Notes References Index",
    "venue": "",
    "year": 2004,
    "referenceCount": 0,
    "citationCount": 118,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "History",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "History",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "66196923",
        "name": "Don Snow"
      }
    ]
  },
  "144678737": {
    "paperId": "484cdbf507139714232d40a6139b17dc72c7919f",
    "externalIds": {
      "MAG": "2056183950",
      "DOI": "10.1515/MULTI.2007.004",
      "CorpusId": 144678737
    },
    "publicationVenue": null,
    "title": "Cantonese as an additional language in Hong Kong: Problems and prospects",
    "abstract": "Abstract Based on data obtained from a questionnaire survey and in-depth interviews with four Caucasians and four dark-skinned Asians, this study shows that while some \u2018foreigners\u2019 do make an effort to learn Cantonese, many find the teaching methods not so useful and the language difficult to master, especially its tone system. The data are analyzed following the interactive multicultural model of acculturation. The findings point toward a huge chasm between non-local groups and the Cantonese-speaking community. The receptivity of Hong Kong Chinese towards attempts by members of non-local groups to speak Cantonese varies, depending on their racial identity and socioeconomic status.",
    "venue": "",
    "year": 2007,
    "referenceCount": 59,
    "citationCount": 12,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Sociology",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "71255084",
        "name": "G. Sachs"
      },
      {
        "authorId": "1575956608",
        "name": "David C. S. Li"
      }
    ]
  },
  "198953378": {
    "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
    "externalIds": {
      "DBLP": "journals/corr/abs-1907-11692",
      "MAG": "2965373594",
      "ArXiv": "1907.11692",
      "CorpusId": 198953378
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
    "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
    "venue": "arXiv.org",
    "year": 2019,
    "referenceCount": 68,
    "citationCount": 21568,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "11323179",
        "name": "Yinhan Liu"
      },
      {
        "authorId": "40511414",
        "name": "Myle Ott"
      },
      {
        "authorId": "39589154",
        "name": "Naman Goyal"
      },
      {
        "authorId": "3048577",
        "name": "Jingfei Du"
      },
      {
        "authorId": "144863691",
        "name": "Mandar Joshi"
      },
      {
        "authorId": "50536468",
        "name": "Danqi Chen"
      },
      {
        "authorId": "39455775",
        "name": "Omer Levy"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      },
      {
        "authorId": "1759422",
        "name": "Veselin Stoyanov"
      }
    ]
  },
  "125977708": {
    "paperId": "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
    "externalIds": {
      "DBLP": "journals/corr/abs-1904-09223",
      "ArXiv": "1904.09223",
      "MAG": "2938830017",
      "CorpusId": 125977708
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "ERNIE: Enhanced Representation through Knowledge Integration",
    "abstract": "We present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.",
    "venue": "arXiv.org",
    "year": 2019,
    "referenceCount": 23,
    "citationCount": 825,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2117103617",
        "name": "Yu Sun"
      },
      {
        "authorId": "104463827",
        "name": "Shuohuan Wang"
      },
      {
        "authorId": "1710861",
        "name": "Yukun Li"
      },
      {
        "authorId": "1718657",
        "name": "Shikun Feng"
      },
      {
        "authorId": "2109214103",
        "name": "Xuyi Chen"
      },
      {
        "authorId": "48213346",
        "name": "Han Zhang"
      },
      {
        "authorId": "2117127187",
        "name": "Xin Tian"
      },
      {
        "authorId": "152867082",
        "name": "Danxiang Zhu"
      },
      {
        "authorId": "50007795",
        "name": "Hao Tian"
      },
      {
        "authorId": "40354707",
        "name": "Hua Wu"
      }
    ]
  },
  "218719869": {
    "paperId": "72cdd6ebe0221fb568ef20534f44ba5b35190a56",
    "externalIds": {
      "MAG": "3027440908",
      "ACL": "2020.emnlp-demos.2",
      "ArXiv": "2005.10200",
      "DBLP": "conf/emnlp/NguyenVN20",
      "DOI": "10.18653/v1/2020.emnlp-demos.2",
      "CorpusId": 218719869
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "BERTweet: A pre-trained language model for English Tweets",
    "abstract": "We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 41,
    "citationCount": 812,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34691913",
        "name": "Dat Quoc Nguyen"
      },
      {
        "authorId": "143768607",
        "name": "Thanh Vu"
      },
      {
        "authorId": "1398541475",
        "name": "A. Nguyen"
      }
    ]
  },
  "7771402": {
    "paperId": "e72e5ee5de14fd463ab58ce830474157258e3578",
    "externalIds": {
      "MAG": "2252123671",
      "ACL": "W13-2322",
      "DBLP": "conf/acllaw/BanarescuBCGGHK13",
      "CorpusId": 7771402
    },
    "publicationVenue": null,
    "title": "Abstract Meaning Representation for Sembanking",
    "abstract": "We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.",
    "venue": "LAW@ACL",
    "year": 2013,
    "referenceCount": 27,
    "citationCount": 1375,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Law",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3460261",
        "name": "L. Banarescu"
      },
      {
        "authorId": "3202888",
        "name": "C. Bonial"
      },
      {
        "authorId": "2112618394",
        "name": "Shu Cai"
      },
      {
        "authorId": "2065872210",
        "name": "Madalina Georgescu"
      },
      {
        "authorId": "3168985",
        "name": "Kira Griffitt"
      },
      {
        "authorId": "1791311",
        "name": "U. Hermjakob"
      },
      {
        "authorId": "152971314",
        "name": "Kevin Knight"
      },
      {
        "authorId": "1755162",
        "name": "Philipp Koehn"
      },
      {
        "authorId": "145755155",
        "name": "Martha Palmer"
      },
      {
        "authorId": "145254207",
        "name": "Nathan Schneider"
      }
    ]
  },
  "5271395": {
    "paperId": "e867a965033a074e4074875e0916ce1ca42f3bf6",
    "externalIds": {
      "MAG": "2568471475",
      "DOI": "10.1007/S11168-006-6327-9",
      "CorpusId": 5271395
    },
    "publicationVenue": null,
    "title": "Minimal Recursion Semantics: An Introduction",
    "abstract": null,
    "venue": "",
    "year": 2005,
    "referenceCount": 54,
    "citationCount": 1229,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "15379653",
        "name": "Ann A. Copestake"
      },
      {
        "authorId": "3209288",
        "name": "D. Flickinger"
      },
      {
        "authorId": "144741427",
        "name": "C. Pollard"
      },
      {
        "authorId": "2393013",
        "name": "I. Sag"
      }
    ]
  },
  "61979056": {
    "paperId": "3e8fb256977dca342ef4e86ac4218abee8aa0d03",
    "externalIds": {
      "MAG": "2135007322",
      "DOI": "10.1002/9781118882139.CH12",
      "CorpusId": 61979056
    },
    "publicationVenue": null,
    "title": "Type Theory with Records for Natural Language Semantics",
    "abstract": "Semantic analysis of interaction and coordination in dialogue (SAICD).",
    "venue": "",
    "year": 2015,
    "referenceCount": 51,
    "citationCount": 68,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46887822",
        "name": "R. Cooper"
      },
      {
        "authorId": "2055207",
        "name": "J. Ginzburg"
      }
    ]
  },
  "263897619": {
    "paperId": "0e6967076a5bf740ba168c82886fc09c9c113788",
    "externalIds": {
      "DBLP": "journals/jsemantics/LascaridesS09",
      "MAG": "2150421899",
      "DOI": "10.1093/jos/ffp004",
      "CorpusId": 263897619
    },
    "publicationVenue": {
      "id": "a986ba6e-51cb-478a-967c-8802bafdcc16",
      "name": "Journal of Semantics",
      "type": "journal",
      "alternate_names": [
        "J Semant"
      ],
      "issn": "0167-5133",
      "url": "https://academic.oup.com/jos/issue",
      "alternate_urls": [
        "http://jos.oxfordjournals.org/"
      ]
    },
    "title": "A Formal Semantic Analysis of Gesture",
    "abstract": "The gestures that speakers use in tandem with speech include not only conventionalized actions with identifiable meanings (so-called narrow gloss gestures or emblems) but also productive iconic and deictic gestures whose form and meanings seem largely improvised in context. In this paper, we bridge the descriptive tradition with formal models of reference and discourse structure so as to articulate an approach to the interpretation of these productive gestures. Our model captures gestures' partial and incomplete meanings as derived from form and accounts for the more specific interpretations they derive in context. Our work emphasizes the commonality of the pragmatic mechanisms for interpreting both language and gesture, and the place of formal methods in discovering the principles and knowledge that those mechanisms rely on.",
    "venue": "Journal of Semantics",
    "year": 2009,
    "referenceCount": 104,
    "citationCount": 161,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Sociology",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1876168",
        "name": "A. Lascarides"
      },
      {
        "authorId": "2257292206",
        "name": "Matthew Stone"
      }
    ]
  },
  "216951354": {
    "paperId": "78ce048d39a088554a655e45670cb47066b4551c",
    "externalIds": {
      "DBLP": "journals/jlm/AlahverdzhievaL17",
      "MAG": "2752713499",
      "DOI": "10.15398/jlm.v5i3.167",
      "CorpusId": 216951354
    },
    "publicationVenue": {
      "id": "ebf61c5b-9177-46fb-8aee-20397ea9ca53",
      "name": "Journal of Language Modelling",
      "type": "journal",
      "alternate_names": [
        "J Lang Model"
      ],
      "issn": "2299-8470",
      "url": "http://jlm.ipipan.waw.pl/",
      "alternate_urls": [
        "http://jlm.ipipan.waw.pl/index.php/JLM"
      ]
    },
    "title": "Aligning speech and co-speech gesture in a constraint-based grammar",
    "abstract": "This paper concerns the form-meaning mapping of communicative actions consisting of speech and improvised co-speech gestures. Based on the findings of previous cognitive and computational approaches, we advance a new theory in which this form-meaning mapping is analysed in a constraint-based grammar. Motivated by observations in naturally occurring examples, we propose several construction rules, which use linguistic form, gesture form and their relative timing to constrain the derivation of a single speech-gesture syntax tree, from which a meaning representation can be composed via standard methods for semantic composition. The paper further reports on implementing these speech-gesture construction rules within the English Resource Grammar (Copestake\u00a0and Flickinger 2000). Since gestural form often underspecifies its meaning, the logical formulae that are composed via syntax are underspecified so that current models of the semantics/pragmatics interface support the range of possible interpretations of the speech-gesture act in its context of use.",
    "venue": "Journal of Language Modelling",
    "year": 2018,
    "referenceCount": 61,
    "citationCount": 18,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2743841",
        "name": "K. Alahverdzhieva"
      },
      {
        "authorId": "1876168",
        "name": "A. Lascarides"
      },
      {
        "authorId": "3209288",
        "name": "D. Flickinger"
      }
    ]
  },
  "257985710": {
    "paperId": "295b18b1794b837296959ea3d58e93352c42373c",
    "externalIds": {
      "ACL": "2020.tal-3.2",
      "CorpusId": 257985710
    },
    "publicationVenue": {
      "id": "8a6e871b-6c73-419c-98a8-27e437270a12",
      "name": "ICON",
      "type": "conference",
      "alternate_names": [
        "ICNLP",
        "Int conf nat lang process",
        "International conference natural language processing",
        "Int Conf Nat Lang Process",
        "TAL",
        "IEEE International Conference on Networks",
        "IEEE Int Conf Netw",
        "International Conference on Natural Language Processing"
      ],
      "issn": "1361-8113",
      "url": "http://www.icohtec.org/publications-icon.html",
      "alternate_urls": [
        "https://ieeexplore.ieee.org/xpl/conhome/1000494/all-proceedings",
        "http://www.wikicfp.com/cfp/program?id=1360",
        "http://www.jstor.org/journal/icon"
      ]
    },
    "title": "Situated Meaning in Multimodal Dialogue: Human-Robot and Human-Computer Interactions",
    "abstract": ". The demand for more sophisticated natural human-computer and human-robot interactions is rapidly increasing, as users become more accustomed to conversation-like interactions with their devices. This requires not only the robust recognition and generation of expressions through multiple modalities (language, gesture, vision, action), but also the encoding of situated meaning: (a) the situated grounding of expressions in context; (b) an interpretation of the expression contextualized to the dynamics of the discourse; and (c) an appreciation of the actions and consequences associated with objects in the environment. In this paper, we introduce VoxWorld, a multimodal simulation platform for modeling human-computer interactions. It is built on the language VoxML",
    "venue": "ICON",
    "year": 2020,
    "referenceCount": 66,
    "citationCount": 11,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1707726",
        "name": "J. Pustejovsky"
      },
      {
        "authorId": "34079649",
        "name": "Nikhil Krishnaswamy"
      }
    ]
  },
  "240531414": {
    "paperId": "f8f021f47f185aec7a0200e22eeb5c7570f44b64",
    "externalIds": {
      "MAG": "3201415876",
      "DBLP": "journals/ki/PustejovskyK21",
      "DOI": "10.1007/s13218-021-00727-5",
      "CorpusId": 240531414
    },
    "publicationVenue": null,
    "title": "Embodied Human Computer Interaction",
    "abstract": null,
    "venue": "KI - K\u00fcnstliche Intelligenz",
    "year": 2021,
    "referenceCount": 115,
    "citationCount": 42,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1707726",
        "name": "J. Pustejovsky"
      },
      {
        "authorId": "34079649",
        "name": "Nikhil Krishnaswamy"
      }
    ]
  },
  "168169824": {
    "paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
    "externalIds": {
      "MAG": "2971008823",
      "DBLP": "conf/nips/ZellersHRBFRC19",
      "ArXiv": "1905.12616",
      "CorpusId": 168169824
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Defending Against Neural Fake News",
    "abstract": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",
    "venue": "Neural Information Processing Systems",
    "year": 2019,
    "referenceCount": 73,
    "citationCount": 908,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2545335",
        "name": "Rowan Zellers"
      },
      {
        "authorId": "14487640",
        "name": "Ari Holtzman"
      },
      {
        "authorId": "2516777",
        "name": "Hannah Rashkin"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "143787583",
        "name": "Ali Farhadi"
      },
      {
        "authorId": "3268360",
        "name": "Franziska Roesner"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      }
    ]
  },
  "197547831": {
    "paperId": "c3b3a6d27dbbfed4df630b39fc0a8a6692b1828a",
    "externalIds": {
      "MAG": "2992405724",
      "CorpusId": 197547831
    },
    "publicationVenue": null,
    "title": "Deepfakes : How a pervert shook the world",
    "abstract": "Recently a software has made it easy to create hyper-realistic face swaps in videos that leaves little-to-no traces of manipulation, in what is known as \u201cdeepfake\u201d videos. Scenarios, where these AI manipulated/generated videos, are used for political distress, blackmail or even terrorism are easily envisioned as a near dystopia. This paper explores the various aspects of deepfake videos including its consequences and newly developed innovations in detecting deepfakes.",
    "venue": "",
    "year": 2019,
    "referenceCount": 4,
    "citationCount": 36,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1576730528",
        "name": "R. Chawla"
      }
    ]
  },
  "236460257": {
    "paperId": "f95a4568a714c34984aa32327fa66344ebe52861",
    "externalIds": {
      "ACL": "2021.acl-long.62",
      "DBLP": "conf/acl/HuYZZTSD020",
      "DOI": "10.18653/v1/2021.acl-long.62",
      "CorpusId": 236460257
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge",
    "abstract": "Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted. In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake news detection. Considering that fake news detection is correlated with topics, we also incorporate topics to enrich the news representation. Specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and entities. Based on the graph, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and KB. Finally, the topic-enriched news representation combining the entity comparison features is fed into a fake news classifier. Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 36,
    "citationCount": 141,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1771202",
        "name": "Linmei Hu"
      },
      {
        "authorId": "2800519",
        "name": "Tianchi Yang"
      },
      {
        "authorId": "2156145866",
        "name": "Luhao Zhang"
      },
      {
        "authorId": "81970097",
        "name": "Wanjun Zhong"
      },
      {
        "authorId": "39483833",
        "name": "Duyu Tang"
      },
      {
        "authorId": "144123161",
        "name": "C. Shi"
      },
      {
        "authorId": "46429989",
        "name": "Nan Duan"
      },
      {
        "authorId": "92660691",
        "name": "Ming Zhou"
      }
    ]
  },
  "236460326": {
    "paperId": "bbfed74eed1796b4534bcce6811b2c7c0b74024a",
    "externalIds": {
      "ACL": "2021.acl-long.133",
      "DBLP": "conf/acl/FungTRPJCMBS20",
      "DOI": "10.18653/v1/2021.acl-long.133",
      "CorpusId": 236460326
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "InfoSurgeon: Cross-Media Fine-grained Information Consistency Checking for Fake News Detection",
    "abstract": "To defend against machine-generated fake news, an effective mechanism is urgently needed. We contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative. Due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific, hard to detect, known inconsistencies. Our detection approach outperforms the state-of-the-art (up to 16.8% accuracy gain), and more critically, yields fine-grained explanations.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 56,
    "citationCount": 56,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51135899",
        "name": "Y. Fung"
      },
      {
        "authorId": "2150796972",
        "name": "Christopher Thomas"
      },
      {
        "authorId": "47016316",
        "name": "Revanth Reddy Gangi Reddy"
      },
      {
        "authorId": "1944118877",
        "name": "Sandeep Polisetty"
      },
      {
        "authorId": "2113323573",
        "name": "Heng Ji"
      },
      {
        "authorId": "2122374530",
        "name": "Shih-Fu Chang"
      },
      {
        "authorId": "145590324",
        "name": "K. McKeown"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      },
      {
        "authorId": "2707234",
        "name": "Avirup Sil"
      }
    ]
  },
  "237304047": {
    "paperId": "02e46711fc86877bdd279c736abe5415a2415e48",
    "externalIds": {
      "ArXiv": "2108.11896",
      "DBLP": "journals/tacl/GuoSV22",
      "ACL": "2022.tacl-1.11",
      "DOI": "10.1162/tacl_a_00454",
      "CorpusId": 237304047
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "A Survey on Automated Fact-Checking",
    "abstract": "Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 216,
    "citationCount": 351,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2681038",
        "name": "Zhijiang Guo"
      },
      {
        "authorId": "8804828",
        "name": "M. Schlichtkrull"
      },
      {
        "authorId": "2064056928",
        "name": "Andreas Vlachos"
      }
    ]
  },
  "222216945": {
    "paperId": "3e42fea41d2737bb5ef7a6ebf9bbb4f9006a503b",
    "externalIds": {
      "PubMedCentral": "7541057",
      "MAG": "3092568232",
      "DOI": "10.1371/journal.pone.0239666",
      "CorpusId": 222216945,
      "PubMed": "33027262"
    },
    "publicationVenue": {
      "id": "0aed7a40-85f3-4c66-9e1b-c1556c57001b",
      "name": "PLoS ONE",
      "type": "journal",
      "alternate_names": [
        "Plo ONE",
        "PLOS ONE",
        "PLO ONE"
      ],
      "issn": "1932-6203",
      "url": "https://journals.plos.org/plosone/",
      "alternate_urls": [
        "http://www.plosone.org/"
      ]
    },
    "title": "Why do people spread false information online? The effects of message and viewer characteristics on self-reported likelihood of sharing social media disinformation",
    "abstract": "Individuals who encounter false information on social media may actively spread it further, by sharing or otherwise engaging with it. Much of the spread of disinformation can thus be attributed to human action. Four studies (total N = 2,634) explored the effect of message attributes (authoritativeness of source, consensus indicators), viewer characteristics (digital literacy, personality, and demographic variables) and their interaction (consistency between message and recipient beliefs) on self-reported likelihood of spreading examples of disinformation. Participants also reported whether they had shared real-world disinformation in the past. Reported likelihood of sharing was not influenced by authoritativeness of the source of the material, nor indicators of how many other people had previously engaged with it. Participants\u2019 level of digital literacy had little effect on their responses. The people reporting the greatest likelihood of sharing disinformation were those who thought it likely to be true, or who had pre-existing attitudes consistent with it. They were likely to have previous familiarity with the materials. Across the four studies, personality (lower Agreeableness and Conscientiousness, higher Extraversion and Neuroticism) and demographic variables (male gender, lower age and lower education) were weakly and inconsistently associated with self-reported likelihood of sharing. These findings have implications for strategies more or less likely to work in countering disinformation in social media.",
    "venue": "PLoS ONE",
    "year": 2020,
    "referenceCount": 56,
    "citationCount": 122,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "29789516",
        "name": "T. Buchanan"
      }
    ]
  },
  "236459973": {
    "paperId": "acfcb88fbd0ece7956cf5ad5eb0f8087311b5b3d",
    "externalIds": {
      "ACL": "2021.acl-long.158",
      "DBLP": "conf/acl/DaFZZHBC20",
      "DOI": "10.18653/v1/2021.acl-long.158",
      "CorpusId": 236459973
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Edited Media Understanding Frames: Reasoning About the Intent and Implications of Visual Misinformation",
    "abstract": "Understanding manipulated media, from automatically generated \u2018deepfakes\u2019 to manually edited ones, raises novel research challenges. Because the vast majority of edited or manipulated images are benign, such as photoshopped images for visual enhancements, the key challenge is to understand the complex layers of underlying intents of media edits and their implications with respect to disinformation. In this paper, we study Edited Media Frames, a new formalism to understand visual media manipulation as structured annotations with respect to the intents, emotional reactions, attacks on individuals, and the overall implications of disinformation. We introduce a dataset for our task, EMU, with 56k question-answer pairs written in rich natural language. We evaluate a wide variety of vision-and-language models for our task, and introduce a new model PELICAN, which builds upon recent progress in pretrained multimodal representations. Our model obtains promising results on our dataset, with humans rating its answers as accurate 48.2% of the time. At the same time, there is still much work to be done \u2013 and we provide analysis that highlights areas for further progress.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 45,
    "citationCount": 11,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1380616323",
        "name": "Jeff Da"
      },
      {
        "authorId": "39191185",
        "name": "Maxwell Forbes"
      },
      {
        "authorId": "2545335",
        "name": "Rowan Zellers"
      },
      {
        "authorId": "146227594",
        "name": "Anthony Zheng"
      },
      {
        "authorId": "2012510",
        "name": "Jena D. Hwang"
      },
      {
        "authorId": "8536286",
        "name": "Antoine Bosselut"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      }
    ]
  },
  "245916820": {
    "paperId": "9963b8bdbfa3e35220757fef2a2667372241b2e5",
    "externalIds": {
      "DOI": "10.1038/s44159-021-00006-y",
      "CorpusId": 245916820
    },
    "publicationVenue": {
      "id": "e5aa582c-d942-45b5-9b96-a1501eead56b",
      "name": "Nature Reviews Psychology",
      "type": "journal",
      "alternate_names": [
        "Nat Rev Psychol"
      ],
      "issn": "2731-0574"
    },
    "title": "The psychological drivers of misinformation belief and its resistance to correction",
    "abstract": null,
    "venue": "Nature Reviews Psychology",
    "year": 2022,
    "referenceCount": 322,
    "citationCount": 509,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3139999",
        "name": "Ullrich K. H. Ecker"
      },
      {
        "authorId": "2573193",
        "name": "S. Lewandowsky"
      },
      {
        "authorId": "2115139230",
        "name": "J. Cook"
      },
      {
        "authorId": "2068468483",
        "name": "P. Schmid"
      },
      {
        "authorId": "2587105",
        "name": "Lisa K. Fazio"
      },
      {
        "authorId": "3439222",
        "name": "Nadia M. Brashier"
      },
      {
        "authorId": "4247735",
        "name": "Panayiota Kendeou"
      },
      {
        "authorId": "2469467",
        "name": "E. Vraga"
      },
      {
        "authorId": "41094808",
        "name": "Michelle A. Amazeen"
      }
    ]
  },
  "237100274": {
    "paperId": "b0fe3bf02e16bbea17df617fed6d367a0cc5e739",
    "externalIds": {
      "DBLP": "conf/ijcai/BevilacquaPRN21",
      "DOI": "10.24963/ijcai.2021/593",
      "CorpusId": 237100274
    },
    "publicationVenue": {
      "id": "67f7f831-711a-43c8-8785-1e09005359b5",
      "name": "International Joint Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Int Jt Conf Artif Intell",
        "IJCAI"
      ],
      "url": "http://www.ijcai.org/"
    },
    "title": "Recent Trends in Word Sense Disambiguation: A Survey",
    "abstract": "Word Sense Disambiguation (WSD) aims at making explicit the semantics of a word in context by identifying the most suitable meaning from a predefined sense inventory. Recent breakthroughs in representation learning have fueled intensive WSD research, resulting in considerable performance improvements, breaching the 80% glass ceiling set by the inter-annotator agreement. In this survey, we provide an extensive overview of current advances in WSD, describing the state of the art in terms of i) resources for the task, i.e., sense inventories and reference datasets for training and testing, as well as ii) automatic disambiguation approaches, detailing their peculiarities, strengths and weaknesses. Finally, we highlight the current limitations of the task itself, but also point out recent trends that could help expand the scope and applicability of WSD, setting up new promising directions for the future.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2021,
    "referenceCount": 73,
    "citationCount": 119,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143802044",
        "name": "Michele Bevilacqua"
      },
      {
        "authorId": "40438851",
        "name": "Tommaso Pasini"
      },
      {
        "authorId": "3106437",
        "name": "Alessandro Raganato"
      },
      {
        "authorId": "1733928",
        "name": "Roberto Navigli"
      }
    ]
  },
  "218517044": {
    "paperId": "9a25609275bb1113aaf7c92b28477ed7ff0677a8",
    "externalIds": {
      "DBLP": "journals/corr/abs-2005-02590",
      "MAG": "3034675880",
      "ArXiv": "2005.02590",
      "ACL": "2020.acl-main.95",
      "DOI": "10.18653/v1/2020.acl-main.95",
      "CorpusId": 218517044
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders",
    "abstract": "A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training. We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense. The encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding. Our system outperforms previous state-of-the-art models on English all-words WSD; these gains predominantly come from improved performance on rare senses, leading to a 31.1% error reduction on less frequent senses over prior work. This demonstrates that rare senses can be more effectively disambiguated by modeling their definitions.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 50,
    "citationCount": 155,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3443287",
        "name": "Terra Blevins"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ]
  },
  "235097236": {
    "paperId": "486df033e3689be451a5e6137b88493d01c8de43",
    "externalIds": {
      "ACL": "2021.naacl-main.371",
      "MAG": "3169262646",
      "DBLP": "conf/naacl/BarbaPN21",
      "DOI": "10.18653/V1/2021.NAACL-MAIN.371",
      "CorpusId": 235097236
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "ESC: Redesigning WSD with Extractive Sense Comprehension",
    "abstract": "Word Sense Disambiguation (WSD) is a historical NLP task aimed at linking words in contexts to discrete sense inventories and it is usually cast as a multi-label classification task. Recently, several neural approaches have employed sense definitions to better represent word meanings. Yet, these approaches do not observe the input sentence and the sense definition candidates all at once, thus potentially reducing the model performance and generalization power. We cope with this issue by reframing WSD as a span extraction problem \u2014 which we called Extractive Sense Comprehension (ESC) \u2014 and propose ESCHER, a transformer-based neural architecture for this new formulation. By means of an extensive array of experiments, we show that ESC unleashes the full potential of our model, leading it to outdo all of its competitors and to set a new state of the art on the English WSD task. In the few-shot scenario, ESCHER proves to exploit training data efficiently, attaining the same performance as its closest competitor while relying on almost three times fewer annotations. Furthermore, ESCHER can nimbly combine data annotated with senses from different lexical resources, achieving performances that were previously out of everyone\u2019s reach. The model along with data is available at https://github.com/SapienzaNLP/esc.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 52,
    "citationCount": 71,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1810690342",
        "name": "Edoardo Barba"
      },
      {
        "authorId": "40438851",
        "name": "Tommaso Pasini"
      },
      {
        "authorId": "1733928",
        "name": "Roberto Navigli"
      }
    ]
  },
  "1134497": {
    "paperId": "f84529b40be07ba1f8fdd3d318ddeacd7394c908",
    "externalIds": {
      "MAG": "1978620866",
      "DBLP": "journals/coling/MarquezCLS08",
      "ACL": "J08-2001",
      "DOI": "10.1162/coli.2008.34.2.145",
      "CorpusId": 1134497
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Special Issue Introduction: Semantic Role Labeling: An Introduction to the Special Issue",
    "abstract": "Semantic role labeling, the computational identification and labeling of arguments in text, has become a leading task in computational linguistics today. Although the issues for this task have been studied for decades, the availability of large resources and the development of statistical machine learning methods have heightened the amount of effort in this field. This special issue presents selected and representative work in the field. This overview describes linguistic background of the problem, the movement from linguistic theories to computational practice, the major resources that are being used, an overview of steps taken in computational systems, and a description of the key issues and results in semantic role labeling (as revealed in several international evaluations). We assess weaknesses in semantic role labeling and identify important challenges facing the field. Overall, the opportunities and the potential for useful further research in semantic role labeling are considerable.",
    "venue": "International Conference on Computational Logic",
    "year": 2008,
    "referenceCount": 53,
    "citationCount": 285,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3049328",
        "name": "Llu\u00eds M\u00e0rquez i Villodre"
      },
      {
        "authorId": "1701734",
        "name": "X. Carreras"
      },
      {
        "authorId": "2262551743",
        "name": "Kenneth C. Litkowski"
      },
      {
        "authorId": "145584212",
        "name": "S. Stevenson"
      }
    ]
  },
  "9210201": {
    "paperId": "c7d3f610b528226f1c862c4f9cd6b37623f7390f",
    "externalIds": {
      "ACL": "W09-1201",
      "DBLP": "conf/conll/HajicCJKMMMNPSSSXZ09",
      "MAG": "2142222368",
      "DOI": "10.3115/1596409.1596411",
      "CorpusId": 9210201
    },
    "publicationVenue": null,
    "title": "The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages",
    "abstract": "For the 11th straight year, the Conference on Computational Natural Language Learning has been accompanied by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2009, the shared task was dedicated to the joint parsing of syntactic and semantic dependencies in multiple languages. This shared task combines the shared tasks of the previous five years under a unique dependency-based formalism similar to the 2008 task. In this paper, we define the shared task, describe how the data sets were created and show their quantitative properties, report the results and summarize the approaches of the participating systems.",
    "venue": "CoNLL Shared Task",
    "year": 2009,
    "referenceCount": 33,
    "citationCount": 614,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144002335",
        "name": "Jan Hajic"
      },
      {
        "authorId": "2754495",
        "name": "Massimiliano Ciaramita"
      },
      {
        "authorId": "145341661",
        "name": "Richard Johansson"
      },
      {
        "authorId": "2368642",
        "name": "Daisuke Kawahara"
      },
      {
        "authorId": "40430085",
        "name": "M. A. Mart\u00ed"
      },
      {
        "authorId": "3049328",
        "name": "Llu\u00eds M\u00e0rquez i Villodre"
      },
      {
        "authorId": "144817783",
        "name": "Adam Meyers"
      },
      {
        "authorId": "1720988",
        "name": "Joakim Nivre"
      },
      {
        "authorId": "1708581",
        "name": "Sebastian Pad\u00f3"
      },
      {
        "authorId": "153593239",
        "name": "J. Step\u00e1nek"
      },
      {
        "authorId": "1788237",
        "name": "P. Stran\u00e1k"
      },
      {
        "authorId": "1760868",
        "name": "M. Surdeanu"
      },
      {
        "authorId": "1702849",
        "name": "Nianwen Xue"
      },
      {
        "authorId": "49889438",
        "name": "Yi Zhang"
      }
    ]
  },
  "202540311": {
    "paperId": "e6ae88330fcc31ff744d8669c9f2f51114494418",
    "externalIds": {
      "DBLP": "conf/emnlp/HeLZ19",
      "ACL": "D19-1538",
      "ArXiv": "1909.00310",
      "MAG": "2970626985",
      "DOI": "10.18653/v1/D19-1538",
      "CorpusId": 202540311
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Syntax-aware Multilingual Semantic Role Labeling",
    "abstract": "Recently, semantic role labeling (SRL) has earned a series of success with even higher performance improvements, which can be mainly attributed to syntactic integration and enhanced word representation. However, most of these efforts focus on English, while SRL on multiple languages more than English has received relatively little attention so that is kept underdevelopment. Thus this paper intends to fill the gap on multilingual SRL with special focus on the impact of syntax and contextualized word representation. Unlike existing work, we propose a novel method guided by syntactic rule to prune arguments, which enables us to integrate syntax into multilingual SRL model simply and effectively. We present a unified SRL model designed for multiple languages together with the proposed uniform syntax enhancement. Our model achieves new state-of-the-art results on the CoNLL-2009 benchmarks of all seven languages. Besides, we pose a discussion on the syntactic role among different languages and verify the effectiveness of deep enhanced representation for multilingual SRL.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 38,
    "citationCount": 49,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51129953",
        "name": "Shexia He"
      },
      {
        "authorId": "30658665",
        "name": "Z. Li"
      },
      {
        "authorId": "36225434",
        "name": "Zhao Hai"
      }
    ]
  },
  "235097227": {
    "paperId": "2172c289b97e4d709f1c54683a242ce9a4c2f37c",
    "externalIds": {
      "ACL": "2021.naacl-main.31",
      "MAG": "3170956458",
      "DBLP": "conf/naacl/ConiaBN21",
      "DOI": "10.18653/V1/2021.NAACL-MAIN.31",
      "CorpusId": 235097227
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Unifying Cross-Lingual Semantic Role Labeling with Heterogeneous Linguistic Resources",
    "abstract": "While cross-lingual techniques are finding increasing success in a wide range of Natural Language Processing tasks, their application to Semantic Role Labeling (SRL) has been strongly limited by the fact that each language adopts its own linguistic formalism, from PropBank for English to AnCora for Spanish and PDT-Vallex for Czech, inter alia. In this work, we address this issue and present a unified model to perform cross-lingual SRL over heterogeneous linguistic resources. Our model implicitly learns a high-quality mapping for different formalisms across diverse languages without resorting to word alignment and/or translation techniques. We find that, not only is our cross-lingual system competitive with the current state of the art but that it is also robust to low-data scenarios. Most interestingly, our unified model is able to annotate a sentence in a single forward pass with all the inventories it was trained with, providing a tool for the analysis and comparison of linguistic theories across different languages. We release our code and model at https://github.com/SapienzaNLP/unify-srl.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 53,
    "citationCount": 34,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1396456007",
        "name": "Simone Conia"
      },
      {
        "authorId": "151426607",
        "name": "Andrea Bacciu"
      },
      {
        "authorId": "1733928",
        "name": "Roberto Navigli"
      }
    ]
  },
  "199022404": {
    "paperId": "5fd6339f299304a7541c805c5ee443fbfb0bac3c",
    "externalIds": {
      "ACL": "P19-4002",
      "MAG": "2966753611",
      "DBLP": "conf/acl/KollerOS19",
      "DOI": "10.18653/v1/P19-4002",
      "CorpusId": 199022404
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Graph-Based Meaning Representations: Design and Processing",
    "abstract": "This tutorial is on representing and processing sentence meaning in the form of labeled directed graphs. The tutorial will (a) briefly review relevant background in formal and linguistic semantics; (b) semi-formally define a unified abstract view on different flavors of semantic graphs and associated terminology; (c) survey common frameworks for graph-based meaning representation and available graph banks; and (d) offer a technical overview of a representative selection of different parsing approaches.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 40,
    "citationCount": 31,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145542037",
        "name": "Alexander Koller"
      },
      {
        "authorId": "2949607",
        "name": "S. Oepen"
      },
      {
        "authorId": "144780931",
        "name": "Weiwei SUN"
      }
    ]
  },
  "226283995": {
    "paperId": "dff49c89b2d15704b7122c309e76bf7c545200b2",
    "externalIds": {
      "DBLP": "conf/conll/OepenAABHHLOXZ20",
      "MAG": "3103028291",
      "ACL": "2020.conll-shared.1",
      "DOI": "10.18653/v1/2020.conll-shared.1",
      "CorpusId": 226283995
    },
    "publicationVenue": {
      "id": "3779a5a7-9119-4f69-84fe-f7eef193eb49",
      "name": "Conference on Computational Natural Language Learning",
      "type": "conference",
      "alternate_names": [
        "CoNLL",
        "Conf Comput Nat Lang Learn"
      ]
    },
    "title": "MRP 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing",
    "abstract": "The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four of these representation frameworks, additional training and evaluation data was provided for one additional language per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu",
    "venue": "Conference on Computational Natural Language Learning",
    "year": 2020,
    "referenceCount": 100,
    "citationCount": 65,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2949607",
        "name": "S. Oepen"
      },
      {
        "authorId": "2769805",
        "name": "Omri Abend"
      },
      {
        "authorId": "2453967",
        "name": "Lasha Abzianidze"
      },
      {
        "authorId": "3461596",
        "name": "Johan Bos"
      },
      {
        "authorId": "144002335",
        "name": "Jan Hajic"
      },
      {
        "authorId": "2086349",
        "name": "Daniel Hershcovich"
      },
      {
        "authorId": "2185910490",
        "name": "Bin Li"
      },
      {
        "authorId": "1388957618",
        "name": "Timothy J. O'Gorman"
      },
      {
        "authorId": "1702849",
        "name": "Nianwen Xue"
      },
      {
        "authorId": "1771298",
        "name": "Daniel Zeman"
      }
    ]
  },
  "235349016": {
    "paperId": "25e7c9dcc294d77d184c4c1122c8304cdb58c69d",
    "externalIds": {
      "DBLP": "conf/aaai/BevilacquaBN21",
      "DOI": "10.1609/aaai.v35i14.17489",
      "CorpusId": 235349016
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline",
    "abstract": "In Text-to-AMR parsing, current state-of-the-art semantic parsers use cumbersome pipelines integrating several different modules or components, and exploit graph recategorization, i.e., a set of content-specific heuristics that are developed on the basis of the training set. However, the generalizability of graph recategorization in an out-of-distribution setting is unclear. In contrast, state-of-the-art AMR-to-Text generation, which can be seen as the inverse to parsing, is based on simpler seq2seq. In this paper, we cast Text-to-AMR and AMR-to-Text as a symmetric transduction task and show that by devising a careful graph linearization and extending a pretrained encoder-decoder model, it is possible to obtain state-of-the-art performances in both tasks using the very same seq2seq approach, i.e., SPRING (Symmetric PaRsIng aNd Generation). Our model does not require complex pipelines, nor heuristics built on heavy assumptions. In fact, we drop the need for graph recategorization, showing that this technique is actually harmful outside of the standard benchmark. Finally, we outperform the previous state of the art on the English AMR 2.0 dataset by a large margin: on Text-to-AMR we obtain an improvement of 3.6 Smatch points, while on AMR-to-Text we outperform the state of the art by 11.2 BLEU points. \nWe release the software at github.com/SapienzaNLP/spring.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2021,
    "referenceCount": 51,
    "citationCount": 150,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143802044",
        "name": "Michele Bevilacqua"
      },
      {
        "authorId": "2008183673",
        "name": "Rexhina Blloshmi"
      },
      {
        "authorId": "1733928",
        "name": "Roberto Navigli"
      }
    ]
  },
  "4460617": {
    "paperId": "57b101db87fb0b67fbe8b57f90b83f8e9efe81a6",
    "externalIds": {
      "MAG": "2406493898",
      "DOI": "10.1038/533452a",
      "CorpusId": 4460617,
      "PubMed": "27225100"
    },
    "publicationVenue": {
      "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
      "name": "Nature",
      "type": "journal",
      "issn": "0028-0836",
      "url": "https://www.nature.com/",
      "alternate_urls": [
        "http://www.nature.com/nature/",
        "https://www.nature.com/nature/",
        "http://www.nature.com/nature/archive/index.html"
      ]
    },
    "title": "1,500 scientists lift the lid on reproducibility",
    "abstract": null,
    "venue": "Nature",
    "year": 2016,
    "referenceCount": 4,
    "citationCount": 2931,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Geology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2225440",
        "name": "M. Baker"
      }
    ]
  },
  "210936371": {
    "paperId": "83c550e602a7cbabbdf8a157b6014458c1f8bc5a",
    "externalIds": {
      "DBLP": "conf/ranlp/MieskesFNGC19",
      "MAG": "2972619658",
      "ACL": "R19-1089",
      "DOI": "10.26615/978-954-452-056-4_089",
      "CorpusId": 210936371
    },
    "publicationVenue": {
      "id": "3413b6f7-e718-4940-a26a-e208f732ada0",
      "name": "Recent Advances in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "RANLP",
        "Recent Adv Nat Lang Process"
      ]
    },
    "title": "Community Perspective on Replicability in Natural Language Processing",
    "abstract": "With recent efforts in drawing attention to the task of replicating and/or reproducing results, for example in the context of COLING 2018 and various LREC workshops, the question arises how the NLP community views the topic of replicability in general. Using a survey, in which we involve members of the NLP community, we investigate how our community perceives this topic, its relevance and options for improvement. Based on over two hundred participants, the survey results confirm earlier observations, that successful reproducibility requires more than having access to code and data. Additionally, the results show that the topic has to be tackled from the authors\u2019, reviewers\u2019 and community\u2019s side.",
    "venue": "Recent Advances in Natural Language Processing",
    "year": 2019,
    "referenceCount": 20,
    "citationCount": 13,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2921990",
        "name": "Margot Mieskes"
      },
      {
        "authorId": "3196675",
        "name": "Kar\u00ebn Fort"
      },
      {
        "authorId": "48680958",
        "name": "Aur\u00e9lie N\u00e9v\u00e9ol"
      },
      {
        "authorId": "2105490",
        "name": "Cyril Grouin"
      },
      {
        "authorId": "145468230",
        "name": "K. Cohen"
      }
    ]
  },
  "232232827": {
    "paperId": "59e7ed6132ce9992a6790a0a179b9eed73959780",
    "externalIds": {
      "ACL": "2021.eacl-main.29",
      "DBLP": "journals/corr/abs-2103-07929",
      "ArXiv": "2103.07929",
      "DOI": "10.18653/v1/2021.eacl-main.29",
      "CorpusId": 232232827
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "A Systematic Review of Reproducibility Research in Natural Language Processing",
    "abstract": "Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how reproducibility should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wide-angle, and as near as possible complete, snapshot of current work on reproducibility in NLP,",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 84,
    "citationCount": 55,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "41052836",
        "name": "Anya Belz"
      },
      {
        "authorId": "2114358275",
        "name": "Shubham Agarwal"
      },
      {
        "authorId": "2181869",
        "name": "Anastasia Shimorina"
      },
      {
        "authorId": "144568312",
        "name": "Ehud Reiter"
      }
    ]
  },
  "227247527": {
    "paperId": "38a24433220d3c1251c9d69bb3d2d242c52c2241",
    "externalIds": {
      "ArXiv": "2012.01172",
      "DBLP": "journals/corr/abs-2012-01172",
      "MAG": "3108226083",
      "DOI": "10.1007/978-3-030-76423-4_1",
      "CorpusId": 227247527
    },
    "publicationVenue": {
      "id": "3ca59fb5-82d5-4f30-b6e4-66ef6ca1be24",
      "name": "International Workshop on Reproducible Research in Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "RRPR",
        "Int Workshop Reprod Res Pattern Recognit"
      ]
    },
    "title": "ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility",
    "abstract": null,
    "venue": "International Workshop on Reproducible Research in Pattern Recognition",
    "year": 2020,
    "referenceCount": 31,
    "citationCount": 12,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2099875322",
        "name": "Burak Yildiz"
      },
      {
        "authorId": "2064627681",
        "name": "Hayley Hung"
      },
      {
        "authorId": "3308507",
        "name": "J. Krijthe"
      },
      {
        "authorId": "1968667",
        "name": "Cynthia C. S. Liem"
      },
      {
        "authorId": "1380498259",
        "name": "M. Loog"
      },
      {
        "authorId": "2766199",
        "name": "Gosia Migut"
      },
      {
        "authorId": "2064861783",
        "name": "Frans Oliehoek"
      },
      {
        "authorId": "3013302",
        "name": "Annibale Panichella"
      },
      {
        "authorId": "2784409",
        "name": "P. Pawe\u0142czak"
      },
      {
        "authorId": "1686538",
        "name": "S. Picek"
      },
      {
        "authorId": "1788228",
        "name": "M. D. Weerdt"
      },
      {
        "authorId": "1738975",
        "name": "J. V. Gemert"
      }
    ]
  },
  "158046772": {
    "paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde",
    "externalIds": {
      "MAG": "2953356739",
      "DBLP": "conf/acl/ZhangHLJSL19",
      "ArXiv": "1905.07129",
      "ACL": "P19-1139",
      "DOI": "10.18653/v1/P19-1139",
      "CorpusId": 158046772
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "ERNIE: Enhanced Language Representation with Informative Entities",
    "abstract": "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 59,
    "citationCount": 1288,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2621696",
        "name": "Zhengyan Zhang"
      },
      {
        "authorId": "48506411",
        "name": "Xu Han"
      },
      {
        "authorId": "49293587",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "145820291",
        "name": "Xin Jiang"
      },
      {
        "authorId": "1753344",
        "name": "Maosong Sun"
      },
      {
        "authorId": "1688015",
        "name": "Qun Liu"
      }
    ]
  },
  "202542757": {
    "paperId": "bfeb827d06c1a3583b5cc6d25241203a81f6af09",
    "externalIds": {
      "MAG": "2970986510",
      "DBLP": "conf/emnlp/PetersNLSJSS19",
      "ACL": "D19-1005",
      "ArXiv": "1909.04164",
      "DOI": "10.18653/v1/D19-1005",
      "CorpusId": 202542757
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Knowledge Enhanced Contextual Word Representations",
    "abstract": "Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert\u2019s runtime is comparable to BERT\u2019s and it scales to large KBs.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 74,
    "citationCount": 631,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39139825",
        "name": "Matthew E. Peters"
      },
      {
        "authorId": "50043859",
        "name": "Mark Neumann"
      },
      {
        "authorId": "1387915630",
        "name": "IV RobertL.Logan"
      },
      {
        "authorId": "4671928",
        "name": "Roy Schwartz"
      },
      {
        "authorId": "24887082",
        "name": "Vidur Joshi"
      },
      {
        "authorId": "34650964",
        "name": "Sameer Singh"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      }
    ]
  },
  "202583325": {
    "paperId": "06a73ad09664435f8b3cd90293f4e05a047cf375",
    "externalIds": {
      "MAG": "2998385486",
      "DBLP": "journals/corr/abs-1909-07606",
      "ArXiv": "1909.07606",
      "DOI": "10.1609/AAAI.V34I03.5681",
      "CorpusId": 202583325
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "K-BERT: Enabling Language Representation with Knowledge Graph",
    "abstract": "Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "referenceCount": 25,
    "citationCount": 698,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46642223",
        "name": "Weijie Liu"
      },
      {
        "authorId": "2113325955",
        "name": "Peng Zhou"
      },
      {
        "authorId": "48634137",
        "name": "Zhe Zhao"
      },
      {
        "authorId": "1390877035",
        "name": "Zhiruo Wang"
      },
      {
        "authorId": "34974680",
        "name": "Qi Ju"
      },
      {
        "authorId": "153807713",
        "name": "Haotang Deng"
      },
      {
        "authorId": "2152210622",
        "name": "Ping Wang"
      }
    ]
  },
  "153312687": {
    "paperId": "5728919676a85553b3c3063626c220fe7a5634e4",
    "externalIds": {
      "DBLP": "conf/acl/DingZCYT19",
      "MAG": "2951350922",
      "ArXiv": "1905.05460",
      "ACL": "P19-1259",
      "DOI": "10.18653/v1/P19-1259",
      "CorpusId": 153312687
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Cognitive Graph for Multi-Hop Reading Comprehension at Scale",
    "abstract": "We propose a new CogQA framework for multi-hop reading comprehension question answering in web-scale documents. Founded on the dual process theory in cognitive science, the framework gradually builds a cognitive graph in an iterative process by coordinating an implicit extraction module (System 1) and an explicit reasoning module (System 2). While giving accurate answers, our framework further provides explainable reasoning paths. Specifically, our implementation based on BERT and graph neural network efficiently handles millions of documents for multi-hop reasoning questions in the HotpotQA fullwiki dataset, achieving a winning joint F_1 score of 34.9 on the leaderboard, compared to 23.1 of the best competitor.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 35,
    "citationCount": 214,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145573466",
        "name": "Ming Ding"
      },
      {
        "authorId": "144161025",
        "name": "Chang Zhou"
      },
      {
        "authorId": "50282546",
        "name": "Qibin Chen"
      },
      {
        "authorId": "38385080",
        "name": "Hongxia Yang"
      },
      {
        "authorId": "46199760",
        "name": "Jie Tang"
      }
    ]
  },
  "202565512": {
    "paperId": "693dcbf7a78dc80c9d36eaf968625810655e4e0b",
    "externalIds": {
      "DBLP": "conf/aaai/LvGXTDGSJCH20",
      "MAG": "2972851234",
      "ArXiv": "1909.05311",
      "DOI": "10.1609/AAAI.V34I05.6364",
      "CorpusId": 202565512
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering",
    "abstract": "Commonsense question answering aims to answer questions which require background knowledge that is not explicitly expressed in the question. The key challenge is how to obtain evidence from external knowledge and make predictions based on the evidence. Recent studies either learn to generate evidence from human-annotated evidence which is expensive to collect, or extract evidence from either structured or unstructured knowledge bases which fails to take advantages of both sources simultaneously. In this work, we propose to automatically extract evidence from heterogeneous knowledge sources, and answer questions based on the extracted evidence. Specifically, we extract evidence from both structured knowledge base (i.e. ConceptNet) and Wikipedia plain texts. We construct graphs for both sources to obtain the relational structures of evidence. Based on these graphs, we propose a graph-based approach consisting of a graph-based contextual word representation learning module and a graph-based inference module. The first module utilizes graph structural information to re-define the distance between words for learning better contextual word representations. The second module adopts graph convolutional network to encode neighbor information into the representations of nodes, and aggregates evidence with graph attention mechanism for predicting the final answer. Experimental results on CommonsenseQA dataset illustrate that our graph-based approach over both knowledge sources brings improvement over strong baselines. Our approach achieves the state-of-the-art accuracy (75.3%) on the CommonsenseQA dataset.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "referenceCount": 38,
    "citationCount": 192,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "28128416",
        "name": "Shangwen Lv"
      },
      {
        "authorId": "2278834796",
        "name": "Daya Guo"
      },
      {
        "authorId": "47883405",
        "name": "Jingjing Xu"
      },
      {
        "authorId": "39483833",
        "name": "Duyu Tang"
      },
      {
        "authorId": "46429989",
        "name": "Nan Duan"
      },
      {
        "authorId": "50175330",
        "name": "Ming Gong"
      },
      {
        "authorId": "24962156",
        "name": "Linjun Shou"
      },
      {
        "authorId": "71790825",
        "name": "Daxin Jiang"
      },
      {
        "authorId": "3320836",
        "name": "Guihong Cao"
      },
      {
        "authorId": "40845069",
        "name": "Songlin Hu"
      }
    ]
  },
  "222125285": {
    "paperId": "37bf0bf34603145246c3311df19e2afdf6e0270a",
    "externalIds": {
      "DBLP": "conf/aaai/Yu0Y022",
      "MAG": "3090656107",
      "ArXiv": "2010.00796",
      "DOI": "10.1609/aaai.v36i10.21417",
      "CorpusId": 222125285
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "JAKET: Joint Pre-training of Knowledge Graph and Language Understanding",
    "abstract": "Knowledge graphs (KGs) contain rich information about world knowledge, entities, and relations. Thus, they can be great supplements to existing pre-trained language models. However, it remains a challenge to efficiently integrate information from KG into language modeling. And the understanding of a knowledge graph requires related context. We propose a novel joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module and language module provide essential information to mutually assist each other: the knowledge module produces embeddings for entities in text while the language module generates context-aware initial embeddings for entities and relations in the graph. Our design enables the pre-trained model to easily adapt to unseen knowledge graphs in new domains. Experiment results on several knowledge-aware NLP tasks show that our proposed framework achieves superior performance by effectively leveraging knowledge in language understanding.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2020,
    "referenceCount": 51,
    "citationCount": 127,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "15121583",
        "name": "Donghan Yu"
      },
      {
        "authorId": "1456009348",
        "name": "Chenguang Zhu"
      },
      {
        "authorId": "35729970",
        "name": "Yiming Yang"
      },
      {
        "authorId": "48262024",
        "name": "Michael Zeng"
      }
    ]
  },
  "214802401": {
    "paperId": "57ff1baa79ea7d52f64989358d673fa2da0eccbb",
    "externalIds": {
      "MAG": "3034569646",
      "ACL": "2020.acl-main.184",
      "DBLP": "conf/acl/ZhangLXL20",
      "ArXiv": "1911.02707",
      "DOI": "10.18653/v1/2020.acl-main.184",
      "CorpusId": 214802401
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs",
    "abstract": "Human conversations naturally evolve around related concepts and hop to distant concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow\u2019s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 51,
    "citationCount": 136,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "92156482",
        "name": "Houyu Zhang"
      },
      {
        "authorId": "49047064",
        "name": "Zhenghao Liu"
      },
      {
        "authorId": "144628574",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "49293587",
        "name": "Zhiyuan Liu"
      }
    ]
  },
  "221879025": {
    "paperId": "7e8457393ff1b40ddd099f195af9d3b14c5a934f",
    "externalIds": {
      "DBLP": "journals/corr/abs-2009-11692",
      "ArXiv": "2009.11692",
      "MAG": "3104007871",
      "ACL": "2020.emnlp-main.54",
      "DOI": "10.18653/v1/2020.emnlp-main.54",
      "CorpusId": 221879025
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Language Generation with Multi-hop Reasoning on Commonsense Knowledge Graph",
    "abstract": "Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 43,
    "citationCount": 108,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51111817",
        "name": "Haozhe Ji"
      },
      {
        "authorId": "1886879",
        "name": "Pei Ke"
      },
      {
        "authorId": "3110003",
        "name": "Shaohan Huang"
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei"
      },
      {
        "authorId": "145213540",
        "name": "Xiaoyan Zhu"
      },
      {
        "authorId": "1730108",
        "name": "Minlie Huang"
      }
    ]
  },
  "218869575": {
    "paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
    "externalIds": {
      "ArXiv": "2005.11401",
      "MAG": "3027879771",
      "DBLP": "conf/nips/LewisPPPKGKLYR020",
      "CorpusId": 218869575
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 66,
    "citationCount": 3619,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145222654",
        "name": "Patrick Lewis"
      },
      {
        "authorId": "3439053",
        "name": "Ethan Perez"
      },
      {
        "authorId": "1716179427",
        "name": "Aleksandara Piktus"
      },
      {
        "authorId": "40052301",
        "name": "F. Petroni"
      },
      {
        "authorId": "2067091563",
        "name": "Vladimir Karpukhin"
      },
      {
        "authorId": "39589154",
        "name": "Naman Goyal"
      },
      {
        "authorId": "103131985",
        "name": "Heinrich Kuttler"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "144105277",
        "name": "Wen-tau Yih"
      },
      {
        "authorId": "2620211",
        "name": "Tim Rockt\u00e4schel"
      },
      {
        "authorId": "48662861",
        "name": "Sebastian Riedel"
      },
      {
        "authorId": "1743722",
        "name": "Douwe Kiela"
      }
    ]
  },
  "235166616": {
    "paperId": "5c95de51ac81569fd515df1a91fe0a6617536fd9",
    "externalIds": {
      "DBLP": "journals/corr/abs-2105-11174",
      "ACL": "2021.findings-acl.269",
      "ArXiv": "2105.11174",
      "DOI": "10.18653/v1/2021.findings-acl.269",
      "CorpusId": 235166616
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Retrieval Enhanced Model for Commonsense Generation",
    "abstract": "Commonsense generation is a challenging task of generating a plausible sentence describing an everyday scenario using provided concepts. Its requirement of reasoning over commonsense knowledge and compositional generalization ability even puzzles strong pre-trained language generation models. We propose a novel framework using retrieval methods to enhance both the pre-training and fine-tuning for commonsense generation. We retrieve prototype sentence candidates by concept matching and use them as auxiliary input. For fine-tuning, we further boost its performance with a trainable sentence retriever. We demonstrate experimentally on the large-scale CommonGen benchmark that our approach achieves new state-of-the-art results.",
    "venue": "Findings",
    "year": 2021,
    "referenceCount": 29,
    "citationCount": 27,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144407394",
        "name": "Han Wang"
      },
      {
        "authorId": "39798499",
        "name": "Yang Liu"
      },
      {
        "authorId": "1456009348",
        "name": "Chenguang Zhu"
      },
      {
        "authorId": "24962156",
        "name": "Linjun Shou"
      },
      {
        "authorId": "50175330",
        "name": "Ming Gong"
      },
      {
        "authorId": "2110197273",
        "name": "Yichong Xu"
      },
      {
        "authorId": "48262024",
        "name": "Michael Zeng"
      }
    ]
  },
  "202540096": {
    "paperId": "710d183174844da5b7f392667f3cc25d2b098dde",
    "externalIds": {
      "MAG": "2971986145",
      "DBLP": "journals/corr/abs-1909-02151",
      "ACL": "D19-1282",
      "ArXiv": "1909.02151",
      "DOI": "10.18653/v1/D19-1282",
      "CorpusId": 202540096
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning",
    "abstract": "Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 41,
    "citationCount": 433,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51583409",
        "name": "Bill Yuchen Lin"
      },
      {
        "authorId": "121022966",
        "name": "Xinyue Chen"
      },
      {
        "authorId": "4787155",
        "name": "Jamin Chen"
      },
      {
        "authorId": "145201124",
        "name": "Xiang Ren"
      }
    ]
  },
  "207757487": {
    "paperId": "a078d53c1eff50123e2b065276663de539a40aa1",
    "externalIds": {
      "DBLP": "journals/corr/abs-1910-14087",
      "MAG": "2982346747",
      "ArXiv": "1910.14087",
      "ACL": "D19-6003",
      "DOI": "10.18653/v1/D19-6003",
      "CorpusId": 207757487
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering",
    "abstract": "Non-extractive commonsense QA remains a challenging AI task, as it requires systems to reason about, synthesize, and gather disparate pieces of information, in order to generate responses to queries. Recent approaches on such tasks show increased performance, only when models are either pre-trained with additional information or when domain-specific heuristics are used, without any special consideration regarding the knowledge resource type. In this paper, we perform a survey of recent commonsense QA methods and we provide a systematic analysis of popular knowledge resources and knowledge-integration methods, across benchmarks from multiple commonsense datasets. Our results and analysis show that attention-based injection seems to be a preferable choice for knowledge integration and that the degree of domain overlap, between knowledge bases and datasets, plays a crucial role in determining model success.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 44,
    "citationCount": 85,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "22244290",
        "name": "Kaixin Ma"
      },
      {
        "authorId": "26253744",
        "name": "Jonathan M Francis"
      },
      {
        "authorId": "50447732",
        "name": "Quanyang Lu"
      },
      {
        "authorId": "144287919",
        "name": "Eric Nyberg"
      },
      {
        "authorId": "49930888",
        "name": "A. Oltramari"
      }
    ]
  },
  "227231215": {
    "paperId": "023b508aca0e776f6face93548fedb8921cf35ab",
    "externalIds": {
      "MAG": "3107930758",
      "DBLP": "journals/corr/abs-2012-00366",
      "ACL": "2020.coling-main.182",
      "ArXiv": "2012.00366",
      "DOI": "10.18653/V1/2020.COLING-MAIN.182",
      "CorpusId": 227231215
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "An Enhanced Knowledge Injection Model for Commonsense Generation",
    "abstract": "Commonsense generation aims at generating plausible everyday scenario description based on a set of provided concepts. Digging the relationship of concepts from scratch is non-trivial, therefore, we retrieve prototypes from external knowledge to assist the understanding of the scenario for better description generation. We integrate two additional modules into the pretrained encoder-decoder model for prototype modeling to enhance the knowledge injection procedure. We conduct experiment on CommonGen benchmark, experimental results show that our method significantly improves the performance on all the metrics.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2020,
    "referenceCount": 42,
    "citationCount": 27,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "9610143",
        "name": "Zhihao Fan"
      },
      {
        "authorId": "2171182",
        "name": "Yeyun Gong"
      },
      {
        "authorId": "2712533",
        "name": "Zhongyu Wei"
      },
      {
        "authorId": "2116420560",
        "name": "Siyuan Wang"
      },
      {
        "authorId": "2143258217",
        "name": "Ya-Chieh Huang"
      },
      {
        "authorId": "49097406",
        "name": "Jian Jiao"
      },
      {
        "authorId": "1790227",
        "name": "Xuanjing Huang"
      },
      {
        "authorId": "46429989",
        "name": "Nan Duan"
      },
      {
        "authorId": "2124601065",
        "name": "Ruofei Zhang"
      }
    ]
  },
  "221970785": {
    "paperId": "baa8f524c82735f174b8d1ab512ac5750146d67e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2009-12677",
      "MAG": "3089060322",
      "ArXiv": "2009.12677",
      "DOI": "10.1609/aaai.v35i7.16796",
      "CorpusId": 221970785
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
    "abstract": "Generative commonsense reasoning which aims to empower machines to generate sentences with the capacity of reasoning over a set of concepts is a critical bottleneck for text generation. Even the state-of-the-art pre-trained language generation models struggle at this task and often produce implausible and anomalous sentences. One reason is that they rarely consider incorporating the knowledge graph which can provide rich relational information among the commonsense concepts. To promote the ability of commonsense reasoning for text generation, we propose a novel knowledge graph augmented pre-trained language generation model KG-BART, which encompasses the complex relations of concepts through the knowledge graph and produces more logical and natural sentences as output. Moreover, KG-BART can leverage the graph attention to aggregate the rich concept semantics that enhances the model generalization on unseen concept sets. Experiments on benchmark CommonGen dataset verify the effectiveness of our proposed approach by comparing with several strong pre-trained language generation models, particularly KG-BART outperforms BART by 5.80, 4.60, in terms of BLEU-3, 4. Moreover, we also show that the generated context by our model can work as background scenarios to benefit downstream commonsense QA tasks.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2020,
    "referenceCount": 40,
    "citationCount": 169,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": null,
        "name": "Ye Liu"
      },
      {
        "authorId": "2389866",
        "name": "Yao Wan"
      },
      {
        "authorId": "40901818",
        "name": "Lifang He"
      },
      {
        "authorId": null,
        "name": "Hao Peng"
      },
      {
        "authorId": "144019071",
        "name": "Philip S. Yu"
      }
    ]
  },
  "52136077": {
    "paperId": "d03d24241fc95e018517d6b1e3be40c5dc31ee66",
    "externalIds": {
      "ArXiv": "1808.10113",
      "DBLP": "journals/corr/abs-1808-10113",
      "MAG": "2889002152",
      "DOI": "10.1609/AAAI.V33I01.33016473",
      "CorpusId": 52136077
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge",
    "abstract": "Generating a reasonable ending for a given story context, i.e., story ending generation, is a strong indication of story comprehension. This task requires not only to understand the context clues which play an important role in planning the plot, but also to handle implicit knowledge to make a reasonable, coherent story. In this paper, we devise a novel model for story ending generation. The model adopts an incremental encoding scheme to represent context clues which are spanning in the story context. In addition, commonsense knowledge is applied through multi-source attention to facilitate story comprehension, and thus to help generate coherent and reasonable endings. Through building context clues and using implicit knowledge, the model is able to produce reasonable story endings. Automatic and manual evaluation shows that our model can generate more reasonable story endings than state-of-the-art baselines1.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2018,
    "referenceCount": 35,
    "citationCount": 160,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2323534462",
        "name": "Jian Guan"
      },
      {
        "authorId": "2952472",
        "name": "Yansen Wang"
      },
      {
        "authorId": "1730108",
        "name": "Minlie Huang"
      }
    ]
  },
  "210701352": {
    "paperId": "c6a84615bc36486cd0170f8a3e1b7e5ec8f5344e",
    "externalIds": {
      "DBLP": "journals/tacl/GuanHHZZ20",
      "ACL": "2020.tacl-1.7",
      "MAG": "3014521650",
      "ArXiv": "2001.05139",
      "DOI": "10.1162/tacl_a_00302",
      "CorpusId": 210701352
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation",
    "abstract": "Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 59,
    "citationCount": 228,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2323534462",
        "name": "Jian Guan"
      },
      {
        "authorId": "152159016",
        "name": "Fei Huang"
      },
      {
        "authorId": "1491087002",
        "name": "Zhihao Zhao"
      },
      {
        "authorId": "145213540",
        "name": "Xiaoyan Zhu"
      },
      {
        "authorId": "1730108",
        "name": "Minlie Huang"
      }
    ]
  },
  "238253193": {
    "paperId": "b5148f8324570a845a7ae9a5843102d0693d006f",
    "externalIds": {
      "DBLP": "journals/corr/abs-2110-00269",
      "CorpusId": 238253193
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Survey of Knowledge Enhanced Pre-trained Models",
    "abstract": "\u2014Pre-trained models learn informative representations on large-scale training data through a self-supervised or supervised learning method, which has achieved promising performance in natural language processing (NLP), computer vision (CV), and cross-modal \ufb01elds after \ufb01ne-tuning. These models, however, suffer from poor robustness and lack of interpretability. Pre-trained models with knowledge injection, which we call knowledge enhanced pre-trained models (KEPTMs), possess deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPTMs in NLP and CV. We \ufb01rst introduce the progress of pre-trained models and knowledge representation learning. Then we systematically categorize existing KEPTMs from three different perspectives. Finally, we outline some potential directions of KEPTMs for future research.",
    "venue": "arXiv.org",
    "year": 2021,
    "referenceCount": 191,
    "citationCount": 45,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2118801449",
        "name": "Jian Yang"
      },
      {
        "authorId": "2146499268",
        "name": "Gang Xiao"
      },
      {
        "authorId": "2117688339",
        "name": "Yulong Shen"
      },
      {
        "authorId": "2152124097",
        "name": "Wei Jiang"
      },
      {
        "authorId": "2110049666",
        "name": "Xinyu Hu"
      },
      {
        "authorId": null,
        "name": "Ying Zhang"
      },
      {
        "authorId": "2122808169",
        "name": "Jinghui Peng"
      }
    ]
  },
  "248005954": {
    "paperId": "42b103b6b4d4f6ba58d13ab9b3c3f201bb65dd1e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2204-03508",
      "ACL": "2023.eacl-main.66",
      "ArXiv": "2204.03508",
      "DOI": "10.48550/arXiv.2204.03508",
      "CorpusId": 248005954
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods",
    "abstract": "Multi-task learning (MTL) has become increasingly popular in natural language processing (NLP) because it improves the performance of related tasks by exploiting their commonalities and differences. Nevertheless, it is still not understood very well how multi-task learning can be implemented based on the relatedness of training tasks. In this survey, we review recent advances of multi-task learning methods in NLP, with the aim of summarizing them into two general multi-task training methods based on their task relatedness: (i) joint training and (ii) multi-step training. We present examples in various NLP downstream applications, summarize the task relationships and discuss future directions of this promising topic.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 108,
    "citationCount": 62,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "72871419",
        "name": "Zhihan Zhang"
      },
      {
        "authorId": "38767143",
        "name": "W. Yu"
      },
      {
        "authorId": "2115909133",
        "name": "Mengxia Yu"
      },
      {
        "authorId": "2109411071",
        "name": "Zhichun Guo"
      },
      {
        "authorId": "2152153656",
        "name": "Meng Jiang"
      }
    ]
  },
  "239015890": {
    "paperId": "290867638c5ca520de5c48aa4336f196d426c226",
    "externalIds": {
      "DBLP": "journals/corr/abs-2110-08455",
      "ArXiv": "2110.08455",
      "CorpusId": 239015890
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey",
    "abstract": "Pretrained Language Models (PLM) have established a new paradigm through learning informative contextualized representations on large-scale text corpus. This new paradigm has revolutionized the entire field of natural language processing, and set the new state-of-the-art performance for a wide variety of NLP tasks. However, though PLMs could store certain knowledge/facts from training corpus, their knowledge awareness is still far from satisfactory. To address this issue, integrating knowledge into PLMs have recently become a very active research area and a variety of approaches have been developed. In this paper, we provide a comprehensive survey of the literature on this emerging and fast-growing field - Knowledge Enhanced Pretrained Language Models (KE-PLMs). We introduce three taxonomies to categorize existing work. Besides, we also survey the various NLU and NLG applications on which KE-PLM has demonstrated superior performance over vanilla PLMs. Finally, we discuss challenges that face KE-PLMs and also promising directions for future research.",
    "venue": "arXiv.org",
    "year": 2021,
    "referenceCount": 111,
    "citationCount": 37,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2336542",
        "name": "Xiaokai Wei"
      },
      {
        "authorId": "2151226033",
        "name": "Shen Wang"
      },
      {
        "authorId": "2358258",
        "name": "Dejiao Zhang"
      },
      {
        "authorId": "50339091",
        "name": "Parminder Bhatia"
      },
      {
        "authorId": "2112031035",
        "name": "Andrew O. Arnold"
      }
    ]
  },
  "3480671": {
    "paperId": "15e81c8d1c21f9e928c72721ac46d458f3341454",
    "externalIds": {
      "DBLP": "conf/iclr/Gu0XLS18",
      "MAG": "2767206889",
      "ArXiv": "1711.02281",
      "CorpusId": 3480671
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Non-Autoregressive Neural Machine Translation",
    "abstract": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English-German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English-Romanian.",
    "venue": "International Conference on Learning Representations",
    "year": 2017,
    "referenceCount": 32,
    "citationCount": 763,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3016273",
        "name": "Jiatao Gu"
      },
      {
        "authorId": "40518045",
        "name": "James Bradbury"
      },
      {
        "authorId": "2228109",
        "name": "Caiming Xiong"
      },
      {
        "authorId": "2052674293",
        "name": "V. Li"
      },
      {
        "authorId": "2166511",
        "name": "R. Socher"
      }
    ]
  },
  "202538740": {
    "paperId": "5efadc9019ce3378a0eb6c8f939cdde6c8918b1e",
    "externalIds": {
      "MAG": "2988975212",
      "DBLP": "conf/emnlp/GhazvininejadLL19",
      "ACL": "D19-1633",
      "DOI": "10.18653/v1/D19-1633",
      "CorpusId": 202538740
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models",
    "abstract": "Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 19,
    "citationCount": 538,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2320509",
        "name": "Marjan Ghazvininejad"
      },
      {
        "authorId": "39455775",
        "name": "Omer Levy"
      },
      {
        "authorId": "11323179",
        "name": "Yinhan Liu"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ]
  },
  "216056470": {
    "paperId": "bed87e8fb3e7e9bc87e1c2ee459ae405a35d3267",
    "externalIds": {
      "ACL": "2020.acl-main.15",
      "ArXiv": "2004.10454",
      "DBLP": "journals/corr/abs-2004-10454",
      "MAG": "3018753103",
      "DOI": "10.18653/v1/2020.acl-main.15",
      "CorpusId": 216056470
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Study of Non-autoregressive Model for Sequence Generation",
    "abstract": "Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others. In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer: (1) Why NAR models can catch up with AR models in some tasks but not all? (2) Why techniques like knowledge distillation and source-target alignment can help NAR models. Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens. To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks. We have several interesting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least. 2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models. 3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 33,
    "citationCount": 59,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1500435161",
        "name": "Yi Ren"
      },
      {
        "authorId": "48211720",
        "name": "Jinglin Liu"
      },
      {
        "authorId": "48391466",
        "name": "Xu Tan"
      },
      {
        "authorId": "47601191",
        "name": "Sheng Zhao"
      },
      {
        "authorId": "47122432",
        "name": "Zhou Zhao"
      },
      {
        "authorId": "2110264337",
        "name": "Tie-Yan Liu"
      }
    ]
  },
  "235422524": {
    "paperId": "013eb12ce5468f79d58bf859653f4929c5a2bd14",
    "externalIds": {
      "DBLP": "journals/tacl/ChenTRBY23",
      "ACL": "2023.tacl-1.12",
      "ArXiv": "2106.07499",
      "DOI": "10.1162/tacl_a_00542",
      "CorpusId": 235422524
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP",
    "abstract": "NLP has achieved great progress in the past decade through the use of neural models and large labeled datasets. The dependence on abundant data prevents NLP models from being applied to low-resource settings or novel tasks where significant time, money, or expertise is required to label massive amounts of textual data. Recently, data augmentation methods have been explored as a means of improving data efficiency in NLP. To date, there has been no systematic empirical overview of data augmentation for NLP in the limited labeled data setting, making it difficult to understand which methods work in which settings. In this paper, we provide an empirical survey of recent progress on data augmentation for NLP in the limited labeled data setting, summarizing the landscape of methods (including token-level augmentations, sentence-level augmentations, adversarial augmentations, and hidden-space augmentations) and carrying out experiments on 11 datasets covering topics/news classification, inference tasks, paraphrasing tasks, and single-sentence tasks. Based on the results, we draw several conclusions to help practitioners choose appropriate augmentations in different settings and discuss the current challenges and future directions for limited data learning in NLP.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 170,
    "citationCount": 141,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47739850",
        "name": "Jiaao Chen"
      },
      {
        "authorId": "1390031652",
        "name": "Derek Tam"
      },
      {
        "authorId": "2402716",
        "name": "Colin Raffel"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      },
      {
        "authorId": "2143919864",
        "name": "Diyi Yang"
      }
    ]
  },
  "216553182": {
    "paperId": "ae2c03cbe6162dadf65edd2ff7dfc5333524dca5",
    "externalIds": {
      "MAG": "3035542229",
      "ArXiv": "2004.12239",
      "DBLP": "journals/corr/abs-2004-12239",
      "ACL": "2020.acl-main.194",
      "DOI": "10.18653/v1/2020.acl-main.194",
      "CorpusId": 216553182
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification",
    "abstract": "This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data. By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks. The improvement is especially prominent when supervision is extremely limited. We have publicly released our code at https://github.com/GT-SALT/MixText.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 48,
    "citationCount": 326,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47739850",
        "name": "Jiaao Chen"
      },
      {
        "authorId": "8387085",
        "name": "Zichao Yang"
      },
      {
        "authorId": "2022168",
        "name": "Diyi Yang"
      }
    ]
  },
  "58981712": {
    "paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
    "externalIds": {
      "DBLP": "journals/corr/abs-1901-07291",
      "ArXiv": "1901.07291",
      "MAG": "2970049541",
      "CorpusId": 58981712
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Cross-lingual Language Model Pretraining",
    "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT\u201916 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT\u201916 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",
    "venue": "Neural Information Processing Systems",
    "year": 2019,
    "referenceCount": 52,
    "citationCount": 2588,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1830914",
        "name": "Guillaume Lample"
      },
      {
        "authorId": "2480903",
        "name": "Alexis Conneau"
      }
    ]
  },
  "221995566": {
    "paperId": "110c13fbf4ff87b52ee1fd9eb2d3616c839ceb41",
    "externalIds": {
      "ArXiv": "2009.14124",
      "DBLP": "journals/corr/abs-2009-14124",
      "MAG": "3108406411",
      "ACL": "2020.findings-emnlp.118",
      "DOI": "10.18653/v1/2020.findings-emnlp.118",
      "CorpusId": 221995566
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Parsing with Multilingual BERT, a Small Treebank, and a Small Corpus",
    "abstract": "Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar to these models, whose labeled and unlabeled data is too limited to train a monolingual model effectively. We propose the use of additional language-specific pretraining and vocabulary augmentation to adapt multilingual models to low-resource settings. Using dependency parsing of four diverse low-resource language varieties as a case study, we show that these methods significantly improve performance over baselines, especially in the lowest-resource cases, and demonstrate the importance of the relationship between such models\u2019 pretraining data and target language varieties.",
    "venue": "Findings",
    "year": 2020,
    "referenceCount": 43,
    "citationCount": 16,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1974400874",
        "name": "Ethan C. Chau"
      },
      {
        "authorId": "49478117",
        "name": "Lucy H. Lin"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      }
    ]
  },
  "220714040": {
    "paperId": "c9b56cb026a38e39bb0228faac57accd6f65e6f7",
    "externalIds": {
      "MAG": "3105604018",
      "DBLP": "conf/emnlp/MorrisLYGJQ20",
      "ACL": "2020.emnlp-demos.16",
      "DOI": "10.18653/v1/2020.emnlp-demos.16",
      "CorpusId": 220714040
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP",
    "abstract": "While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack\u2019s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 43,
    "citationCount": 653,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "153769695",
        "name": "John X. Morris"
      },
      {
        "authorId": "1453652787",
        "name": "Eli Lifland"
      },
      {
        "authorId": "1693182792",
        "name": "Jin Yong Yoo"
      },
      {
        "authorId": "1829303908",
        "name": "J. Grigsby"
      },
      {
        "authorId": "2068347799",
        "name": "Di Jin"
      },
      {
        "authorId": "121817403",
        "name": "Yanjun Qi"
      }
    ]
  },
  "222132916": {
    "paperId": "a321d1ec561a23512a5aa687c0d89c971bb5687b",
    "externalIds": {
      "MAG": "3168921237",
      "DBLP": "journals/corr/abs-2010-02194",
      "ACL": "2021.naacl-main.426",
      "ArXiv": "2010.02194",
      "DOI": "10.18653/V1/2021.NAACL-MAIN.426",
      "CorpusId": 222132916
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Self-training Improves Pre-training for Natural Language Understanding",
    "abstract": "Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semi-supervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 57,
    "citationCount": 156,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3048577",
        "name": "Jingfei Du"
      },
      {
        "authorId": "3024698",
        "name": "Edouard Grave"
      },
      {
        "authorId": "7653327",
        "name": "Beliz Gunel"
      },
      {
        "authorId": "113810201",
        "name": "Vishrav Chaudhary"
      },
      {
        "authorId": "2061077885",
        "name": "Onur \u00c7elebi"
      },
      {
        "authorId": "2325985",
        "name": "Michael Auli"
      },
      {
        "authorId": "1389924486",
        "name": "Ves Stoyanov"
      },
      {
        "authorId": "2480903",
        "name": "Alexis Conneau"
      }
    ]
  },
  "221703107": {
    "paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9",
    "externalIds": {
      "ArXiv": "2009.07118",
      "MAG": "3085177480",
      "DBLP": "conf/naacl/SchickS21",
      "ACL": "2021.naacl-main.185",
      "DOI": "10.18653/V1/2021.NAACL-MAIN.185",
      "CorpusId": 221703107
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
    "abstract": "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much \u201cgreener\u201d in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 64,
    "citationCount": 876,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32246932",
        "name": "Timo Schick"
      },
      {
        "authorId": "144418438",
        "name": "Hinrich Sch\u00fctze"
      }
    ]
  },
  "237416585": {
    "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
    "externalIds": {
      "DBLP": "journals/corr/abs-2109-01652",
      "ArXiv": "2109.01652",
      "CorpusId": 237416585
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Finetuned Language Models Are Zero-Shot Learners",
    "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "referenceCount": 169,
    "citationCount": 3014,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144026731",
        "name": "Jason Wei"
      },
      {
        "authorId": "40377863",
        "name": "Maarten Bosma"
      },
      {
        "authorId": "2664737",
        "name": "Vincent Zhao"
      },
      {
        "authorId": "2091768",
        "name": "Kelvin Guu"
      },
      {
        "authorId": "40625240",
        "name": "Adams Wei Yu"
      },
      {
        "authorId": "144104130",
        "name": "Brian Lester"
      },
      {
        "authorId": "2140321952",
        "name": "Nan Du"
      },
      {
        "authorId": "2555924",
        "name": "Andrew M. Dai"
      },
      {
        "authorId": "2827616",
        "name": "Quoc V. Le"
      }
    ]
  },
  "235899116": {
    "paperId": "2ee03e28208a9310a9be4032c2b04ebdddb83cc7",
    "externalIds": {
      "ArXiv": "2107.07170",
      "DBLP": "conf/nips/BraggCLB21",
      "CorpusId": 235899116
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "FLEX: Unifying Evaluation for Few-Shot NLP",
    "abstract": "Few-shot NLP research is highly active, yet conducted in disjoint research threads with evaluation suites that lack challenging-yet-realistic testing setups and fail to employ careful experimental design. Consequently, the community does not know which techniques perform best or even if they outperform simple baselines. In response, we formulate the FLEX Principles, a set of requirements and best practices for unified, rigorous, valid, and cost-sensitive few-shot NLP evaluation. These principles include Sample Size Design, a novel approach to benchmark design that optimizes statistical accuracy and precision while keeping evaluation costs manageable. Following the principles, we release the FLEX benchmark, which includes four few-shot transfer settings, zero-shot evaluation, and a public leaderboard that covers diverse NLP tasks. In addition, we present UniFew, a prompt-based model for few-shot learning that unifies pretraining and finetuning prompt formats, eschewing complex machinery of recent prompt-based approaches in adapting downstream task formats to language model pretraining objectives. We demonstrate that despite simplicity, UniFew achieves results competitive with both popular meta-learning and prompt-based approaches.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "referenceCount": 82,
    "citationCount": 95,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2699105",
        "name": "Jonathan Bragg"
      },
      {
        "authorId": "2527954",
        "name": "Arman Cohan"
      },
      {
        "authorId": "46258841",
        "name": "Kyle Lo"
      },
      {
        "authorId": "46181066",
        "name": "Iz Beltagy"
      }
    ]
  },
  "6941275": {
    "paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6",
    "externalIds": {
      "ArXiv": "1505.04870",
      "DBLP": "conf/iccv/PlummerWCCHL15",
      "MAG": "2568262903",
      "DOI": "10.1007/s11263-016-0965-7",
      "CorpusId": 6941275
    },
    "publicationVenue": {
      "id": "939ee07c-6009-43f8-b884-69238b40659e",
      "name": "International Journal of Computer Vision",
      "type": "journal",
      "alternate_names": [
        "Int J Comput Vis"
      ],
      "issn": "0920-5691",
      "url": "https://www.springer.com/computer/image+processing/journal/11263",
      "alternate_urls": [
        "https://link.springer.com/journal/11263",
        "http://link.springer.com/journal/11263"
      ]
    },
    "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
    "abstract": null,
    "venue": "International Journal of Computer Vision",
    "year": 2015,
    "referenceCount": 83,
    "citationCount": 1822,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2856622",
        "name": "Bryan A. Plummer"
      },
      {
        "authorId": "39060743",
        "name": "Liwei Wang"
      },
      {
        "authorId": "6648406",
        "name": "Christopher M. Cervantes"
      },
      {
        "authorId": "145507543",
        "name": "Juan C. Caicedo"
      },
      {
        "authorId": "3118681",
        "name": "J. Hockenmaier"
      },
      {
        "authorId": "1749609",
        "name": "Svetlana Lazebnik"
      }
    ]
  },
  "6308361": {
    "paperId": "92c141447f51b6732242376164ff961e464731c8",
    "externalIds": {
      "ACL": "D14-1086",
      "DBLP": "conf/emnlp/KazemzadehOMB14",
      "MAG": "2251512949",
      "DOI": "10.3115/v1/D14-1086",
      "CorpusId": 6308361
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes",
    "abstract": "In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2014,
    "referenceCount": 47,
    "citationCount": 1126,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3006072",
        "name": "Sahar Kazemzadeh"
      },
      {
        "authorId": "2004053",
        "name": "Vicente Ordonez"
      },
      {
        "authorId": "65828281",
        "name": "M. Matten"
      },
      {
        "authorId": "1685538",
        "name": "Tamara L. Berg"
      }
    ]
  },
  "8745888": {
    "paperId": "e65142010431ffc089b272a1174214e00693e503",
    "externalIds": {
      "MAG": "2963109634",
      "ArXiv": "1511.02283",
      "DBLP": "journals/corr/MaoHTCYM15",
      "DOI": "10.1109/CVPR.2016.9",
      "CorpusId": 8745888
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "Generation and Comprehension of Unambiguous Object Descriptions",
    "abstract": "We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MSCOCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/ mjhucla/Google_Refexp_toolbox.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2015,
    "referenceCount": 60,
    "citationCount": 1172,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "36010601",
        "name": "Junhua Mao"
      },
      {
        "authorId": "2136435893",
        "name": "Jonathan Huang"
      },
      {
        "authorId": "1726415",
        "name": "Alexander Toshev"
      },
      {
        "authorId": "3317152",
        "name": "Oana-Maria Camburu"
      },
      {
        "authorId": "145081362",
        "name": "A. Yuille"
      },
      {
        "authorId": "1702318",
        "name": "K. Murphy"
      }
    ]
  },
  "2210455": {
    "paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628",
    "externalIds": {
      "ArXiv": "1504.00325",
      "DBLP": "journals/corr/ChenFLVGDZ15",
      "MAG": "1889081078",
      "CorpusId": 2210455
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Microsoft COCO Captions: Data Collection and Evaluation Server",
    "abstract": "In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.",
    "venue": "arXiv.org",
    "year": 2015,
    "referenceCount": 46,
    "citationCount": 2238,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39717886",
        "name": "Xinlei Chen"
      },
      {
        "authorId": "2113484216",
        "name": "Hao Fang"
      },
      {
        "authorId": "33493200",
        "name": "Tsung-Yi Lin"
      },
      {
        "authorId": "8137017",
        "name": "Ramakrishna Vedantam"
      },
      {
        "authorId": "144157872",
        "name": "Saurabh Gupta"
      },
      {
        "authorId": "3127283",
        "name": "Piotr Doll\u00e1r"
      },
      {
        "authorId": "1699161",
        "name": "C. L. Zitnick"
      }
    ]
  },
  "3180429": {
    "paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
    "externalIds": {
      "MAG": "1933349210",
      "DBLP": "conf/iccv/AntolALMBZP15",
      "ArXiv": "1505.00468",
      "DOI": "10.1007/s11263-016-0966-6",
      "CorpusId": 3180429
    },
    "publicationVenue": {
      "id": "939ee07c-6009-43f8-b884-69238b40659e",
      "name": "International Journal of Computer Vision",
      "type": "journal",
      "alternate_names": [
        "Int J Comput Vis"
      ],
      "issn": "0920-5691",
      "url": "https://www.springer.com/computer/image+processing/journal/11263",
      "alternate_urls": [
        "https://link.springer.com/journal/11263",
        "http://link.springer.com/journal/11263"
      ]
    },
    "title": "VQA: Visual Question Answering",
    "abstract": null,
    "venue": "International Journal of Computer Vision",
    "year": 2015,
    "referenceCount": 73,
    "citationCount": 4955,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2801949",
        "name": "Aishwarya Agrawal"
      },
      {
        "authorId": "8553015",
        "name": "Jiasen Lu"
      },
      {
        "authorId": "1963421",
        "name": "Stanislaw Antol"
      },
      {
        "authorId": "2067793408",
        "name": "Margaret Mitchell"
      },
      {
        "authorId": "1699161",
        "name": "C. L. Zitnick"
      },
      {
        "authorId": "153432684",
        "name": "Devi Parikh"
      },
      {
        "authorId": "1746610",
        "name": "Dhruv Batra"
      }
    ]
  },
  "4492210": {
    "paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
    "externalIds": {
      "ArXiv": "1602.07332",
      "DBLP": "journals/corr/KrishnaZGJHKCKL16",
      "MAG": "2277195237",
      "DOI": "10.1007/s11263-016-0981-7",
      "CorpusId": 4492210
    },
    "publicationVenue": {
      "id": "939ee07c-6009-43f8-b884-69238b40659e",
      "name": "International Journal of Computer Vision",
      "type": "journal",
      "alternate_names": [
        "Int J Comput Vis"
      ],
      "issn": "0920-5691",
      "url": "https://www.springer.com/computer/image+processing/journal/11263",
      "alternate_urls": [
        "https://link.springer.com/journal/11263",
        "http://link.springer.com/journal/11263"
      ]
    },
    "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
    "abstract": null,
    "venue": "International Journal of Computer Vision",
    "year": 2016,
    "referenceCount": 130,
    "citationCount": 5230,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145237361",
        "name": "Ranjay Krishna"
      },
      {
        "authorId": "2117748",
        "name": "Yuke Zhu"
      },
      {
        "authorId": "50499889",
        "name": "O. Groth"
      },
      {
        "authorId": "2115231104",
        "name": "Justin Johnson"
      },
      {
        "authorId": "1382195702",
        "name": "K. Hata"
      },
      {
        "authorId": "40591424",
        "name": "Joshua Kravitz"
      },
      {
        "authorId": "2110910215",
        "name": "Stephanie Chen"
      },
      {
        "authorId": "1944225",
        "name": "Yannis Kalantidis"
      },
      {
        "authorId": "2040091191",
        "name": "Li-Jia Li"
      },
      {
        "authorId": "1760364",
        "name": "David A. Shamma"
      },
      {
        "authorId": "145879842",
        "name": "Michael S. Bernstein"
      },
      {
        "authorId": "48004138",
        "name": "Li Fei-Fei"
      }
    ]
  },
  "152282269": {
    "paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c",
    "externalIds": {
      "MAG": "2950104027",
      "DBLP": "conf/cvpr/HudsonM19",
      "DOI": "10.1109/CVPR.2019.00686",
      "CorpusId": 152282269
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
    "abstract": "We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages Visual Genome scene graph structures to create 22M diverse reasoning questions, which all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. A careful analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains a mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%, offering ample opportunity for new research to explore. We hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding of vision and language.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2019,
    "referenceCount": 44,
    "citationCount": 1545,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "152951058",
        "name": "Drew A. Hudson"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      }
    ]
  },
  "8849206": {
    "paperId": "2c1890864c1c2b750f48316dc8b650ba4772adc5",
    "externalIds": {
      "MAG": "2950318722",
      "DBLP": "conf/cvpr/YangHGDS16",
      "ArXiv": "1511.02274",
      "DOI": "10.1109/CVPR.2016.10",
      "CorpusId": 8849206
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "Stacked Attention Networks for Image Question Answering",
    "abstract": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2015,
    "referenceCount": 35,
    "citationCount": 1810,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8387085",
        "name": "Zichao Yang"
      },
      {
        "authorId": "144137069",
        "name": "Xiaodong He"
      },
      {
        "authorId": "1800422",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "144718788",
        "name": "L. Deng"
      },
      {
        "authorId": "46234526",
        "name": "Alex Smola"
      }
    ]
  },
  "868693": {
    "paperId": "fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b",
    "externalIds": {
      "MAG": "2963668159",
      "ArXiv": "1606.00061",
      "DBLP": "journals/corr/LuYBP16",
      "CorpusId": 868693
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering",
    "abstract": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.",
    "venue": "Neural Information Processing Systems",
    "year": 2016,
    "referenceCount": 32,
    "citationCount": 1539,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8553015",
        "name": "Jiasen Lu"
      },
      {
        "authorId": "145743311",
        "name": "Jianwei Yang"
      },
      {
        "authorId": "1746610",
        "name": "Dhruv Batra"
      },
      {
        "authorId": "153432684",
        "name": "Devi Parikh"
      }
    ]
  },
  "3753452": {
    "paperId": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8",
    "externalIds": {
      "MAG": "2745461083",
      "DBLP": "conf/cvpr/00010BT0GZ18",
      "ArXiv": "1707.07998",
      "DOI": "10.1109/CVPR.2018.00636",
      "CorpusId": 3753452
    },
    "publicationVenue": null,
    "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
    "abstract": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",
    "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
    "year": 2017,
    "referenceCount": 67,
    "citationCount": 3965,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "6965856",
        "name": "Peter Anderson"
      },
      {
        "authorId": "144137069",
        "name": "Xiaodong He"
      },
      {
        "authorId": "31790073",
        "name": "Chris Buehler"
      },
      {
        "authorId": "2406263",
        "name": "Damien Teney"
      },
      {
        "authorId": "145177220",
        "name": "Mark Johnson"
      },
      {
        "authorId": "145273587",
        "name": "Stephen Gould"
      },
      {
        "authorId": "39089563",
        "name": "Lei Zhang"
      }
    ]
  },
  "2840197": {
    "paperId": "12f7de07f9b00315418e381b2bd797d21f12b419",
    "externalIds": {
      "MAG": "2952140939",
      "DBLP": "journals/corr/FukuiPYRDR16",
      "ACL": "D16-1044",
      "ArXiv": "1606.01847",
      "DOI": "10.18653/v1/D16-1044",
      "CorpusId": 2840197
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
    "abstract": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2016,
    "referenceCount": 57,
    "citationCount": 1415,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2135128997",
        "name": "Akira Fukui"
      },
      {
        "authorId": "3422202",
        "name": "Dong Huk Park"
      },
      {
        "authorId": "3422876",
        "name": "Daylen Yang"
      },
      {
        "authorId": "34721166",
        "name": "Anna Rohrbach"
      },
      {
        "authorId": "1753210",
        "name": "Trevor Darrell"
      },
      {
        "authorId": "34849128",
        "name": "Marcus Rohrbach"
      }
    ]
  },
  "5276660": {
    "paperId": "21c99706bb26e9012bfb4d8d48009a3d45af59b2",
    "externalIds": {
      "MAG": "2950099562",
      "DBLP": "conf/cvpr/AndreasRDK16",
      "ArXiv": "1511.02799",
      "DOI": "10.1109/CVPR.2016.12",
      "CorpusId": 5276660
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "Neural Module Networks",
    "abstract": "Visual question answering is fundamentally compositional in nature-a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2015,
    "referenceCount": 43,
    "citationCount": 1025,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2112400",
        "name": "Jacob Andreas"
      },
      {
        "authorId": "34849128",
        "name": "Marcus Rohrbach"
      },
      {
        "authorId": "1753210",
        "name": "Trevor Darrell"
      },
      {
        "authorId": "38666915",
        "name": "D. Klein"
      }
    ]
  },
  "216080982": {
    "paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290",
    "externalIds": {
      "DBLP": "conf/eccv/ChenLYK0G0020",
      "MAG": "3090449556",
      "DOI": "10.1007/978-3-030-58577-8_7",
      "CorpusId": 216080982
    },
    "publicationVenue": {
      "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
      "name": "European Conference on Computer Vision",
      "type": "conference",
      "alternate_names": [
        "ECCV",
        "Eur Conf Comput Vis"
      ],
      "url": "https://link.springer.com/conference/eccv"
    },
    "title": "UNITER: UNiversal Image-TExt Representation Learning",
    "abstract": null,
    "venue": "European Conference on Computer Vision",
    "year": 2019,
    "referenceCount": 64,
    "citationCount": 1965,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2378902",
        "name": "Yen-Chun Chen"
      },
      {
        "authorId": "50703697",
        "name": "Linjie Li"
      },
      {
        "authorId": "1714982",
        "name": "Licheng Yu"
      },
      {
        "authorId": "1877430",
        "name": "Ahmed El Kholy"
      },
      {
        "authorId": "2054472958",
        "name": "Faisal Ahmed"
      },
      {
        "authorId": "144702900",
        "name": "Zhe Gan"
      },
      {
        "authorId": "145215470",
        "name": "Yu Cheng"
      },
      {
        "authorId": "46700348",
        "name": "Jingjing Liu"
      }
    ]
  },
  "215754208": {
    "paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0",
    "externalIds": {
      "DBLP": "journals/corr/abs-2004-06165",
      "MAG": "3091588028",
      "ArXiv": "2004.06165",
      "DOI": "10.1007/978-3-030-58577-8_8",
      "CorpusId": 215754208
    },
    "publicationVenue": {
      "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
      "name": "European Conference on Computer Vision",
      "type": "conference",
      "alternate_names": [
        "ECCV",
        "Eur Conf Comput Vis"
      ],
      "url": "https://link.springer.com/conference/eccv"
    },
    "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
    "abstract": null,
    "venue": "European Conference on Computer Vision",
    "year": 2020,
    "referenceCount": 52,
    "citationCount": 1733,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47058148",
        "name": "Xiujun Li"
      },
      {
        "authorId": "1629039205",
        "name": "Xi Yin"
      },
      {
        "authorId": "2109737569",
        "name": "Chunyuan Li"
      },
      {
        "authorId": "2148941781",
        "name": "Xiaowei Hu"
      },
      {
        "authorId": "9325940",
        "name": "Pengchuan Zhang"
      },
      {
        "authorId": "39089563",
        "name": "Lei Zhang"
      },
      {
        "authorId": "29957038",
        "name": "Lijuan Wang"
      },
      {
        "authorId": "35431603",
        "name": "Houdong Hu"
      },
      {
        "authorId": "145307652",
        "name": "Li Dong"
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      },
      {
        "authorId": "1800422",
        "name": "Jianfeng Gao"
      }
    ]
  },
  "235692795": {
    "paperId": "63c74d15940af1af9b386b5762e4445e54c73719",
    "externalIds": {
      "DBLP": "conf/cvpr/ZhangLHY0WCG21",
      "DOI": "10.1109/CVPR46437.2021.00553",
      "CorpusId": 235692795
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
    "abstract": "This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model OSCAR [20], and utilize an improved approach OSCAR+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. Code, models and pre-extracted features are released at https://github.com/pzzhang/VinVL.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "referenceCount": 45,
    "citationCount": 803,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "9325940",
        "name": "Pengchuan Zhang"
      },
      {
        "authorId": "47058148",
        "name": "Xiujun Li"
      },
      {
        "authorId": "2148941781",
        "name": "Xiaowei Hu"
      },
      {
        "authorId": "120157163",
        "name": "Jianwei Yang"
      },
      {
        "authorId": "2152828578",
        "name": "Lei Zhang"
      },
      {
        "authorId": "29957038",
        "name": "Lijuan Wang"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      },
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      }
    ]
  },
  "231802355": {
    "paperId": "cb596bffc5c5042c254058b62317a57fa156fea4",
    "externalIds": {
      "ArXiv": "2102.02779",
      "DBLP": "journals/corr/abs-2102-02779",
      "CorpusId": 231802355
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Unifying Vision-and-Language Tasks via Text Generation",
    "abstract": "Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5",
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "referenceCount": 84,
    "citationCount": 481,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2706729",
        "name": "Jaemin Cho"
      },
      {
        "authorId": "46665218",
        "name": "Jie Lei"
      },
      {
        "authorId": "80940793",
        "name": "Hao Tan"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      }
    ]
  },
  "237291550": {
    "paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13",
    "externalIds": {
      "DBLP": "journals/corr/abs-2108-10904",
      "ArXiv": "2108.10904",
      "CorpusId": 237291550
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
    "abstract": "With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.",
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "referenceCount": 66,
    "citationCount": 699,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2257550085",
        "name": "Zirui Wang"
      },
      {
        "authorId": "150167366",
        "name": "Jiahui Yu"
      },
      {
        "authorId": "40625240",
        "name": "Adams Wei Yu"
      },
      {
        "authorId": "3422912",
        "name": "Zihang Dai"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "145144022",
        "name": "Yuan Cao"
      }
    ]
  },
  "231591445": {
    "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
    "externalIds": {
      "DBLP": "conf/icml/RadfordKHRGASAM21",
      "ArXiv": "2103.00020",
      "CorpusId": 231591445
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Learning Transferable Visual Models From Natural Language Supervision",
    "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "referenceCount": 220,
    "citationCount": 20470,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "38909097",
        "name": "Alec Radford"
      },
      {
        "authorId": "2110935237",
        "name": "Jong Wook Kim"
      },
      {
        "authorId": "2004021329",
        "name": "Chris Hallacy"
      },
      {
        "authorId": "1992922591",
        "name": "A. Ramesh"
      },
      {
        "authorId": "40087786",
        "name": "Gabriel Goh"
      },
      {
        "authorId": "144517868",
        "name": "Sandhini Agarwal"
      },
      {
        "authorId": "144864359",
        "name": "Girish Sastry"
      },
      {
        "authorId": "119609682",
        "name": "Amanda Askell"
      },
      {
        "authorId": "2051714782",
        "name": "Pamela Mishkin"
      },
      {
        "authorId": "2115193883",
        "name": "Jack Clark"
      },
      {
        "authorId": "2064404342",
        "name": "Gretchen Krueger"
      },
      {
        "authorId": "1701686",
        "name": "I. Sutskever"
      }
    ]
  },
  "231879586": {
    "paperId": "141a5033d9994242b18bb3b217e79582f1ee9306",
    "externalIds": {
      "ArXiv": "2102.05918",
      "DBLP": "conf/icml/JiaYXCPPLSLD21",
      "CorpusId": 231879586
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
    "abstract": "Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.",
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "referenceCount": 81,
    "citationCount": 3031,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2082436672",
        "name": "Chao Jia"
      },
      {
        "authorId": "2781059",
        "name": "Yinfei Yang"
      },
      {
        "authorId": "2110629849",
        "name": "Ye Xia"
      },
      {
        "authorId": "2109060286",
        "name": "Yi-Ting Chen"
      },
      {
        "authorId": "27456119",
        "name": "Zarana Parekh"
      },
      {
        "authorId": "143950636",
        "name": "Hieu Pham"
      },
      {
        "authorId": "2827616",
        "name": "Quoc V. Le"
      },
      {
        "authorId": "2305450",
        "name": "Yun-Hsuan Sung"
      },
      {
        "authorId": "2110121852",
        "name": "Zhen Li"
      },
      {
        "authorId": "2066508193",
        "name": "Tom Duerig"
      }
    ]
  },
  "222341606": {
    "paperId": "dedcdc1fb3a6def9772dce674d89150923dd75b9",
    "externalIds": {
      "DBLP": "journals/corr/abs-2010-06775",
      "MAG": "3092787421",
      "ACL": "2020.emnlp-main.162",
      "ArXiv": "2010.06775",
      "DOI": "10.18653/v1/2020.emnlp-main.162",
      "CorpusId": 222341606
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Vokenization: Improving Language Understanding via Contextualized, Visually-Grounded Supervision",
    "abstract": "Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named \"vokenization\" that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call \"vokens\"). The \"vokenizer\" is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG. Code and pre-trained models publicly available at this https URL",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 84,
    "citationCount": 111,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "80940793",
        "name": "Hao Tan"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      }
    ]
  },
  "219303834": {
    "paperId": "e3575ca62373639152adf84ca0b33c52a4f892ef",
    "externalIds": {
      "DBLP": "conf/cvpr/NiHSCBW0D21",
      "DOI": "10.1109/CVPR46437.2021.00397",
      "CorpusId": 219303834
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training",
    "abstract": "We present M3P, a Multitask Multilingual Multimodal Pre-trained model that combines multilingual pre-training and multimodal pre-training into a unified framework via multitask pre-training. Our goal is to learn universal representations that can map objects occurred in different modalities or texts expressed in different languages into a common semantic space. In addition, to explicitly encourage fine-grained alignment between images and non-English languages, we also propose Multimodal Code-switched Training (MCT) to combine monolingual pre-training and multimodal pre-training via a code-switch strategy. Experiments are performed on the multilingual image retrieval task across two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve comparable results for English and new state-of-the-art results for non-English languages.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "referenceCount": 38,
    "citationCount": 91,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "15086992",
        "name": "Haoyang Huang"
      },
      {
        "authorId": "2069855086",
        "name": "Lin Su"
      },
      {
        "authorId": "1380129958",
        "name": "Di Qi"
      },
      {
        "authorId": "46429989",
        "name": "Nan Duan"
      },
      {
        "authorId": "144530394",
        "name": "Edward Cui"
      },
      {
        "authorId": "1490606819",
        "name": "Taroon Bharti"
      },
      {
        "authorId": "1452981772",
        "name": "Lei Zhang"
      },
      {
        "authorId": "29957038",
        "name": "Lijuan Wang"
      },
      {
        "authorId": "1800422",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "2108662284",
        "name": "Bei Liu"
      },
      {
        "authorId": "3247966",
        "name": "Jianlong Fu"
      },
      {
        "authorId": "40232931",
        "name": "Dongdong Zhang"
      },
      {
        "authorId": "2146075914",
        "name": "Xin Liu"
      },
      {
        "authorId": "92660691",
        "name": "Ming Zhou"
      }
    ]
  },
  "231740629": {
    "paperId": "81002fbb777f860f9aac2bbc24467a62345af279",
    "externalIds": {
      "ArXiv": "2102.00529",
      "DBLP": "journals/corr/abs-2102-00529",
      "DOI": "10.1162/tacl_a_00385",
      "CorpusId": 231740629
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers",
    "abstract": "Abstract Recently, multimodal transformer models have gained popularity because their performance on downstream tasks suggests they learn rich visual-linguistic representations. Focusing on zero-shot image retrieval tasks, we study three important factors that can impact the quality of learned representations: pretraining data, the attention mechanism, and loss functions. By pretraining models on six datasets, we observe that dataset noise and language similarity to our downstream task are important indicators of model performance. Through architectural analysis, we learn that models with a multimodal attention mechanism can outperform deeper models with modality-specific attention mechanisms. Finally, we show that successful contrastive losses used in the self-supervised learning literature do not yield similar performance gains when used in multimodal transformers.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 51,
    "citationCount": 102,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2234342",
        "name": "Lisa Anne Hendricks"
      },
      {
        "authorId": "1386957852",
        "name": "John F. J. Mellor"
      },
      {
        "authorId": "145721402",
        "name": "R. Schneider"
      },
      {
        "authorId": "2285263",
        "name": "Jean-Baptiste Alayrac"
      },
      {
        "authorId": "3208081",
        "name": "Aida Nematzadeh"
      }
    ]
  },
  "237452717": {
    "paperId": "de2f6a8921f9bf984fdc2b46964a03d3b83e2fcd",
    "externalIds": {
      "ArXiv": "2109.04448",
      "DBLP": "journals/corr/abs-2109-04448",
      "ACL": "2021.emnlp-main.775",
      "DOI": "10.18653/v1/2021.emnlp-main.775",
      "CorpusId": 237452717
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers",
    "abstract": "Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 34,
    "citationCount": 77,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "37922370",
        "name": "Stella Frank"
      },
      {
        "authorId": "83574123",
        "name": "Emanuele Bugliarello"
      },
      {
        "authorId": "50369944",
        "name": "Desmond Elliott"
      }
    ]
  },
  "235458570": {
    "paperId": "5c09c7b9d749e7a1f90573b0cfd53606f1038d73",
    "externalIds": {
      "DBLP": "journals/corr/abs-2106-09141",
      "ACL": "2021.findings-acl.318",
      "ArXiv": "2106.09141",
      "DOI": "10.18653/v1/2021.findings-acl.318",
      "CorpusId": 235458570
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Probing Image-Language Transformers for Verb Understanding",
    "abstract": "Multimodal image-language transformers have achieved impressive results on a variety of tasks that rely on fine-tuning (e.g., visual question answering and image retrieval). We are interested in shedding light on the quality of their pretrained representations -- in particular, if these models can distinguish different types of verbs or if they rely solely on nouns in a given sentence. To do so, we collect a dataset of image-sentence pairs (in English) consisting of 421 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset). We use this dataset to evaluate pretrained image-language transformers and find that they fail more in situations that require verb understanding compared to other parts of speech. We also investigate what category of verbs are particularly challenging.",
    "venue": "Findings",
    "year": 2021,
    "referenceCount": 30,
    "citationCount": 102,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2234342",
        "name": "Lisa Anne Hendricks"
      },
      {
        "authorId": "3208081",
        "name": "Aida Nematzadeh"
      }
    ]
  },
  "227238841": {
    "paperId": "dec2f6d3215de9aa2d87d358b7933fb21eeb3bc0",
    "externalIds": {
      "DBLP": "journals/tacl/BugliarelloCOE21",
      "ArXiv": "2011.15124",
      "DOI": "10.1162/tacl_a_00408",
      "CorpusId": 227238841
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs",
    "abstract": "Abstract Large-scale pretraining and task-specific fine- tuning is now the standard methodology for many tasks in computer vision and natural language processing. Recently, a multitude of methods have been proposed for pretraining vision and language BERTs to tackle challenges at the intersection of these two key areas of AI. These models can be categorized into either single-stream or dual-stream encoders. We study the differences between these two categories, and show how they can be unified under a single theoretical framework. We then conduct controlled experiments to discern the empirical differences between five vision and language BERTs. Our experiments show that training data and hyperparameters are responsible for most of the differences between the reported results, but they also reveal that the embedding layer plays a crucial role in these massive models.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 56,
    "citationCount": 110,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "83574123",
        "name": "Emanuele Bugliarello"
      },
      {
        "authorId": "1750769",
        "name": "Ryan Cotterell"
      },
      {
        "authorId": "1764004",
        "name": "Naoaki Okazaki"
      },
      {
        "authorId": "50369944",
        "name": "Desmond Elliott"
      }
    ]
  },
  "12304778": {
    "paperId": "8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5",
    "externalIds": {
      "ArXiv": "1606.07356",
      "DBLP": "conf/emnlp/AgrawalBP16",
      "MAG": "2949866484",
      "ACL": "D16-1203",
      "DOI": "10.18653/v1/D16-1203",
      "CorpusId": 12304778
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Analyzing the Behavior of Visual Question Answering Models",
    "abstract": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016. \nOur behavior analysis reveals that despite recent progress, today's VQA models are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump to conclusions\" (converge on a predicted answer after 'listening' to just half the question), and are \"stubborn\" (do not change their answers across images).",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2016,
    "referenceCount": 28,
    "citationCount": 300,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2801949",
        "name": "Aishwarya Agrawal"
      },
      {
        "authorId": "1746610",
        "name": "Dhruv Batra"
      },
      {
        "authorId": "153432684",
        "name": "Devi Parikh"
      }
    ]
  },
  "52176506": {
    "paperId": "4921243268c81d0d6db99053a9d004852225a622",
    "externalIds": {
      "MAG": "2962735233",
      "DBLP": "conf/emnlp/RohrbachHBDS18",
      "ArXiv": "1809.02156",
      "ACL": "D18-1437",
      "DOI": "10.18653/v1/D18-1437",
      "CorpusId": 52176506
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Object Hallucination in Image Captioning",
    "abstract": "Despite continuously improving performance, contemporary image captioning models are prone to \u201challucinating\u201d objects that are not actually in a scene. One problem is that standard metrics only measure similarity to ground truth captions and may not fully capture image relevance. In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination. We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination. We investigate these questions on the standard image captioning benchmark, MSCOCO, using a diverse set of models. Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 24,
    "citationCount": 318,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34721166",
        "name": "Anna Rohrbach"
      },
      {
        "authorId": "2234342",
        "name": "Lisa Anne Hendricks"
      },
      {
        "authorId": "40895688",
        "name": "Kaylee Burns"
      },
      {
        "authorId": "1753210",
        "name": "Trevor Darrell"
      },
      {
        "authorId": "2903226",
        "name": "Kate Saenko"
      }
    ]
  },
  "219573512": {
    "paperId": "2f5f81bc516a6d085d39479378af1fc27104f91e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2006-06195",
      "MAG": "3102995547",
      "ArXiv": "2006.06195",
      "CorpusId": 219573512
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning",
    "abstract": "We present VILLA, the first known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the \"free\" adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 92,
    "citationCount": 460,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144702900",
        "name": "Zhe Gan"
      },
      {
        "authorId": "2118386757",
        "name": "Yen-Chun Chen"
      },
      {
        "authorId": "50703697",
        "name": "Linjie Li"
      },
      {
        "authorId": "1431754650",
        "name": "Chen Zhu"
      },
      {
        "authorId": "145215470",
        "name": "Yu Cheng"
      },
      {
        "authorId": "46700348",
        "name": "Jingjing Liu"
      }
    ]
  },
  "211204926": {
    "paperId": "0af07469cd031061ac6c5783d5bd0399ac48812e",
    "externalIds": {
      "ArXiv": "2002.08911",
      "DBLP": "journals/corr/abs-2002-08911",
      "ACL": "2021.naacl-main.78",
      "MAG": "3172141633",
      "DOI": "10.18653/V1/2021.NAACL-MAIN.78",
      "CorpusId": 211204926
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Measuring Social Biases in Grounded Vision and Language Embeddings",
    "abstract": "We generalize the notion of measuring social biases in word embeddings to visually grounded word embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these biases in systems will begin to have real-world consequences as they are deployed, making carefully measuring bias and then mitigating it critical to building a fair society.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 27,
    "citationCount": 56,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51519704",
        "name": "Candace Ross"
      },
      {
        "authorId": "143912599",
        "name": "B. Katz"
      },
      {
        "authorId": "21570451",
        "name": "Andrei Barbu"
      }
    ]
  },
  "18279904": {
    "paperId": "f4b654433c53b26b71d5f39dd8ecdc4d8a2acb1f",
    "externalIds": {
      "MAG": "2951315305",
      "ArXiv": "1605.06083",
      "DBLP": "journals/corr/Miltenburg16",
      "CorpusId": 18279904
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Stereotyping and Bias in the Flickr30K Dataset",
    "abstract": "An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they \"focus only on the information that can be obtained from the image alone\" (Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.",
    "venue": "arXiv.org",
    "year": 2016,
    "referenceCount": 15,
    "citationCount": 85,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3192572",
        "name": "Emiel van Miltenburg"
      }
    ]
  },
  "15166472": {
    "paperId": "9bbc952adb3e3c6091d45d800e806d3373a52bac",
    "externalIds": {
      "DBLP": "journals/corr/MisraZMG15",
      "MAG": "2209712595",
      "CorpusId": 15166472
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Learning Visual Classifiers using Human-centric Annotations",
    "abstract": "Many datasets contain human-centric annotations that are the result of humans applying their own subjective judgements on what to describe and what to ignore. Examples include image tags and keywords found on photo sharing sites, or in datasets containing image captions. In this paper, we explore the use of human-centric annotations for learning image classifiers. Due to human reporting bias, these annotations miss a significant amount of the information present in an image. We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. Our algorithm provides results that are highly interpretable for reporting \u201cwhat\u2019s in the image\u201d versus \u201cwhat\u2019s worth saying.\u201d We show improvements over traditional learning algorithms for both image classification and image captioning, and evaluate the algorithm\u2019s efficacy along a variety of metrics and datasets, including MS COCO and Yahoo Flickr 100M.",
    "venue": "arXiv.org",
    "year": 2015,
    "referenceCount": 60,
    "citationCount": 3,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1806773",
        "name": "Ishan Misra"
      },
      {
        "authorId": "1699161",
        "name": "C. L. Zitnick"
      },
      {
        "authorId": "49501003",
        "name": "Margaret Mitchell"
      },
      {
        "authorId": "2983898",
        "name": "Ross B. Girshick"
      }
    ]
  },
  "209862419": {
    "paperId": "22a0e7c06f1e64729211177f80d1b9dcaab3c706",
    "externalIds": {
      "MAG": "3004542466",
      "DBLP": "conf/aies/RajiGMBLD20",
      "ArXiv": "2001.00964",
      "DOI": "10.1145/3375627.3375820",
      "CorpusId": 209862419
    },
    "publicationVenue": {
      "id": "ace94611-0469-4818-ae70-43bdb8082d73",
      "name": "AAAI/ACM Conference on AI, Ethics, and Society",
      "type": "conference",
      "alternate_names": [
        "AAAI/ACM conference Artificial Intelligence, Ethics, and Society",
        "AIES",
        "AAAI/ACM Conf AI Ethics Soc",
        "AAAI/ACM conf Artif Intell Ethics Soc",
        "AIES "
      ]
    },
    "title": "Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing",
    "abstract": "Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of fiveethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.",
    "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
    "year": 2020,
    "referenceCount": 47,
    "citationCount": 246,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Law",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "81316798",
        "name": "Inioluwa Deborah Raji"
      },
      {
        "authorId": "2076288",
        "name": "Timnit Gebru"
      },
      {
        "authorId": "118707418",
        "name": "Margaret Mitchell"
      },
      {
        "authorId": "38222513",
        "name": "Joy Buolamwini"
      },
      {
        "authorId": "2119006",
        "name": "Joonseok Lee"
      },
      {
        "authorId": "40081727",
        "name": "Emily L. Denton"
      }
    ]
  },
  "1389483": {
    "paperId": "135bafc83e9a73c88e759f98a28edfdb5c02f81d",
    "externalIds": {
      "DBLP": "journals/corr/ZhaoWYOC17",
      "MAG": "2951685111",
      "ACL": "D17-1323",
      "ArXiv": "1707.09457",
      "DOI": "10.18653/v1/D17-1323",
      "CorpusId": 1389483
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
    "abstract": "Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively\u3002",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2017,
    "referenceCount": 35,
    "citationCount": 906,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "33524946",
        "name": "Jieyu Zhao"
      },
      {
        "authorId": "1785372925",
        "name": "Tianlu Wang"
      },
      {
        "authorId": "2064210",
        "name": "Mark Yatskar"
      },
      {
        "authorId": "2004053",
        "name": "Vicente Ordonez"
      },
      {
        "authorId": "2782886",
        "name": "Kai-Wei Chang"
      }
    ]
  },
  "19298149": {
    "paperId": "90873a97aa9a43775e5aeea01b03aea54b28bfbd",
    "externalIds": {
      "DBLP": "conf/cvpr/AgrawalBPK18",
      "ArXiv": "1712.00377",
      "MAG": "2963644680",
      "DOI": "10.1109/CVPR.2018.00522",
      "CorpusId": 19298149
    },
    "publicationVenue": null,
    "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering",
    "abstract": "A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.",
    "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
    "year": 2017,
    "referenceCount": 41,
    "citationCount": 553,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2801949",
        "name": "Aishwarya Agrawal"
      },
      {
        "authorId": "1746610",
        "name": "Dhruv Batra"
      },
      {
        "authorId": "153432684",
        "name": "Devi Parikh"
      },
      {
        "authorId": "2684226",
        "name": "Aniruddha Kembhavi"
      }
    ]
  },
  "195584122": {
    "paperId": "ab9cc2c6a8d35d7a145cf608ff9dd7af87213253",
    "externalIds": {
      "DBLP": "conf/nips/CadeneDBCP19",
      "ArXiv": "1906.10169",
      "MAG": "2970017794",
      "CorpusId": 195584122
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "RUBi: Reducing Unimodal Biases in Visual Question Answering",
    "abstract": "Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. \nWe propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. \nOur code is available: this http URL",
    "venue": "Neural Information Processing Systems",
    "year": 2019,
    "referenceCount": 45,
    "citationCount": 336,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "7535126",
        "name": "R\u00e9mi Cad\u00e8ne"
      },
      {
        "authorId": "41020827",
        "name": "Corentin Dancette"
      },
      {
        "authorId": "1405301761",
        "name": "H. Ben-younes"
      },
      {
        "authorId": "51021910",
        "name": "M. Cord"
      },
      {
        "authorId": "153432684",
        "name": "Devi Parikh"
      }
    ]
  },
  "52946763": {
    "paperId": "45ec1446f42c0a7c7fe74319118335c76e0f7b19",
    "externalIds": {
      "MAG": "2890952061",
      "DBLP": "conf/nips/RamakrishnanAL18",
      "ArXiv": "1810.03649",
      "CorpusId": 52946763
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Overcoming Language Priors in Visual Question Answering with Adversarial Regularization",
    "abstract": "Modern Visual Question Answering (VQA) models have been shown to rely heavily on superficial correlations between question and answer words learned during training such as overwhelmingly reporting the type of room as kitchen or the sport being played as tennis, irrespective of the image. Most alarmingly, this shortcoming is often not well reflected during evaluation because the same strong priors exist in test distributions; however, a VQA system that fails to ground questions in image content would likely perform poorly in real-world settings. In this work, we present a novel regularization scheme for VQA that reduces this effect. We introduce a question-only model that takes as input the question encoding from the VQA model and must leverage language biases in order to succeed. We then pose training as an adversarial game between the VQA model and this question-only adversary -- discouraging the VQA model from capturing language biases in its question encoding. Further,we leverage this question-only model to estimate the increase in model confidence after considering the image, which we maximize explicitly to encourage visual grounding. Our approach is a model agnostic training procedure and simple to implement. We show empirically that it can improve performance significantly on a bias-sensitive split of the VQA dataset for multiple base models -- achieving state-of-the-art on this task. Further, on standard VQA tasks, our approach shows significantly less drop in accuracy compared to existing bias-reducing VQA models.",
    "venue": "Neural Information Processing Systems",
    "year": 2018,
    "referenceCount": 32,
    "citationCount": 218,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "31448527",
        "name": "S. Ramakrishnan"
      },
      {
        "authorId": "2801949",
        "name": "Aishwarya Agrawal"
      },
      {
        "authorId": "2297229",
        "name": "Stefan Lee"
      }
    ]
  },
  "218684820": {
    "paperId": "ad322ec0617a9bdf1dabd2a51e626a9c474ed9e3",
    "externalIds": {
      "DBLP": "conf/nips/TeneyAKSKH20",
      "MAG": "3100160322",
      "ArXiv": "2005.09241",
      "CorpusId": 218684820
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "On the Value of Out-of-Distribution Testing: An Example of Goodhart's Law",
    "abstract": "Out-of-distribution (OOD) testing is increasingly popular for evaluating a machine learning system's ability to generalize beyond the biases of a training set. OOD benchmarks are designed to present a different joint distribution of data and labels between training and test time. VQA-CP has become the standard OOD benchmark for visual question answering, but we discovered three troubling practices in its current use. First, most published methods rely on explicit knowledge of the construction of the OOD splits. They often rely on ``inverting'' the distribution of labels, e.g. answering mostly 'yes' when the common training answer is 'no'. Second, the OOD test set is used for model selection. Third, a model's in-domain performance is assessed after retraining it on in-domain splits (VQA v2) that exhibit a more balanced distribution of labels. These three practices defeat the objective of evaluating generalization, and put into question the value of methods specifically designed for this dataset. We show that embarrassingly-simple methods, including one that generates answers at random, surpass the state of the art on some question types. We provide short- and long-term solutions to avoid these pitfalls and realize the benefits of OOD evaluation.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 44,
    "citationCount": 136,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Economics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2406263",
        "name": "Damien Teney"
      },
      {
        "authorId": "33315685",
        "name": "Kushal Kafle"
      },
      {
        "authorId": "153677280",
        "name": "Robik Shrestha"
      },
      {
        "authorId": "8088602",
        "name": "Ehsan Abbasnejad"
      },
      {
        "authorId": "3290098",
        "name": "Christopher Kanan"
      },
      {
        "authorId": "5546141",
        "name": "A. Hengel"
      }
    ]
  },
  "195820364": {
    "paperId": "753b7a701adc1b6072378bd048cfa8567885d9c7",
    "externalIds": {
      "MAG": "2953494151",
      "DBLP": "journals/corr/abs-1907-02893",
      "ArXiv": "1907.02893",
      "CorpusId": 195820364
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Invariant Risk Minimization",
    "abstract": "We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.",
    "venue": "arXiv.org",
    "year": 2019,
    "referenceCount": 77,
    "citationCount": 1941,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2877311",
        "name": "Mart\u00edn Arjovsky"
      },
      {
        "authorId": "52184096",
        "name": "L. Bottou"
      },
      {
        "authorId": "2708454",
        "name": "Ishaan Gulrajani"
      },
      {
        "authorId": "1401804750",
        "name": "David Lopez-Paz"
      }
    ]
  },
  "211532227": {
    "paperId": "55daceb1d28be049b457ec53bc3ffa582c021317",
    "externalIds": {
      "DBLP": "conf/iccv/TeneyAH21",
      "MAG": "3007641164",
      "ArXiv": "2002.11894",
      "DOI": "10.1109/ICCV48922.2021.00145",
      "CorpusId": 211532227
    },
    "publicationVenue": {
      "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision",
      "type": "conference",
      "alternate_names": [
        "ICCV",
        "IEEE Int Conf Comput Vis",
        "ICCV Workshops",
        "ICCV Work"
      ],
      "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
    },
    "title": "Unshuffling Data for Improved Generalization in Visual Question Answering",
    "abstract": "Generalization beyond the training distribution is a core challenge in machine learning. The common practice of mixing and shuffling examples when training neural networks may not be optimal in this regard. We show that partitioning the data into well-chosen, non-i.i.d. subsets treated as multiple training environments can guide the learning of models with better out-of-distribution generalization. We describe a training procedure to capture the patterns that are stable across environments while discarding spurious ones. The method makes a step beyond correlation-based learning: the choice of the partitioning allows injecting information about the task that cannot be otherwise recovered from the joint distribution of the training data.We demonstrate multiple use cases with the task of visual question answering, which is notorious for dataset biases. We obtain significant improvements on VQA-CP, using environments built from prior knowledge, existing meta data, or unsupervised clustering. We also get improvements on GQA using annotations of \"equivalent questions\", and on multi-dataset training (VQA v2 / Visual Genome) by treating them as distinct environments.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2020,
    "referenceCount": 90,
    "citationCount": 69,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2406263",
        "name": "Damien Teney"
      },
      {
        "authorId": "8088602",
        "name": "Ehsan Abbasnejad"
      },
      {
        "authorId": "5546141",
        "name": "A. Hengel"
      }
    ]
  },
  "221802738": {
    "paperId": "72eb95e6424d0e667fe8df8d6ebd439b831406f6",
    "externalIds": {
      "DBLP": "journals/corr/abs-2009-08566",
      "MAG": "3104788521",
      "ArXiv": "2009.08566",
      "ACL": "2020.emnlp-main.63",
      "DOI": "10.18653/v1/2020.emnlp-main.63",
      "CorpusId": 221802738
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering",
    "abstract": "While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a $10.57\\%$ improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 53,
    "citationCount": 127,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "120838645",
        "name": "Tejas Gokhale"
      },
      {
        "authorId": "120722271",
        "name": "Pratyay Banerjee"
      },
      {
        "authorId": "1760291",
        "name": "Chitta Baral"
      },
      {
        "authorId": "1784500",
        "name": "Yezhou Yang"
      }
    ]
  },
  "215828445": {
    "paperId": "29121a31e4d684839cfd0bb358f33ea1266cece5",
    "externalIds": {
      "DBLP": "conf/eccv/TeneyAH20",
      "MAG": "3106544837",
      "ArXiv": "2004.09034",
      "DOI": "10.1007/978-3-030-58607-2_34",
      "CorpusId": 215828445
    },
    "publicationVenue": {
      "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
      "name": "European Conference on Computer Vision",
      "type": "conference",
      "alternate_names": [
        "ECCV",
        "Eur Conf Comput Vis"
      ],
      "url": "https://link.springer.com/conference/eccv"
    },
    "title": "Learning What Makes a Difference from Counterfactual Examples and Gradient Supervision",
    "abstract": null,
    "venue": "European Conference on Computer Vision",
    "year": 2020,
    "referenceCount": 85,
    "citationCount": 109,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2406263",
        "name": "Damien Teney"
      },
      {
        "authorId": "8088602",
        "name": "Ehsan Abbasnejad"
      },
      {
        "authorId": "5546141",
        "name": "A. Hengel"
      }
    ]
  },
  "225053997": {
    "paperId": "74246b7d26078538c01b02586c56b01486a029a6",
    "externalIds": {
      "DBLP": "conf/icml/IlseTF21",
      "CorpusId": 225053997
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Selecting Data Augmentation for Simulating Interventions",
    "abstract": "Machine learning models trained with purely observational data and the principle of empirical risk minimization (Vapnik, 1992) can fail to generalize to unseen domains. In this paper, we focus on the case where the problem arises through spurious correlation between the observed domains and the actual task labels. We \ufb01nd that many domain generalization methods do not explicitly take this spurious correlation into account. Instead, especially in more application-oriented research areas like medical imaging or robotics, data augmentation techniques that are based on heuristics are used to learn domain invariant features. To bridge the gap between theory and practice, we develop a causal perspective on the problem of domain generalization. We argue that causal concepts can be used to explain the success of data augmentation by describing how they can weaken the spurious correlation between the observed domains and the task labels. We demonstrate that data augmentation can serve as a tool for simulating interventional data. We use these theoretical insights to derive a simple algorithm that is able to select data augmentation techniques that will lead to better domain generalization.",
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "referenceCount": 49,
    "citationCount": 46,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2232363631",
        "name": "Maximilian Ilse"
      },
      {
        "authorId": "1849327",
        "name": "Jakub M. Tomczak"
      },
      {
        "authorId": "2232216375",
        "name": "Patrick Forr\u00e9"
      }
    ]
  },
  "209376944": {
    "paperId": "10e597be22f83757b2cbc9426d0ad7f397336764",
    "externalIds": {
      "MAG": "3035512383",
      "ArXiv": "1912.07538",
      "DBLP": "conf/cvpr/AgarwalSF20",
      "DOI": "10.1109/cvpr42600.2020.00971",
      "CorpusId": 209376944
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "Towards Causal VQA: Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing",
    "abstract": "Despite significant success in Visual Question Answering (VQA), VQA models have been shown to be notoriously brittle to linguistic variations in the questions. Due to deficiencies in models and datasets, today\u2019s models often rely on correlations rather than predictions that are causal w.r.t. data. In this paper, we propose a novel way to analyze and measure the robustness of the state of the art models w.r.t semantic visual variations as well as propose ways to make models more robust against spurious correlations. Our method performs automated semantic image manipulations and tests for consistency in model predictions to quantify the model robustness as well as generate synthetic data to counter these problems. We perform our analysis on three diverse, state of the art VQA models and diverse question types with a particular focus on challenging counting questions. In addition, we show that models can be made significantly more robust against inconsistent predictions using our edited data. Finally, we show that results also translate to real-world error cases of state of the art models, which results in improved overall performance",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2019,
    "referenceCount": 34,
    "citationCount": 142,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48189355",
        "name": "Vedika Agarwal"
      },
      {
        "authorId": "38314306",
        "name": "Rakshith Shetty"
      },
      {
        "authorId": "1739548",
        "name": "Mario Fritz"
      }
    ]
  },
  "10565222": {
    "paperId": "0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa",
    "externalIds": {
      "DBLP": "journals/corr/WenGMRSUVY16",
      "MAG": "2963797754",
      "ACL": "E17-1042",
      "ArXiv": "1604.04562",
      "DOI": "10.18653/V1/E17-1042",
      "CorpusId": 10565222
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System",
    "abstract": "Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 39,
    "citationCount": 1066,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1388702112",
        "name": "L. Rojas-Barahona"
      },
      {
        "authorId": "51175233",
        "name": "M. Ga\u0161i\u0107"
      },
      {
        "authorId": "3334541",
        "name": "N. Mrksic"
      },
      {
        "authorId": "2131709",
        "name": "Pei-hao Su"
      },
      {
        "authorId": "2295429",
        "name": "Stefan Ultes"
      },
      {
        "authorId": "144256365",
        "name": "Tsung-Hsien Wen"
      },
      {
        "authorId": "145259603",
        "name": "S. Young"
      },
      {
        "authorId": "92480907",
        "name": "David Vandyke"
      }
    ]
  },
  "238744120": {
    "paperId": "3ea8767b852253e2636b6e57925be7fcc1d739df",
    "externalIds": {
      "ArXiv": "2110.06733",
      "DBLP": "journals/corr/abs-2110-06733",
      "ACL": "2022.acl-long.376",
      "DOI": "10.18653/v1/2022.acl-long.376",
      "CorpusId": 238744120
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Systematic Inequalities in Language Technology Performance across the World\u2019s Languages",
    "abstract": "Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world\u2019s \\approx6,500 languages. We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP. Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as foundational NLP tasks (dependency parsing, morphological inflection). In the process, we (1) quantify disparities in the current state of NLP research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies. Data and code to reproduce the findings discussed in this paper areavailable on GitHub (https://github.com/neubig/globalutility).",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 69,
    "citationCount": 113,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "6894443",
        "name": "Dami\u00e1n E. Blasi"
      },
      {
        "authorId": "49513989",
        "name": "Antonios Anastasopoulos"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  "235313293": {
    "paperId": "d29036946152bddf950fec7a08c2828a8a8f902e",
    "externalIds": {
      "ArXiv": "2104.08570",
      "DBLP": "journals/jair/RazumovskaiaGMP22",
      "DOI": "10.1613/jair.1.13083",
      "CorpusId": 235313293
    },
    "publicationVenue": {
      "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
      "name": "Journal of Artificial Intelligence Research",
      "type": "journal",
      "alternate_names": [
        "JAIR",
        "J Artif Intell Res",
        "The Journal of Artificial Intelligence Research"
      ],
      "issn": "1076-9757",
      "url": "http://www.jair.org/"
    },
    "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
    "abstract": "In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent\u00a0 with the aim of completing a concrete task. Although this technology represents one of\u00a0 the central objectives of AI and has been the focus of ever more intense research and\u00a0 development efforts, it is currently limited to a few narrow domains (e.g., food ordering,\u00a0 ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an\u00a0 extensive overview of existing methods and resources in multilingual ToD as an entry point\u00a0 to this exciting and emerging field. We find that the most critical factor preventing the\u00a0 creation of truly multilingual ToD systems is the lack of datasets in most languages for\u00a0 both training and evaluation. In fact, acquiring annotations or human feedback for each\u00a0 component of modular systems or for data-hungry end-to-end systems is expensive and\u00a0 tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or\u00a0 few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English),\u00a0 either by means of (i) machine translation or (ii) multilingual representations. These\u00a0 approaches are currently viable only for typologically similar languages and languages with\u00a0 parallel / monolingual corpora available. On the other hand, their effectiveness beyond these\u00a0 boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks\u00a0 (especially for natural language generation and end-to-end evaluation). To overcome this\u00a0 limitation, we draw parallels between components of the ToD pipeline and other NLP tasks,\u00a0 which can inspire solutions for learning in low-resource scenarios. Finally, we list additional\u00a0 challenges that multilinguality poses for related areas (such as speech, fluency in generated\u00a0 text, and human-centred evaluation), and indicate future directions that hold promise to\u00a0 further expand language coverage and dialogue capabilities of current ToD systems.\u00a0",
    "venue": "Journal of Artificial Intelligence Research",
    "year": 2021,
    "referenceCount": 317,
    "citationCount": 28,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "66879943",
        "name": "E. Razumovskaia"
      },
      {
        "authorId": "1666177566",
        "name": "Goran Glavavs"
      },
      {
        "authorId": "46963731",
        "name": "Olga Majewska"
      },
      {
        "authorId": "3381663",
        "name": "E. Ponti"
      },
      {
        "authorId": "145762466",
        "name": "A. Korhonen"
      },
      {
        "authorId": "1747849",
        "name": "Ivan Vulic"
      }
    ]
  },
  "2486369": {
    "paperId": "99d2dcdcf4cf05facaa101a48c7e31d140b4736d",
    "externalIds": {
      "DBLP": "journals/coling/PalmerKG05",
      "ACL": "J05-1004",
      "MAG": "2158847908",
      "DOI": "10.1162/0891201053630264",
      "CorpusId": 2486369
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "The Proposition Bank: An Annotated Corpus of Semantic Roles",
    "abstract": "The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty trace categories of the treebank.",
    "venue": "International Conference on Computational Logic",
    "year": 2005,
    "referenceCount": 60,
    "citationCount": 2584,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145755155",
        "name": "Martha Palmer"
      },
      {
        "authorId": "2489901",
        "name": "Paul R. Kingsbury"
      },
      {
        "authorId": "1793218",
        "name": "D. Gildea"
      }
    ]
  },
  "235563506": {
    "paperId": "b1c1bfe5f7a5696909c0ee7de7fbb4092a04c907",
    "externalIds": {
      "DBLP": "journals/ki/GyselVCLMYOCCHH21",
      "MAG": "3159560594",
      "DOI": "10.1007/s13218-021-00722-w",
      "CorpusId": 235563506
    },
    "publicationVenue": null,
    "title": "Designing a Uniform Meaning Representation for Natural Language Processing",
    "abstract": null,
    "venue": "KI - K\u00fcnstliche Intelligenz",
    "year": 2021,
    "referenceCount": 99,
    "citationCount": 65,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "116305713",
        "name": "J. V. Gysel"
      },
      {
        "authorId": "51882643",
        "name": "Meagan Vigus"
      },
      {
        "authorId": "41124366",
        "name": "Jayeol Chun"
      },
      {
        "authorId": "2715566",
        "name": "Kenneth Lai"
      },
      {
        "authorId": "51500425",
        "name": "Sarah Moeller"
      },
      {
        "authorId": "40040342",
        "name": "Jiarui Yao"
      },
      {
        "authorId": "1388957618",
        "name": "Timothy J. O'Gorman"
      },
      {
        "authorId": "2070241255",
        "name": "Andrew Cowell"
      },
      {
        "authorId": "144456145",
        "name": "W. Bruce Croft"
      },
      {
        "authorId": "1405994600",
        "name": "Chu-Ren Huang"
      },
      {
        "authorId": "144002335",
        "name": "Jan Hajic"
      },
      {
        "authorId": "1740728360",
        "name": "James H. Martin"
      },
      {
        "authorId": "2949607",
        "name": "S. Oepen"
      },
      {
        "authorId": "145755155",
        "name": "Martha Palmer"
      },
      {
        "authorId": "1707726",
        "name": "J. Pustejovsky"
      },
      {
        "authorId": "143886116",
        "name": "Rosa Vallejos"
      },
      {
        "authorId": "1702849",
        "name": "Nianwen Xue"
      }
    ]
  },
  "2440012": {
    "paperId": "1ae5c1646ea445a670fe6cc8bf72b589dd9f6e5c",
    "externalIds": {
      "MAG": "2113914762",
      "ACL": "P05-1072",
      "DBLP": "conf/acl/PradhanWHMJ05",
      "DOI": "10.3115/1219840.1219912",
      "CorpusId": 2440012
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Semantic Role Labeling Using Different Syntactic Views",
    "abstract": "Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2005,
    "referenceCount": 24,
    "citationCount": 156,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1735131",
        "name": "Sameer Pradhan"
      },
      {
        "authorId": "1866226",
        "name": "Wayne H. Ward"
      },
      {
        "authorId": "2483422",
        "name": "K. Hacioglu"
      },
      {
        "authorId": "10796472",
        "name": "James H. Martin"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      }
    ]
  },
  "5000956": {
    "paperId": "33a9d1a702eb75da709d26c44aaeb7c2015c870b",
    "externalIds": {
      "ACL": "P14-1134",
      "DBLP": "conf/acl/FlaniganTCDS14",
      "MAG": "2149837184",
      "DOI": "10.3115/v1/P14-1134",
      "CorpusId": 5000956
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Discriminative Graph-Based Parser for the Abstract Meaning Representation",
    "abstract": "Abstract Meaning Representation (AMR) is a semantic formalism for which a grow- ing set of annotated examples is avail- able. We introduce the first approach to parse sentences into this representa- tion, providing a strong baseline for fu- ture improvement. The method is based on a novel algorithm for finding a maxi- mum spanning, connected subgraph, em- bedded within a Lagrangian relaxation of an optimization problem that imposes lin- guistically inspired constraints. Our ap- proach is described in the general frame- work of structured prediction, allowing fu- ture incorporation of additional features and constraints, and may extend to other formalisms as well. Our open-source sys- tem, JAMR, is available at: http://github.com/jflanigan/jamr",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2014,
    "referenceCount": 45,
    "citationCount": 321,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144683841",
        "name": "Jeffrey Flanigan"
      },
      {
        "authorId": "38094552",
        "name": "Sam Thomson"
      },
      {
        "authorId": "143712374",
        "name": "J. Carbonell"
      },
      {
        "authorId": "1745899",
        "name": "Chris Dyer"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      }
    ]
  },
  "46889674": {
    "paperId": "63ef50238ba765edf47c86e3e3fe9f608d8ea00b",
    "externalIds": {
      "ACL": "P18-1037",
      "DBLP": "conf/acl/TitovL18",
      "MAG": "2950858079",
      "ArXiv": "1805.05286",
      "DOI": "10.18653/v1/P18-1037",
      "CorpusId": 46889674
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "AMR Parsing as Graph Prediction with Latent Alignment",
    "abstract": "Abstract meaning representations (AMRs) are broad-coverage sentence-level semantic representations. AMRs represent sentences as rooted labeled directed acyclic graphs. AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences. We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts, relations and alignments. As exact inference requires marginalizing over alignments and is infeasible, we use the variational autoencoding framework and a continuous relaxation of the discrete alignments. We show that joint modeling is preferable to using a pipeline of align and parse. The parser achieves the best reported results on the standard benchmark (74.4% on LDC2016E25).",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 48,
    "citationCount": 126,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2753561",
        "name": "Chunchuan Lyu"
      },
      {
        "authorId": "144889265",
        "name": "Ivan Titov"
      }
    ]
  },
  "222133032": {
    "paperId": "12b28c2d1b58234daa0f06ab43353c401eda1958",
    "externalIds": {
      "ACL": "2020.emnlp-main.196",
      "DBLP": "conf/emnlp/XuLZZZ20",
      "ArXiv": "2010.01771",
      "MAG": "3104890489",
      "DOI": "10.18653/v1/2020.emnlp-main.196",
      "CorpusId": 222133032
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Improving AMR Parsing with Sequence-to-Sequence Pre-training",
    "abstract": "In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than complex models. We make our code and model available at this https URL.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 49,
    "citationCount": 68,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1510477221",
        "name": "Dong Xu"
      },
      {
        "authorId": "2108988344",
        "name": "Junhui Li"
      },
      {
        "authorId": "145490067",
        "name": "Muhua Zhu"
      },
      {
        "authorId": "2156053331",
        "name": "Min Zhang"
      },
      {
        "authorId": "143740945",
        "name": "Guodong Zhou"
      }
    ]
  },
  "227162992": {
    "paperId": "618a84ea5d9ac23c6a93961ba798154026754ddd",
    "externalIds": {
      "MAG": "3108716566",
      "DBLP": "journals/corr/abs-2011-12631",
      "ArXiv": "2011.12631",
      "DOI": "10.1145/3447735",
      "CorpusId": 227162992
    },
    "publicationVenue": {
      "id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
      "name": "Communications of the ACM",
      "type": "journal",
      "alternate_names": [
        "Commun ACM",
        "Communications of The ACM"
      ],
      "issn": "0001-0782",
      "url": "http://www.acm.org/pubs/cacm/",
      "alternate_urls": [
        "http://portal.acm.org/cacm",
        "http://www.acm.org/pubs/contents/journals/cacm/",
        "https://cacm.acm.org/"
      ]
    },
    "title": "A panoramic survey of natural language processing in the Arab world",
    "abstract": "The term natural language refers to any system of symbolic communication (spoken, signed or written) without intentional human planning and design. This distinguishes natural languages such as Arabic and Japanese from artificially constructed languages such as Esperanto or Python. Natural language processing (NLP) is the sub-field of artificial intelligence (AI) focused on modeling natural languages to build applications such as speech recognition and synthesis, machine translation, optical character recognition (OCR), sentiment analysis (SA), question answering, dialogue systems, etc. NLP is a highly interdisciplinary field with connections to computer science, linguistics, cognitive science, psychology, mathematics and others. Some of the earliest AI applications were in NLP (e.g., machine translation); and the last decade (2010-2020) in particular has witnessed an incredible increase in quality, matched with a rise in public awareness, use, and expectations of what may have seemed like science fiction in the past. NLP researchers pride themselves on developing language independent models and tools that can be applied to all human languages, e.g. machine translation systems can be built for a variety of languages using the same basic mechanisms and models. However, the reality is that some languages do get more attention (e.g., English and Chinese) than others (e.g., Hindi and Swahili). Arabic, the primary language of the Arab world and the religious language of millions of non-Arab Muslims is somewhere in the middle of this continuum. Though Arabic NLP has many challenges, it has seen many successes and developments. Next we discuss Arabic's main challenges as a necessary background, and we present a brief history of Arabic NLP. We then survey a number of its research areas, and close with a critical discussion of the future of Arabic NLP.",
    "venue": "Communications of the ACM",
    "year": 2020,
    "referenceCount": 207,
    "citationCount": 87,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143758717",
        "name": "Kareem Darwish"
      },
      {
        "authorId": "1696645",
        "name": "Nizar Habash"
      },
      {
        "authorId": "32273807",
        "name": "Mourad Abbas"
      },
      {
        "authorId": "1556678958",
        "name": "H. Al-Khalifa"
      },
      {
        "authorId": "2028792734",
        "name": "Huseein T. Al-Natsheh"
      },
      {
        "authorId": "1399214738",
        "name": "S. El-Beltagy"
      },
      {
        "authorId": "2063374",
        "name": "Houda Bouamor"
      },
      {
        "authorId": "2512179",
        "name": "Karim Bouzoubaa"
      },
      {
        "authorId": "1399577744",
        "name": "V. Cavalli-Sforza"
      },
      {
        "authorId": "1402224224",
        "name": "W. El-Hajj"
      },
      {
        "authorId": "1764681",
        "name": "Mustafa Jarrar"
      },
      {
        "authorId": "143779235",
        "name": "Hamdy Mubarak"
      }
    ]
  },
  "219260403": {
    "paperId": "6463f532a45f68624cb172d247d19a6601dda270",
    "externalIds": {
      "MAG": "3033076904",
      "DBLP": "journals/corr/abs-2006-02419",
      "ArXiv": "2006.02419",
      "CorpusId": 219260403
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Emergent Multi-Agent Communication in the Deep Learning Era",
    "abstract": "The ability to cooperate through language is a defining feature of humans. As the perceptual, motory and planning capabilities of deep artificial networks increase, researchers are studying whether they also can develop a shared language to interact. From a scientific perspective, understanding the conditions under which language evolves in communities of deep agents and its emergent features can shed light on human language evolution. From an applied perspective, endowing deep networks with the ability to solve problems interactively by communicating with each other and with us should make them more flexible and useful in everyday life. \nThis article surveys representative recent language emergence studies from \nboth of these two angles.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 121,
    "citationCount": 177,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2672644",
        "name": "Angeliki Lazaridou"
      },
      {
        "authorId": "145283199",
        "name": "Marco Baroni"
      }
    ]
  },
  "259253713": {
    "paperId": "1e3ee4a75451ef74febb720a7bdda561f16b964a",
    "externalIds": {
      "DBLP": "journals/corr/abs-1809-09337",
      "CorpusId": 259253713
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Survey of Learning Causality with Data: Problems and Methods",
    "abstract": "The era of big data provides researchers with convenient access to copious data. However, we often have little knowledge of such data. The increasing prevalence of massive data is challenging the traditional methods of learning causality because they were developed for the cases with limited amount of data and strong prior causal knowledge. This survey aims to close the gap between big data and learning causality with a comprehensive and structured review of both traditional and frontier methods followed by a discussion about some open problems of learning causality. We begin with preliminaries of learning causality. Then we categorize and revisit methods of learning causality for the typical problems and data types. After that, we discuss the connections between learning causality and machine learning. At the end, some open problems are presented to show the great potential of learning causality with data",
    "venue": "arXiv.org",
    "year": 2018,
    "referenceCount": 154,
    "citationCount": 223,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2773849",
        "name": "Ruocheng Guo"
      },
      {
        "authorId": "2140175677",
        "name": "Lu Cheng"
      },
      {
        "authorId": "2040455",
        "name": "Jundong Li"
      },
      {
        "authorId": "144974208",
        "name": "P. R. Hahn"
      },
      {
        "authorId": "2146398099",
        "name": "Huan Liu"
      }
    ]
  },
  "261325982": {
    "paperId": "3803ea42e1fc773db3b1d0fa05f41b5ebf0a61d1",
    "externalIds": {
      "DBLP": "journals/pieee/ScholkopfLBKKGB21",
      "DOI": "10.1109/JPROC.2021.3058954",
      "CorpusId": 261325982
    },
    "publicationVenue": {
      "id": "6faaccca-1cc4-45a9-aeb6-96a4901d2606",
      "name": "Proceedings of the IEEE",
      "type": "journal",
      "alternate_names": [
        "Proc IEEE"
      ],
      "issn": "0018-9219",
      "alternate_issns": [
        "1558-2256"
      ],
      "url": "http://www.ieee.org/portal/pages/pubs/proceedings/",
      "alternate_urls": [
        "http://www.ieee.org/products/onlinepubs/pub/about_conference.html",
        "https://ieeexplore.ieee.org/servlet/opac?punumber=5",
        "http://proceedingsoftheieee.ieee.org/"
      ]
    },
    "title": "Toward Causal Representation Learning",
    "abstract": "The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.",
    "venue": "Proceedings of the IEEE",
    "year": 2021,
    "referenceCount": 285,
    "citationCount": 726,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2231240655",
        "name": "Bernhard Sch\u00f6lkopf"
      },
      {
        "authorId": "9557137",
        "name": "Francesco Locatello"
      },
      {
        "authorId": "153125952",
        "name": "Stefan Bauer"
      },
      {
        "authorId": "145604319",
        "name": "Nan Rosemary Ke"
      },
      {
        "authorId": "2583391",
        "name": "Nal Kalchbrenner"
      },
      {
        "authorId": "1996705",
        "name": "Anirudh Goyal"
      },
      {
        "authorId": "1865800402",
        "name": "Y. Bengio"
      }
    ]
  },
  "237386009": {
    "paperId": "130d432ccbc836380a212bea618f84ff094a6a52",
    "externalIds": {
      "ArXiv": "2109.00725",
      "DBLP": "journals/corr/abs-2109-00725",
      "DOI": "10.1162/tacl_a_00511",
      "CorpusId": 237386009
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond",
    "abstract": "Abstract A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the challenges and opportunities in the application of causal inference to the textual domain, with its unique properties. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects with text, encompassing settings where text is used as an outcome, treatment, or to address confounding. In addition, we explore potential uses of causal inference to improve the robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the NLP community.1",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 127,
    "citationCount": 209,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46609506",
        "name": "Amir Feder"
      },
      {
        "authorId": "145137850",
        "name": "Katherine A. Keith"
      },
      {
        "authorId": "2125374460",
        "name": "Emaad A. Manzoor"
      },
      {
        "authorId": "2253657208",
        "name": "Reid Pryzant"
      },
      {
        "authorId": "153485411",
        "name": "Dhanya Sridhar"
      },
      {
        "authorId": "1411379613",
        "name": "Zach Wood-Doughty"
      },
      {
        "authorId": "144154709",
        "name": "Jacob Eisenstein"
      },
      {
        "authorId": "2361828",
        "name": "Justin Grimmer"
      },
      {
        "authorId": "1762757",
        "name": "Roi Reichart"
      },
      {
        "authorId": "2464550",
        "name": "Margaret E. Roberts"
      },
      {
        "authorId": "28924497",
        "name": "Brandon M Stewart"
      },
      {
        "authorId": "2974320",
        "name": "Victor Veitch"
      },
      {
        "authorId": "2143919864",
        "name": "Diyi Yang"
      }
    ]
  },
  "8451212": {
    "paperId": "57a10537978600fd33dcdd48922c791609a4851a",
    "externalIds": {
      "ACL": "D16-1139",
      "DBLP": "conf/emnlp/KimR16",
      "ArXiv": "1606.07947",
      "MAG": "2463507112",
      "DOI": "10.18653/v1/D16-1139",
      "CorpusId": 8451212
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Sequence-Level Knowledge Distillation",
    "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2016,
    "referenceCount": 65,
    "citationCount": 1021,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "38367242",
        "name": "Yoon Kim"
      },
      {
        "authorId": "2531268",
        "name": "Alexander M. Rush"
      }
    ]
  },
  "235435717": {
    "paperId": "128b6540b23cb8316edc496a2c532ea194dbf10e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2106-08122",
      "ACL": "2021.cl-4.29",
      "ArXiv": "2106.08122",
      "DOI": "10.1162/coli_a_00421",
      "CorpusId": 235435717
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Sequence-Level Training for Non-Autoregressive Neural Machine Translation",
    "abstract": "Abstract In recent years, Neural Machine Translation (NMT) has achieved notable results in various translation tasks. However, the word-by-word generation manner determined by the autoregressive mechanism leads to high translation latency of the NMT and restricts its low-latency applications. Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive mechanism and achieves significant decoding speedup by generating target words independently and simultaneously. Nevertheless, NAT still takes the word-level cross-entropy loss as the training objective, which is not optimal because the output of NAT cannot be properly evaluated due to the multimodality problem. In this article, we propose using sequence-level training objectives to train NAT models, which evaluate the NAT outputs as a whole and correlates well with the real translation quality. First, we propose training NAT models to optimize sequence-level evaluation metrics (e.g., BLEU) based on several novel reinforcement algorithms customized for NAT, which outperform the conventional method by reducing the variance of gradient estimation. Second, we introduce a novel training objective for NAT models, which aims to minimize the Bag-of-N-grams (BoN) difference between the model output and the reference sentence. The BoN training objective is differentiable and can be calculated efficiently without doing any approximations. Finally, we apply a three-stage training strategy to combine these two methods to train the NAT model. We validate our approach on four translation tasks (WMT14 En\u2194De, WMT16 En\u2194Ro), which shows that our approach largely outperforms NAT baselines and achieves remarkable performance on all translation tasks. The source code is available at https://github.com/ictnlp/Seq-NAT.",
    "venue": "International Conference on Computational Logic",
    "year": 2021,
    "referenceCount": 69,
    "citationCount": 25,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "81050636",
        "name": "Chenze Shao"
      },
      {
        "authorId": "49771779",
        "name": "Yang Feng"
      },
      {
        "authorId": "2108970018",
        "name": "Jinchao Zhang"
      },
      {
        "authorId": "33427918",
        "name": "Fandong Meng"
      },
      {
        "authorId": "2108485135",
        "name": "Jie Zhou"
      }
    ]
  },
  "201103818": {
    "paperId": "d6cfb4e345b1031040ccd3683730854c560a2b0d",
    "externalIds": {
      "ArXiv": "1908.07181",
      "MAG": "2996843693",
      "DBLP": "conf/aaai/ShuLNC20",
      "DOI": "10.1609/AAAI.V34I05.6413",
      "CorpusId": 201103818
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference using a Delta Posterior",
    "abstract": "Although neural machine translation models reached high translation quality, the autoregressive nature makes inference difficult to parallelize and leads to high translation latency. Inspired by recent refinement-based approaches, we propose LaNMT, a latent-variable non-autoregressive model with continuous latent variables and deterministic inference procedure. In contrast to existing approaches, we use a deterministic inference algorithm to find the target sequence that maximizes the lowerbound to the log-probability. During inference, the length of translation automatically adapts itself. Our experiments show that the lowerbound can be greatly increased by running the inference algorithm, resulting in significantly improved translation quality. Our proposed model closes the performance gap between non-autoregressive and autoregressive approaches on ASPEC Ja-En dataset with 8.6x faster decoding. On WMT'14 En-De dataset, our model narrows the gap with autoregressive baseline to 2.0 BLEU points with 12.5x speedup. By decoding multiple initial latent variables in parallel and rescore using a teacher model, the proposed model further brings the gap down to 1.0 BLEU point on WMT'14 En-De task with 6.8x speedup.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "referenceCount": 35,
    "citationCount": 112,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "7412686",
        "name": "Raphael Shu"
      },
      {
        "authorId": "100811091",
        "name": "Jason Lee"
      },
      {
        "authorId": "48731103",
        "name": "Hideki Nakayama"
      },
      {
        "authorId": "1979489",
        "name": "Kyunghyun Cho"
      }
    ]
  },
  "9901844": {
    "paperId": "261a056f8b21918e8616a429b2df6e1d5d33be41",
    "externalIds": {
      "CorpusId": 9901844
    },
    "publicationVenue": null,
    "title": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
    "abstract": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un-segmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.",
    "venue": "",
    "year": null,
    "referenceCount": 18,
    "citationCount": 4443,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2251771699",
        "name": "Alex Graves"
      },
      {
        "authorId": "2313915717",
        "name": "Santiago Fern\u00b4andez"
      },
      {
        "authorId": "145842938",
        "name": "Faustino J. Gomez"
      },
      {
        "authorId": "2286647360",
        "name": "J. Schmidhuber"
      }
    ]
  },
  "249017760": {
    "paperId": "8ba66cb690ff3a37c63ff0f67b595f03dd78dc75",
    "externalIds": {
      "DBLP": "journals/corr/abs-2205-12209",
      "ArXiv": "2205.12209",
      "DOI": "10.48550/arXiv.2205.12209",
      "CorpusId": 249017760
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start",
    "abstract": "We present EdiT5 - a novel semi-autoregressive text-editing model designed to combine the strengths of non-autoregressive text-editing and autoregressive decoding. EdiT5 is faster during inference than conventional sequence-to-sequence (seq2seq) models, while being capable of modelling flexible input-output transformations. This is achieved by decomposing the generation process into three sub-tasks: (1) tagging to decide on the subset of input tokens to be preserved in the output, (2) re-ordering to define their order in the output text, and (3) insertion to infill the missing tokens that are not present in the input. The tagging and re-ordering steps, which are responsible for generating the largest portion of the output, are non-autoregressive, while the insertion step uses an autoregressive decoder. Depending on the task, EdiT5 on average requires significantly fewer autoregressive steps, demonstrating speedups of up to 25x when compared to seq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5 checkpoint yielding comparable performance to T5 in high-resource settings when evaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction, and Decontextualization while clearly outperforming T5 in low-resource settings.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 42,
    "citationCount": 40,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1931758",
        "name": "Jonathan Mallinson"
      },
      {
        "authorId": "50290651",
        "name": "Jakub Adamek"
      },
      {
        "authorId": "3288074",
        "name": "Eric Malmi"
      },
      {
        "authorId": "3091861",
        "name": "Aliaksei Severyn"
      }
    ]
  },
  "195068920": {
    "paperId": "340b59e6ee93d30c055b5e89a7cfbc88874c9958",
    "externalIds": {
      "DBLP": "conf/acl/DongLRC19",
      "ACL": "P19-1331",
      "ArXiv": "1906.08104",
      "MAG": "2949806279",
      "DOI": "10.18653/v1/P19-1331",
      "CorpusId": 195068920
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing",
    "abstract": "We present the first sentence simplification model that learns explicit edit operations (ADD, DELETE, and KEEP) via a neural programmer-interpreter approach. Most current neural sentence simplification systems are variants of sequence-to-sequence models adopted from machine translation. These methods learn to simplify sentences as a byproduct of the fact that they are trained on complex-simple sentence pairs. By contrast, our neural programmer-interpreter is directly trained to predict explicit edit operations on targeted parts of the input sentence, resembling the way that humans perform simplification and revision. Our model outperforms previous state-of-the-art neural sentence simplification models (without external knowledge) by large margins on three benchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89 WikiSmall, +1.41 Newsela), and is judged by humans to produce overall better and simpler output sentences.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 33,
    "citationCount": 154,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49265991",
        "name": "Yue Dong"
      },
      {
        "authorId": "2118274302",
        "name": "Zichao Li"
      },
      {
        "authorId": "1924511",
        "name": "Mehdi Rezagholizadeh"
      },
      {
        "authorId": "3159752",
        "name": "J. Cheung"
      }
    ]
  },
  "214623124": {
    "paperId": "8c881df7a42e6798bf69b6ecb26b9d0792a378e7",
    "externalIds": {
      "ArXiv": "2003.10687",
      "ACL": "2020.findings-emnlp.111",
      "DBLP": "conf/emnlp/MallinsonSMG20",
      "MAG": "3104814493",
      "DOI": "10.18653/v1/2020.findings-emnlp.111",
      "CorpusId": 214623124
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "FELIX: Flexible Text Editing Through Tagging and Insertion",
    "abstract": "We present FELIX \u2013 a flexible text-editing approach for generation, designed to derive maximum benefit from the ideas of decoding with bi-directional contexts and self-supervised pretraining. In contrast to conventional sequenceto-sequence (seq2seq) models, FELIX is efficient in low-resource settings and fast at inference time, while being capable of modeling flexible input-output transformations. We achieve this by decomposing the text-editing task into two sub-tasks: tagging to decide on the subset of input tokens and their order in the output text and insertion to in-fill the missing tokens in the output not present in the input. The tagging model employs a novel Pointer mechanism, while the insertion model is based on a Masked Language Model (MLM). Both of these models are chosen to be non-autoregressive to guarantee faster inference. FELIX performs favourably when compared to recent text-editing methods and strong seq2seq baselines when evaluated on four NLG tasks: Sentence Fusion, Machine Translation Automatic Post-Editing, Summarization, and Text Simplification",
    "venue": "Findings",
    "year": 2020,
    "referenceCount": 49,
    "citationCount": 71,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1931758",
        "name": "Jonathan Mallinson"
      },
      {
        "authorId": "3091861",
        "name": "Aliaksei Severyn"
      },
      {
        "authorId": "3288074",
        "name": "Eric Malmi"
      },
      {
        "authorId": "143944406",
        "name": "Guillermo Garrido"
      }
    ]
  },
  "218889746": {
    "paperId": "9f6b659033da6fff11da1af64fea7c0d728ab433",
    "externalIds": {
      "DBLP": "conf/bea/OmelianchukACS20",
      "MAG": "3029282628",
      "ACL": "2020.bea-1.16",
      "ArXiv": "2005.12592",
      "DOI": "10.18653/v1/2020.bea-1.16",
      "CorpusId": 218889746
    },
    "publicationVenue": {
      "id": "70cb7170-a1b1-440a-8aed-0d0140a013c6",
      "name": "Workshop on Innovative Use of NLP for Building Educational Applications",
      "type": "conference",
      "alternate_names": [
        "Workshop Innov Use NLP Build Educ Appl",
        "UNLPBEA",
        "BEA"
      ]
    },
    "title": "GECToR \u2013 Grammatical Error Correction: Tag, Not Rewrite",
    "abstract": "In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model/ensemble GEC tagger achieves an F_0.5 of 65.3/66.5 on CONLL-2014 (test) and F_0.5 of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.",
    "venue": "Workshop on Innovative Use of NLP for Building Educational Applications",
    "year": 2020,
    "referenceCount": 28,
    "citationCount": 284,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1388819710",
        "name": "Kostiantyn Omelianchuk"
      },
      {
        "authorId": "1720714633",
        "name": "Vitaliy Atrasevych"
      },
      {
        "authorId": "2064101037",
        "name": "Artem Chernodub"
      },
      {
        "authorId": "2291241972",
        "name": "Oleksandr Skurzhanskyi"
      }
    ]
  },
  "247246721": {
    "paperId": "f327a27515c72d0c7c92e8d2e83475477e68f877",
    "externalIds": {
      "ArXiv": "2206.11218",
      "DBLP": "journals/corr/abs-2206-11218",
      "DOI": "10.1609/aaai.v36i10.21331",
      "CorpusId": 247246721
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Hierarchical Context Tagging for Utterance Rewriting",
    "abstract": "Utterance rewriting aims to recover coreferences and omitted information from the latest turn of a multi-turn dialogue. Recently, methods that tag rather than linearly generate sequences have proven stronger in both in- and out-of-domain rewriting settings. This is due to a tagger's smaller search space as it can only copy tokens from the dialogue context. However, these methods may suffer from low coverage when phrases that must be added to a source utterance cannot be covered by a single context span. This can occur in languages like English that introduce tokens such as prepositions into the rewrite for grammaticality. We propose a hierarchical context tagger (HCT) that mitigates this issue by predicting slotted rules (e.g., \"besides _\") whose slots are later filled with context spans. HCT (i) tags the source string with token-level edit actions and slotted rules and (ii) fills in the resulting rule slots with spans from the dialogue context. This rule tagging allows HCT to add out-of-context tokens and multiple spans at once; we further cluster the rules to truncate the long tail of the rule distribution. Experiments on several benchmarks show that HCT can outperform state-of-the-art rewriting systems by ~2 BLEU points.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2022,
    "referenceCount": 40,
    "citationCount": 10,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2152165376",
        "name": "Lisa Jin"
      },
      {
        "authorId": "1748796",
        "name": "Linfeng Song"
      },
      {
        "authorId": "50496698",
        "name": "Lifeng Jin"
      },
      {
        "authorId": "2111505433",
        "name": "Dong Yu"
      },
      {
        "authorId": "1793218",
        "name": "D. Gildea"
      }
    ]
  },
  "202541578": {
    "paperId": "a3707f5ce5cde48960475f6c6013f10d2e851f15",
    "externalIds": {
      "DBLP": "journals/corr/abs-1909-01187",
      "MAG": "2971505041",
      "ACL": "D19-1510",
      "ArXiv": "1909.01187",
      "DOI": "10.18653/v1/D19-1510",
      "CorpusId": 202541578
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Encode, Tag, Realize: High-Precision Text Editing",
    "abstract": "We propose LaserTagger - a sequence tagging approach that casts text generation as a text editing task. Target texts are reconstructed from the inputs using three main edit operations: keeping a token, deleting it, and adding a phrase before the token. To predict the edit operations, we propose a novel model, which combines a BERT encoder with an autoregressive Transformer decoder. This approach is evaluated on English text on four tasks: sentence fusion, sentence splitting, abstractive summarization, and grammar correction. LaserTagger achieves new state-of-the-art results on three of these tasks, performs comparably to a set of strong seq2seq baselines with a large number of training examples, and outperforms them when the number of examples is limited. Furthermore, we show that at inference time tagging can be more than two orders of magnitude faster than comparable seq2seq models, making it more attractive for running in a live environment.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 51,
    "citationCount": 162,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3288074",
        "name": "Eric Malmi"
      },
      {
        "authorId": "32632038",
        "name": "Sebastian Krause"
      },
      {
        "authorId": "2204815",
        "name": "S. Rothe"
      },
      {
        "authorId": "1789341",
        "name": "Daniil Mirylenka"
      },
      {
        "authorId": "3091861",
        "name": "Aliaksei Severyn"
      }
    ]
  },
  "166227937": {
    "paperId": "f87de21b46683b5743c4d82af3c9cb8bbcd26f21",
    "externalIds": {
      "MAG": "2971167892",
      "DBLP": "journals/corr/abs-1905-11006",
      "ArXiv": "1905.11006",
      "CorpusId": 166227937
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Levenshtein Transformer",
    "abstract": "Modern neural sequence generation models are built to either generate tokens step-by-step from scratch or (iteratively) modify a sequence of tokens bounded by a fixed length. In this work, we develop Levenshtein Transformer, a new partially autoregressive model devised for more flexible and amenable sequence generation. Unlike previous approaches, the atomic operations of our model are insertion and deletion. The combination of them facilitates not only generation but also sequence refinement allowing dynamic length changes. We also propose a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable performance but much-improved efficiency on both generation (e.g. machine translation, text summarization) and refinement tasks (e.g. automatic post-editing). We further confirm the flexibility of our model by showing a Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing.",
    "venue": "Neural Information Processing Systems",
    "year": 2019,
    "referenceCount": 32,
    "citationCount": 345,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3016273",
        "name": "Jiatao Gu"
      },
      {
        "authorId": "20132361",
        "name": "Changhan Wang"
      },
      {
        "authorId": "2109914894",
        "name": "Jake Zhao"
      }
    ]
  },
  "234762899": {
    "paperId": "fd90d2d2853c5b550eab7db203db9f4e7e5a2aaa",
    "externalIds": {
      "ArXiv": "2105.08206",
      "DBLP": "conf/acl/ReidZ21",
      "ACL": "2021.findings-acl.344",
      "DOI": "10.18653/v1/2021.findings-acl.344",
      "CorpusId": 234762899
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "LEWIS: Levenshtein Editing for Unsupervised Text Style Transfer",
    "abstract": "Many types of text style transfer can be achieved with only small, precise edits (e.g. sentiment transfer from I had a terrible time... to I had a great time...). We propose a coarse-to-fine editor for style transfer that transforms text using Levenshtein edit operations (e.g. insert, replace, delete). Unlike prior single-span edit methods, our method concurrently edits multiple spans in the source text. To train without parallel style text pairs (e.g. pairs of +/- sentiment statements), we propose an unsupervised data synthesis procedure. We first convert text to style-agnostic templates using style classifier attention (e.g. I had a SLOT time...), then fill in slots in these templates using fine-tuned pretrained language models. Our method outperforms existing generation and editing style transfer methods on sentiment (Yelp, Amazon) and politeness (Polite) transfer. In particular, multi-span editing achieves higher performance and more diverse output than single-span editing. Moreover, compared to previous methods on unsupervised data synthesis, our method results in higher quality parallel style pairs and improves model performance.",
    "venue": "Findings",
    "year": 2021,
    "referenceCount": 37,
    "citationCount": 68,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1557386977",
        "name": "Machel Reid"
      },
      {
        "authorId": "3428769",
        "name": "Victor Zhong"
      }
    ]
  },
  "222125019": {
    "paperId": "9616236d5b19006d30cd512001bb217d88c1f830",
    "externalIds": {
      "DBLP": "conf/emnlp/MalmiSR20",
      "ACL": "2020.emnlp-main.699",
      "MAG": "3091502979",
      "ArXiv": "2010.01054",
      "DOI": "10.18653/v1/2020.emnlp-main.699",
      "CorpusId": 222125019
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Unsupervised Text Style Transfer with Masked Language Models",
    "abstract": "We propose Masker, an unsupervised text-editing method for style transfer. To tackle cases when no parallel source-target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text spans where the two models disagree the most in terms of likelihood. This allows us to identify the source tokens to delete to transform the source text to match the style of the target domain. The deleted tokens are replaced with the target MLM, and by using a padded MLM variant, we avoid having to predetermine the number of inserted tokens. Our experiments on sentence fusion and sentiment transfer demonstrate that Masker performs competitively in a fully unsupervised setting. Moreover, in low-resource settings, it improves supervised methods' accuracy by over 10 percentage points when pre-training them on silver training data generated by Masker.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 35,
    "citationCount": 11,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3288074",
        "name": "Eric Malmi"
      },
      {
        "authorId": "3091861",
        "name": "Aliaksei Severyn"
      },
      {
        "authorId": "2204815",
        "name": "S. Rothe"
      }
    ]
  },
  "202765548": {
    "paperId": "9da95e99afd4ea899bd1fb40dd350e0be0a12a84",
    "externalIds": {
      "DBLP": "journals/corr/abs-1910-02893",
      "ACL": "D19-1435",
      "MAG": "2977610714",
      "ArXiv": "1910.02893",
      "DOI": "10.18653/v1/D19-1435",
      "CorpusId": 202765548
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Parallel Iterative Edit Models for Local Sequence Transduction",
    "abstract": "We present a Parallel Iterative Edit (PIE) model for the problem of local sequence transduction arising in tasks like Grammatical error correction (GEC). Recent approaches are based on the popular encoder-decoder (ED) model for sequence to sequence learning. The ED model auto-regressively captures full dependency among output tokens but is slow due to sequential decoding. The PIE model does parallel decoding, giving up the advantage of modeling full dependency in the output, yet it achieves accuracy competitive with the ED model for four reasons: 1. predicting edits instead of tokens, 2. labeling sequences instead of generating sequences, 3. iteratively refining predictions to capture dependencies, and 4. factorizing logits over edits and their token argument to harness pre-trained language models like BERT. Experiments on tasks spanning GEC, OCR correction and spell correction demonstrate that the PIE model is an accurate and significantly faster alternative for local sequence transduction.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 42,
    "citationCount": 138,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "67126112",
        "name": "Abhijeet Awasthi"
      },
      {
        "authorId": "1770124",
        "name": "Sunita Sarawagi"
      },
      {
        "authorId": "1381285886",
        "name": "Rasna Goyal"
      },
      {
        "authorId": "3168730",
        "name": "Sabyasachi Ghosh"
      },
      {
        "authorId": "2748067",
        "name": "Vihari Piratla"
      }
    ]
  },
  "221856672": {
    "paperId": "58ef9f9682c0ae4561dc30079a52867f108f704e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2009-11136",
      "ACL": "2020.emnlp-main.418",
      "MAG": "3104681546",
      "ArXiv": "2009.11136",
      "DOI": "10.18653/v1/2020.emnlp-main.418",
      "CorpusId": 221856672
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Seq2Edits: Sequence Transduction Using Span-level Edit Operations",
    "abstract": "We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 62,
    "citationCount": 76,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48404632",
        "name": "Felix Stahlberg"
      },
      {
        "authorId": "2109681515",
        "name": "Shankar Kumar"
      }
    ]
  },
  "12836470": {
    "paperId": "a1765ca8c4aa99a0d35f82d9e310ec3f79004e62",
    "externalIds": {
      "MAG": "2774217864",
      "DBLP": "conf/ijcnlp/Alva-ManchegoBP17",
      "ACL": "I17-1030",
      "CorpusId": 12836470
    },
    "publicationVenue": {
      "id": "e783305c-5d8a-44b9-b7a2-449d474a85b2",
      "name": "International Joint Conference on Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "IJCNLP",
        "Int Jt Conf Nat Lang Process"
      ],
      "url": "https://www.aclweb.org/portal/ijcnlp"
    },
    "title": "Learning How to Simplify From Explicit Labeling of Complex-Simplified Text Pairs",
    "abstract": "Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degree that generalization becomes difficult. End-to-end models also make it hard to interpret what is actually learned from data. We propose a method that decomposes the task of TS into its sub-problems. We devise a way to automatically identify operations in a parallel corpus and introduce a sequence-labeling approach based on these annotations. Finally, we provide insights on the types of transformations that different approaches can model.",
    "venue": "International Joint Conference on Natural Language Processing",
    "year": 2017,
    "referenceCount": 32,
    "citationCount": 68,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "69930782",
        "name": "Fernando Alva-Manchego"
      },
      {
        "authorId": "3053695",
        "name": "Joachim Bingel"
      },
      {
        "authorId": "3302745",
        "name": "Gustavo Paetzold"
      },
      {
        "authorId": "2797847",
        "name": "Carolina Scarton"
      },
      {
        "authorId": "1702974",
        "name": "Lucia Specia"
      }
    ]
  },
  "239017006": {
    "paperId": "aa62d5e43cb151cd574e4df058b4c6a509d62644",
    "externalIds": {
      "DBLP": "journals/corr/abs-2110-09327",
      "ArXiv": "2110.09327",
      "DOI": "10.1109/MSP.2021.3134634",
      "CorpusId": 239017006
    },
    "publicationVenue": {
      "id": "f62e5eab-173a-4e0a-a963-ed8de9835d22",
      "name": "IEEE Signal Processing Magazine",
      "type": "journal",
      "alternate_names": [
        "IEEE Signal Process Mag"
      ],
      "issn": "1053-5888",
      "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79",
      "alternate_urls": [
        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=79"
      ]
    },
    "title": "Self-Supervised Representation Learning: Introduction, advances, and challenges",
    "abstract": "Self-supervised representation learning (SSRL) methods aim to provide powerful, deep feature learning without the requirement of large annotated data sets, thus alleviating the annotation bottleneck\u2014one of the main barriers to the practical deployment of deep learning today. These techniques have advanced rapidly in recent years, with their efficacy approaching and sometimes surpassing fully supervised pretraining alternatives across a variety of data modalities, including image, video, sound, text, and graphs. This article introduces this vibrant area, including key concepts, the four main families of approaches and associated state-of-the-art techniques, and how self-supervised methods are applied to diverse modalities of data. We further discuss practical considerations including workflows, representation transferability, and computational cost. Finally, we survey major open challenges in the field, that provide fertile ground for future work.",
    "venue": "IEEE Signal Processing Magazine",
    "year": 2021,
    "referenceCount": 99,
    "citationCount": 225,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "37151799",
        "name": "Linus Ericsson"
      },
      {
        "authorId": "2319565",
        "name": "H. Gouk"
      },
      {
        "authorId": "1717179",
        "name": "Chen Change Loy"
      },
      {
        "authorId": "1697755",
        "name": "Timothy M. Hospedales"
      }
    ]
  },
  "211532403": {
    "paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0",
    "externalIds": {
      "ArXiv": "2002.12327",
      "MAG": "3006881356",
      "DBLP": "journals/corr/abs-2002-12327",
      "DOI": "10.1162/tacl_a_00349",
      "CorpusId": 211532403
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "A Primer in BERTology: What We Know About How BERT Works",
    "abstract": "Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 208,
    "citationCount": 1329,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145046059",
        "name": "Anna Rogers"
      },
      {
        "authorId": "152176221",
        "name": "Olga Kovaleva"
      },
      {
        "authorId": "1681193",
        "name": "Anna Rumshisky"
      }
    ]
  },
  "236493269": {
    "paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069",
    "externalIds": {
      "DBLP": "journals/csur/LiuYFJHN23",
      "ArXiv": "2107.13586",
      "DOI": "10.1145/3560815",
      "CorpusId": 236493269
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
    "abstract": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub \u201cprompt-based learning.\u201d Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x\u2032 that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x\u0302, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia\u2013Pretrain including constantly updated survey and paperlist.",
    "venue": "ACM Computing Surveys",
    "year": 2021,
    "referenceCount": 223,
    "citationCount": 3158,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144118452",
        "name": "Pengfei Liu"
      },
      {
        "authorId": "30300197",
        "name": "Weizhe Yuan"
      },
      {
        "authorId": "41037252",
        "name": "Jinlan Fu"
      },
      {
        "authorId": "2669515",
        "name": "Zhengbao Jiang"
      },
      {
        "authorId": "50376014",
        "name": "Hiroaki Hayashi"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  "212747830": {
    "paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a",
    "externalIds": {
      "DBLP": "journals/corr/abs-2003-08271",
      "MAG": "3088409176",
      "ArXiv": "2003.08271",
      "DOI": "10.1007/s11431-020-1647-3",
      "CorpusId": 212747830
    },
    "publicationVenue": {
      "id": "e66be951-1e6a-417b-b768-fa43c83c31f6",
      "name": "Science China Technological Sciences",
      "type": "journal",
      "alternate_names": [
        "Sci China Technol Sci",
        "Sci China-technological Sci",
        "Science China-technological Sciences"
      ],
      "issn": "1869-1900",
      "url": "https://link.springer.com/journal/volumesAndIssues/11431"
    },
    "title": "Pre-trained models for natural language processing: A survey",
    "abstract": null,
    "venue": "Science China Technological Sciences",
    "year": 2020,
    "referenceCount": 263,
    "citationCount": 1314,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1767521",
        "name": "Xipeng Qiu"
      },
      {
        "authorId": "153345698",
        "name": "Tianxiang Sun"
      },
      {
        "authorId": "26339093",
        "name": "Yige Xu"
      },
      {
        "authorId": "95329799",
        "name": "Yunfan Shao"
      },
      {
        "authorId": "145493218",
        "name": "Ning Dai"
      },
      {
        "authorId": "1790227",
        "name": "Xuanjing Huang"
      }
    ]
  },
  "233297055": {
    "paperId": "dbfc17833434243e07c4629e58f3d8ed7112dbfe",
    "externalIds": {
      "ArXiv": "2104.08656",
      "ACL": "2021.emnlp-main.437",
      "DBLP": "journals/corr/abs-2104-08656",
      "DOI": "10.18653/v1/2021.emnlp-main.437",
      "CorpusId": 233297055
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Learning from Noisy Labels for Entity-Centric Information Extraction",
    "abstract": "Recent information extraction approaches have relied on training deep neural models. However, such models can easily overfit noisy labels and suffer from performance degradation. While it is very costly to filter noisy labels in large learning resources, recent studies show that such labels take more training steps to be memorized and are more frequently forgotten than clean labels, therefore are identifiable in training. Motivated by such properties, we propose a simple co-regularization framework for entity-centric information extraction, which consists of several neural models with identical structures but different parameter initialization. These models are jointly optimized with the task-specific losses and are regularized to generate similar predictions based on an agreement loss, which prevents overfitting on noisy labels. Extensive experiments on two widely used but noisy benchmarks for information extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework. We release our code to the community for future research.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 50,
    "citationCount": 60,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2203076",
        "name": "Wenxuan Zhou"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      }
    ]
  },
  "233296689": {
    "paperId": "a9b04a3e0cf5766df9b3af8c442f2d85ac5e2c7e",
    "externalIds": {
      "ACL": "2021.emnlp-main.84",
      "ArXiv": "2104.08812",
      "DBLP": "journals/corr/abs-2104-08812",
      "DOI": "10.18653/v1/2021.emnlp-main.84",
      "CorpusId": 233296689
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Contrastive Out-of-Distribution Detection for Pretrained Transformers",
    "abstract": "Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the model often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable model should identify such instances, and then either reject them during inference or pass them over to models that handle another distribution. In this paper, we develop an unsupervised OOD detection method, in which only the in-distribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the Mahalanobis distance in the model\u2019s penultimate layer. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming baselines drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 53,
    "citationCount": 89,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2203076",
        "name": "Wenxuan Zhou"
      },
      {
        "authorId": "144097210",
        "name": "Fangyu Liu"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      }
    ]
  },
  "219179670": {
    "paperId": "0ba05a24b090435d0edd3865abdd70a9168290b4",
    "externalIds": {
      "MAG": "3035690155",
      "ACL": "2020.acl-main.438",
      "ArXiv": "2006.01209",
      "DBLP": "journals/corr/abs-2006-01209",
      "DOI": "10.18653/v1/2020.acl-main.438",
      "CorpusId": 219179670
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Learning Constraints for Structured Prediction Using Rectifier Networks",
    "abstract": "Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 43,
    "citationCount": 8,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3265780",
        "name": "Xingyuan Pan"
      },
      {
        "authorId": "41016174",
        "name": "Maitrey Mehta"
      },
      {
        "authorId": "3052879",
        "name": "Vivek Srikumar"
      }
    ]
  },
  "237485601": {
    "paperId": "0cdc27a99c1520c2ec604b97470ae75227e096ee",
    "externalIds": {
      "ACL": "2021.emnlp-main.134",
      "DBLP": "conf/emnlp/HeZNR21",
      "ArXiv": "2006.05500",
      "DOI": "10.18653/v1/2021.emnlp-main.134",
      "CorpusId": 237485601
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Foreseeing the Benefits of Incidental Supervision",
    "abstract": "Real-world applications often require improved models by leveraging *a range of cheap incidental supervision signals*. These could include partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations \u2013 all having statistical associations with gold annotations but not exactly the same. However, we currently lack a principled way to measure the benefits of these signals to a given target task, and the common practice of evaluating these benefits is through exhaustive experiments with various models and hyperparameters. This paper studies whether we can, *in a single framework, quantify the benefits of various types of incidental signals for a given target task without going through combinatorial experiments*. We propose a unified PAC-Bayesian motivated informativeness measure, PABI, that characterizes the uncertainty reduction provided by incidental supervision signals. We demonstrate PABI\u2019s effectiveness by quantifying the value added by various types of incidental signals to sequence tagging tasks. Experiments on named entity recognition (NER) and question answering (QA) show that PABI\u2019s predictions correlate well with learning performance, providing a promising way to determine, ahead of learning, which supervision signals would be beneficial.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 76,
    "citationCount": 11,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "7146703",
        "name": "Hangfeng He"
      },
      {
        "authorId": "2119682611",
        "name": "Mingyuan Zhang"
      },
      {
        "authorId": "3333257",
        "name": "Qiang Ning"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "222142136": {
    "paperId": "e2d38543bd3cf813c63df336b21b003156ed48a8",
    "externalIds": {
      "MAG": "3092219520",
      "DBLP": "conf/emnlp/YinRRSX20",
      "ACL": "2020.emnlp-main.660",
      "ArXiv": "2010.02584",
      "DOI": "10.18653/v1/2020.emnlp-main.660",
      "CorpusId": 222142136
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start",
    "abstract": "A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pretrained textual entailment system generalize across domains with only a handful of domain-specific examples? and (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations. \nUniversal NLP can be probably achieved through different routines. In this work, we introduce Universal Few-shot textual Entailment (UFO-Entail). We demonstrate that this framework enables a pretrained entailment model to work well on new entailment domains in a few-shot setting, and show its effectiveness as a unified solver for several downstream NLP tasks such as question answering and coreference resolution when the end-task annotations are limited. Code: this https URL",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 36,
    "citationCount": 66,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40483594",
        "name": "Wenpeng Yin"
      },
      {
        "authorId": "8937909",
        "name": "Nazneen Rajani"
      },
      {
        "authorId": "9215251",
        "name": "Dragomir R. Radev"
      },
      {
        "authorId": "2166511",
        "name": "R. Socher"
      },
      {
        "authorId": "2228109",
        "name": "Caiming Xiong"
      }
    ]
  },
  "246822845": {
    "paperId": "ef25f1586cf6630f4a30d41ee5a2848b064dede3",
    "externalIds": {
      "ACL": "2022.tacl-1.35",
      "ArXiv": "2202.06167",
      "DBLP": "journals/tacl/LiYC22",
      "DOI": "10.1162/tacl_a_00479",
      "CorpusId": 246822845
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference",
    "abstract": "The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large number of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics because types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE\ud83c\udf7b, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.1",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 49,
    "citationCount": 28,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1596827240",
        "name": "Bangzheng Li"
      },
      {
        "authorId": "40483594",
        "name": "Wenpeng Yin"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      }
    ]
  },
  "1240016": {
    "paperId": "5a3abc60f0c91a255b1a86843d9e97ab7d63bf08",
    "externalIds": {
      "ACL": "P18-1201",
      "MAG": "2730696199",
      "DBLP": "conf/acl/DaganJVHCR18",
      "ArXiv": "1707.01066",
      "DOI": "10.18653/v1/P18-1201",
      "CorpusId": 1240016
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Zero-Shot Transfer Learning for Event Extraction",
    "abstract": "Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 58,
    "citationCount": 195,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34170717",
        "name": "Lifu Huang"
      },
      {
        "authorId": "2113323573",
        "name": "Heng Ji"
      },
      {
        "authorId": "1979489",
        "name": "Kyunghyun Cho"
      },
      {
        "authorId": "1817166",
        "name": "Clare R. Voss"
      }
    ]
  },
  "202544143": {
    "paperId": "ad3b204c495b049fbd25ccf26eb49e78655f8512",
    "externalIds": {
      "ACL": "D19-1030",
      "DBLP": "conf/emnlp/SubburathinamLJ19",
      "MAG": "2971145411",
      "DOI": "10.18653/v1/D19-1030",
      "CorpusId": 202544143
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Cross-lingual Structure Transfer for Relation and Event Extraction",
    "abstract": "The identification of complex semantic structures such as events and entity relations, already a challenging Information Extraction task, is doubly difficult from sources written in under-resourced and under-annotated languages. We investigate the suitability of cross-lingual structure transfer techniques for these tasks. We exploit relation- and event-relevant language-universal features, leveraging both symbolic (including part-of-speech and dependency path) and distributional (including type representation and contextualized representation) information. By representing all entity mentions, event triggers, and contexts into this complex and structured multilingual common space, using graph convolutional networks, we can train a relation or event extractor from source language annotations and apply it to the target language. Extensive experiments on cross-lingual relation and event transfer among English, Chinese, and Arabic demonstrate that our approach achieves performance comparable to state-of-the-art supervised models trained on up to 3,000 manually annotated mentions: up to 62.6% F-score for Relation Extraction, and 63.1% F-score for Event Argument Role Labeling. The event argument role labeling model transferred from English to Chinese achieves similar performance as the model trained from Chinese. We thus find that language-universal symbolic and distributional representations are complementary for cross-lingual structure transfer.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 77,
    "citationCount": 74,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3393606",
        "name": "Ananya Subburathinam"
      },
      {
        "authorId": "152347526",
        "name": "Di Lu"
      },
      {
        "authorId": "2113323573",
        "name": "Heng Ji"
      },
      {
        "authorId": "143823227",
        "name": "Jonathan May"
      },
      {
        "authorId": "9546964",
        "name": "Shih-Fu Chang"
      },
      {
        "authorId": "2707234",
        "name": "Avirup Sil"
      },
      {
        "authorId": "1817166",
        "name": "Clare R. Voss"
      }
    ]
  },
  "71147690": {
    "paperId": "64584150548d79aadad3a0b3e7a3c949967c5d55",
    "externalIds": {
      "MAG": "2963733234",
      "ArXiv": "1903.02588",
      "DBLP": "conf/naacl/WangXYGCW19",
      "ACL": "N19-1086",
      "DOI": "10.18653/v1/N19-1086",
      "CorpusId": 71147690
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Sentence Embedding Alignment for Lifelong Relation Extraction",
    "abstract": "Conventional approaches to relation extraction usually require a fixed set of pre-defined relations. Such requirement is hard to meet in many real applications, especially when new data and relations are emerging incessantly and it is computationally expensive to store all data and re-train the whole model every time new data and relations come in. We formulate such challenging problem as lifelong relation extraction and investigate memory-efficient incremental learning methods without catastrophically forgetting knowledge learned from previous tasks. We first investigate a modified version of the stochastic gradient methods with a replay memory, which surprisingly outperforms recent state-of-the-art lifelong learning methods. We further propose to improve this approach to alleviate the forgetting problem by anchoring the sentence embedding space. Specifically, we utilize an explicit alignment model to mitigate the sentence embedding distortion of learned model when training on new data and new relations. Experiment results on multiple benchmarks show that our proposed method significantly outperforms the state-of-the-art lifelong learning approaches.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 33,
    "citationCount": 118,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46507182",
        "name": "Hong Wang"
      },
      {
        "authorId": "22253126",
        "name": "Wenhan Xiong"
      },
      {
        "authorId": "2482533",
        "name": "Mo Yu"
      },
      {
        "authorId": "1955964",
        "name": "Xiaoxiao Guo"
      },
      {
        "authorId": "3307026",
        "name": "Shiyu Chang"
      },
      {
        "authorId": "1682479",
        "name": "William Yang Wang"
      }
    ]
  },
  "53845347": {
    "paperId": "2718cd594d2aa09315da52594877cd71d377dfcf",
    "externalIds": {
      "ArXiv": "1811.11683",
      "DBLP": "conf/cvpr/AkbariKBCVC19",
      "MAG": "2954830955",
      "DOI": "10.1109/CVPR.2019.01276",
      "CorpusId": 53845347
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "Multi-Level Multimodal Common Semantic Space for Image-Phrase Grounding",
    "abstract": "We address the problem of phrase grounding by learning a multi-level common semantic space shared by the textual and visual modalities. We exploit multiple levels of feature maps of a Deep Convolutional Neural Network, as well as contextualized word and sentence embeddings extracted from a character-based language model. Following dedicated non-linear mappings for visual features at each level, word, and sentence embeddings, we obtain multiple instantiations of our common semantic space in which comparisons between any target text and the visual content is performed with cosine similarity. We guide the model by a multi-level multimodal attention mechanism which outputs attended visual features at each level. The best level is chosen to be compared with text content for maximizing the pertinence scores of image-sentence pairs of the ground truth. Experiments conducted on three publicly available datasets show significant performance gains (20%-60% relative) over the state-of-the-art in phrase localization and set a new performance record on those datasets. We provide a detailed ablation study to show the contribution of each element of our approach and release our code on GitHub.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2018,
    "referenceCount": 64,
    "citationCount": 73,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "153769937",
        "name": "Hassan Akbari"
      },
      {
        "authorId": "35862299",
        "name": "Svebor Karaman"
      },
      {
        "authorId": "1754397",
        "name": "Surabhi Bhargava"
      },
      {
        "authorId": "2108342501",
        "name": "Brian Chen"
      },
      {
        "authorId": "1856025",
        "name": "Carl Vondrick"
      },
      {
        "authorId": "9546964",
        "name": "Shih-Fu Chang"
      }
    ]
  },
  "218501728": {
    "paperId": "04f7834936bf8f455f804c4d84b52fcffc6784ee",
    "externalIds": {
      "ACL": "2020.acl-main.230",
      "DBLP": "journals/corr/abs-2005-02472",
      "MAG": "3022137941",
      "ArXiv": "2005.02472",
      "DOI": "10.18653/v1/2020.acl-main.230",
      "CorpusId": 218501728
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Cross-media Structured Common Space for Multimedia Event Extraction",
    "abstract": "We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 70,
    "citationCount": 94,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3361240",
        "name": "Manling Li"
      },
      {
        "authorId": "2778637",
        "name": "Alireza Zareian"
      },
      {
        "authorId": "145653969",
        "name": "Qi Zeng"
      },
      {
        "authorId": "153188991",
        "name": "Spencer Whitehead"
      },
      {
        "authorId": "152347526",
        "name": "Di Lu"
      },
      {
        "authorId": "2113323573",
        "name": "Heng Ji"
      },
      {
        "authorId": "9546964",
        "name": "Shih-Fu Chang"
      }
    ]
  },
  "199380526": {
    "paperId": "27ea18d852729a397eea4ad10d7b1478c2c6c55e",
    "externalIds": {
      "DBLP": "journals/topics/AronowitzL20",
      "DOI": "10.1111/tops.12445",
      "CorpusId": 199380526,
      "PubMed": "31373171"
    },
    "publicationVenue": {
      "id": "c2da8960-5aa5-430f-938c-2c2c811e5c96",
      "name": "Topics in Cognitive Science",
      "type": "journal",
      "alternate_names": [
        "Top Cogn Sci"
      ],
      "issn": "1756-8757",
      "url": "http://www3.interscience.wiley.com/journal/121673067/toc",
      "alternate_urls": [
        "http://www.cognitivesciencesociety.org/",
        "https://onlinelibrary.wiley.com/journal/17568765",
        "http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1756-8765"
      ]
    },
    "title": "Experiential Explanation",
    "abstract": "People often answer why-questions with what we call experiential explanations: narratives or stories with temporal structure and concrete details. In contrast, on most theories of the epistemic function of explanation, explanations should be abstractive: structured by general relationships and lacking extraneous details. We suggest that abstractive and experiential explanations differ not only in level of abstraction, but also in structure, and that each form of explanation contributes to the epistemic goals of individual learners and of science. In particular, experiential explanations support mental simulation and survive transitions across background theories; as a result, they support learning and help us translate between competing frameworks. Experiential explanations play an irreducible role in human cognition-and perhaps in science.",
    "venue": "Topics in Cognitive Science",
    "year": 2019,
    "referenceCount": 65,
    "citationCount": 10,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "151419874",
        "name": "S. Aronowitz"
      },
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      }
    ]
  },
  "245707671": {
    "paperId": "ff0f6f8597637a11b68d6b4c20321647fbc47512",
    "externalIds": {
      "PubMedCentral": "8770803",
      "DOI": "10.3389/fpsyg.2021.745586",
      "CorpusId": 245707671,
      "PubMed": "35069325"
    },
    "publicationVenue": {
      "id": "89097a03-8be6-4e2d-ae2c-a6df64c77a06",
      "name": "Frontiers in Psychology",
      "type": "journal",
      "alternate_names": [
        "Front Psychol"
      ],
      "issn": "1664-1078",
      "url": "http://www.frontiersin.org/Cultural_Psychology",
      "alternate_urls": [
        "http://frontiersin.org/psychology/",
        "https://www.frontiersin.org/journals/psychology",
        "http://journal.frontiersin.org/journal/psychology",
        "http://www.frontiersin.org/psychology"
      ]
    },
    "title": "The Explanatory Effect of a Label: Its Influence on a Category Persists Even If We Forget the Label",
    "abstract": "In this study we replicated the explanatory effect of a label which had been found by Giffin et al. (2017). In their experiments, they used vignettes describing an odd behavior of a person based on culturally specific disorders that were unfamiliar to respondents. It turned out that explanations which explain an odd behavior through a person\u2019s tendency to behave that way (circulus vitiosus) seemed more persuasive if the disorder was given a label that was used in the explanation. We replicated these results in Experiment 1, and in a follow-up Experiment 2 we examined the familiarity with category information and the evaluation of that category over time (the delay lasted one week). We realized that the label effect persists even when people make judgments based on their recollections about a category. Furthermore, according to a content analysis of the recollections, participants in the label condition remembered more information from the vignettes but tended to forget an artificial label; however, they used other words from the disorder domain instead (like \u201cdisease\u201d or \u201ckleptomania\u201d). This allowed us to suggest a new interpretation of this effect: we suppose that in the Giffin et al. (2017) experiments the label did not bring any new features to a category itself, but pointed to a relevant domain instead, so the effect appeared from the activation of areas of knowledge in semantic memory and the application of relevant schema for learning a new phenomenon.",
    "venue": "Frontiers in Psychology",
    "year": 2022,
    "referenceCount": 28,
    "citationCount": 5,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2133596475",
        "name": "I. Aslanov"
      },
      {
        "authorId": "2127304399",
        "name": "Yu.V. Sudorgina"
      },
      {
        "authorId": "2064241881",
        "name": "Alexey Kotov"
      }
    ]
  },
  "131768597": {
    "paperId": "a8a4f55a0e8e3c6f2ea5bf15c4425ca236413f7d",
    "externalIds": {
      "MAG": "2747318853",
      "DOI": "10.1007/S11098-017-0958-6",
      "CorpusId": 131768597
    },
    "publicationVenue": null,
    "title": "Stability, breadth and guidance",
    "abstract": null,
    "venue": "",
    "year": 2018,
    "referenceCount": 28,
    "citationCount": 23,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "90002703",
        "name": "Thomas Blanchard"
      },
      {
        "authorId": "145541797",
        "name": "N. Vasilyeva"
      },
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      }
    ]
  },
  "206867154": {
    "paperId": "276cb1bd74ae16e400021a5e17ec8405a2baad54",
    "externalIds": {
      "MAG": "2742426565",
      "DOI": "10.1016/j.cognition.2017.07.011",
      "CorpusId": 206867154,
      "PubMed": "28800495"
    },
    "publicationVenue": {
      "id": "a3c74f56-9d9e-4dca-b159-87ffb93d8e47",
      "name": "Cognition",
      "type": "journal",
      "alternate_names": [
        "Int Conf Augment Cogn",
        "International Conference on Augmented Cognition"
      ],
      "issn": "0010-0277",
      "alternate_issns": [
        "2392-4624"
      ],
      "url": "https://www.journals.elsevier.com/cognition",
      "alternate_urls": [
        "http://www.sciencedirect.com/science/journal/00100277"
      ]
    },
    "title": "The explanatory effect of a label: Explanations with named categories are more satisfying",
    "abstract": null,
    "venue": "Cognition",
    "year": 2017,
    "referenceCount": 37,
    "citationCount": 26,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39795740",
        "name": "Carly Giffin"
      },
      {
        "authorId": "1941739",
        "name": "D. Wilkenfeld"
      },
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      }
    ]
  },
  "1751077": {
    "paperId": "84d7ac28daee0216fb1d58c1d9ce73ad47d6d995",
    "externalIds": {
      "MAG": "1498303196",
      "DBLP": "journals/mima/WilsonK98",
      "DOI": "10.1023/A:1008259020140",
      "CorpusId": 1751077
    },
    "publicationVenue": {
      "id": "76f182a2-ed15-4abe-add6-99f2dcd59171",
      "name": "Minds and Machines",
      "type": "journal",
      "alternate_names": [
        "Mind Mach"
      ],
      "issn": "0924-6495",
      "url": "https://www.springer.com/computer/ai/journal/11023",
      "alternate_urls": [
        "http://www.springer.com/computer/ai/journal/11023",
        "https://link.springer.com/journal/11023",
        "https://www.springer.com/computer/ai/journal/11023/PSE"
      ]
    },
    "title": "The Shadows and Shallows of Explanation",
    "abstract": null,
    "venue": "Minds and Machines",
    "year": 1998,
    "referenceCount": 57,
    "citationCount": 154,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144910421",
        "name": "Robert A. Wilson"
      },
      {
        "authorId": "2391042",
        "name": "F. Keil"
      }
    ]
  },
  "52282459": {
    "paperId": "cff1b89f173310ca828eab41928c6f2a9b9350f6",
    "externalIds": {
      "MAG": "2890687379",
      "DOI": "10.1037/xge0000478",
      "CorpusId": 52282459,
      "PubMed": "30221963"
    },
    "publicationVenue": {
      "id": "ba388b36-981e-4c1b-8048-464cdaa9c9fc",
      "name": "Journal of experimental psychology. General",
      "type": "journal",
      "alternate_names": [
        "J Exp Psychol Gen",
        "J exp psychol Gen",
        "Journal of Experimental Psychology: General"
      ],
      "issn": "0096-3445",
      "url": "http://www.apa.org/journals/xge.html",
      "alternate_urls": [
        "http://content.apa.org/journals/xge",
        "http://www.apa.org/pubs/journals/xge/index.aspx"
      ]
    },
    "title": "Community Appeal: Explanation Without Information",
    "abstract": "Formal or categorical explanation involves the use of a label to explain a property of an object or group of objects. In four experiments, we provide evidence that label entrenchment, the degree to which a label is accepted and used by members of the community, influences the judged quality of a categorical explanation whether or not the explanation offers substantive information about the explanandum. Experiment 1 shows that explanations using unentrenched labels are seen as less comprehensive and less natural, independent of the causal information they provide. Experiment 2 shows that these intuitions persist when the community has no additional, relevant featural information, so the label amounts to a mere name for the explanandum. Experiment 3 finds a similar effect when the unentrenched label is not widely used, but is defined by a group of experts and the recipient of the explanation is herself an expert familiar with the topic. The effect also obtains for categories that lack a coherent causal structure. Experiment 4 further demonstrates the domain generality of the entrenchment effect and provides evidence against several interpretations of the results. A majority of participants in Experiments 3 and 4 could not report the impact of entrenchment on their judgments. We argue that this reliance on community cues arose because the community often has useful information to provide about categories. The common use of labels as conduits for this communal knowledge results in reliance on community cues even when they are uninformative.",
    "venue": "Journal of experimental psychology. General",
    "year": 2018,
    "referenceCount": 0,
    "citationCount": 24,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51309124",
        "name": "Babak Hemmatian"
      },
      {
        "authorId": "2404363",
        "name": "S. Sloman"
      }
    ]
  },
  "2901254": {
    "paperId": "60509bfce90600c06945c0019ae8c3997a60b6d7",
    "externalIds": {
      "MAG": "2107961136",
      "DOI": "10.1016/S1364-6613(03)00158-X",
      "CorpusId": 2901254,
      "PubMed": "12907233"
    },
    "publicationVenue": {
      "id": "20cb626a-518d-4016-826a-0157cf2f8fd6",
      "name": "Trends in Cognitive Sciences",
      "type": "journal",
      "alternate_names": [
        "Trends Cogn Sci"
      ],
      "issn": "1364-6613",
      "url": "https://www.cell.com/trends/cognitive-sciences/home",
      "alternate_urls": [
        "http://www.cell.com/trends/cognitive-sciences/home",
        "https://www.journals.elsevier.com/trends-in-cognitive-sciences",
        "http://www.sciencedirect.com/science/journal/13646613"
      ]
    },
    "title": "Folkscience: coarse interpretations of a complex reality",
    "abstract": null,
    "venue": "Trends in Cognitive Sciences",
    "year": 2003,
    "referenceCount": 65,
    "citationCount": 170,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2391042",
        "name": "F. Keil"
      }
    ]
  },
  "15915531": {
    "paperId": "3865295b6111fcf620b674a2ca8b9f758d7f5715",
    "externalIds": {
      "MAG": "2163001257",
      "DOI": "10.1111/1467-9280.00302",
      "CorpusId": 15915531,
      "PubMed": "11294222"
    },
    "publicationVenue": {
      "id": "a990ee76-444c-4bda-b714-df4dad0efa5d",
      "name": "Psychology Science",
      "type": "journal",
      "alternate_names": [
        "Psychol Sci",
        "Psychological Science"
      ],
      "issn": "1614-9947",
      "alternate_issns": [
        "0956-7976"
      ],
      "url": "http://www.psychologie-aktuell.com/index.php?id=204",
      "alternate_urls": [
        "http://www.jstor.org/journals/09567976.html",
        "http://www.jstor.org/action/showPublication?journalCode=psychsci",
        "https://www.jstor.org/journal/psychsci",
        "http://www.blackwellpublishers.co.uk/online",
        "http://pss.sagepub.com/"
      ]
    },
    "title": "How do People Know?",
    "abstract": "To fully understand processes of knowing and knowledge acquisition, it is necessary to examine people's understanding of their own knowing. Individual and developmental differences in what it means to know something, and hence in the criteria for justifying knowledge claims, have potentially wide-ranging implications. In providing support for a claim, young children have difficulty differentiating explanation of why a claim makes sense and evidence that the claim is true. Epistemic understanding progresses developmentally, but substantial variation remains among adults, with few adults achieving understanding of the complementary strengths and weaknesses of evidence and explanation in argument. Epistemic understanding shapes intellectual values and hence the disposition (as opposed to competence) to exercise intellectual skills. Only its most advanced levels support a disposition to engage in the intellectual effort that reasoned argument entails. The sample case of juror reasoning illustrates how epistemic understanding underlies and shapes intellectual performance.",
    "venue": "Psychology Science",
    "year": 2001,
    "referenceCount": 54,
    "citationCount": 373,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143633493",
        "name": "D. Kuhn"
      }
    ]
  },
  "251048300": {
    "paperId": "572175048f460d76de1780f715fed53178e259f8",
    "externalIds": {
      "DOI": "10.1017/S1358246100005130",
      "CorpusId": 251048300
    },
    "publicationVenue": {
      "id": "d3292315-bbf7-46f7-9627-ea69b554c4fe",
      "name": "Royal Institute of Philosophy Supplement",
      "type": "journal",
      "alternate_names": [
        "R Inst Philos Suppl"
      ],
      "issn": "1358-2461",
      "url": "https://www.cambridge.org/core/journals/royal-institute-of-philosophy-supplements"
    },
    "title": "Contrastive Explanation",
    "abstract": "According to a causal model of explanation, we explain phenomena by giving their causes or, where the phenomena are themselves causal regularities, we explain them by giving a mechanism linking cause and effect. If we explain why smoking causes cancer, we do not give the cause of this causal connection, but we do give the causal mechanism that makes it. The claim that to explain is to give a cause is not only natural and plausible, but it also avoids many of the objections to other accounts of explanation, such as the views that to explain is to give a reason to believe the phenomenon occurred, to somehow make the phenomenon familiar, or to give a Deductive-Nomological argument. Unlike the reason for belief account, a causal model makes a clear distinction between understanding why a phenomenon occurs and merely knowing that it does, and the model does so in a way that makes understanding unmysterious and objective. Understanding is not some sort of super-knowledge, but simply more knowledge: knowledge of the phenomenon and knowledge of its causal history. A causal model makes it clear how something can explain without itself being explained, and so avoids the regress of whys, since we can know a phenomenon's cause without knowing the cause of the cause. It also accounts for legitimate self-evidencing explanations, explanations where the phenomenon is an essential part of the evidence that the explanation is correct, so the explanation can not supply a non-circular reason for believing the phenomenon occurred. There is no barrier to knowing a cause through its effects and also knowing that it is their cause. The speed of recession of a star explains its observed red-shift, even though the shift is an essential part of the evidence for its speed of recession. The model also avoids the most serious objection to the familiarity view, which is that some phenomena are familiar yet not understood, since a phenomenon can be perfectly familiar, such as the blueness of the sky or the fact that the same side of the moon always faces the earth, even if we do not know its cause. Finally, a causal model avoids many of the objections to the Deductive-Nomological model. Ordinary explanations do not have to meet the requirements of the Deductive-Nomological model, because one does not need to give a law to give a cause, and one does not need to know a law to have good reason to believe that a cause is a cause. As for the notorious over-permissiveness of the Deductive-Nomological model, the reason recession explains red-shift but not conversely, is simply that causes explain effects but not conversely, and the reason a conjunction of laws does not explain its conjuncts is that conjunctions do not cause their conjuncts.",
    "venue": "Royal Institute of Philosophy Supplement",
    "year": 1990,
    "referenceCount": 9,
    "citationCount": 276,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2063321",
        "name": "P. Lipton"
      }
    ]
  },
  "149066570": {
    "paperId": "49caff587dc7e08913eed9acf407bd3d4db7e363",
    "externalIds": {
      "MAG": "2627159663",
      "DOI": "10.1093/OXFORDHB/9780199734689.013.0014",
      "CorpusId": 149066570
    },
    "publicationVenue": null,
    "title": "Explanation and Abductive Inference",
    "abstract": "Everyday cognition reveals a sophisticated capacity to seek, generate, and evaluate explanations for the social and physical worlds around us. Why are we so driven to explain, and what accounts for our systematic explanatory preferences? This chapter reviews evidence from cognitive psychology and cognitive development concerning the structure and function of explanations, with a focus on the role of explanations in learning and inference. The findings highlight the value of understanding explanation and abductive inference both as phenomena in their own right and for the insights they provide concerning foundational aspects of human cognition, such as representation, learning, and inference.",
    "venue": "",
    "year": 2012,
    "referenceCount": 139,
    "citationCount": 240,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      }
    ]
  },
  "3632849": {
    "paperId": "866b6c8285b8cb6e6bdef5edb8933ee2f4aebd66",
    "externalIds": {
      "MAG": "2513866468",
      "DOI": "10.1016/j.tics.2016.08.001",
      "CorpusId": 3632849,
      "PubMed": "27567318"
    },
    "publicationVenue": {
      "id": "20cb626a-518d-4016-826a-0157cf2f8fd6",
      "name": "Trends in Cognitive Sciences",
      "type": "journal",
      "alternate_names": [
        "Trends Cogn Sci"
      ],
      "issn": "1364-6613",
      "url": "https://www.cell.com/trends/cognitive-sciences/home",
      "alternate_urls": [
        "http://www.cell.com/trends/cognitive-sciences/home",
        "https://www.journals.elsevier.com/trends-in-cognitive-sciences",
        "http://www.sciencedirect.com/science/journal/13646613"
      ]
    },
    "title": "Explanatory Preferences Shape Learning and Inference",
    "abstract": null,
    "venue": "Trends in Cognitive Sciences",
    "year": 2016,
    "referenceCount": 84,
    "citationCount": 162,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      }
    ]
  },
  "202740177": {
    "paperId": "87a65c172ff4ab532486523560224c4ab2f2aa02",
    "externalIds": {
      "MAG": "2973893193",
      "DOI": "10.1093/oso/9780190860974.003.0011",
      "CorpusId": 202740177
    },
    "publicationVenue": null,
    "title": "Mechanistic versus Functional Understanding",
    "abstract": "Many natural and artificial entities can be predicted and explained both mechanistically, in term of parts and proximate causal processes, as well as functionally, in terms of functions and goals. Do these distinct \u201cstances\u201d or \u201cmodes of construal\u201d support fundamentally different kinds of understanding? Based on recent work in epistemology and philosophy of science, as well as empirical evidence from cognitive and developmental psychology, this chapter argues for the \u201cweak differentiation thesis\u201d: the claim that mechanistic and functional understanding are distinct in that they involve importantly different objects. The chapter also considers more tentative arguments for the \u201cstrong differentiation thesis\u201d: the claim that mechanistic and functional understanding involve different epistemic relationships between mind and world.",
    "venue": "Varieties of Understanding",
    "year": 2019,
    "referenceCount": 47,
    "citationCount": 28,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Chemistry",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      },
      {
        "authorId": "1941739",
        "name": "D. Wilkenfeld"
      }
    ]
  },
  "119309561": {
    "paperId": "70dbe3e740a5e7927ccce00fd615365b08a6eaae",
    "externalIds": {
      "ArXiv": "1904.07451",
      "MAG": "2950932789",
      "DBLP": "journals/corr/abs-1904-07451",
      "CorpusId": 119309561
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Counterfactual Visual Explanations",
    "abstract": "In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query' image $I$ for which a vision system predicts class $c$, a counterfactual visual explanation identifies how $I$ could change such that the system would output a different specified class $c'$. To do this, we select a 'distractor' image $I'$ that the system predicts as class $c'$ and identify spatial regions in $I$ and $I'$ such that replacing the identified region in $I$ with the identified region in $I'$ would push the system towards classifying $I$ as $c'$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.",
    "venue": "International Conference on Machine Learning",
    "year": 2019,
    "referenceCount": 31,
    "citationCount": 471,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "37226164",
        "name": "Yash Goyal"
      },
      {
        "authorId": "2109685046",
        "name": "Ziyan Wu"
      },
      {
        "authorId": "39497207",
        "name": "Jan Ernst"
      },
      {
        "authorId": "1746610",
        "name": "Dhruv Batra"
      },
      {
        "authorId": "153432684",
        "name": "Devi Parikh"
      },
      {
        "authorId": "2297229",
        "name": "Stefan Lee"
      }
    ]
  },
  "253510293": {
    "paperId": "f9145d932ae9e454d9a45ae23fdb0ec0171e4ef4",
    "externalIds": {
      "ArXiv": "2010.10596",
      "DOI": "10.1145/3677119",
      "CorpusId": 253510293
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review",
    "abstract": "\n Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of\n explainability\n in machine learning. In this paper, we seek to review and categorize research on\n counterfactual explanations\n , a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.\n",
    "venue": "ACM Computing Surveys",
    "year": 2020,
    "referenceCount": 376,
    "citationCount": 122,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1780214785",
        "name": "Sahil Verma"
      },
      {
        "authorId": "2190750501",
        "name": "Varich Boonsanong"
      },
      {
        "authorId": "2190750431",
        "name": "Minh Hoang"
      },
      {
        "authorId": "4634403",
        "name": "Keegan E. Hines"
      },
      {
        "authorId": "1718974",
        "name": "John P. Dickerson"
      },
      {
        "authorId": "2145672392",
        "name": "Chirag Shah"
      }
    ]
  },
  "199543734": {
    "paperId": "d6bf6420d78efd64ce8161ec3fb1e93574b67c44",
    "externalIds": {
      "MAG": "3003420231",
      "DBLP": "conf/icdm/GurumoorthyDCA19",
      "DOI": "10.1109/ICDM.2019.00036",
      "CorpusId": 199543734
    },
    "publicationVenue": {
      "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
      "name": "Industrial Conference on Data Mining",
      "type": "conference",
      "alternate_names": [
        "Ind Conf Data Min",
        "ICDM"
      ],
      "url": "http://www.data-mining-forum.de/"
    },
    "title": "Efficient Data Representation by Selecting Prototypes with Importance Weights",
    "abstract": "Prototypical examples that best summarize and compactly represent an underlying complex data distribution, communicate meaningful insights to humans in domains where simple explanations are hard to extract. In this paper, we present algorithms with strong theoretical guarantees to mine these data sets and select prototypes, a.k.a. representatives that optimally describes them. Our work notably generalizes the recent work by Kim et al. (2016) where in addition to selecting prototypes, we also associate non-negative weights which are indicative of their importance. This extension provides a single coherent framework under which both prototypes and criticisms (i.e. outliers) can be found. Furthermore, our framework works for any symmetric positive definite kernel thus addressing one of the key open questions laid out in Kim et al. (2016). By establishing that our objective function enjoys a key property of that of weak submodularity, we present a fast ProtoDash algorithm and also derive approximation guarantees for the same. We demonstrate the efficacy of our method on diverse domains such as retail, digit recognition (MNIST) and on publicly available 40 health questionnaires obtained from the Center for Disease Control (CDC) website maintained by the US Dept. of Health. We validate the results quantitatively as well as qualitatively based on expert feedback and recently published scientific studies on public health, thus showcasing the power of our technique in providing actionability (for retail), utility (for MNIST), and insight (on CDC datasets), which arguably are the hallmarks of an effective interpretable machine learning method.",
    "venue": "Industrial Conference on Data Mining",
    "year": 2017,
    "referenceCount": 26,
    "citationCount": 105,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3097313",
        "name": "Karthik S. Gurumoorthy"
      },
      {
        "authorId": "2145784",
        "name": "Amit Dhurandhar"
      },
      {
        "authorId": "40193335",
        "name": "G. Cecchi"
      },
      {
        "authorId": "1682418",
        "name": "C. Aggarwal"
      }
    ]
  },
  "28671436": {
    "paperId": "d65ce2b8300541414bfe51d03906fca72e93523c",
    "externalIds": {
      "MAG": "2950953798",
      "ArXiv": "1706.04599",
      "DBLP": "journals/corr/GuoPSW17",
      "CorpusId": 28671436
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "On Calibration of Modern Neural Networks",
    "abstract": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.",
    "venue": "International Conference on Machine Learning",
    "year": 2017,
    "referenceCount": 52,
    "citationCount": 5125,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144993411",
        "name": "Chuan Guo"
      },
      {
        "authorId": "10804137",
        "name": "Geoff Pleiss"
      },
      {
        "authorId": "2117103358",
        "name": "Yu Sun"
      },
      {
        "authorId": "7446832",
        "name": "Kilian Q. Weinberger"
      }
    ]
  },
  "52003282": {
    "paperId": "74e9053d6f44f4507bd40bbea999ee65f0cbefb2",
    "externalIds": {
      "MAG": "2950893515",
      "DBLP": "conf/emnlp/FengWGIRB18",
      "ACL": "D18-1407",
      "ArXiv": "1804.07781",
      "DOI": "10.18653/v1/D18-1407",
      "CorpusId": 52003282
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Pathologies of Neural Models Make Interpretations Difficult",
    "abstract": "One way to interpret neural model predictions is to highlight the most important input features\u2014for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word\u2019s importance is determined by either input perturbation\u2014measuring the decrease in model confidence when that word is removed\u2014or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction, without accuracy loss on regular examples.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 50,
    "citationCount": 309,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144588144",
        "name": "Shi Feng"
      },
      {
        "authorId": "145217343",
        "name": "Eric Wallace"
      },
      {
        "authorId": "2778913",
        "name": "Alvin Grissom II"
      },
      {
        "authorId": "2136562",
        "name": "Mohit Iyyer"
      },
      {
        "authorId": "145009056",
        "name": "Pedro Rodriguez"
      },
      {
        "authorId": "1389036863",
        "name": "Jordan L. Boyd-Graber"
      }
    ]
  },
  "237498988": {
    "paperId": "2a456fd1d47a396feef9a1a2cf140a71bbc78ad4",
    "externalIds": {
      "ArXiv": "2104.04515",
      "DBLP": "conf/emnlp/YeND21",
      "ACL": "2021.emnlp-main.447",
      "DOI": "10.18653/v1/2021.emnlp-main.447",
      "CorpusId": 237498988
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Connecting Attributions and QA Model Behavior on Realistic Counterfactuals",
    "abstract": "When a model attribution technique highlights a particular part of the input, a user might understand this highlight as making a statement about counterfactuals (Miller, 2019): if that part of the input were to change, the model\u2019s prediction might change as well. This paper investigates how well different attribution techniques align with this assumption on realistic counterfactuals in the case of reading comprehension (RC). RC is a particularly challenging test case, as token-level attributions that have been extensively studied in other NLP tasks such as sentiment analysis are less suitable to represent the reasoning that RC models perform. We construct counterfactual sets for three different RC settings, and through heuristics that can connect attribution methods\u2019 outputs to high-level model behavior, we can evaluate how useful different attribution methods and even different formats are for understanding counterfactuals. We find that pairwise attributions are better suited to RC than token-level attributions across these different RC settings, with our best performance coming from a modification that we propose to an existing pairwise attribution method.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 64,
    "citationCount": 24,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "50183897",
        "name": "Xi Ye"
      },
      {
        "authorId": "2066327952",
        "name": "Rohan Nair"
      },
      {
        "authorId": "1814094",
        "name": "Greg Durrett"
      }
    ]
  },
  "210837324": {
    "paperId": "4244da5fc9b7e57ae7bb0c1b4c08c662af595a23",
    "externalIds": {
      "MAG": "3001062618",
      "ArXiv": "2001.08298",
      "DBLP": "journals/corr/abs-2001-08298",
      "DOI": "10.1145/3377325.3377498",
      "CorpusId": 210837324
    },
    "publicationVenue": {
      "id": "d43f0f26-43c9-4009-a1ed-63624522c166",
      "name": "International Conference on Intelligent User Interfaces",
      "type": "conference",
      "alternate_names": [
        "IUI",
        "Intell User Interface",
        "Int Conf Intell User Interface",
        "Intelligent User Interfaces"
      ],
      "url": "http://www.iuiconf.org/"
    },
    "title": "Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems",
    "abstract": "Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone.",
    "venue": "International Conference on Intelligent User Interfaces",
    "year": 2020,
    "referenceCount": 51,
    "citationCount": 236,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1490644118",
        "name": "Zana Bu\u00e7inca"
      },
      {
        "authorId": "2113316454",
        "name": "Phoebe Lin"
      },
      {
        "authorId": "1770992",
        "name": "Krzysztof Z Gajos"
      },
      {
        "authorId": "143730651",
        "name": "Elena L. Glassman"
      }
    ]
  },
  "231698739": {
    "paperId": "83f66dfaf78dbb7d10aa914a8f2a790346923c6e",
    "externalIds": {
      "DBLP": "conf/chi/SureshGNS21",
      "ArXiv": "2101.09824",
      "DOI": "10.1145/3411764.3445088",
      "CorpusId": 231698739
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs",
    "abstract": "To ensure accountability and mitigate harm, it is critical that diverse stakeholders can interrogate black-box automated systems and find information that is understandable, relevant, and useful to them. In this paper, we eschew prior expertise- and role-based categorizations of interpretability stakeholders in favor of a more granular framework that decouples stakeholders\u2019 knowledge from their interpretability needs. We characterize stakeholders by their formal, instrumental, and personal knowledge and how it manifests in the contexts of machine learning, the data domain, and the general milieu. We additionally distill a hierarchical typology of stakeholder needs that distinguishes higher-level domain goals from lower-level interpretability tasks. In assessing the descriptive, evaluative, and generative powers of our framework, we find our more nuanced treatment of stakeholders reveals gaps and opportunities in the interpretability literature, adds precision to the design and comparison of user studies, and facilitates a more reflexive approach to conducting this research.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2021,
    "referenceCount": 163,
    "citationCount": 104,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46537606",
        "name": "Harini Suresh"
      },
      {
        "authorId": "21242765",
        "name": "Steven R. Gomez"
      },
      {
        "authorId": "31422836",
        "name": "K. Nam"
      },
      {
        "authorId": "2795670",
        "name": "Arvind Satyanarayan"
      }
    ]
  },
  "210064344": {
    "paperId": "7d089d4cc4aff5c10c1704f02119e2487fc898c9",
    "externalIds": {
      "DBLP": "journals/corr/abs-2001-02478",
      "ArXiv": "2001.02478",
      "MAG": "2999765337",
      "DOI": "10.1145/3313831.3376590",
      "CorpusId": 210064344
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "Questioning the AI: Informing Design Practices for Explainable AI User Experiences",
    "abstract": "A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2020,
    "referenceCount": 107,
    "citationCount": 603,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144921048",
        "name": "Q. Liao"
      },
      {
        "authorId": "72022637",
        "name": "D. Gruen"
      },
      {
        "authorId": "2110279265",
        "name": "Sarah Miller"
      }
    ]
  },
  "263796884": {
    "paperId": "4814c9f9ced14c0dedab7cb80be9e88c8e892604",
    "externalIds": {
      "MAG": "2912324183",
      "DBLP": "journals/corr/abs-1901-07694",
      "ArXiv": "1901.07694",
      "DOI": "10.1145/3301275.3302310",
      "CorpusId": 263796884
    },
    "publicationVenue": {
      "id": "d43f0f26-43c9-4009-a1ed-63624522c166",
      "name": "International Conference on Intelligent User Interfaces",
      "type": "conference",
      "alternate_names": [
        "IUI",
        "Intell User Interface",
        "Int Conf Intell User Interface",
        "Intelligent User Interfaces"
      ],
      "url": "http://www.iuiconf.org/"
    },
    "title": "Explaining models: an empirical study of how explanations impact fairness judgment",
    "abstract": "Ensuring fairness of machine learning systems is a human-in-the-loop process. It relies on developers, users, and the general public to identify fairness problems and make improvements. To facilitate the process we need effective, unbiased, and user-friendly explanations that people can confidently rely on. Towards that end, we conducted an empirical study with four types of programmatically generated explanations to understand how they impact people's fairness judgments of ML systems. With an experiment involving more than 160 Mechanical Turk workers, we show that: 1) Certain explanations are considered inherently less fair, while others can enhance people's confidence in the fairness of the algorithm; 2) Different fairness problems-such as model-wide fairness issues versus case-specific fairness discrepancies-may be more effectively exposed through different styles of explanation; 3) Individual differences, including prior positions and judgment criteria of algorithmic fairness, impact how people react to different styles of explanation. We conclude with a discussion on providing personalized and adaptive explanations to support fairness judgments of ML systems.",
    "venue": "International Conference on Intelligent User Interfaces",
    "year": 2019,
    "referenceCount": 44,
    "citationCount": 104,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2256735536",
        "name": "Jonathan Dodge"
      },
      {
        "authorId": "2256650588",
        "name": "Q. V. Liao"
      },
      {
        "authorId": "2256623236",
        "name": "Yunfeng Zhang"
      },
      {
        "authorId": "2250696333",
        "name": "Rachel K. E. Bellamy"
      },
      {
        "authorId": "2256735073",
        "name": "Casey Dugan"
      }
    ]
  },
  "53774958": {
    "paperId": "127c1cb96b73399f429de553d315561504cc7cd4",
    "externalIds": {
      "DBLP": "journals/corr/abs-1811-07901",
      "MAG": "2901895173",
      "ArXiv": "1811.07901",
      "DOI": "10.1145/3287560.3287590",
      "CorpusId": 53774958
    },
    "publicationVenue": null,
    "title": "On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection",
    "abstract": "Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency. In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (>20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.",
    "venue": "FAT",
    "year": 2018,
    "referenceCount": 84,
    "citationCount": 322,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Law",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "120801533",
        "name": "Vivian Lai"
      },
      {
        "authorId": "40348583",
        "name": "Chenhao Tan"
      }
    ]
  },
  "239050385": {
    "paperId": "5e1746995debd1f17c24af01514c727598cc5613",
    "externalIds": {
      "DBLP": "journals/corr/abs-2110-10790",
      "ArXiv": "2110.10790",
      "CorpusId": 239050385
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Human-Centered Explainable AI (XAI): From Algorithms to User Experiences",
    "abstract": "In recent years, the field of explainable AI (XAI) has produced a vast collection of algorithms, providing a useful toolbox for researchers and practitioners to build XAI applications. With the rich application opportunities, explainability is believed to have moved beyond a demand by data scientists or researchers to comprehend the models they develop, to an essential requirement for people to trust and adopt AI deployed in numerous domains. However, explainability is an inherently human-centric property and the field is starting to embrace human-centered approaches. Human-computer interaction (HCI) research and user experience (UX) design in this area are becoming increasingly important. In this chapter, we begin with a high-level overview of the technical landscape of XAI algorithms, then selectively survey our own and other recent HCI works that take human-centered approaches to design, evaluate, and provide conceptual and methodological tools for XAI. We ask the question\"what are human-centered approaches doing for XAI\"and highlight three roles that they play in shaping XAI technologies by helping navigate, assess and expand the XAI toolbox: to drive technical choices by users' explainability needs, to uncover pitfalls of existing XAI methods and inform new methods, and to provide conceptual frameworks for human-compatible XAI.",
    "venue": "arXiv.org",
    "year": 2021,
    "referenceCount": 110,
    "citationCount": 162,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144921048",
        "name": "Q. Liao"
      },
      {
        "authorId": "68973648",
        "name": "Microsoft Research"
      },
      {
        "authorId": "2226819943",
        "name": "Canada Kush"
      },
      {
        "authorId": "2055470216",
        "name": "R. Varshney"
      },
      {
        "authorId": "1712865",
        "name": "Kush R. Varshney"
      }
    ]
  },
  "210023849": {
    "paperId": "5cc4100a67fd6f2ce3c760655ba7a12f358c7950",
    "externalIds": {
      "DBLP": "journals/corr/abs-2001-02114",
      "ArXiv": "2001.02114",
      "MAG": "2999637955",
      "DOI": "10.1145/3351095.3372852",
      "CorpusId": 210023849
    },
    "publicationVenue": null,
    "title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making",
    "abstract": "Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.",
    "venue": "FAT*",
    "year": 2020,
    "referenceCount": 33,
    "citationCount": 551,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2108127520",
        "name": "Yunfeng Zhang"
      },
      {
        "authorId": "144921048",
        "name": "Q. Liao"
      },
      {
        "authorId": "143725437",
        "name": "R. Bellamy"
      }
    ]
  },
  "233224125": {
    "paperId": "c79ba865574cb9635dbca135cec99ee56125778c",
    "externalIds": {
      "DBLP": "conf/iui/WangY21",
      "DOI": "10.1145/3397481.3450650",
      "CorpusId": 233224125
    },
    "publicationVenue": {
      "id": "d43f0f26-43c9-4009-a1ed-63624522c166",
      "name": "International Conference on Intelligent User Interfaces",
      "type": "conference",
      "alternate_names": [
        "IUI",
        "Intell User Interface",
        "Int Conf Intell User Interface",
        "Intelligent User Interfaces"
      ],
      "url": "http://www.iuiconf.org/"
    },
    "title": "Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making",
    "abstract": "This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy\u2014improve people\u2019s understanding of the AI model, help people recognize the model uncertainty, and support people\u2019s calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making.",
    "venue": "International Conference on Intelligent User Interfaces",
    "year": 2021,
    "referenceCount": 71,
    "citationCount": 234,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2154586618",
        "name": "Xinru Wang"
      },
      {
        "authorId": "2053888438",
        "name": "Ming Yin"
      }
    ]
  },
  "17405787": {
    "paperId": "b611576195d9d883f9191b63009eb9483830a1b3",
    "externalIds": {
      "MAG": "2110078189",
      "DBLP": "journals/isr/McKnightCK02",
      "DOI": "10.1287/isre.13.3.334.81",
      "CorpusId": 17405787
    },
    "publicationVenue": {
      "id": "6b25cc59-3bb5-4b47-819e-d95ba292c4f7",
      "name": "Information systems research",
      "type": "journal",
      "alternate_names": [
        "Information Systems Research",
        "Inf syst res",
        "Inf Syst Res"
      ],
      "issn": "1047-7047",
      "url": "http://isr.journal.informs.org/",
      "alternate_urls": [
        "http://www.jstor.org/action/showPublication?journalCode=infosysres",
        "https://pubsonline.informs.org/",
        "https://www.jstor.org/journal/infosysres"
      ]
    },
    "title": "Developing and Validating Trust Measures for e-Commerce: An Integrative Typology",
    "abstract": "Evidence suggests that consumers often hesitate to transact with Web-based vendors because of uncertainty about vendor behavior or the perceived risk of having personal information stolen by hackers. Trust plays a central role in helping consumers overcome perceptions of risk and insecurity. Trust makes consumers comfortable sharing personal information, making purchases, and acting on Web vendor advice--behaviors essential to widespread adoption of e-commerce. Therefore, trust is critical to both researchers and practitioners. Prior research on e-commerce trust has used diverse, incomplete, and inconsistent definitions of trust, making it difficult to compare results across studies. This paper contributes by proposing and validating measures for a multidisciplinary, multidimensional model of trust in e-commerce. The model includes four high-level constructs--disposition to trust, institution-based trust, trusting beliefs, and trusting intentions--which are further delineated into 16 measurable, literature-grounded subconstructs. The psychometric properties of the measures are demonstrated through use of a hypothetical, legal advice Web site. The results show that trust is indeed a multidimensional concept. Proposed relationships among the trust constructs are tested (for internal nomological validity), as are relationships between the trust constructs and three other e-commerce constructs (for external nomological validity)--Web experience, personal innovativeness, and Web site quality. Suggestions for future research as well as implications for practice are discussed.",
    "venue": "Information systems research",
    "year": 2002,
    "referenceCount": 124,
    "citationCount": 4954,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Business",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2070202945",
        "name": "D. Harrison"
      },
      {
        "authorId": "2223742809",
        "name": "Mcknight bullet"
      },
      {
        "authorId": "1759905",
        "name": "V. Choudhury"
      },
      {
        "authorId": "2666115",
        "name": "C. Kacmar"
      }
    ]
  },
  "140281803": {
    "paperId": "b2b16b3ae577e71b0a7a5c0f2b6fa2a2db8c98f0",
    "externalIds": {
      "MAG": "2942073295",
      "DBLP": "conf/chi/ChengWZOGHZ19",
      "DOI": "10.1145/3290605.3300789",
      "CorpusId": 140281803
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "Explaining Decision-Making Algorithms through UI: Strategies to Help Non-Expert Stakeholders",
    "abstract": "Increasingly, algorithms are used to make important decisions across society. However, these algorithms are usually poorly understood, which can reduce transparency and evoke negative emotions. In this research, we seek to learn design principles for explanation interfaces that communicate how decision-making algorithms work, in order to help organizations explain their decisions to stakeholders, or to support users' \"right to explanation\". We conducted an online experiment where 199 participants used different explanation interfaces to understand an algorithm for making university admissions decisions. We measured users' objective and self-reported understanding of the algorithm. Our results show that both interactive explanations and \"white-box\" explanations (i.e. that show the inner workings of an algorithm) can improve users' comprehension. Although the interactive approach is more effective at improving comprehension, it comes with a trade-off of taking more time. Surprisingly, we also find that users' trust in algorithmic decisions is not affected by the explanation interface or their level of comprehension of the algorithm.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2019,
    "referenceCount": 56,
    "citationCount": 293,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Business",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "10391344",
        "name": "H. Cheng"
      },
      {
        "authorId": "2108694939",
        "name": "Ruotong Wang"
      },
      {
        "authorId": "2148905996",
        "name": "Zheng Zhang"
      },
      {
        "authorId": "2064285507",
        "name": "Fiona O'Connell"
      },
      {
        "authorId": "1401435673",
        "name": "Terrance Gray"
      },
      {
        "authorId": "145192090",
        "name": "F. M. Harper"
      },
      {
        "authorId": "1742431",
        "name": "Haiyi Zhu"
      }
    ]
  },
  "245385821": {
    "paperId": "449ff2404a7b11fa3c47cd228fa5469b00ffef20",
    "externalIds": {
      "ArXiv": "2112.11471",
      "DBLP": "journals/corr/abs-2112-11471",
      "CorpusId": 245385821
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies",
    "abstract": "As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI models and AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our survey highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other's work and produce generalizable scientific knowledge. We also hope this survey will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.",
    "venue": "arXiv.org",
    "year": 2021,
    "referenceCount": 160,
    "citationCount": 156,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "120801533",
        "name": "Vivian Lai"
      },
      {
        "authorId": "2131059444",
        "name": "Chacha Chen"
      },
      {
        "authorId": "144921048",
        "name": "Q. Liao"
      },
      {
        "authorId": "1405364873",
        "name": "Alison Smith-Renner"
      },
      {
        "authorId": "2111727675",
        "name": "Chenhao Tan"
      }
    ]
  },
  "210154117": {
    "paperId": "fb7caddac20dca012f48c90b2e1e2383f7185051",
    "externalIds": {
      "DBLP": "conf/chi/KaurNJCWV20",
      "MAG": "3016099278",
      "DOI": "10.1145/3313831.3376219",
      "CorpusId": 210154117
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning",
    "abstract": "Machine learning (ML) models are now routinely deployed in domains ranging from criminal justice to healthcare. With this newfound ubiquity, ML has moved beyond academia and grown into an engineering discipline. To that end, interpretability tools have been designed to help data scientists and machine learning practitioners better understand how ML models work. However, there has been little evaluation of the extent to which these tools achieve this goal. We study data scientists' use of two existing interpretability tools, the InterpretML implementation of GAMs and the SHAP Python package. We conduct a contextual inquiry (N=11) and a survey (N=197) of data scientists to observe how they use interpretability tools to uncover common issues that arise when building and evaluating ML models. Our results indicate that data scientists over-trust and misuse interpretability tools. Furthermore, few of our participants were able to accurately describe the visualizations output by these tools. We highlight qualitative themes for data scientists' mental models of interpretability tools. We conclude with implications for researchers and tool designers, and contextualize our findings in the social science literature.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2020,
    "referenceCount": 78,
    "citationCount": 412,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48712213",
        "name": "Harmanpreet Kaur"
      },
      {
        "authorId": "40900039",
        "name": "Harsha Nori"
      },
      {
        "authorId": "2057108610",
        "name": "Samuel Jenkins"
      },
      {
        "authorId": "145727186",
        "name": "R. Caruana"
      },
      {
        "authorId": "1831395",
        "name": "Hanna M. Wallach"
      },
      {
        "authorId": "4006636",
        "name": "Jennifer Wortman Vaughan"
      }
    ]
  },
  "211041238": {
    "paperId": "a2dcb75c36120f31f4008ca04d328a84c67db150",
    "externalIds": {
      "MAG": "3013937328",
      "DBLP": "conf/fat/JacobsBBDW20",
      "DOI": "10.1145/3351095.3375671",
      "CorpusId": 211041238
    },
    "publicationVenue": null,
    "title": "The meaning and measurement of bias: lessons from natural language processing",
    "abstract": "The recent interest in identifying and mitigating bias in computational systems has introduced a wide range of different---and occasionally incomparable---proposals for what constitutes bias in such systems. This tutorial introduces the language of measurement modeling from the quantitative social sciences as a framework for examining how social, organizational, and political values enter computational systems and unpacking the varied normative concerns operationalized in different techniques for measuring \"bias.\" We show that this framework helps to clarify the way unobservable theoretical constructs---such as \"creditworthiness,\" \"risk to society,\" or \"tweet toxicity\"---are turned into measurable quantities and how this process may introduce fairness-related harms. In particular, we demonstrate how to systematically assess the construct validity and reliability of these measurements to detect and characterize specific types of harms, which arise from mismatches between constructs and their operationalizations. We then take a critical look at existing approaches to examining \"bias\" in NLP models, ranging from work on embedding spaces to machine translation and hate speech detection. We show that measurement modeling can help uncover the implicit constructs that such work aims to capture when measuring \"bias.\" In so doing, we illustrate the limits of current \"debiasing\" techniques, which have obscured the specific harms whose measurements they implicitly aim to reduce. By introducing the language of measurement modeling, we provide the FAT* community with a framework for making explicit and testing assumptions about unobservable theoretical constructs embedded in computational systems, thereby clarifying and uniting our understandings of fairness-related harms.",
    "venue": "FAT*",
    "year": 2020,
    "referenceCount": 3,
    "citationCount": 28,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47786922",
        "name": "Abigail Z. Jacobs"
      },
      {
        "authorId": "3422038",
        "name": "Su Lin Blodgett"
      },
      {
        "authorId": "2881033",
        "name": "Solon Barocas"
      },
      {
        "authorId": "1722360",
        "name": "Hal Daum\u00e9"
      },
      {
        "authorId": "1831395",
        "name": "Hanna M. Wallach"
      }
    ]
  },
  "3061036": {
    "paperId": "99fe982dce046869f60e4552c7f91c3627304780",
    "externalIds": {
      "DBLP": "conf/naacl/ZaidanEP07",
      "ACL": "N07-1033",
      "CorpusId": 3061036
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Using \u201cAnnotator Rationales\u201d to Improve Machine Learning for Text Categorization",
    "abstract": "We propose a new framework for supervised machine learning. Our goal is to learn from smaller amounts of supervised training data, by collecting a richer kind of training data: annotations with \u201crationales.\u201d When annotating an example, the human teacher will also highlight evidence supporting this annotation\u2014thereby teaching the machine learner why the example belongs to the category. We provide some rationale-annotated data and present a learning method that exploits the rationales during training to boost performance significantly on a sample task, namely sentiment classification of movie reviews. We hypothesize that in some situations, providing rationales is a more fruitful use of an annotator\u2019s time than annotating more examples.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2007,
    "referenceCount": 10,
    "citationCount": 343,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1936277",
        "name": "Omar Zaidan"
      },
      {
        "authorId": "145043214",
        "name": "Jason Eisner"
      },
      {
        "authorId": "1718753",
        "name": "C. Piatko"
      }
    ]
  },
  "5112038": {
    "paperId": "99ad0533f84c110da2d0713d5798e6e14080b159",
    "externalIds": {
      "MAG": "2804897457",
      "DBLP": "conf/naacl/KhashabiCRUR18",
      "ACL": "N18-1023",
      "DOI": "10.18653/v1/N18-1023",
      "CorpusId": 5112038
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences",
    "abstract": "We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 36,
    "citationCount": 467,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1783281",
        "name": "Daniel Khashabi"
      },
      {
        "authorId": "37202877",
        "name": "Snigdha Chaturvedi"
      },
      {
        "authorId": "46617131",
        "name": "Michael Roth"
      },
      {
        "authorId": "33145619",
        "name": "Shyam Upadhyay"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "4711425": {
    "paperId": "b1d24e8e08435b7c52335485a0d635abf9bc604c",
    "externalIds": {
      "MAG": "2789566302",
      "DBLP": "journals/corr/abs-1803-05355",
      "ACL": "N18-1074",
      "ArXiv": "1803.05355",
      "DOI": "10.18653/v1/N18-1074",
      "CorpusId": 4711425
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "abstract": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 42,
    "citationCount": 1399,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144603330",
        "name": "James Thorne"
      },
      {
        "authorId": "2064056928",
        "name": "Andreas Vlachos"
      },
      {
        "authorId": "2718039",
        "name": "Christos Christodoulopoulos"
      },
      {
        "authorId": "2008596",
        "name": "Arpit Mittal"
      }
    ]
  },
  "54040953": {
    "paperId": "c242438dac5aa4d9b13766c14240bb8426690d58",
    "externalIds": {
      "MAG": "2951936329",
      "DBLP": "conf/nips/CamburuRLB18",
      "ArXiv": "1812.01193",
      "CorpusId": 54040953
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "e-SNLI: Natural Language Inference with Natural Language Explanations",
    "abstract": "In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show how our corpus of explanations, which we call e-SNLI, can be used for various goals, such as obtaining full sentence justifications of a model\u2019s decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets. Our dataset thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust",
    "venue": "Neural Information Processing Systems",
    "year": 2018,
    "referenceCount": 27,
    "citationCount": 567,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3317152",
        "name": "Oana-Maria Camburu"
      },
      {
        "authorId": "2620211",
        "name": "Tim Rockt\u00e4schel"
      },
      {
        "authorId": "1690572",
        "name": "Thomas Lukasiewicz"
      },
      {
        "authorId": "1685771",
        "name": "Phil Blunsom"
      }
    ]
  },
  "207847663": {
    "paperId": "087dd95e13efd47aef2a6582e6801b39fc0f83d8",
    "externalIds": {
      "DBLP": "conf/acl/DeYoungJRLXSW20",
      "ACL": "2020.acl-main.408",
      "ArXiv": "1911.03429",
      "MAG": "3035503910",
      "DOI": "10.18653/v1/2020.acl-main.408",
      "CorpusId": 207847663
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models",
    "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the \u2018reasoning\u2019 behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of \u201crationales\u201d (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 75,
    "citationCount": 561,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48727916",
        "name": "Jay DeYoung"
      },
      {
        "authorId": "49837811",
        "name": "Sarthak Jain"
      },
      {
        "authorId": "8937909",
        "name": "Nazneen Rajani"
      },
      {
        "authorId": "51172373",
        "name": "Eric P. Lehman"
      },
      {
        "authorId": "2228109",
        "name": "Caiming Xiong"
      },
      {
        "authorId": "2166511",
        "name": "R. Socher"
      },
      {
        "authorId": "1912476",
        "name": "Byron C. Wallace"
      }
    ]
  },
  "232035689": {
    "paperId": "962aa5b847f1692af058bd14fc0e8c3f0a0fee73",
    "externalIds": {
      "DBLP": "conf/nips/WiegreffeM21",
      "ArXiv": "2102.12060",
      "CorpusId": 232035689
    },
    "publicationVenue": null,
    "title": "Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing",
    "abstract": "Explainable NLP (ExNLP) has increasingly focused on collecting human-annotated textual explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as supervision to train models to produce explanations for their predictions, and as a ground-truth to evaluate model-generated explanations. In this review, we identify 65 datasets with three predominant classes of textual explanations (highlights, free-text, and structured), organize the literature on annotating each type, identify strengths and shortcomings of existing collection methodologies, and give recommendations for collecting ExNLP datasets in the future.",
    "venue": "NeurIPS Datasets and Benchmarks",
    "year": 2021,
    "referenceCount": 172,
    "citationCount": 138,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35823986",
        "name": "Sarah Wiegreffe"
      },
      {
        "authorId": "3451494",
        "name": "Ana Marasovi\u0107"
      }
    ]
  },
  "220920319": {
    "paperId": "64da659c0687762359226b4cf455520c78acd165",
    "externalIds": {
      "MAG": "3046251064",
      "DBLP": "journals/corr/abs-2007-15780",
      "ArXiv": "2007.15780",
      "CorpusId": 220920319
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Neural Language Generation: Formulation, Methods, and Evaluation",
    "abstract": "Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language. Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 504,
    "citationCount": 28,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3360992",
        "name": "Cristina Garbacea"
      },
      {
        "authorId": "1743469",
        "name": "Qiaozhu Mei"
      }
    ]
  },
  "222290670": {
    "paperId": "087087d91598aa62a11061ed156f8f6e699a7930",
    "externalIds": {
      "ArXiv": "2010.04736",
      "DBLP": "conf/emnlp/CartonRT20",
      "ACL": "2020.emnlp-main.747",
      "MAG": "3103035585",
      "DOI": "10.18653/v1/2020.emnlp-main.747",
      "CorpusId": 222290670
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Evaluating and Characterizing Human Rationales",
    "abstract": "Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using \"fidelity curves\" to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 50,
    "citationCount": 49,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40502796",
        "name": "Samuel Carton"
      },
      {
        "authorId": "2064254602",
        "name": "Anirudh Rathore"
      },
      {
        "authorId": "40348583",
        "name": "Chenhao Tan"
      }
    ]
  },
  "222209056": {
    "paperId": "8f60401b03b46c4b3e61834dc691a8587250d46b",
    "externalIds": {
      "MAG": "3102035862",
      "ACL": "2020.findings-emnlp.390",
      "DBLP": "journals/corr/abs-2010-04119",
      "ArXiv": "2010.04119",
      "DOI": "10.18653/v1/2020.findings-emnlp.390",
      "CorpusId": 222209056
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?",
    "abstract": "Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model\u2019s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.",
    "venue": "Findings",
    "year": 2020,
    "referenceCount": 54,
    "citationCount": 91,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144625004",
        "name": "Peter Hase"
      },
      {
        "authorId": "7670835",
        "name": "Shiyue Zhang"
      },
      {
        "authorId": "8303959",
        "name": "Harry Xie"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      }
    ]
  },
  "252118396": {
    "paperId": "63f93a6d9c38d656933706acfc720684470bc108",
    "externalIds": {
      "DBLP": "journals/corr/abs-2209-03430",
      "DOI": "10.48550/arXiv.2209.03430",
      "CorpusId": 252118396
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Foundations and Recent Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions",
    "abstract": "Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining two key principles of modality heterogeneity and interconnections that have driven subsequent innovations, and propose a taxonomy of 6 core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 700,
    "citationCount": 101,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "144802290",
        "name": "Amir Zadeh"
      },
      {
        "authorId": "49933077",
        "name": "Louis-philippe Morency"
      }
    ]
  },
  "10137425": {
    "paperId": "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91",
    "externalIds": {
      "ArXiv": "1705.09406",
      "DBLP": "journals/pami/BaltrusaitisAM19",
      "MAG": "2951127645",
      "DOI": "10.1109/TPAMI.2018.2798607",
      "CorpusId": 10137425,
      "PubMed": "29994351"
    },
    "publicationVenue": {
      "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
      "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "type": "journal",
      "alternate_names": [
        "IEEE Trans Pattern Anal Mach Intell"
      ],
      "issn": "0162-8828",
      "url": "http://www.computer.org/tpami/",
      "alternate_urls": [
        "http://www.computer.org/portal/web/tpami",
        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
      ]
    },
    "title": "Multimodal Machine Learning: A Survey and Taxonomy",
    "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.",
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2017,
    "referenceCount": 273,
    "citationCount": 2511,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1756344",
        "name": "T. Baltru\u0161aitis"
      },
      {
        "authorId": "118242121",
        "name": "Chaitanya Ahuja"
      },
      {
        "authorId": "49933077",
        "name": "Louis-philippe Morency"
      }
    ]
  },
  "393948": {
    "paperId": "184ac0766262312ba76bbdece4e7ffad0aa8180b",
    "externalIds": {
      "ArXiv": "1206.5538",
      "MAG": "2952111767",
      "DBLP": "journals/pami/BengioCV13",
      "DOI": "10.1109/TPAMI.2013.50",
      "CorpusId": 393948,
      "PubMed": "23787338"
    },
    "publicationVenue": {
      "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
      "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "type": "journal",
      "alternate_names": [
        "IEEE Trans Pattern Anal Mach Intell"
      ],
      "issn": "0162-8828",
      "url": "http://www.computer.org/tpami/",
      "alternate_urls": [
        "http://www.computer.org/portal/web/tpami",
        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
      ]
    },
    "title": "Representation Learning: A Review and New Perspectives",
    "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2012,
    "referenceCount": 264,
    "citationCount": 11751,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1751762",
        "name": "Yoshua Bengio"
      },
      {
        "authorId": "1760871",
        "name": "Aaron C. Courville"
      },
      {
        "authorId": "145467703",
        "name": "Pascal Vincent"
      }
    ]
  },
  "213009174": {
    "paperId": "caa31faab39d34ecbb8100911640dfaec76f9ee9",
    "externalIds": {
      "MAG": "2994821360",
      "DBLP": "conf/iclr/JayakumarCMSROT20",
      "CorpusId": 213009174
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Multiplicative Interactions and Where to Find Them",
    "abstract": "We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others. Multiplicative interaction layers as primitive operations have a long-established presence in the literature, though this often not emphasized and thus under-appreciated. We begin by showing that such layers strictly enrich the representable function classes of neural networks. We conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. We therefore argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, we back up our claims and demonstrate the potential of multiplicative interactions by applying them in large-scale complex RL and sequence modelling tasks, where their use allows us to deliver state-of-the-art results, and thereby provides new evidence in support of multiplicative interactions playing a more prominent role when designing new neural network architectures.",
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "referenceCount": 36,
    "citationCount": 123,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35880964",
        "name": "Siddhant M. Jayakumar"
      },
      {
        "authorId": "10698483",
        "name": "Jacob Menick"
      },
      {
        "authorId": "144792148",
        "name": "Wojciech M. Czarnecki"
      },
      {
        "authorId": "144735987",
        "name": "Jonathan Schwarz"
      },
      {
        "authorId": "34269227",
        "name": "Jack W. Rae"
      },
      {
        "authorId": "2217144",
        "name": "Simon Osindero"
      },
      {
        "authorId": "1725303",
        "name": "Y. Teh"
      },
      {
        "authorId": "3367786",
        "name": "Tim Harley"
      },
      {
        "authorId": "1996134",
        "name": "Razvan Pascanu"
      }
    ]
  },
  "710430": {
    "paperId": "adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1",
    "externalIds": {
      "DBLP": "journals/jmlr/SrivastavaS14",
      "MAG": "2164587673",
      "DOI": "10.5555/2627435.2697059",
      "CorpusId": 710430
    },
    "publicationVenue": {
      "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
      "name": "Journal of machine learning research",
      "type": "journal",
      "alternate_names": [
        "Journal of Machine Learning Research",
        "J mach learn res",
        "J Mach Learn Res"
      ],
      "issn": "1532-4435",
      "alternate_issns": [
        "1533-7928"
      ],
      "url": "http://www.ai.mit.edu/projects/jmlr/",
      "alternate_urls": [
        "http://jmlr.csail.mit.edu/",
        "http://www.jmlr.org/",
        "http://portal.acm.org/affiliated/jmlr"
      ]
    },
    "title": "Multimodal learning with deep Boltzmann machines",
    "abstract": "Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bimodal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.",
    "venue": "Journal of machine learning research",
    "year": 2012,
    "referenceCount": 44,
    "citationCount": 1689,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2897313",
        "name": "Nitish Srivastava"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      }
    ]
  },
  "49303347": {
    "paperId": "034f1c5589644a6b42f50bf61b1628a1c5607fd9",
    "externalIds": {
      "DBLP": "journals/corr/abs-1806-06176",
      "ArXiv": "1806.06176",
      "MAG": "2951575728",
      "CorpusId": 49303347
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Learning Factorized Multimodal Representations",
    "abstract": "Learning multimodal representations is a fundamentally complex research problem due to the presence of multiple heterogeneous sources of information. Although the presence of multiple modalities provides additional valuable information, there are two key challenges to address when learning from multimodal data: 1) models must learn the complex intra-modal and cross-modal interactions for prediction and 2) models must be robust to unexpected missing or noisy modalities during testing. In this paper, we propose to optimize for a joint generative-discriminative objective across multimodal data and labels. We introduce a model that factorizes representations into two sets of independent factors: multimodal discriminative and modality-specific generative factors. Multimodal discriminative factors are shared across all modalities and contain joint multimodal features required for discriminative tasks such as sentiment prediction. Modality-specific generative factors are unique for each modality and contain the information required for generating data. Experimental results show that our model is able to learn meaningful multimodal representations that achieve state-of-the-art or competitive performance on six multimodal datasets. Our model demonstrates flexible generative capabilities by conditioning on independent factors and can reconstruct missing modalities without significantly impacting performance. Lastly, we interpret our factorized representations to understand the interactions that influence multimodal learning.",
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "referenceCount": 81,
    "citationCount": 358,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145639633",
        "name": "Yao-Hung Hubert Tsai"
      },
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "144802290",
        "name": "Amir Zadeh"
      },
      {
        "authorId": "49933077",
        "name": "Louis-philippe Morency"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      }
    ]
  },
  "2495132": {
    "paperId": "e2257e3f56ccb12875a57bc0a8cca1d9d7e93ec6",
    "externalIds": {
      "MAG": "1523385540",
      "DBLP": "conf/icml/AndrewABL13",
      "CorpusId": 2495132
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Deep Canonical Correlation Analysis",
    "abstract": "We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation. It can be viewed as a nonlinear extension of the linear method canonical correlation analysis (CCA). It is an alternative to the nonparametric method kernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances. In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.",
    "venue": "International Conference on Machine Learning",
    "year": 2013,
    "referenceCount": 39,
    "citationCount": 1724,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144339350",
        "name": "Galen Andrew"
      },
      {
        "authorId": "144365054",
        "name": "R. Arora"
      },
      {
        "authorId": "1748118",
        "name": "J. Bilmes"
      },
      {
        "authorId": "2924113",
        "name": "Karen Livescu"
      }
    ]
  },
  "221911618": {
    "paperId": "757782a0524d6d23f430d6d8f924c6212d6afeac",
    "externalIds": {
      "MAG": "3036595416",
      "DBLP": "journals/inffus/ZadehLM20",
      "DOI": "10.1016/j.inffus.2020.06.001",
      "CorpusId": 221911618
    },
    "publicationVenue": {
      "id": "06afdd0b-0d85-413f-af8a-c3045c12c561",
      "name": "Information Fusion",
      "type": "journal",
      "alternate_names": [
        "Inf Fusion"
      ],
      "issn": "1566-2535",
      "url": "https://www.journals.elsevier.com/information-fusion",
      "alternate_urls": [
        "http://www.sciencedirect.com/science/journal/15662535"
      ]
    },
    "title": "Foundations of Multimodal Co-learning",
    "abstract": null,
    "venue": "Information Fusion",
    "year": 2020,
    "referenceCount": 25,
    "citationCount": 40,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144802290",
        "name": "Amir Zadeh"
      },
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "49933077",
        "name": "Louis-philippe Morency"
      }
    ]
  },
  "108296442": {
    "paperId": "50f76736c3090c6effac25400e5e40cc0b7b5ad9",
    "externalIds": {
      "ArXiv": "1904.12584",
      "MAG": "2908791737",
      "DBLP": "journals/corr/abs-1904-12584",
      "CorpusId": 108296442
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
    "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "referenceCount": 55,
    "citationCount": 633,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "13589371",
        "name": "Jiayuan Mao"
      },
      {
        "authorId": "144158271",
        "name": "Chuang Gan"
      },
      {
        "authorId": "143967473",
        "name": "Pushmeet Kohli"
      },
      {
        "authorId": "1763295",
        "name": "J. Tenenbaum"
      },
      {
        "authorId": "3045089",
        "name": "Jiajun Wu"
      }
    ]
  },
  "182952502": {
    "paperId": "7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50",
    "externalIds": {
      "DBLP": "conf/ijcai/LuketinaNFFAGWR19",
      "MAG": "2948380112",
      "ArXiv": "1906.03926",
      "DOI": "10.24963/ijcai.2019/880",
      "CorpusId": 182952502
    },
    "publicationVenue": {
      "id": "67f7f831-711a-43c8-8785-1e09005359b5",
      "name": "International Joint Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Int Jt Conf Artif Intell",
        "IJCAI"
      ],
      "url": "http://www.ijcai.org/"
    },
    "title": "A Survey of Reinforcement Learning Informed by Natural Language",
    "abstract": "To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2019,
    "referenceCount": 104,
    "citationCount": 268,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1818756",
        "name": "Jelena Luketina"
      },
      {
        "authorId": "39683441",
        "name": "Nantas Nardelli"
      },
      {
        "authorId": "38698094",
        "name": "Gregory Farquhar"
      },
      {
        "authorId": "145356667",
        "name": "Jakob N. Foerster"
      },
      {
        "authorId": "2112400",
        "name": "Jacob Andreas"
      },
      {
        "authorId": "1864353",
        "name": "Edward Grefenstette"
      },
      {
        "authorId": "1766767",
        "name": "Shimon Whiteson"
      },
      {
        "authorId": "2620211",
        "name": "Tim Rockt\u00e4schel"
      }
    ]
  },
  "211171653": {
    "paperId": "20dc158a6abd1f92a4534ae064d527821a91685d",
    "externalIds": {
      "DBLP": "journals/corr/abs-2002-08325",
      "MAG": "3007439415",
      "ArXiv": "2002.08325",
      "DOI": "10.1007/978-3-030-58589-1_23",
      "CorpusId": 211171653
    },
    "publicationVenue": {
      "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
      "name": "European Conference on Computer Vision",
      "type": "conference",
      "alternate_names": [
        "ECCV",
        "Eur Conf Comput Vis"
      ],
      "url": "https://link.springer.com/conference/eccv"
    },
    "title": "VQA-LOL: Visual Question Answering under the Lens of Logic",
    "abstract": null,
    "venue": "European Conference on Computer Vision",
    "year": 2020,
    "referenceCount": 60,
    "citationCount": 72,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "120838645",
        "name": "Tejas Gokhale"
      },
      {
        "authorId": "120722271",
        "name": "Pratyay Banerjee"
      },
      {
        "authorId": "1760291",
        "name": "Chitta Baral"
      },
      {
        "authorId": "1784500",
        "name": "Yezhou Yang"
      }
    ]
  },
  "220047809": {
    "paperId": "474952c4ceeec59d2677c60e92ebbf6d34140b2d",
    "externalIds": {
      "DBLP": "conf/acl/AlikhaniSLSS20",
      "MAG": "3035532688",
      "ACL": "2020.acl-main.583",
      "ArXiv": "2005.00908",
      "DOI": "10.18653/v1/2020.acl-main.583",
      "CorpusId": 220047809
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Cross-modal Coherence Modeling for Caption Generation",
    "abstract": "We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning. Using an annotation protocol specifically devised for capturing image\u2013caption coherence relations, we annotate 10,000 instances from publicly-available image\u2013caption pairs. We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware, controllable image captioning models. The results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 41,
    "citationCount": 53,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2715920",
        "name": "Malihe Alikhani"
      },
      {
        "authorId": "48267618",
        "name": "Piyush Sharma"
      },
      {
        "authorId": "51019115",
        "name": "Shengjie Li"
      },
      {
        "authorId": "1737285",
        "name": "Radu Soricut"
      },
      {
        "authorId": "144884556",
        "name": "Matthew Stone"
      }
    ]
  },
  "232035663": {
    "paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
    "externalIds": {
      "DBLP": "journals/corr/abs-2102-12092",
      "MAG": "3170016573",
      "ArXiv": "2102.12092",
      "CorpusId": 232035663
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Zero-Shot Text-to-Image Generation",
    "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "referenceCount": 64,
    "citationCount": 4016,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1992922591",
        "name": "A. Ramesh"
      },
      {
        "authorId": "2068123790",
        "name": "Mikhail Pavlov"
      },
      {
        "authorId": "40087786",
        "name": "Gabriel Goh"
      },
      {
        "authorId": "145565184",
        "name": "Scott Gray"
      },
      {
        "authorId": "153387869",
        "name": "Chelsea Voss"
      },
      {
        "authorId": "38909097",
        "name": "Alec Radford"
      },
      {
        "authorId": "2108828435",
        "name": "Mark Chen"
      },
      {
        "authorId": "1701686",
        "name": "I. Sutskever"
      }
    ]
  },
  "235899386": {
    "paperId": "af86df6a0af3226a1b4b5eb27c17c9e45367f896",
    "externalIds": {
      "DBLP": "conf/nips/LiangLFWCWCWLZS21",
      "ArXiv": "2107.07502",
      "CorpusId": 235899386,
      "PubMed": "38774625"
    },
    "publicationVenue": null,
    "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning",
    "abstract": "Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark for multimodal learning spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning spanning innovations in fusion paradigms, optimization objectives, and training approaches. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal machine learning research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized implementations, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.",
    "venue": "NeurIPS Datasets and Benchmarks",
    "year": 2021,
    "referenceCount": 190,
    "citationCount": 137,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "2066413750",
        "name": "Yiwei Lyu"
      },
      {
        "authorId": "2152774190",
        "name": "Xiang Fan"
      },
      {
        "authorId": "1576080679",
        "name": "Zetian Wu"
      },
      {
        "authorId": "2153511720",
        "name": "Yun Cheng"
      },
      {
        "authorId": "2115565414",
        "name": "Jason Wu"
      },
      {
        "authorId": "2146072218",
        "name": "Leslie Chen"
      },
      {
        "authorId": "2111194238",
        "name": "Peter Wu"
      },
      {
        "authorId": "2115303776",
        "name": "Michelle A. Lee"
      },
      {
        "authorId": "2117748",
        "name": "Yuke Zhu"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      },
      {
        "authorId": "49933077",
        "name": "Louis-philippe Morency"
      }
    ]
  },
  "236087303": {
    "paperId": "d7b8014c2a348a631ed466c6fba3825330b2f195",
    "externalIds": {
      "DBLP": "journals/tvcg/WangHJYWQ22",
      "ArXiv": "2107.08264",
      "DOI": "10.1109/TVCG.2021.3114794",
      "CorpusId": 236087303,
      "PubMed": "34587037"
    },
    "publicationVenue": {
      "id": "5e1f6444-5d03-48c7-b202-7f47d492aeae",
      "name": "IEEE Transactions on Visualization and Computer Graphics",
      "type": "journal",
      "alternate_names": [
        "IEEE Trans Vis Comput Graph"
      ],
      "issn": "1077-2626",
      "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=2945"
    },
    "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis",
    "abstract": "Multimodal sentiment analysis aims to recognize people's attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M2Lens, to visualize and explain multimodal models for sentiment analysis. M2Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M2Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.",
    "venue": "IEEE Transactions on Visualization and Computer Graphics",
    "year": 2021,
    "referenceCount": 86,
    "citationCount": 61,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "50141732",
        "name": "Xingbo Wang"
      },
      {
        "authorId": "35573278",
        "name": "Jianben He"
      },
      {
        "authorId": "2111472932",
        "name": "Zhihua Jin"
      },
      {
        "authorId": "8724126",
        "name": "Muqiao Yang"
      },
      {
        "authorId": "2300486168",
        "name": "Huamin Qu"
      }
    ]
  },
  "4384334": {
    "paperId": "b0c5dc3fa19a2bc97606ccb6f55226b913984395",
    "externalIds": {
      "DBLP": "journals/corr/abs-1807-00517",
      "MAG": "2810512327",
      "ArXiv": "1803.09797",
      "DOI": "10.1007/978-3-030-01219-9_47",
      "CorpusId": 4384334
    },
    "publicationVenue": {
      "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
      "name": "European Conference on Computer Vision",
      "type": "conference",
      "alternate_names": [
        "ECCV",
        "Eur Conf Comput Vis"
      ],
      "url": "https://link.springer.com/conference/eccv"
    },
    "title": "Women also Snowboard: Overcoming Bias in Captioning Models",
    "abstract": null,
    "venue": "European Conference on Computer Vision",
    "year": 2018,
    "referenceCount": 50,
    "citationCount": 458,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40895688",
        "name": "Kaylee Burns"
      },
      {
        "authorId": "2234342",
        "name": "Lisa Anne Hendricks"
      },
      {
        "authorId": "1753210",
        "name": "Trevor Darrell"
      },
      {
        "authorId": "34721166",
        "name": "Anna Rohrbach"
      }
    ]
  },
  "211096730": {
    "paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4",
    "externalIds": {
      "DBLP": "conf/icml/ChenK0H20",
      "MAG": "3005680577",
      "ArXiv": "2002.05709",
      "CorpusId": 211096730
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "A Simple Framework for Contrastive Learning of Visual Representations",
    "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
    "venue": "International Conference on Machine Learning",
    "year": 2020,
    "referenceCount": 67,
    "citationCount": 16075,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145358498",
        "name": "Ting Chen"
      },
      {
        "authorId": "40464924",
        "name": "Simon Kornblith"
      },
      {
        "authorId": "144739074",
        "name": "Mohammad Norouzi"
      },
      {
        "authorId": "1695689",
        "name": "Geoffrey E. Hinton"
      }
    ]
  },
  "233296292": {
    "paperId": "c26759e6c701201af2f62f7ee4eb68742b5bf085",
    "externalIds": {
      "ArXiv": "2104.08821",
      "DBLP": "journals/corr/abs-2104-08821",
      "ACL": "2021.emnlp-main.552",
      "DOI": "10.18653/v1/2021.emnlp-main.552",
      "CorpusId": 233296292
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
    "abstract": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \u201centailment\u201d pairs as positives and \u201ccontradiction\u201d pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman\u2019s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show\u2014both theoretically and empirically\u2014that contrastive learning objective regularizes pre-trained embeddings\u2019 anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 68,
    "citationCount": 2806,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "4800645",
        "name": "Tianyu Gao"
      },
      {
        "authorId": "2087141625",
        "name": "Xingcheng Yao"
      },
      {
        "authorId": "50536468",
        "name": "Danqi Chen"
      }
    ]
  },
  "222124366": {
    "paperId": "3f9514630194a9fba9505b594ec921b247fecb48",
    "externalIds": {
      "MAG": "3089631405",
      "ACL": "2020.findings-emnlp.117",
      "DBLP": "conf/emnlp/0001ABBBCDDEGGH20",
      "DOI": "10.18653/v1/2020.findings-emnlp.117",
      "CorpusId": 222124366
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Evaluating Models\u2019 Local Decision Boundaries via Contrast Sets",
    "abstract": "Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model\u2019s decision boundary, which can be used to more accurately evaluate a model\u2019s true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets\u2014up to 25% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.",
    "venue": "Findings",
    "year": 2020,
    "referenceCount": 82,
    "citationCount": 364,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      },
      {
        "authorId": "3167681",
        "name": "Yoav Artzi"
      },
      {
        "authorId": "1750652",
        "name": "Jonathan Berant"
      },
      {
        "authorId": "50757607",
        "name": "Ben Bogin"
      },
      {
        "authorId": "2087205",
        "name": "Sihao Chen"
      },
      {
        "authorId": "33546336",
        "name": "Dheeru Dua"
      },
      {
        "authorId": "51131518",
        "name": "Yanai Elazar"
      },
      {
        "authorId": "1471885977",
        "name": "Ananth Gottumukkala"
      },
      {
        "authorId": "2285178",
        "name": "Nitish Gupta"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      },
      {
        "authorId": "2123694087",
        "name": "Gabriel Ilharco"
      },
      {
        "authorId": "1783281",
        "name": "Daniel Khashabi"
      },
      {
        "authorId": "2154184644",
        "name": "Kevin Lin"
      },
      {
        "authorId": "2262374",
        "name": "Jiangming Liu"
      },
      {
        "authorId": "22243769",
        "name": "Nelson F. Liu"
      },
      {
        "authorId": "46244238",
        "name": "Phoebe Mulcaire"
      },
      {
        "authorId": "3333257",
        "name": "Qiang Ning"
      },
      {
        "authorId": "34650964",
        "name": "Sameer Singh"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      },
      {
        "authorId": "17097887",
        "name": "Sanjay Subramanian"
      },
      {
        "authorId": "145217343",
        "name": "Eric Wallace"
      },
      {
        "authorId": "2153658440",
        "name": "Ally Zhang"
      },
      {
        "authorId": "145360756",
        "name": "Ben Zhou"
      }
    ]
  },
  "12633363": {
    "paperId": "a5a8dfe5dcfa998124c8f115f5f743da2c40d714",
    "externalIds": {
      "ACL": "C18-3006",
      "MAG": "2888875316",
      "DBLP": "conf/coling/ChenCH18",
      "CorpusId": 12633363
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Deep Learning for Dialogue Systems",
    "abstract": "In the past decade, goal-oriented spoken dialogue systems have been the most prominent component in today\u2019s virtual personal assistants. The classic dialogue systems have rather complex and/or modular pipelines. The advance of deep learning technologies has recently risen the applications of neural models to dialogue modeling. However, how to successfully apply deep learning based approaches to a dialogue system is still challenging. Hence, this tutorial is designed to focus on an overview of the dialogue system development while describing most recent research for building dialogue systems and summarizing the challenges, in order to allow researchers to study the potential improvements of the state-of-the-art dialogue systems. The tutorial material is available at http://deepdialogue. miulab.tw.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2018,
    "referenceCount": 57,
    "citationCount": 52,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1725643",
        "name": "Yun-Nung (Vivian) Chen"
      },
      {
        "authorId": "1709797",
        "name": "Asli Celikyilmaz"
      },
      {
        "authorId": "1395813836",
        "name": "Dilek Z. Hakkani-T\u00fcr"
      }
    ]
  },
  "46918362": {
    "paperId": "8cf9c6bccccde9283f4089f7fe99fb1fa2c2df9a",
    "externalIds": {
      "DBLP": "conf/naacl/SuMCV18",
      "MAG": "2807023564",
      "ACL": "N18-6006",
      "DOI": "10.18653/v1/N18-6006",
      "CorpusId": 46918362
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Deep Learning for Conversational AI",
    "abstract": "Spoken Dialogue Systems (SDS) have great commercial potential as they promise to revolutionise the way in which humans interact with machines. The advent of deep learning led to substantial developments in this area of NLP research, and the goal of this tutorial is to familiarise the research community with the recent advances in what some call the most difficult problem in NLP. From a research perspective, the design of spoken dialogue systems provides a number of significant challenges, as these systems depend on: a) solving several difficult NLP and decision-making tasks; and b) combining these into a functional dialogue system pipeline. A key long-term goal of dialogue system research is to enable open-domain systems that can converse about arbitrary topics and assist humans with completing a wide range of tasks. Furthermore, such systems need to autonomously learn on-line to improve their performance and recover from errors using both signals from their environment and from implicit and explicit user feedback. While the design of such systems has traditionally been modular, domain and language-specific, advances in deep learning have alleviated many of the design problems. The main purpose of this tutorial is to encourage dialogue research in the NLP community by providing the research background, a survey of available resources, and giving key insights to application of state-of-the-art SDS methodology into industry-scale conversational AI systems. We plan to introduce researchers to the pipeline framework for modelling goal-oriented dialogue systems, which includes three key components: 1) Language Understanding; 2) Dialogue Management; and 3) Language Generation. The differences between goal-oriented dialogue systems and chat-bot style conversational agents will be explained in order to show the motivation behind the design of both, with the main focus on the pipeline SDS framework. For each key component, we will define the research problem, provide a brief literature review and introduce the current state-of-the-art approaches. Complementary resources (e.g. available datasets and toolkits) will also be discussed. Finally, future work, outstanding challenges, and current industry practices will be presented. All of the presented material will be made available online for future reference.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 45,
    "citationCount": 16,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2131709",
        "name": "Pei-hao Su"
      },
      {
        "authorId": "3334541",
        "name": "N. Mrksic"
      },
      {
        "authorId": "3450866",
        "name": "I. Casanueva"
      },
      {
        "authorId": "1747849",
        "name": "Ivan Vulic"
      }
    ]
  },
  "68167178": {
    "paperId": "83e567c2822aeda91006096a5d7ac0b34721d2a5",
    "externalIds": {
      "MAG": "2948771245",
      "ACL": "P18-5002",
      "DBLP": "journals/corr/abs-1809-08267",
      "ArXiv": "1809.08267",
      "DOI": "10.18653/v1/P18-5002",
      "CorpusId": 68167178
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Neural Approaches to Conversational AI",
    "abstract": "This tutorial surveys neural approaches to conversational AI that were developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) social bots. For each category, we present a review of state-of-the-art neural approaches, draw the connection between neural approaches and traditional symbolic approaches, and discuss the progress we have made and challenges we are facing, using specific systems and models as case studies.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 428,
    "citationCount": 644,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "1947267",
        "name": "Michel Galley"
      },
      {
        "authorId": "47681372",
        "name": "Lihong Li"
      }
    ]
  },
  "220730260": {
    "paperId": "2165904b8041784ac545e7934d235664b6e77a08",
    "externalIds": {
      "MAG": "3044704040",
      "DBLP": "conf/sigir/GaoXB20",
      "DOI": "10.1145/3397271.3401418",
      "CorpusId": 220730260
    },
    "publicationVenue": {
      "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
      "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "type": "conference",
      "alternate_names": [
        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "Int ACM SIGIR Conf Res Dev Inf Retr",
        "SIGIR",
        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
      ],
      "url": "http://www.acm.org/sigir/"
    },
    "title": "Recent Advances in Conversational Information Retrieval",
    "abstract": "Recent progress in deep learning has brought tremendous improvements in conversational AI, leading to a plethora of commercial conversational services that allow naturally spoken interactions, increasing the need for more human-centric interactions in IR. As a result, we have witnessed a resurgent interest in developing modern CIR systems in research communities and industry. This tutorial presents recent advances in CIR, focusing mainly on neural approaches and new applications developed in the past five years. Our goal is to provide a thorough and in-depth overview of the general definition of CIR, the components of CIR systems, new applications raised for its conversational aspects, and the (neural) techniques recently developed for it.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2020,
    "referenceCount": 21,
    "citationCount": 19,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "144628574",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "144609235",
        "name": "Paul N. Bennett"
      }
    ]
  },
  "250340235": {
    "paperId": "8bc6162766b4e6cd616ad508ea488ecc628cf4ac",
    "externalIds": {
      "DBLP": "conf/sigir/0001FORRTZ22",
      "DOI": "10.1145/3477495.3532678",
      "CorpusId": 250340235
    },
    "publicationVenue": {
      "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
      "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "type": "conference",
      "alternate_names": [
        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "Int ACM SIGIR Conf Res Dev Inf Retr",
        "SIGIR",
        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
      ],
      "url": "http://www.acm.org/sigir/"
    },
    "title": "Conversational Information Seeking: Theory and Application",
    "abstract": "Conversational information seeking (CIS) involves interaction sequences between one or more users and an information system. Interactions in CIS are primarily based on natural language dialogue, while they may include other types of interactions, such as click, touch, and body gestures. CIS recently attracted significant attention and advancements continue to be made. This tutorial follows the content of the recent Conversational Information Seeking book authored by several of the tutorial presenters. The tutorial aims to be an introduction to CIS for newcomers to CIS in addition to the recent advanced topics and state-of-the-art approaches for students and researchers with moderate knowledge of the topic. A significant part of the tutorial is dedicated to hands-on experiences based on toolkits developed by the presenters for conversational passage retrieval and multi-modal task-oriented dialogues. The outcomes of this tutorial include theoretical and practical knowledge, including a forum to meet researchers interested in CIS.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2022,
    "referenceCount": 35,
    "citationCount": 22,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145269114",
        "name": "Jeffrey Dalton"
      },
      {
        "authorId": "2164704207",
        "name": "Sophie Fischer"
      },
      {
        "authorId": "2105439683",
        "name": "Paul Owoicho"
      },
      {
        "authorId": "2065812052",
        "name": "Filip Radlinski"
      },
      {
        "authorId": "48890086",
        "name": "Federico Rossetto"
      },
      {
        "authorId": "2528063",
        "name": "Johanne R. Trippas"
      },
      {
        "authorId": "2499986",
        "name": "Hamed Zamani"
      }
    ]
  },
  "5523008": {
    "paperId": "a6401e102c03a441992b3e45f7b63eec09d4b89d",
    "externalIds": {
      "MAG": "2953126480",
      "ArXiv": "1711.01731",
      "DBLP": "journals/corr/abs-1711-01731",
      "DOI": "10.1145/3166054.3166058",
      "CorpusId": 5523008
    },
    "publicationVenue": null,
    "title": "A Survey on Dialogue Systems: Recent Advances and New Frontiers",
    "abstract": "Dialogue systems have attracted more and more attention. Recent advances on dialogue systems are overwhelmingly contributed by deep learning techniques, which have been employed to enhance a wide range of big data applications such as computer vision, natural language processing, and recommender systems. For dialogue systems, deep learning can leverage a massive amount of data to learn meaningful feature representations and response generation strategies, while requiring a minimum amount of hand-crafting. In this article, we give an overview to these recent advances on dialogue systems from various perspectives and discuss some possible research directions. In particular, we generally divide existing dialogue systems into task-oriented and nontask- oriented models, then detail how deep learning techniques help them with representative algorithms and finally discuss some appealing research directions that can bring the dialogue system research into a new frontier",
    "venue": "SKDD",
    "year": 2017,
    "referenceCount": 131,
    "citationCount": 658,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2957953",
        "name": "Hongshen Chen"
      },
      {
        "authorId": "1390612725",
        "name": "Xiaorui Liu"
      },
      {
        "authorId": "50559722",
        "name": "Dawei Yin"
      },
      {
        "authorId": "1736632",
        "name": "Jiliang Tang"
      }
    ]
  },
  "153312680": {
    "paperId": "89e65078d37d076627818d9dba2c8ca9bf8f66bc",
    "externalIds": {
      "MAG": "3015322406",
      "DBLP": "journals/corr/abs-1905-05709",
      "ArXiv": "1905.05709",
      "DOI": "10.1145/3383123",
      "CorpusId": 153312680
    },
    "publicationVenue": null,
    "title": "Challenges in Building Intelligent Open-domain Dialog Systems",
    "abstract": "There is a resurgent interest in developing intelligent open-domain dialog systems due to the availability of large amounts of conversational data and the recent progress on neural approaches to conversational AI [33]. Unlike traditional task-oriented bots, an open-domain dialog system aims to establish long-term connections with users by satisfying the human need for communication, affection, and social belonging. This article reviews the recent work on neural approaches that are devoted to addressing three challenges in developing such systems: semantics, consistency, and interactiveness. Semantics requires a dialog system to not only understand the content of the dialog but also identify users\u2019 emotional and social needs during the conversation. Consistency requires the system to demonstrate a consistent personality to win users\u2019 trust and gain their long-term confidence. Interactiveness refers to the system\u2019s ability to generate interpersonal responses to achieve particular social goals such as entertainment and conforming. The studies we select to present in this survey are based on our unique views and are by no means complete. Nevertheless, we hope that the discussion will inspire new research in developing more intelligent open-domain dialog systems.",
    "venue": "ACM Trans. Inf. Syst.",
    "year": 2019,
    "referenceCount": 205,
    "citationCount": 285,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1730108",
        "name": "Minlie Huang"
      },
      {
        "authorId": "145213540",
        "name": "Xiaoyan Zhu"
      },
      {
        "authorId": "1800422",
        "name": "Jianfeng Gao"
      }
    ]
  },
  "246210119": {
    "paperId": "4ad0dc7a0a6a0142d308439b6c7375d27d81db36",
    "externalIds": {
      "ArXiv": "2201.08808",
      "DBLP": "journals/ftir/ZamaniTDR23",
      "DOI": "10.1561/1500000081",
      "CorpusId": 246210119
    },
    "publicationVenue": {
      "id": "f84f2e3a-8af9-48e4-911c-c3c324b02627",
      "name": "Foundations and Trends in Information Retrieval",
      "type": "journal",
      "alternate_names": [
        "Found Trends Inf Retr"
      ],
      "issn": "1554-0669",
      "url": "https://www.nowpublishers.com/ir",
      "alternate_urls": [
        "https://www.nowpublishers.com/product.aspx?product=INR"
      ]
    },
    "title": "Conversational Information Seeking",
    "abstract": "Conversational information seeking (CIS) is concerned with a sequence of interactions between one or more users and an information system. Interactions in CIS are primarily based on natural language dialogue, while they may include other types of interactions, such as click, touch, and body gestures. This monograph provides a thorough overview of CIS definitions, applications, interactions, interfaces, design, implementation, and evaluation. This monograph views CIS applications as including conversational search, conversational question answering, and conversational recommendation. Our aim is to provide an overview of past research related to CIS, introduce the current state-of-the-art in CIS, highlight the challenges still being faced in the community. and suggest future directions.",
    "venue": "Foundations and Trends in Information Retrieval",
    "year": 2022,
    "referenceCount": 0,
    "citationCount": 76,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2499986",
        "name": "Hamed Zamani"
      },
      {
        "authorId": "2528063",
        "name": "Johanne R. Trippas"
      },
      {
        "authorId": "145269114",
        "name": "Jeffrey Dalton"
      },
      {
        "authorId": "2065812052",
        "name": "Filip Radlinski"
      }
    ]
  },
  "245986359": {
    "paperId": "b97a33933541c276778c3fe63baad6964f4bdf44",
    "externalIds": {
      "DBLP": "journals/corr/abs-2201-05176",
      "ArXiv": "2201.05176",
      "DOI": "10.1007/978-3-031-23080-6",
      "CorpusId": 245986359
    },
    "publicationVenue": null,
    "title": "Neural Approaches to Conversational Information Retrieval",
    "abstract": null,
    "venue": "The Information Retrieval Series",
    "year": 2022,
    "referenceCount": 0,
    "citationCount": 66,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "144609235",
        "name": "Paul N. Bennett"
      },
      {
        "authorId": "2286321410",
        "name": "Nick Craswell"
      }
    ]
  },
  "249808144": {
    "paperId": "a7c86246a3cfdfb5219a0fec1426ecd9072c66ab",
    "externalIds": {
      "DBLP": "journals/ftir/YanLY22",
      "DOI": "10.1561/1500000083",
      "CorpusId": 249808144
    },
    "publicationVenue": {
      "id": "f84f2e3a-8af9-48e4-911c-c3c324b02627",
      "name": "Foundations and Trends in Information Retrieval",
      "type": "journal",
      "alternate_names": [
        "Found Trends Inf Retr"
      ],
      "issn": "1554-0669",
      "url": "https://www.nowpublishers.com/ir",
      "alternate_urls": [
        "https://www.nowpublishers.com/product.aspx?product=INR"
      ]
    },
    "title": "Deep Learning for Dialogue Systems: Chit-Chat and Beyond",
    "abstract": "Recommendation, information retrieval, and other information access systems pose unique challenges for investigating and applying the fairness and non-discrimination concepts that have been developed for studying other machine learning systems. While fair information access shares many commonalities with fair classification, there are important differences: the multistakeholder nature of information access applications, the rank-based problem setting, the centrality of personalization in many cases, and the role of user response all complicate the problem of identifying precisely what types and operationalizations of fairness may be relevant. In this monograph, we present a taxonomy of the various dimensions of fair information access and survey the literature to date on this new and rapidly-growing topic. We Michael D. Ekstrand, Anubrata Das, Robin Burke and Fernando Diaz (2022), \u201cFairness in Information Access Systems\u201d, Foundations and Trends\u00ae in Information Retrieval: Vol. 16, No. 1-2, pp 1\u2013177. DOI: 10.1561/1500000079. \u00a92022 M. D. Ekstrand et al. Full text available at: http://dx.doi.org/10.1561/1500000079",
    "venue": "Foundations and Trends in Information Retrieval",
    "year": 2022,
    "referenceCount": 279,
    "citationCount": 18,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2055863231",
        "name": "Rui Yan"
      },
      {
        "authorId": "143959787",
        "name": "Juntao Li"
      },
      {
        "authorId": "144007938",
        "name": "Zhou Yu"
      }
    ]
  },
  "236447339": {
    "paperId": "0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",
    "externalIds": {
      "ArXiv": "2107.12708",
      "DBLP": "journals/corr/abs-2107-12708",
      "DOI": "10.1145/3560260",
      "CorpusId": 236447339
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
    "abstract": "Alongside huge volumes of research on deep learning models in NLP in the recent years, there has been much work on benchmark datasets needed to track modeling progress. Question answering and reading comprehension have been particularly prolific in this regard, with more than 80 new datasets appearing in the past 2 years. This study is the largest survey of the field to date. We provide an overview of the various formats and domains of the current resources, highlighting the current lacunae for future work. We further discuss the current classifications of \u201cskills\u201d that question answering/reading comprehension systems are supposed to acquire and propose a new taxonomy. The supplementary materials survey the current multilingual resources and monolingual resources for languages other than English, and we discuss the implications of overfocusing on English. The study is aimed at both practitioners looking for pointers to the wealth of existing data and at researchers working on new resources.",
    "venue": "ACM Computing Surveys",
    "year": 2021,
    "referenceCount": 348,
    "citationCount": 143,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145046059",
        "name": "Anna Rogers"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      },
      {
        "authorId": "1736067",
        "name": "Isabelle Augenstein"
      }
    ]
  },
  "252519203": {
    "paperId": "285d13bf3cbe6a8a0f164f584d84f8b74067271f",
    "externalIds": {
      "ArXiv": "2209.11326",
      "DBLP": "journals/corr/abs-2209-11326",
      "DOI": "10.48550/arXiv.2209.11326",
      "CorpusId": 252519203
    },
    "publicationVenue": {
      "id": "ee37a78c-f3d8-407a-bd24-bb97fe6dbab9",
      "name": "Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Comput Linguistics"
      ],
      "issn": "0891-2017",
      "alternate_issns": [
        "1530-9312",
        "0362-613x",
        "0362-613X"
      ],
      "url": "http://aclanthology.info/venues/cl",
      "alternate_urls": [
        "http://mitpress.mit.edu/catalog/item/default.asp?ttype=4&tid=10",
        "https://www.mitpressjournals.org/loi/coli"
      ]
    },
    "title": "Towards Faithful Model Explanation in NLP: A Survey",
    "abstract": "Abstract End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, that is, an explanation should accurately represent the reasoning process behind the model\u2019s prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.",
    "venue": "Computational Linguistics",
    "year": 2022,
    "referenceCount": 272,
    "citationCount": 74,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1904906987",
        "name": "Qing Lyu"
      },
      {
        "authorId": "2817917",
        "name": "Marianna Apidianaki"
      },
      {
        "authorId": "1763608",
        "name": "Chris Callison-Burch"
      }
    ]
  },
  "252734772": {
    "paperId": "f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab",
    "externalIds": {
      "DBLP": "journals/corr/abs-2210-02875",
      "ArXiv": "2210.02875",
      "DOI": "10.48550/arXiv.2210.02875",
      "CorpusId": 252734772
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Binding Language Models in Symbolic Languages",
    "abstract": "Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "referenceCount": 67,
    "citationCount": 169,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1471878967",
        "name": "Zhoujun Cheng"
      },
      {
        "authorId": "2057038673",
        "name": "Tianbao Xie"
      },
      {
        "authorId": "2055356856",
        "name": "Peng Shi"
      },
      {
        "authorId": "2155795167",
        "name": "Chengzu Li"
      },
      {
        "authorId": "40027281",
        "name": "Rahul Nadkarni"
      },
      {
        "authorId": "2112209725",
        "name": "Yushi Hu"
      },
      {
        "authorId": "2054594326",
        "name": "Caiming Xiong"
      },
      {
        "authorId": "9215251",
        "name": "Dragomir R. Radev"
      },
      {
        "authorId": "81444299",
        "name": "M. Ostendorf"
      },
      {
        "authorId": "2137813791",
        "name": "Luke S. Zettlemoyer"
      },
      {
        "authorId": "2116827887",
        "name": "N. A. Smith"
      },
      {
        "authorId": null,
        "name": "Tao Yu"
      }
    ]
  },
  "253708270": {
    "paperId": "6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",
    "externalIds": {
      "ArXiv": "2211.10435",
      "DBLP": "journals/corr/abs-2211-10435",
      "DOI": "10.48550/arXiv.2211.10435",
      "CorpusId": 253708270
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "PAL: Program-aided Language Models",
    "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .",
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "referenceCount": 67,
    "citationCount": 354,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49715441",
        "name": "Luyu Gao"
      },
      {
        "authorId": "21626987",
        "name": "Aman Madaan"
      },
      {
        "authorId": "2149163534",
        "name": "Shuyan Zhou"
      },
      {
        "authorId": "47051926",
        "name": "Uri Alon"
      },
      {
        "authorId": "144118452",
        "name": "Pengfei Liu"
      },
      {
        "authorId": "46286308",
        "name": "Yiming Yang"
      },
      {
        "authorId": "144987107",
        "name": "Jamie Callan"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  "202540839": {
    "paperId": "093d9253a2fe765ca6577b091d3f99bab3155a7d",
    "externalIds": {
      "ACL": "D19-1404",
      "MAG": "2970200208",
      "DBLP": "journals/corr/abs-1909-00161",
      "ArXiv": "1909.00161",
      "DOI": "10.18653/v1/D19-1404",
      "CorpusId": 202540839
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",
    "abstract": "Zero-shot text classification (0Shot-TC) is a challenging NLU problem to which little attention has been paid by the research community. 0Shot-TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, emotion, event, etc.) described by the label. And there are only a few articles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include: i) The datasets we provide facilitate studying 0Shot-TC relative to conceptually different and diverse aspects: the \u201ctopic\u201d aspect includes \u201csports\u201d and \u201cpolitics\u201d as labels; the \u201cemotion\u201d aspect includes \u201cjoy\u201d and \u201canger\u201d; the \u201csituation\u201d aspect includes \u201cmedical assistance\u201d and \u201cwater shortage\u201d. ii) We extend the existing evaluation setup (label-partially-unseen) \u2013 given a dataset, train on some labels, test on all labels \u2013 to include a more challenging yet realistic evaluation label-fully-unseen 0Shot-TC (Chang et al., 2008), aiming at classifying text snippets without seeing task specific training data at all. iii) We unify the 0Shot-TC of diverse aspects within a textual entailment formulation and study it this way.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 29,
    "citationCount": 493,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40483594",
        "name": "Wenpeng Yin"
      },
      {
        "authorId": "153035400",
        "name": "Jamaal Hay"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "248505827": {
    "paperId": "6f0650a429add68e9f9430b09e1c6e8780ca787c",
    "externalIds": {
      "DBLP": "conf/naacl/SainzGLMA22",
      "ArXiv": "2205.01376",
      "DOI": "10.48550/arXiv.2205.01376",
      "CorpusId": 248505827
    },
    "publicationVenue": null,
    "title": "Textual Entailment for Event Argument Extraction: Zero- and Few-Shot with Multi-Source Learning",
    "abstract": "Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as Textual Entailment tasks using verbalizations, with strong performance in zero-shot and few-shot settings thanks to pre-trained entailment models. The fact that relations in current RE datasets are easily verbalized casts doubts on whether entailment would be effective in more complex tasks. In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE and WikiEvents respectively, while achieving the same performance as with full training. More importantly, we show that recasting EAE as entailment alleviates the dependency on schemas, which has been a road-block for transferring annotations between domains. Thanks to the entailment, the multi-source transfer between ACE and WikiEvents further reduces annotation down to 10% and 5% (respectively) of the full training without transfer. Our analysis shows that the key to good results is the use of several entailment datasets to pre-train the entailment model. Similar to previous approaches, our method requires a small amount of effort for manual verbalization: only less than 15 minutes per event argument type is needed, and comparable results can be achieved with users with different level of expertise.",
    "venue": "NAACL-HLT",
    "year": 2022,
    "referenceCount": 71,
    "citationCount": 44,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1724648481",
        "name": "Oscar Sainz"
      },
      {
        "authorId": "1404791152",
        "name": "Itziar Gonzalez-Dios"
      },
      {
        "authorId": "1715983",
        "name": "Oier Lopez de Lacalle"
      },
      {
        "authorId": "1875233",
        "name": "Bonan Min"
      },
      {
        "authorId": "1733049",
        "name": "Eneko Agirre"
      }
    ]
  },
  "238408158": {
    "paperId": "f18a9dc44d66346a8d75005f7b5ab9e7e4899de5",
    "externalIds": {
      "DBLP": "conf/iclr/ZhangHS22",
      "ArXiv": "2110.02369",
      "CorpusId": 238408158
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "EntQA: Entity Linking as Question Answering",
    "abstract": "A conventional approach to entity linking is to first find mentions in a given document and then infer their underlying entities in the knowledge base. A well-known limitation of this approach is that it requires finding mentions without knowing their entities, which is unnatural and difficult. We present a new model that does not suffer from this limitation called EntQA, which stands for Entity linking as Question Answering. EntQA first proposes candidate entities with a fast retrieval module, and then scrutinizes the document to find mentions of each candidate with a powerful reader module. Our approach combines progress in entity linking with that in open-domain question answering and capitalizes on pretrained models for dense entity retrieval and reading comprehension. Unlike in previous works, we do not rely on a mention-candidates dictionary or large-scale weak supervision. EntQA achieves strong results on the GERBIL benchmarking platform.",
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "referenceCount": 48,
    "citationCount": 43,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2107940644",
        "name": "Wenzheng Zhang"
      },
      {
        "authorId": "2007245028",
        "name": "Wenyue Hua"
      },
      {
        "authorId": "1714215",
        "name": "K. Stratos"
      }
    ]
  },
  "248965470": {
    "paperId": "11fc7b4e459479ec5facb344b42a9bad940da37a",
    "externalIds": {
      "ArXiv": "2205.09837",
      "DBLP": "conf/emnlp/LuHZMC22",
      "DOI": "10.48550/arXiv.2205.09837",
      "CorpusId": 248965470
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Summarization as Indirect Supervision for Relation Extraction",
    "abstract": "Relation extraction (RE) models have been challenged by their reliance on training data with expensive annotations. Considering that summarization tasks aim at acquiring concise expressions of synoptical information from the longer context, these tasks naturally align with the objective of RE, i.e., extracting a kind of synoptical information that describes the relation of entity mentions. We present SuRE, which converts RE into a summarization formulation. SuRE leads to more precise and resource-efficient RE based on indirect supervision from summarization tasks. To achieve this goal, we develop sentence and relation conversion techniques that essentially bridge the formulation of summarization and RE tasks. We also incorporate constraint decoding techniques with Trie scoring to further enhance summarization-based RE with robust inference. Experiments on three RE datasets demonstrate the effectiveness of SuRE in both full-dataset and low-resource settings, showing that summarization is a promising source of indirect supervision to improve RE models.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 59,
    "citationCount": 50,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1515662094",
        "name": "K. Lu"
      },
      {
        "authorId": "34809425",
        "name": "I-Hung Hsu"
      },
      {
        "authorId": "2203076",
        "name": "Wenxuan Zhou"
      },
      {
        "authorId": "144592155",
        "name": "Mingyu Derek Ma"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      }
    ]
  },
  "245219282": {
    "paperId": "4081aeb7ff148cc4678efca4e44a72dece4542e3",
    "externalIds": {
      "DBLP": "journals/corr/abs-2112-08674",
      "ACL": "2022.naacl-main.47",
      "ArXiv": "2112.08674",
      "DOI": "10.18653/v1/2022.naacl-main.47",
      "CorpusId": 245219282
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Reframing Human-AI Collaboration for Generating Free-Text Explanations",
    "abstract": "Large language models are increasingly capable of generating fluent-appearing text with relatively little task-specific supervision. But can these models accurately explain classification decisions? We consider the task of generating free-text explanations using human-written examples in a few-shot manner. We find that (1) authoring higher quality prompts results in higher quality generations; and (2) surprisingly, in a head-to-head comparison, crowdworkers often prefer explanations generated by GPT-3 to crowdsourced explanations in existing datasets. Our human studies also show, however, that while models often produce factual, grammatical, and sufficient explanations, they have room to improve along axes such as providing novel information and supporting the label. We create a pipeline that combines GPT-3 with a supervised filter that incorporates binary acceptability judgments from humans in the loop. Despite the intrinsic subjectivity of acceptability judgments, we demonstrate that acceptability is partially correlated with various fine-grained attributes of explanations. Our approach is able to consistently filter GPT-3-generated explanations deemed acceptable by humans.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 68,
    "citationCount": 128,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35823986",
        "name": "Sarah Wiegreffe"
      },
      {
        "authorId": "2689239",
        "name": "Jack Hessel"
      },
      {
        "authorId": "2133324514",
        "name": "Swabha Swayamdipta"
      },
      {
        "authorId": "2065904932",
        "name": "Mark O. Riedl"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      }
    ]
  },
  "253237669": {
    "paperId": "1d417bdd331912a458de920459f23fcc7f6e8699",
    "externalIds": {
      "DBLP": "conf/emnlp/Zhou0YR22",
      "ACL": "2022.emnlp-main.142",
      "ArXiv": "2210.16865",
      "DOI": "10.48550/arXiv.2210.16865",
      "CorpusId": 253237669
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts",
    "abstract": "Explicit decomposition modeling, which involves breaking down complex tasks into more straightforward and often more interpretable sub-tasks, has long been a central theme in developing robust and interpretable NLU systems. However, despite the many datasets and resources built as part of this effort, the majority have small-scale annotations and limited scope, which is insufficient to solve general decomposition tasks. In this paper, we look at large-scale intermediate pre-training of decomposition-based transformers using distant supervision from comparable texts, particularly large-scale parallel news. We show that with such intermediate pre-training, developing robust decomposition-based models for a diverse range of tasks becomes more feasible. For example, on semantic parsing, our model, DecompT5, improves 20% to 30% on two datasets, Overnight and TORQUE, over the baseline language model. We further use DecompT5 to build a novel decomposition-based QA system named DecompEntail, improving over state-of-the-art models, including GPT-3, on both HotpotQA and StrategyQA by 8% and 4%, respectively.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 39,
    "citationCount": 17,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2108536188",
        "name": "Ben Zhou"
      },
      {
        "authorId": "46666605",
        "name": "Kyle Richardson"
      },
      {
        "authorId": "3099583",
        "name": "Xiaodong Yu"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "219708822": {
    "paperId": "952fdd071c124691ee24c216e24474b7dec0f70e",
    "externalIds": {
      "MAG": "3035640035",
      "ArXiv": "2006.08791",
      "DBLP": "journals/corr/abs-2006-08791",
      "CorpusId": 219708822
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Learnability with Indirect Supervision Signals",
    "abstract": "Learning from indirect supervision signals is important in real-world AI applications when, often, gold labels are missing or too costly. In this paper, we develop a unified theoretical framework for multi-class classification when the supervision is provided by a variable that contains nonzero mutual information with the gold label. The nature of this problem is determined by (i) the transition probability from the gold labels to the indirect supervision variables and (ii) the learner's prior knowledge about the transition. Our framework relaxes assumptions made in the literature, and supports learning with unknown, non-invertible and instance-dependent transitions. Our theory introduces a novel concept called \\emph{separation}, which characterizes the learnability and generalization bounds. We also demonstrate the application of our framework via concrete novel results in a variety of learning scenarios such as learning with superset annotations and joint supervision signals.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 41,
    "citationCount": 7,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2148355232",
        "name": "Kaifu Wang"
      },
      {
        "authorId": "3333257",
        "name": "Qiang Ning"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "53734356": {
    "paperId": "6dfc2ff03534a4325d06c6f88c3144831996629b",
    "externalIds": {
      "MAG": "2958882215",
      "ArXiv": "1811.10830",
      "DBLP": "conf/cvpr/ZellersBFC19",
      "DOI": "10.1109/CVPR.2019.00688",
      "CorpusId": 53734356
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "From Recognition to Cognition: Visual Commonsense Reasoning",
    "abstract": "Visual understanding goes well beyond object recognition. With one glance at an image, we can effortlessly imagine the world beyond the pixels: for instance, we can infer people's actions, goals, and mental states. While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense Reasoning. Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer. Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe for generating non-trivial and high-quality problems at scale is Adversarial Matching, a new approach to transform rich annotations into multiple choice questions with minimal bias. Experimental results show that while humans find VCR easy (over 90% accuracy), state-of-the-art vision models struggle (~45%). To move towards cognition-level understanding, we present a new reasoning engine, Recognition to Cognition Networks (R2C), that models the necessary layered inferences for grounding, contextualization, and reasoning. R2C helps narrow the gap between humans and machines (~65%); still, the challenge is far from solved, and we provide analysis that suggests avenues for future work.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2018,
    "referenceCount": 97,
    "citationCount": 801,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2545335",
        "name": "Rowan Zellers"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "143787583",
        "name": "Ali Farhadi"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      }
    ]
  },
  "249097975": {
    "paperId": "4f4a409f701f7552d45c46a5b0fea69dca6f8e84",
    "externalIds": {
      "ArXiv": "2112.09118",
      "DBLP": "journals/tmlr/IzacardCHRBJG22",
      "CorpusId": 249097975
    },
    "publicationVenue": null,
    "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
    "abstract": "Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2021,
    "referenceCount": 68,
    "citationCount": 594,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1410231361",
        "name": "Gautier Izacard"
      },
      {
        "authorId": "2062862676",
        "name": "Mathilde Caron"
      },
      {
        "authorId": "26360550",
        "name": "Lucas Hosseini"
      },
      {
        "authorId": "48662861",
        "name": "Sebastian Riedel"
      },
      {
        "authorId": "2329288",
        "name": "Piotr Bojanowski"
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin"
      },
      {
        "authorId": "3024698",
        "name": "Edouard Grave"
      }
    ]
  },
  "253581733": {
    "paperId": "8cf05ed2b7cd3b0f601c454914a678c24d393de3",
    "externalIds": {
      "DBLP": "journals/corr/abs-2211-09260",
      "ArXiv": "2211.09260",
      "DOI": "10.48550/arXiv.2211.09260",
      "CorpusId": 253581733
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Task-aware Retrieval with Instructions",
    "abstract": "We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose task-aware retrieval system using multi-task instruction tuning, which can follow human-written instructions to find the best documents for a given query. We introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, BERRI, and present TART, a multi-task retrieval system trained on BERRI with instructions. TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X^2-Retrieval to better reflect real-world scenarios, where diverse domains and tasks are pooled and a system needs to find documents aligning users' intents. In this setup, TART significantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 99,
    "citationCount": 70,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35584853",
        "name": "Akari Asai"
      },
      {
        "authorId": "32246932",
        "name": "Timo Schick"
      },
      {
        "authorId": "145222654",
        "name": "Patrick Lewis"
      },
      {
        "authorId": "1769736",
        "name": "Xilun Chen"
      },
      {
        "authorId": "1410231361",
        "name": "Gautier Izacard"
      },
      {
        "authorId": "48662861",
        "name": "Sebastian Riedel"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      },
      {
        "authorId": "2072801764",
        "name": "Wen-tau Yih"
      }
    ]
  },
  "251371732": {
    "paperId": "916be31cbf847faa65cad0549e153f0c25b9f424",
    "externalIds": {
      "DBLP": "journals/jmlr/IzacardLLHPSDJRG23",
      "ArXiv": "2208.03299",
      "DOI": "10.48550/arXiv.2208.03299",
      "CorpusId": 251371732
    },
    "publicationVenue": {
      "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
      "name": "Journal of machine learning research",
      "type": "journal",
      "alternate_names": [
        "Journal of Machine Learning Research",
        "J mach learn res",
        "J Mach Learn Res"
      ],
      "issn": "1532-4435",
      "alternate_issns": [
        "1533-7928"
      ],
      "url": "http://www.ai.mit.edu/projects/jmlr/",
      "alternate_urls": [
        "http://jmlr.csail.mit.edu/",
        "http://www.jmlr.org/",
        "http://portal.acm.org/affiliated/jmlr"
      ]
    },
    "title": "Few-shot Learning with Retrieval Augmented Language Models",
    "abstract": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.",
    "venue": "Journal of machine learning research",
    "year": 2022,
    "referenceCount": 94,
    "citationCount": 572,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1410231361",
        "name": "Gautier Izacard"
      },
      {
        "authorId": "145222654",
        "name": "Patrick Lewis"
      },
      {
        "authorId": "3376175",
        "name": "M. Lomeli"
      },
      {
        "authorId": "26360550",
        "name": "Lucas Hosseini"
      },
      {
        "authorId": "40052301",
        "name": "F. Petroni"
      },
      {
        "authorId": "32246932",
        "name": "Timo Schick"
      },
      {
        "authorId": "2129456957",
        "name": "Jane A. Yu"
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin"
      },
      {
        "authorId": "48662861",
        "name": "Sebastian Riedel"
      },
      {
        "authorId": "3024698",
        "name": "Edouard Grave"
      }
    ]
  },
  "244954723": {
    "paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641",
    "externalIds": {
      "DBLP": "journals/corr/abs-2112-04426",
      "ArXiv": "2112.04426",
      "CorpusId": 244954723
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Improving language models by retrieving from trillions of tokens",
    "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "referenceCount": 65,
    "citationCount": 823,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "148016269",
        "name": "Sebastian Borgeaud"
      },
      {
        "authorId": "1697879",
        "name": "A. Mensch"
      },
      {
        "authorId": "46616544",
        "name": "Jordan Hoffmann"
      },
      {
        "authorId": "2072572294",
        "name": "Trevor Cai"
      },
      {
        "authorId": "2143538252",
        "name": "Eliza Rutherford"
      },
      {
        "authorId": "2143434227",
        "name": "Katie Millican"
      },
      {
        "authorId": "47568983",
        "name": "George van den Driessche"
      },
      {
        "authorId": "143783339",
        "name": "Jean-Baptiste Lespiau"
      },
      {
        "authorId": "2143374656",
        "name": "Bogdan Damoc"
      },
      {
        "authorId": "31993415",
        "name": "Aidan Clark"
      },
      {
        "authorId": "40550616",
        "name": "Diego de Las Casas"
      },
      {
        "authorId": "40895205",
        "name": "Aurelia Guy"
      },
      {
        "authorId": "10698483",
        "name": "Jacob Menick"
      },
      {
        "authorId": "81387328",
        "name": "Roman Ring"
      },
      {
        "authorId": "4629007",
        "name": "T. Hennigan"
      },
      {
        "authorId": "2148653469",
        "name": "Saffron Huang"
      },
      {
        "authorId": "108173905",
        "name": "Lorenzo Maggiore"
      },
      {
        "authorId": "2115601070",
        "name": "Chris Jones"
      },
      {
        "authorId": "51042571",
        "name": "Albin Cassirer"
      },
      {
        "authorId": "2065040422",
        "name": "Andy Brock"
      },
      {
        "authorId": "35550664",
        "name": "Michela Paganini"
      },
      {
        "authorId": "2060655766",
        "name": "G. Irving"
      },
      {
        "authorId": "1689108",
        "name": "O. Vinyals"
      },
      {
        "authorId": "2217144",
        "name": "Simon Osindero"
      },
      {
        "authorId": "34838386",
        "name": "K. Simonyan"
      },
      {
        "authorId": "34269227",
        "name": "Jack W. Rae"
      },
      {
        "authorId": "152585800",
        "name": "Erich Elsen"
      },
      {
        "authorId": "2175946",
        "name": "L. Sifre"
      }
    ]
  },
  "248545122": {
    "paperId": "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7",
    "externalIds": {
      "DBLP": "conf/iclr/JongZFSC22",
      "ArXiv": "2110.06176",
      "CorpusId": 248545122
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "MENTION MEMORY : INCORPORATING TEXTUAL KNOWLEDGE INTO TRANSFORMERS THROUGH ENTITY MENTION ATTENTION",
    "abstract": "Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with `mention memory', a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining.",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "referenceCount": 27,
    "citationCount": 42,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "21379393",
        "name": "Michiel de Jong"
      },
      {
        "authorId": "51199981",
        "name": "Yury Zemlyanskiy"
      },
      {
        "authorId": "143883142",
        "name": "Nicholas FitzGerald"
      },
      {
        "authorId": "145757665",
        "name": "Fei Sha"
      },
      {
        "authorId": "2058480371",
        "name": "W. Cohen"
      }
    ]
  },
  "254220735": {
    "paperId": "9492ee1435e183cb62b65d8d7f39be0dfd17377a",
    "externalIds": {
      "DBLP": "conf/acl/MinSL0YHZ23",
      "ArXiv": "2212.01349",
      "DOI": "10.48550/arXiv.2212.01349",
      "CorpusId": 254220735
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Nonparametric Masked Language Modeling",
    "abstract": "Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 96,
    "citationCount": 44,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48872685",
        "name": "Sewon Min"
      },
      {
        "authorId": "3040379",
        "name": "Weijia Shi"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "1769736",
        "name": "Xilun Chen"
      },
      {
        "authorId": "2072801764",
        "name": "Wen-tau Yih"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ]
  },
  "249062699": {
    "paperId": "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1",
    "externalIds": {
      "DBLP": "journals/corr/abs-2205-12674",
      "ArXiv": "2205.12674",
      "ACL": "2022.emnlp-main.382",
      "DOI": "10.48550/arXiv.2205.12674",
      "CorpusId": 249062699
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Training Language Models with Memory Augmentation",
    "abstract": "Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories\u2014local, long-term, and external memory\u2014at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 61,
    "citationCount": 115,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49164966",
        "name": "Zexuan Zhong"
      },
      {
        "authorId": "49986267",
        "name": "Tao Lei"
      },
      {
        "authorId": "50536468",
        "name": "Danqi Chen"
      }
    ]
  },
  "249152130": {
    "paperId": "563a851106623b9f112d0e2a290d3950a871079c",
    "externalIds": {
      "ArXiv": "2205.13792",
      "DBLP": "journals/corr/abs-2205-13792",
      "ACL": "2022.emnlp-main.214",
      "DOI": "10.48550/arXiv.2205.13792",
      "CorpusId": 249152130
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Nearest Neighbor Zero-Shot Inference",
    "abstract": "Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy. We extensively study one such model, the k-nearest neighbor LM (kNN-LM), showing that the gains marginally transfer. The main challenge is to achieve coverage of the verbalizer tokens that define the different end-task class labels. To address this challenge, we also introduce kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers (e.g. to expand \u201cterrible\u201d to also include \u201csilly\u201d and other task-specific synonyms for sentiment classification). Across nine diverse end-tasks, using kNN-Prompt with GPT-2 large yields significant performance boosts over strong zeroshot baselines (13.4% absolute improvement over the base LM on average). We also show that other advantages of non-parametric augmentation hold for end tasks; kNN-Prompt is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 39,
    "citationCount": 30,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3040379",
        "name": "Weijia Shi"
      },
      {
        "authorId": "38614754",
        "name": "Julian Michael"
      },
      {
        "authorId": "40895369",
        "name": "Suchin Gururangan"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ]
  },
  "246431219": {
    "paperId": "f92535edac9d1c735feabdb4d94c1157f12d899c",
    "externalIds": {
      "ArXiv": "2201.12431",
      "DBLP": "conf/icml/0002XHSRN22",
      "CorpusId": 246431219
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
    "abstract": "Retrieval-based language models (R-LM) model the probability of natural language text by combining a standard language model (LM) with examples retrieved from an external datastore at test time. While effective, a major bottleneck of using these models in practice is the computationally costly datastore search, which can be performed as frequently as every time step. In this paper, we present RetoMaton - retrieval automaton - which approximates the datastore search, based on (1) saving pointers between consecutive datastore entries, and (2) clustering of entries into\"states\". This effectively results in a weighted finite automaton built on top of the datastore, instead of representing the datastore as a flat list. The creation of the automaton is unsupervised, and a RetoMaton can be constructed from any text collection: either the original training corpus or from another domain. Traversing this automaton at inference time, in parallel to the LM inference, reduces its perplexity by up to 1.85, or alternatively saves up to 83% of the nearest neighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting perplexity. Our code and trained models are available at https://github.com/neulab/retomaton .",
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "referenceCount": 47,
    "citationCount": 53,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47051926",
        "name": "Uri Alon"
      },
      {
        "authorId": "40027632",
        "name": "Frank F. Xu"
      },
      {
        "authorId": "6215698",
        "name": "Junxian He"
      },
      {
        "authorId": "2072419570",
        "name": "Sudipta Sengupta"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  "51609977": {
    "paperId": "0957dc1a757f292ff8ba7a8e186ffc63a63d6b5a",
    "externalIds": {
      "MAG": "2842153692",
      "DBLP": "conf/ijcai/CabrioV18",
      "DOI": "10.24963/ijcai.2018/766",
      "CorpusId": 51609977
    },
    "publicationVenue": {
      "id": "67f7f831-711a-43c8-8785-1e09005359b5",
      "name": "International Joint Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Int Jt Conf Artif Intell",
        "IJCAI"
      ],
      "url": "http://www.ijcai.org/"
    },
    "title": "Five Years of Argument Mining: a Data-driven Analysis",
    "abstract": "Argument mining is the research area aiming at extracting natural language arguments and their relations from text, with the final goal of providing machine-processable structured data for computational models of argument. This research topic has started to attract the attention of a small community of researchers around 2014, and it is nowadays counted as one of the most promising research areas in Artificial Intelligence in terms of growing of the community, funded projects, and involvement of companies. In this paper, we present the argument mining tasks, and we discuss the obtained results in the area from a data-driven perspective. An open discussion highlights the main weaknesses suffered by the existing work in the literature, and proposes open challenges to be faced in the future.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2018,
    "referenceCount": 45,
    "citationCount": 161,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1772891",
        "name": "Elena Cabrio"
      },
      {
        "authorId": "1725656",
        "name": "S. Villata"
      }
    ]
  },
  "236460206": {
    "paperId": "dcb0b23685c9c116d8d53fe47e5157753659d3bd",
    "externalIds": {
      "DBLP": "conf/acl/VecchiFJL20",
      "ACL": "2021.acl-long.107",
      "DOI": "10.18653/v1/2021.acl-long.107",
      "CorpusId": 236460206
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Towards Argument Mining for Social Good: A Survey",
    "abstract": "This survey builds an interdisciplinary picture of Argument Mining (AM), with a strong focus on its potential to address issues related to Social and Political Science. More specifically, we focus on AM challenges related to its applications to social media and in the multilingual domain, and then proceed to the widely debated notion of argument quality. We propose a novel definition of argument quality which is integrated with that of deliberative quality from the Social Science literature. Under our definition, the quality of a contribution needs to be assessed at multiple levels: the contribution itself, its preceding context, and the consequential effect on the development of the upcoming discourse. The latter has not received the deserved attention within the community. We finally define an application of AM for Social Good: (semi-)automatic moderation, a highly integrative application which (a) represents a challenging testbed for the integrated notion of quality we advocate, (b) allows the empirical quantification of argument/deliberative quality to benefit from the developments in other NLP fields (i.e. hate speech detection, fact checking, debiasing), and (c) has a clearly beneficial potential at the level of its societal thanks to its real-world application (even if extremely ambitious).",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 100,
    "citationCount": 36,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "36274653",
        "name": "Eva Maria Vecchi"
      },
      {
        "authorId": "1724540796",
        "name": "Neele Falk"
      },
      {
        "authorId": "2121302675",
        "name": "Iman Jundi"
      },
      {
        "authorId": "2424572",
        "name": "Gabriella Lapesa"
      }
    ]
  },
  "5252401": {
    "paperId": "834d68b9befcc6c68415b460b33435a1822799fb",
    "externalIds": {
      "ACL": "J17-1004",
      "ArXiv": "1601.02403",
      "DBLP": "journals/coling/HabernalG17",
      "MAG": "2236647290",
      "DOI": "10.1162/COLI_a_00276",
      "CorpusId": 5252401
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Argumentation Mining in User-Generated Web Discourse",
    "abstract": "The goal of argumentation mining, an evolving research field in computational linguistics, is to design methods capable of analyzing people\u2019s argumentation. In this article, we go beyond the state of the art in several ways. (i) We deal with actual Web data and take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse. (ii) We bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study. (iii) We create a new gold standard corpus (90k tokens in 340 documents) and experiment with several machine learning methods to identify argument components. We offer the data, source codes, and annotation guidelines to the community under free licenses. Our findings show that argumentation mining in user-generated Web discourse is a feasible but challenging task.",
    "venue": "International Conference on Computational Logic",
    "year": 2016,
    "referenceCount": 213,
    "citationCount": 256,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2572366",
        "name": "Ivan Habernal"
      },
      {
        "authorId": "1730400",
        "name": "Iryna Gurevych"
      }
    ]
  },
  "11014757": {
    "paperId": "08f6819e66318cd49cddefd5d690a752d1098da7",
    "externalIds": {
      "MAG": "2610091188",
      "DBLP": "conf/emnlp/DaxenbergerEHSG17",
      "ArXiv": "1704.07203",
      "ACL": "D17-1218",
      "DOI": "10.18653/v1/D17-1218",
      "CorpusId": 11014757
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "What is the Essence of a Claim? Cross-Domain Claim Identification",
    "abstract": "Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent conceptualization of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2017,
    "referenceCount": 61,
    "citationCount": 92,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1790638",
        "name": "Johannes Daxenberger"
      },
      {
        "authorId": "2620186",
        "name": "Steffen Eger"
      },
      {
        "authorId": "2572366",
        "name": "Ivan Habernal"
      },
      {
        "authorId": "3067663",
        "name": "Christian Stab"
      },
      {
        "authorId": "1730400",
        "name": "Iryna Gurevych"
      }
    ]
  },
  "31107411": {
    "paperId": "29e4e14a5613be06a39e76bf0d8a4c8217573c2f",
    "externalIds": {
      "MAG": "2760347205",
      "DBLP": "conf/emnlp/DusmanuCV17",
      "ACL": "D17-1245",
      "DOI": "10.18653/v1/D17-1245",
      "CorpusId": 31107411
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Argument Mining on Twitter: Arguments, Facts and Sources",
    "abstract": "Social media collect and spread on the Web personal opinions, facts, fake news and all kind of information users may be interested in. Applying argument mining methods to such heterogeneous data sources is a challenging open research issue, in particular considering the peculiarities of the language used to write textual messages on social media. In addition, new issues emerge when dealing with arguments posted on such platforms, such as the need to make a distinction between personal opinions and actual facts, and to detect the source disseminating information about such facts to allow for provenance verification. In this paper, we apply supervised classification to identify arguments on Twitter, and we present two new tasks for argument mining, namely facts recognition and source identification. We study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexit news topics.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2017,
    "referenceCount": 20,
    "citationCount": 76,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "24871970",
        "name": "Mihai Dusmanu"
      },
      {
        "authorId": "1772891",
        "name": "Elena Cabrio"
      },
      {
        "authorId": "1725656",
        "name": "S. Villata"
      }
    ]
  },
  "219177237": {
    "paperId": "ebe7c37c60024330bc8e90f7057961f9b849ff8d",
    "externalIds": {
      "MAG": "3032612850",
      "ArXiv": "2006.00843",
      "DBLP": "journals/corr/abs-2006-00843",
      "ACL": "2020.coling-main.402",
      "DOI": "10.18653/V1/2020.COLING-MAIN.402",
      "CorpusId": 219177237
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Rhetoric, Logic, and Dialectic: Advancing Theory-based Argument Quality Assessment in Natural Language Processing",
    "abstract": "Though preceding work in computational argument quality (AQ) mostly focuses on assessing overall AQ, researchers agree that writers would benefit from feedback targeting individual dimensions of argumentation theory. However, a large-scale theory-based corpus and corresponding computational models are missing. We fill this gap by conducting an extensive analysis covering three diverse domains of online argumentative writing and presenting GAQCorpus: the first large-scale English multi-domain (community Q&A forums, debate forums, review forums) corpus annotated with theory-based AQ scores. We then propose the first computational approaches to theory-based assessment, which can serve as strong baselines for future work. We demonstrate the feasibility of large-scale AQ annotation, show that exploiting relations between dimensions yields performance improvements, and explore the synergies between theory-based prediction and practical AQ assessment.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2020,
    "referenceCount": 41,
    "citationCount": 40,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "29891652",
        "name": "Anne Lauscher"
      },
      {
        "authorId": "2069505550",
        "name": "Lily Ng"
      },
      {
        "authorId": "3047950",
        "name": "Courtney Napoles"
      },
      {
        "authorId": "1739099",
        "name": "Joel R. Tetreault"
      }
    ]
  },
  "256631011": {
    "paperId": "f2b93fa29a948b7699ed35e80889b74e70ca7b4c",
    "externalIds": {
      "DBLP": "conf/emnlp/MarroCV22",
      "DOI": "10.18653/v1/2022.findings-emnlp.306",
      "CorpusId": 256631011
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Graph Embeddings for Argumentation Quality Assessment",
    "abstract": ",",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 39,
    "citationCount": 9,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "122316116",
        "name": "Santiago Marro"
      },
      {
        "authorId": "1772891",
        "name": "Elena Cabrio"
      },
      {
        "authorId": "1725656",
        "name": "S. Villata"
      }
    ]
  },
  "56161114": {
    "paperId": "3ac244867115f42c255fea0b0460022e55e72c73",
    "externalIds": {
      "MAG": "2092284551",
      "DOI": "10.1057/PALGRAVE.CEP.6110002",
      "CorpusId": 56161114
    },
    "publicationVenue": null,
    "title": "Measuring Political Deliberation: A Discourse Quality Index",
    "abstract": null,
    "venue": "",
    "year": 2003,
    "referenceCount": 71,
    "citationCount": 570,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Sociology",
        "source": "external"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "72113330",
        "name": "M. Steenbergen"
      },
      {
        "authorId": "67002751",
        "name": "Andr\u00e9 B\u00e4chtiger"
      },
      {
        "authorId": "117113588",
        "name": "Markus Sp\u00f6rndli"
      },
      {
        "authorId": "31852878",
        "name": "J. Steiner"
      }
    ]
  },
  "157370225": {
    "paperId": "3d1605960bc44899f0e1f9db9122a57cc0e1305f",
    "externalIds": {
      "MAG": "2518877660",
      "DOI": "10.1017/S0007123416000144",
      "CorpusId": 157370225
    },
    "publicationVenue": {
      "id": "fbdd3843-cc9e-480a-8369-6a16f60297bf",
      "name": "British Journal of Political Science",
      "type": "journal",
      "alternate_names": [
        "Br J Political Sci"
      ],
      "issn": "0007-1234",
      "url": "https://www.cambridge.org/core/journals/british-journal-of-political-science",
      "alternate_urls": [
        "http://journals.cambridge.org/action/displayJournal?jid=JPS",
        "https://www.jstor.org/journal/britjpoliscie"
      ]
    },
    "title": "Deliberative Abilities and Influence in a Transnational Deliberative Poll (EuroPolis)",
    "abstract": "This article investigates the deliberative abilities of ordinary citizens in the context of \u2018EuroPolis\u2019, a transnational deliberative poll. Drawing upon a philosophically grounded instrument, an updated version of the Discourse Quality Index (DQI), it explores how capable European citizens are of meeting deliberative ideals; whether socio-economic, cultural and psychological biases affect the ability to deliberate; and whether opinion change results from the exchange of arguments. On the positive side, EuroPolis shows that the ideal deliberator scoring high on all deliberative standards does actually exist, and that participants change their opinions more often when rational justification is used in the discussions. On the negative side, deliberative abilities are unequally distributed: in particular, working-class members are less likely to contribute to a high standard of deliberation.",
    "venue": "British Journal of Political Science",
    "year": 2016,
    "referenceCount": 86,
    "citationCount": 61,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Economics",
        "source": "external"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "50466938",
        "name": "Marl\u00e8ne Gerber"
      },
      {
        "authorId": "67002751",
        "name": "Andr\u00e9 B\u00e4chtiger"
      },
      {
        "authorId": "47480319",
        "name": "Susumu Shikano"
      },
      {
        "authorId": "121576386",
        "name": "Simon Reber"
      },
      {
        "authorId": "2073637685",
        "name": "Samuel Rohr"
      }
    ]
  },
  "44099358": {
    "paperId": "01f401fefac301b8a49371099e4039b4c74d5d73",
    "externalIds": {
      "MAG": "2799126217",
      "ArXiv": "1805.10254",
      "DBLP": "journals/corr/abs-1805-10254",
      "ACL": "P18-1021",
      "DOI": "10.18653/v1/P18-1021",
      "CorpusId": 44099358
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Neural Argument Generation Augmented with Externally Retrieved Evidence",
    "abstract": "High quality arguments are essential elements for human reasoning and decision-making processes. However, effective argument construction is a challenging task for both human and machines. In this work, we study a novel task on automatically generating arguments of a different stance for a given statement. We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia. Our model first generates a set of talking point phrases as intermediate representation, followed by a separate decoder producing the final argument based on both input and the keyphrases. Experiments on a large-scale dataset collected from Reddit show that our model constructs arguments with more topic-relevant content than popular sequence-to-sequence generation models according to automatic evaluation and human assessments.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 45,
    "citationCount": 58,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "7156360",
        "name": "Xinyu Hua"
      },
      {
        "authorId": null,
        "name": "Lu Wang"
      }
    ]
  },
  "219065800": {
    "paperId": "356466a042a763de6cff0fbaa3ceaa6ac65b3d80",
    "externalIds": {
      "MAG": "3022758713",
      "DBLP": "conf/acl/AlshomarySPW20",
      "ACL": "2020.acl-main.399",
      "DOI": "10.18653/V1/2020.ACL-MAIN.399",
      "CorpusId": 219065800
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Target Inference in Argument Conclusion Generation",
    "abstract": "In argumentation, people state premises to reason towards a conclusion. The conclusion conveys a stance towards some target, such as a concept or statement. Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons. However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation. We thus study the question to what extent an argument\u2019s conclusion can be reconstructed from its premises. In particular, we argue here that a decisive step is to infer a conclusion\u2019s target, and we hypothesize that this target is related to the premises\u2019 targets. We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network. Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines. According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 42,
    "citationCount": 21,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2300829",
        "name": "Milad Alshomary"
      },
      {
        "authorId": "18417916",
        "name": "S. Syed"
      },
      {
        "authorId": "3046200",
        "name": "Martin Potthast"
      },
      {
        "authorId": "2626599",
        "name": "Henning Wachsmuth"
      }
    ]
  },
  "239885913": {
    "paperId": "1aeb7e6b23e913a1c9bd3d6879d125a042184c64",
    "externalIds": {
      "ACL": "2021.argmining-1.7",
      "DBLP": "journals/corr/abs-2110-13495",
      "ArXiv": "2110.13495",
      "DOI": "10.18653/v1/2021.argmining-1.7",
      "CorpusId": 239885913
    },
    "publicationVenue": {
      "id": "033381f9-4ca6-43a6-8634-c0e5d389dba9",
      "name": "Workshop on Argument Mining",
      "type": "conference",
      "alternate_names": [
        "ArgMining",
        "Workshop Argum Min"
      ]
    },
    "title": "Assessing the Sufficiency of Arguments through Conclusion Generation",
    "abstract": "The premises of an argument give evidence or other reasons to support a conclusion. However, the amount of support required depends on the generality of a conclusion, the nature of the individual premises, and similar. An argument whose premises make its conclusion rationally worthy to be drawn is called sufficient in argument quality research. Previous work tackled sufficiency assessment as a standard text classification problem, not modeling the inherent relation of premises and conclusion. In this paper, we hypothesize that the conclusion of a sufficient argument can be generated from its premises. To study this hypothesis, we explore the potential of assessing sufficiency based on the output of large-scale pre-trained language models. Our best model variant achieves an F1-score of .885, outperforming the previous state-of-the-art and being on par with human experts. While manual evaluation reveals the quality of the generated conclusions, their impact remains low ultimately.",
    "venue": "Workshop on Argument Mining",
    "year": 2021,
    "referenceCount": 23,
    "citationCount": 21,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2047307140",
        "name": "Timon Ziegenbein"
      },
      {
        "authorId": "2300829",
        "name": "Milad Alshomary"
      },
      {
        "authorId": "2626599",
        "name": "Henning Wachsmuth"
      }
    ]
  },
  "202768765": {
    "paperId": "d13004d17c61c6ca423b74707bb5b8d7440613b7",
    "externalIds": {
      "ACL": "D19-1568",
      "ArXiv": "2004.03034",
      "DBLP": "conf/emnlp/DurmusLC19",
      "MAG": "2970462997",
      "DOI": "10.18653/v1/D19-1568",
      "CorpusId": 202768765
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "The Role of Pragmatic and Discourse Context in Determining Argument Impact",
    "abstract": "Research in the social sciences and psychology has shown that the persuasiveness of an argument depends not only the language employed, but also on attributes of the source/communicator, the audience, and the appropriateness and strength of the argument\u2019s claims given the pragmatic and discourse context of the argument. Among these characteristics of persuasive arguments, prior work in NLP does not explicitly investigate the effect of the pragmatic and discourse context when determining argument quality. This paper presents a new dataset to initiate the study of this aspect of argumentation: it consists of a diverse collection of arguments covering 741 controversial topics and comprising over 47,000 claims. We further propose predictive models that incorporate the pragmatic and discourse context of argumentative claims and show that they outperform models that rely only on claim-specific linguistic features for predicting the perceived impact of individual claims within a particular line of argument.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 46,
    "citationCount": 24,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "41152329",
        "name": "Esin Durmus"
      },
      {
        "authorId": "8759332",
        "name": "Faisal Ladhak"
      },
      {
        "authorId": "1748501",
        "name": "Claire Cardie"
      }
    ]
  },
  "222310410": {
    "paperId": "eb64f8c3e56ccfb3263def4edd4249a7cf1541ad",
    "externalIds": {
      "ACL": "2020.findings-emnlp.29",
      "DBLP": "conf/emnlp/Toledo-RonenOBS20",
      "MAG": "3099640445",
      "ArXiv": "2010.06432",
      "DOI": "10.18653/v1/2020.findings-emnlp.29",
      "CorpusId": 222310410
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Multilingual Argument Mining: Datasets and Analysis",
    "abstract": "The growing interest in argument mining and computational argumentation brings with it a plethora of Natural Language Understanding (NLU) tasks and corresponding datasets. However, as with many other NLU tasks, the dominant language is English, with resources in other languages being few and far between. In this work, we explore the potential of transfer learning using the multilingual BERT model to address argument mining tasks in non-English languages, based on English datasets and the use of machine translation. We show that such methods are well suited for classifying the stance of arguments and detecting evidence, but less so for assessing the quality of arguments, presumably because quality is harder to preserve under translation. In addition, focusing on the translate-train approach, we show how the choice of languages for translation, and the relations among them, affect the accuracy of the resultant model. Finally, to facilitate evaluation of transfer learning on argument mining tasks, we provide a human-generated dataset with more than 10k arguments in multiple languages, as well as machine translation of the English datasets.",
    "venue": "Findings",
    "year": 2020,
    "referenceCount": 31,
    "citationCount": 31,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3022994",
        "name": "Orith Toledo-Ronen"
      },
      {
        "authorId": "80108223",
        "name": "Matan Orbach"
      },
      {
        "authorId": "2911299",
        "name": "Yonatan Bilu"
      },
      {
        "authorId": "51451979",
        "name": "Artem Spector"
      },
      {
        "authorId": "1766595",
        "name": "N. Slonim"
      }
    ]
  },
  "227151662": {
    "paperId": "32f16fa23ee77456400ddacfceeb1b06b99220ec",
    "externalIds": {
      "DBLP": "journals/corr/abs-2011-12014",
      "MAG": "3109302007",
      "ACL": "2020.argmining-1.9",
      "ArXiv": "2011.12014",
      "CorpusId": 227151662
    },
    "publicationVenue": {
      "id": "033381f9-4ca6-43a6-8634-c0e5d389dba9",
      "name": "Workshop on Argument Mining",
      "type": "conference",
      "alternate_names": [
        "ArgMining",
        "Workshop Argum Min"
      ]
    },
    "title": "Argument from Old Man\u2019s View: Assessing Social Bias in Argumentation",
    "abstract": "Social bias in language - towards genders, ethnicities, ages, and other social groups - poses a problem with ethical impact for many NLP applications. Recent research has shown that machine learning models trained on respective data may not only adopt, but even amplify the bias. So far, however, little attention has been paid to bias in computational argumentation. In this paper, we study the existence of social biases in large English debate portals. In particular, we train word embedding models on portal-specific corpora and systematically evaluate their bias using WEAT, an existing metric to measure bias in word embeddings. In a word co-occurrence analysis, we then investigate causes of bias. The results suggest that all tested debate corpora contain unbalanced and biased data, mostly in favor of male people with European-American names. Our empirical insights contribute towards an understanding of bias in argumentative data sources.",
    "venue": "Workshop on Argument Mining",
    "year": 2020,
    "referenceCount": 44,
    "citationCount": 18,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Sociology",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "83854974",
        "name": "Maximilian Splieth\u00f6ver"
      },
      {
        "authorId": "2626599",
        "name": "Henning Wachsmuth"
      }
    ]
  },
  "10784127": {
    "paperId": "435f22636f6fe13797951fd6cbe4532bc88f89ee",
    "externalIds": {
      "MAG": "2152193375",
      "DOI": "10.1177/1754073910361984",
      "CorpusId": 10784127
    },
    "publicationVenue": null,
    "title": "Emotion and Motivation: Toward Consensus Definitions and a Common Research Purpose",
    "abstract": "Historically, the hypothesis driving emotion research has been that emotion\u2019s data-base\u2014in language, physiology, and behavior\u2014 is organized around specific mental states, as reflected in evaluative language. It is suggested that this approach has not greatly advanced a natural science of emotion and that the developing motivational model of emotion defines a better path: emotion is an evolved trait founded on motivational neural circuitry shared by mammalian species, primitively prompting heightened perceptual processing and reflex mobilization for action to appetitive or threatening survival cues. As the field moves forward with increasingly sophisticated measurement technology and assessing more complex affective functioning, scientific understanding of human emotion will proceed best within the framework of this mammalian brain model.",
    "venue": "",
    "year": 2010,
    "referenceCount": 40,
    "citationCount": 138,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143853826",
        "name": "P. Lang"
      }
    ]
  },
  "221320207": {
    "paperId": "1ab38e55ed557f4dd03158268235fd2050baa730",
    "externalIds": {
      "MAG": "2091084672",
      "DOI": "10.1511/2001.4.344",
      "CorpusId": 221320207
    },
    "publicationVenue": {
      "id": "f09e3439-2129-47ae-81c3-42fac2e6bf5c",
      "name": "American Scientist",
      "type": "journal",
      "alternate_names": [
        "Am Sci"
      ],
      "issn": "0003-0996",
      "url": "https://www.americanscientist.org/",
      "alternate_urls": [
        "http://www.jstor.org/action/showPublication?journalCode=amerscie",
        "https://www.jstor.org/journal/amerscie"
      ]
    },
    "title": "The Nature of Emotions",
    "abstract": null,
    "venue": "American Scientist",
    "year": 2001,
    "referenceCount": 0,
    "citationCount": 754,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "84527386",
        "name": "R. Plutchik"
      }
    ]
  },
  "52012769": {
    "paperId": "723f744455914bf1ead6ad267976a1500b7dee4a",
    "externalIds": {
      "MAG": "2874464011",
      "ACL": "C18-1179",
      "DBLP": "conf/coling/BostanK18",
      "CorpusId": 52012769
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "An Analysis of Annotated Corpora for Emotion Classification in Text",
    "abstract": "Several datasets have been annotated and published for classification of emotions. They differ in several ways: (1) the use of different annotation schemata (e. g., discrete label sets, including joy, anger, fear, or sadness or continuous values including valence, or arousal), (2) the domain, and, (3) the file formats. This leads to several research gaps: supervised models often only use a limited set of available resources. Additionally, no previous work has compared emotion corpora in a systematic manner. We aim at contributing to this situation with a survey of the datasets, and aggregate them in a common file format with a common annotation schema. Based on this aggregation, we perform the first cross-corpus classification experiments in the spirit of future research enabled by this paper, in order to gain insight and a better understanding of differences of models inferred from the data. This work also simplifies the choice of the most appropriate resources for developing a model for a novel domain. One result from our analysis is that a subset of corpora is better classified with models trained on a different corpus. For none of the corpora, training on all data altogether is better than using a subselection of the resources. Our unified corpus is available at http://www.ims.uni-stuttgart.de/data/unifyemotion.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2018,
    "referenceCount": 72,
    "citationCount": 184,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51199643",
        "name": "Laura Ana Maria Bostan"
      },
      {
        "authorId": "66339110",
        "name": "Roman Klinger"
      }
    ]
  },
  "226237153": {
    "paperId": "eef65f8affe3dceba40f87b57789a02a40366d30",
    "externalIds": {
      "ArXiv": "2011.01612",
      "ACL": "2020.coling-main.575",
      "DBLP": "journals/corr/abs-2011-01612",
      "MAG": "3093456628",
      "DOI": "10.18653/V1/2020.COLING-MAIN.575",
      "CorpusId": 226237153
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "XED: A Multilingual Dataset for Sentiment Analysis and Emotion Detection",
    "abstract": "We introduce XED, a multilingual fine-grained emotion dataset. The dataset consists of human-annotated Finnish (25k) and English sentences (30k), as well as projected annotations for 30 additional languages, providing new resources for many low-resource languages. We use Plutchik\u2019s core emotions to annotate the dataset with the addition of neutral to create a multilabel multiclass dataset. The dataset is carefully evaluated using language-specific BERT models and SVMs to show that XED performs on par with other similar datasets and is therefore a useful tool for sentiment analysis and emotion detection.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2020,
    "referenceCount": 45,
    "citationCount": 46,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35497520",
        "name": "Emily \u00d6hman"
      },
      {
        "authorId": "1850527789",
        "name": "Marc P\u00e0mies"
      },
      {
        "authorId": "41033530",
        "name": "Kaisla Kajava"
      },
      {
        "authorId": "143675545",
        "name": "J. Tiedemann"
      }
    ]
  },
  "249605436": {
    "paperId": "0ed26f2ec23335da48d03418cd067dd807fbb330",
    "externalIds": {
      "ArXiv": "2206.05238",
      "DBLP": "journals/corr/abs-2206-05238",
      "ACL": "2023.cl-1.1",
      "DOI": "10.1162/coli_a_00461",
      "CorpusId": 249605436
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction",
    "abstract": "The most prominent tasks in emotion analysis are to assign emotions to texts and to understand how emotions manifest in language. An important observation for natural language processing is that emotions can be communicated implicitly by referring to events alone, appealing to an empathetic, intersubjective understanding of events, even without explicitly mentioning an emotion name. In psychology, the class of emotion theories known as appraisal theories aims at explaining the link between events and emotions. Appraisals can be formalized as variables that measure a cognitive evaluation by people living through an event that they consider relevant. They include the assessment if an event is novel, if the person considers themselves to be responsible, if it is in line with their own goals, and so forth. Such appraisals explain which emotions are developed based on an event, for example, that a novel situation can induce surprise or one with uncertain consequences could evoke fear. We analyze the suitability of appraisal theories for emotion analysis in text with the goal of understanding if appraisal concepts can reliably be reconstructed by annotators, if they can be predicted by text classifiers, and if appraisal concepts help to identify emotion categories. To achieve that, we compile a corpus by asking people to textually describe events that triggered particular emotions and to disclose their appraisals. Then, we ask readers to reconstruct emotions and appraisals from the text. This set-up allows us to measure if emotions and appraisals can be recovered purely from text and provides a human baseline to judge a model\u2019s performance measures. Our comparison of text classification methods to human annotators shows that both can reliably detect emotions and appraisals with similar performance. Therefore, appraisals constitute an alternative computational emotion analysis paradigm and further improve the categorization of emotions in text with joint models.",
    "venue": "International Conference on Computational Logic",
    "year": 2022,
    "referenceCount": 134,
    "citationCount": 31,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "82895194",
        "name": "Enrica Troiano"
      },
      {
        "authorId": "2000278193",
        "name": "Laura Oberl\u00e4nder"
      },
      {
        "authorId": "66339110",
        "name": "Roman Klinger"
      }
    ]
  },
  "208010268": {
    "paperId": "f9700e31a1d0ae34d4571ab056dfb268c1543349",
    "externalIds": {
      "DBLP": "journals/corr/abs-1911-12237",
      "ACL": "D19-5409",
      "ArXiv": "1911.12237",
      "MAG": "2989743967",
      "DOI": "10.18653/v1/D19-5409",
      "CorpusId": 208010268
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization",
    "abstract": "This paper introduces the SAMSum Corpus, a new dataset with abstractive dialogue summaries. We investigate the challenges it poses for automated summarization by testing several models and comparing their results with those obtained on a corpus of news articles. We show that model-generated summaries of dialogues achieve higher ROUGE scores than the model-generated summaries of news \u2013 in contrast with human evaluators\u2019 judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated models and non-standard quality measures. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 32,
    "citationCount": 526,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1782426",
        "name": "Bogdan Gliwa"
      },
      {
        "authorId": "103241417",
        "name": "Iwona Mochol"
      },
      {
        "authorId": "2065251929",
        "name": "M. Biesek"
      },
      {
        "authorId": "1744393",
        "name": "A. Wawer"
      }
    ]
  },
  "221749138": {
    "paperId": "86cb79083bfa5dc6329ab1b8c7099af76fefde36",
    "externalIds": {
      "MAG": "3085629518",
      "DBLP": "conf/emnlp/ZhuXZ020",
      "ACL": "2020.findings-emnlp.19",
      "DOI": "10.18653/V1/2020.FINDINGS-EMNLP.19",
      "CorpusId": 221749138
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining",
    "abstract": "With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization intractable. Meanwhile, there are a handful of deep neural models for text summarization and dialogue systems. However, the semantic structure and styles of meeting transcripts are quite different from articles and conversations. In this paper, we propose a novel abstractive summary network that adapts to the meeting scenario. We design a hierarchical structure to accommodate long meeting transcripts and a role vector to depict the difference among speakers. Furthermore, due to the inadequacy of meeting summary data, we pretrain the model on large-scale news summary data. Empirical results show that our model outperforms previous approaches in both automatic metrics and human evaluation. For example, on ICSI dataset, the ROUGE-1 score increases from 34.66% to 46.28%.",
    "venue": "Findings",
    "year": 2020,
    "referenceCount": 40,
    "citationCount": 134,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8652308",
        "name": "Chenguang Zhu"
      },
      {
        "authorId": "8233965",
        "name": "Ruochen Xu"
      },
      {
        "authorId": "48262024",
        "name": "Michael Zeng"
      },
      {
        "authorId": "144531812",
        "name": "Xuedong Huang"
      }
    ]
  },
  "237420813": {
    "paperId": "ac95a18762133d4065ac8af518c33084d83c5582",
    "externalIds": {
      "ArXiv": "2109.02492",
      "DBLP": "journals/corr/abs-2109-02492",
      "DOI": "10.1609/aaai.v36i10.21432",
      "CorpusId": 237420813
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "DialogLM: Pre-trained Model for Long Dialogue Understanding and Summarization",
    "abstract": "Dialogue is an essential part of human communication and cooperation. Existing research mainly focuses on short dialogue scenarios in a one-on-one fashion. However, multi-person interactions in the real world, such as meetings or interviews, are frequently over a few thousand words. There is still a lack of corresponding research and powerful tools to understand and process such long dialogues. Therefore, in this work, we present a pre-training framework for long dialogue understanding and summarization. Considering the nature of long conversations, we propose a window-based denoising approach for generative pre-training. For a dialogue, it corrupts a window of text with dialogue-inspired noise, and guides the model to reconstruct this window based on the content of the remaining conversation. Furthermore, to process longer input, we augment the model with sparse attention which is combined with conventional attention in a hybrid manner. We conduct extensive experiments on five datasets of long dialogues, covering tasks of dialogue summarization, abstractive question answering and topic segmentation. Experimentally, we show that our pre-trained model DialogLM significantly surpasses the state-of-the-art models across datasets and tasks. Source code and all the pre-trained models are available on our GitHub repository (https://github.com/microsoft/DialogLM).",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2021,
    "referenceCount": 48,
    "citationCount": 113,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1606040932",
        "name": "Ming Zhong"
      },
      {
        "authorId": "39798499",
        "name": "Yang Liu"
      },
      {
        "authorId": "2110197273",
        "name": "Yichong Xu"
      },
      {
        "authorId": "1456009348",
        "name": "Chenguang Zhu"
      },
      {
        "authorId": "48262024",
        "name": "Michael Zeng"
      }
    ]
  },
  "222133028": {
    "paperId": "0ac7c7279f52e8cc98171254534276d9644cf92c",
    "externalIds": {
      "ACL": "2020.emnlp-main.336",
      "MAG": "3092133063",
      "DBLP": "journals/corr/abs-2010-01672",
      "ArXiv": "2010.01672",
      "DOI": "10.18653/v1/2020.emnlp-main.336",
      "CorpusId": 222133028
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization",
    "abstract": "Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations---an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers---remains relatively under-investigated. This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries. Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment. We also discussed specific challenges that current approaches faced with this task. We have publicly released our code at this https URL",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 63,
    "citationCount": 136,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47739850",
        "name": "Jiaao Chen"
      },
      {
        "authorId": "2022168",
        "name": "Diyi Yang"
      }
    ]
  },
  "243865654": {
    "paperId": "b519511993b63423be6e3580cf3ce63ea77e9e2f",
    "externalIds": {
      "DBLP": "conf/emnlp/ChenY21a",
      "ACL": "2021.emnlp-main.530",
      "DOI": "10.18653/v1/2021.emnlp-main.530",
      "CorpusId": 243865654
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Simple Conversational Data Augmentation for Semi-supervised Abstractive Dialogue Summarization",
    "abstract": "Abstractive conversation summarization has received growing attention while most current state-of-the-art summarization models heavily rely on human-annotated summaries. To reduce the dependence on labeled summaries, in this work, we present a simple yet effective set of Conversational Data Augmentation (CODA) methods for semi-supervised abstractive conversation summarization, such as random swapping/deletion to perturb the discourse relations inside conversations, dialogue-acts-guided insertion to interrupt the development of conversations, and conditional-generation-based substitution to substitute utterances with their paraphrases generated based on the conversation context. To further utilize unlabeled conversations, we combine CODA with two-stage noisy self-training where we first pre-train the summarization model on unlabeled conversations with pseudo summaries and then fine-tune it on labeled conversations. Experiments conducted on the recent conversation summarization datasets demonstrate the effectiveness of our methods over several state-of-the-art data augmentation baselines.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 61,
    "citationCount": 34,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47739850",
        "name": "Jiaao Chen"
      },
      {
        "authorId": "2143919864",
        "name": "Diyi Yang"
      }
    ]
  },
  "233219904": {
    "paperId": "aa28873534c24e4a8c5deb7bff723cd5fc69a6f0",
    "externalIds": {
      "ACL": "2021.naacl-main.472",
      "MAG": "3171639395",
      "ArXiv": "2104.05938",
      "DBLP": "journals/corr/abs-2104-05938",
      "DOI": "10.18653/V1/2021.NAACL-MAIN.472",
      "CorpusId": 233219904
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization",
    "abstract": "Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at https://github.com/Yale-LILY/QMSum.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 66,
    "citationCount": 278,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1606040932",
        "name": "Ming Zhong"
      },
      {
        "authorId": "144508458",
        "name": "Da Yin"
      },
      {
        "authorId": "48881008",
        "name": "Tao Yu"
      },
      {
        "authorId": "2056402211",
        "name": "A. Zaidi"
      },
      {
        "authorId": "2074120594",
        "name": "Mutethia Mutuma"
      },
      {
        "authorId": "144598922",
        "name": "Rahul Jha"
      },
      {
        "authorId": "2072795428",
        "name": "A. Awadallah"
      },
      {
        "authorId": "1709797",
        "name": "Asli Celikyilmaz"
      },
      {
        "authorId": "39798499",
        "name": "Yang Liu"
      },
      {
        "authorId": "1767521",
        "name": "Xipeng Qiu"
      },
      {
        "authorId": "9215251",
        "name": "Dragomir R. Radev"
      }
    ]
  },
  "235294200": {
    "paperId": "fa51076458b7bcf9a60f476d525755e47199a6d8",
    "externalIds": {
      "ACL": "2021.acl-long.535",
      "ArXiv": "2106.00829",
      "DBLP": "conf/acl/FabbriRRWLMR20",
      "DOI": "10.18653/v1/2021.acl-long.535",
      "CorpusId": 235294200
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining",
    "abstract": "While online conversations can cover a vast amount of information in many different formats, abstractive text summarization has primarily focused on modeling solely news articles. This research gap is due, in part, to the lack of standardized datasets for summarizing online discussions. To address this gap, we design annotation protocols motivated by an issues\u2013viewpoints\u2013assertions framework to crowdsource four new datasets on diverse online conversation forms of news comments, discussion forums, community question answering forums, and email threads. We benchmark state-of-the-art models on our datasets and analyze characteristics associated with the data. To create a comprehensive benchmark, we also evaluate these models on widely-used conversation summarization datasets to establish strong baselines in this domain. Furthermore, we incorporate argument mining through graph construction to directly model the issues, viewpoints, and assertions present in a conversation and filter noisy input, showing comparable or improved results according to automatic and human evaluations.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 79,
    "citationCount": 54,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46255971",
        "name": "Alexander R. Fabbri"
      },
      {
        "authorId": "151002933",
        "name": "Faiaz Rahman"
      },
      {
        "authorId": "2106627934",
        "name": "Imad Rizvi"
      },
      {
        "authorId": "2203187",
        "name": "Borui Wang"
      },
      {
        "authorId": "1391218521",
        "name": "Haoran Li"
      },
      {
        "authorId": "2263803",
        "name": "Yashar Mehdad"
      },
      {
        "authorId": "9215251",
        "name": "Dragomir R. Radev"
      }
    ]
  },
  "51859670": {
    "paperId": "4acd2c229ef9dfa3fa903911ed7447e62f726edc",
    "externalIds": {
      "MAG": "937307955",
      "ACL": "2010.jec-1.2",
      "CorpusId": 51859670
    },
    "publicationVenue": null,
    "title": "Shared Resources, Shared Values? Ethical Implications of Sharing Translation Resources",
    "abstract": "The exploitation of large corpora to create and populate shared translation resources has been hampered in two areas: first, practical problems (\u201clocked-in\u201d data, ineffective exchange formats, client reservations); and second, ethical and legal problems. Recent developments, notably online collaborative translation environments (Desillets, 2007) and greater industry openness, might have been expected to highlight such issues. Yet the growing use of shared data is being addressed only gingerly. Good reasons lie behind the failure to broach the ethics of shared resources. The issues are challenging: confidentiality, ownership, copyright, authorial rights, attribution, the law, protectionism, costs, fairness, motivation, trust, quality, reliability. However, we argue that, though complex, these issues should not be swept under the carpet. The huge demand for translation cannot be met without intelligent sharing of resources (Kelly, 2009). Relevant ethical considerations have already been identified in translation and related domains, in such texts as Codes of Ethics, international conventions and declarations, and Codes of Professional Conduct; these can be useful here. We outline two case studies from current industry initiatives, highlighting their ethical implications. We identify questions which users and developers should be asking and relate these to existing debates and codes as a practical framework for their consideration.",
    "venue": "JEC",
    "year": 2010,
    "referenceCount": 2,
    "citationCount": 15,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Political Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2438114",
        "name": "Bogdan Babych"
      }
    ]
  },
  "37634159": {
    "paperId": "c82848399fa374093afbdf4184310ec2c72ebeda",
    "externalIds": {
      "MAG": "605075121",
      "ACL": "L14-1363",
      "DBLP": "conf/lrec/CouillaultFA14",
      "CorpusId": 37634159
    },
    "publicationVenue": {
      "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
      "name": "International Conference on Language Resources and Evaluation",
      "type": "conference",
      "alternate_names": [
        "LREC",
        "Int Conf Lang Resour Evaluation"
      ],
      "url": "http://www.lrec-conf.org/"
    },
    "title": "Evaluating corpora documentation with regards to the Ethics and Big Data Charter",
    "abstract": "The authors have written the Ethic and Big Data Charter in collaboration with various agencies, private bodies and associations. This Charter aims at describing any large or complex resources, and in particular language resources, from a legal and ethical viewpoint and ensuring the transparency of the process of creating and distributing such resources. We propose in this article an analysis of the documentation coverage of the most frequently mentioned language resources with regards to the Charter, in order to show the benefit it offers.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2014,
    "referenceCount": 14,
    "citationCount": 14,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Law",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2078469025",
        "name": "Alain Couillault"
      },
      {
        "authorId": "3196675",
        "name": "Kar\u00ebn Fort"
      },
      {
        "authorId": "1707447",
        "name": "G. Adda"
      },
      {
        "authorId": "2864622",
        "name": "Hugues de Mazancourt"
      }
    ]
  },
  "16399410": {
    "paperId": "a95f16aab62250fba20bf222349be29c5838f143",
    "externalIds": {
      "MAG": "2740834240",
      "ACL": "W17-1603",
      "DBLP": "conf/ethnlp/Mieskes17",
      "DOI": "10.18653/v1/W17-1603",
      "CorpusId": 16399410
    },
    "publicationVenue": null,
    "title": "A Quantitative Study of Data in the NLP community",
    "abstract": "We present results on a quantitative analysis of publications in the NLP domain on collecting, publishing and availability of research data. We find that a wide range of publications rely on data crawled from the web, but few give details on how potentially sensitive data was treated. Additionally, we find that while links to repositories of data are given, they often do not work even a short time after publication. We put together several suggestions on how to improve this situation based on publications from the NLP domain, but also other research areas.",
    "venue": "EthNLP@EACL",
    "year": 2017,
    "referenceCount": 27,
    "citationCount": 30,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2921990",
        "name": "Margot Mieskes"
      }
    ]
  },
  "52255687": {
    "paperId": "97bfa89addc6e5d76361e4c1e296949cad887b86",
    "externalIds": {
      "ACL": "Q18-1041",
      "DBLP": "journals/tacl/BenderF18",
      "MAG": "2911227954",
      "DOI": "10.1162/tacl_a_00041",
      "CorpusId": 52255687
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    "abstract": "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 64,
    "citationCount": 781,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2471699",
        "name": "Emily M. Bender"
      },
      {
        "authorId": "144029598",
        "name": "Batya Friedman"
      }
    ]
  },
  "202539668": {
    "paperId": "ab70d071223476265d7b077f290c6133a96ef677",
    "externalIds": {
      "ACL": "D19-1329",
      "DBLP": "conf/emnlp/KannCB19",
      "MAG": "2971418718",
      "ArXiv": "1909.01522",
      "DOI": "10.18653/v1/D19-1329",
      "CorpusId": 202539668
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Towards Realistic Practices In Low-Resource Natural Language Processing: The Development Set",
    "abstract": "Development sets are impractical to obtain for real low-resource languages, since using all available data for training is often more effective. However, development sets are widely used in research papers that purport to deal with low-resource natural language processing (NLP). Here, we aim to answer the following questions: Does using a development set for early stopping in the low-resource setting influence results as compared to a more realistic alternative, where the number of training epochs is tuned on development languages? And does it lead to overestimation or underestimation of performance? We repeat multiple experiments from recent work on neural models for low-resource NLP and compare results for models obtained by training with and without development sets. On average over languages, absolute accuracy differs by up to 1.4%. However, for some languages and tasks, differences are as big as 18.0% accuracy. Our results highlight the importance of realistic experimental setups in the publication of low-resource NLP research results.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 38,
    "citationCount": 43,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3422953",
        "name": "Katharina Kann"
      },
      {
        "authorId": "1979489",
        "name": "Kyunghyun Cho"
      },
      {
        "authorId": "3644767",
        "name": "Samuel R. Bowman"
      }
    ]
  },
  "237504133": {
    "paperId": "48709b65320bf979d19c1874aebf6bf8bd85de88",
    "externalIds": {
      "ArXiv": "2109.06598",
      "DBLP": "journals/corr/abs-2109-06598",
      "DOI": "10.18653/v1/2021.findings-emnlp.414",
      "CorpusId": 237504133
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Just What do You Think You're Doing, Dave?' A Checklist for Responsible Data Use in NLP",
    "abstract": "A key part of the NLP ethics movement is responsible use of data, but exactly what that means or how it can be best achieved remain unclear. This position paper discusses the core legal and ethical principles for collection and sharing of textual data, and the tensions between them. We propose a potential checklist for responsible data (re-)use that could both standardise the peer review of conference submissions, as well as enable a more in-depth view of published research across the community. Our proposal aims to contribute to the development of a consistent standard for data (re-)use, embraced across NLP conferences.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 78,
    "citationCount": 60,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Law",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145046059",
        "name": "Anna Rogers"
      },
      {
        "authorId": "145465286",
        "name": "Timothy Baldwin"
      },
      {
        "authorId": "10836604",
        "name": "Kobi Leins"
      }
    ]
  },
  "4421027": {
    "paperId": "0df347f5e3118fac7c351917e3a497899b071d1e",
    "externalIds": {
      "DBLP": "journals/cacm/GebruMVVWDC21",
      "ArXiv": "1803.09010",
      "MAG": "2795038878",
      "DOI": "10.1145/3458723",
      "CorpusId": 4421027
    },
    "publicationVenue": {
      "id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
      "name": "Communications of the ACM",
      "type": "journal",
      "alternate_names": [
        "Commun ACM",
        "Communications of The ACM"
      ],
      "issn": "0001-0782",
      "url": "http://www.acm.org/pubs/cacm/",
      "alternate_urls": [
        "http://portal.acm.org/cacm",
        "http://www.acm.org/pubs/contents/journals/cacm/",
        "https://cacm.acm.org/"
      ]
    },
    "title": "Datasheets for datasets",
    "abstract": "Documentation to facilitate communication between dataset creators and consumers.",
    "venue": "Communications of the ACM",
    "year": 2018,
    "referenceCount": 58,
    "citationCount": 1890,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2076288",
        "name": "Timnit Gebru"
      },
      {
        "authorId": "144848816",
        "name": "Jamie Morgenstern"
      },
      {
        "authorId": "40808865",
        "name": "Briana Vecchione"
      },
      {
        "authorId": "4006636",
        "name": "Jennifer Wortman Vaughan"
      },
      {
        "authorId": "1831395",
        "name": "Hanna M. Wallach"
      },
      {
        "authorId": "1722360",
        "name": "Hal Daum\u00e9"
      },
      {
        "authorId": "1810680217",
        "name": "Kate Crawford"
      }
    ]
  },
  "7532652": {
    "paperId": "a6ecf4481a3af03dbf7c676f5ebc79f7562c4d4c",
    "externalIds": {
      "DBLP": "conf/chi/BedersonQ11",
      "MAG": "2133685705",
      "DOI": "10.1145/1979742.1979606",
      "CorpusId": 7532652
    },
    "publicationVenue": null,
    "title": "Web workers unite! addressing challenges of online laborers",
    "abstract": "The ongoing rise of human computation as a means of solving computational problems has created an environment where human workers are often regarded as nameless, faceless computational resources. Some people have begun to think of online tasks as a \"remote person call\". In this paper, we summarize ethical and practical labor issues surrounding online labor, and offer a set of guidelines for designing and using online labor in ways that support more positive relationships between workers and requesters, so that both can gain the most benefit from the interaction.",
    "venue": "CHI Extended Abstracts",
    "year": 2011,
    "referenceCount": 22,
    "citationCount": 114,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1799187",
        "name": "B. Bederson"
      },
      {
        "authorId": "1847884",
        "name": "Alexander J. Quinn"
      }
    ]
  },
  "1051130": {
    "paperId": "afaae4ddf8bd819eef26a50451d67e2a51c692e2",
    "externalIds": {
      "ACL": "J11-2010",
      "DOI": "10.1162/COLI_a_00057",
      "CorpusId": 1051130
    },
    "publicationVenue": {
      "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
      "name": "International Conference on Computational Logic",
      "type": "conference",
      "alternate_names": [
        "CL",
        "Int Conf Comput Log"
      ]
    },
    "title": "Last Words: Amazon Mechanical Turk: Gold Mine or Coal Mine?",
    "abstract": null,
    "venue": "International Conference on Computational Logic",
    "year": null,
    "referenceCount": 0,
    "citationCount": 372,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3196675",
        "name": "Kar\u00ebn Fort"
      },
      {
        "authorId": "1707447",
        "name": "G. Adda"
      },
      {
        "authorId": "145468230",
        "name": "K. Cohen"
      }
    ]
  },
  "6172923": {
    "paperId": "b938313d32a2d72bc370d5ba12532e641629aff1",
    "externalIds": {
      "MAG": "1681309541",
      "DBLP": "conf/hcomp/Callison-Burch14",
      "DOI": "10.1609/hcomp.v2i1.13198",
      "CorpusId": 6172923
    },
    "publicationVenue": {
      "id": "b01f161c-f4c9-46eb-9461-a6c5fa8fd002",
      "name": "AAAI Conference on Human Computation & Crowdsourcing",
      "type": "conference",
      "alternate_names": [
        "HCOMP",
        "AAAI Conf Hum Comput  Crowdsourcing"
      ]
    },
    "title": "Crowd-Workers: Aggregating Information Across Turkers to Help Them Find Higher Paying Work",
    "abstract": "\n \n The Mechanical Turk crowdsourcing platform currently fails to provide the most basic piece of information to enable workers to make informed decisions about which tasks to undertake: what is the expected hourly pay? \u00a0Mechanical Turk advertises a reward amount per assignment, but does not give any indication of how long each assignment will take. \u00a0We have developed a browser plugin that tracks the length of time it takes to complete a task, and a web service that aggregates the information across many workers. Our web service, crowd-workers.com, allows workers to discovery higher paying work by sorting tasks by estimated hourly rate.\n \n",
    "venue": "AAAI Conference on Human Computation & Crowdsourcing",
    "year": 2014,
    "referenceCount": 9,
    "citationCount": 43,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Economics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1763608",
        "name": "Chris Callison-Burch"
      }
    ]
  },
  "46159245": {
    "paperId": "64b437ac0a8308a12377dfbc4df7613392d1c571",
    "externalIds": {
      "MAG": "978146473",
      "DBLP": "conf/ltconf/FortASMC11",
      "DOI": "10.1007/978-3-319-08958-4_25",
      "CorpusId": 46159245
    },
    "publicationVenue": {
      "id": "63175cb5-3882-47ea-9b9f-81f3270bf97e",
      "name": "Language and Technology Conference",
      "type": "conference",
      "alternate_names": [
        "LTC",
        "Lang Technol Conf"
      ]
    },
    "title": "Crowdsourcing for Language Resource Development: Criticisms About Amazon Mechanical Turk Overpowering Use",
    "abstract": null,
    "venue": "Language and Technology Conference",
    "year": 2011,
    "referenceCount": 45,
    "citationCount": 26,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3196675",
        "name": "Kar\u00ebn Fort"
      },
      {
        "authorId": "1707447",
        "name": "G. Adda"
      },
      {
        "authorId": "68990982",
        "name": "Beno\u00eet Sagot"
      },
      {
        "authorId": "8201467",
        "name": "J. Mariani"
      },
      {
        "authorId": "2078469025",
        "name": "Alain Couillault"
      }
    ]
  },
  "5040507": {
    "paperId": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b",
    "externalIds": {
      "DBLP": "conf/chi/HaraAMSCB18",
      "MAG": "2952172018",
      "ArXiv": "1712.05796",
      "DOI": "10.1145/3173574.3174023",
      "CorpusId": 5040507
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk",
    "abstract": "A growing number of people are working as part of on-line crowd work. Crowd work is often thought to be low wage work. However, we know little about the wage distribution in practice and what causes low/high earnings in this setting. We recorded 2,676 workers performing 3.8 million tasks on Amazon Mechanical Turk. Our task-level analysis revealed that workers earned a median hourly wage of only ~$2/h, and only 4% earned more than $7.25/h. While the average requester pays more than $11/h, lower-paying requesters post much more work. Our wage calculations are influenced by how unpaid work is accounted for, e.g., time spent searching for tasks, working on tasks that are rejected, and working on tasks that are ultimately not submitted. We further explore the characteristics of tasks and working patterns that yield higher hourly wages. Our analysis informs platform design and worker tools to create a more positive future for crowd work.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2017,
    "referenceCount": 72,
    "citationCount": 409,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Economics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Economics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "39496240",
        "name": "Kotaro Hara"
      },
      {
        "authorId": "120519667",
        "name": "Abigail Adams"
      },
      {
        "authorId": "1844994",
        "name": "Kristy Milland"
      },
      {
        "authorId": "1983163",
        "name": "Saiph Savage"
      },
      {
        "authorId": "1763608",
        "name": "Chris Callison-Burch"
      },
      {
        "authorId": "1744846",
        "name": "Jeffrey P. Bigham"
      }
    ]
  },
  "237362773": {
    "paperId": "fb874843e39f3a46ff70f9c06ad2a8ff1b5ae12e",
    "externalIds": {
      "ArXiv": "2110.00169",
      "DBLP": "journals/corr/abs-2110-00169",
      "DOI": "10.1145/3476060",
      "CorpusId": 237362773
    },
    "publicationVenue": null,
    "title": "Quantifying the Invisible Labor in Crowd Work",
    "abstract": "Crowdsourcing markets provide workers with a centralized place to find paid work. What may not be obvious at first glance is that, in addition to the work they do for pay, crowd workers also have to shoulder a variety of unpaid invisible labor in these markets, which ultimately reduces workers' hourly wages. Invisible labor includes finding good tasks, messaging requesters, or managing payments. However, we currently know little about how much time crowd workers actually spend on invisible labor or how much it costs them economically. To ensure a fair and equitable future for crowd work, we need to be certain that workers are being paid fairly for all of the work they do. In this paper, we conduct a field study to quantify the invisible labor in crowd work. We build a plugin to record the amount of time that 100 workers on Amazon Mechanical Turk dedicate to invisible labor while completing 40,903 tasks. If we ignore the time workers spent on invisible labor, workers' median hourly wage was $3.76. But, we estimated that crowd workers in our study spent 33% of their time daily on invisible labor, dropping their median hourly wage to $2.83. We found that the invisible labor differentially impacts workers depending on their skill level and workers' demographics. The invisible labor category that took the most time and that was also the most common revolved around workers having to manage their payments. The second most time-consuming invisible labor category involved hyper-vigilance, where workers vigilantly watched over requesters' profiles for newly posted work or vigilantly searched for labor. We hope that through our paper, the invisible labor in crowdsourcing becomes more visible, and our results help to reveal the larger implications of the continuing invisibility of labor in crowdsourcing.",
    "venue": "Proc. ACM Hum. Comput. Interact.",
    "year": 2021,
    "referenceCount": 118,
    "citationCount": 70,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Economics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2602469",
        "name": "Carlos Toxtli"
      },
      {
        "authorId": "38730378",
        "name": "Siddharth Suri"
      },
      {
        "authorId": "1983163",
        "name": "Saiph Savage"
      }
    ]
  },
  "218971825": {
    "paperId": "d47a682723f710395454687319bb55635e653105",
    "externalIds": {
      "DBLP": "journals/corr/abs-2005-14050",
      "MAG": "3032388710",
      "ArXiv": "2005.14050",
      "ACL": "2020.acl-main.485",
      "DOI": "10.18653/v1/2020.acl-main.485",
      "CorpusId": 218971825
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP",
    "abstract": "We survey 146 papers analyzing \u201cbias\u201d in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 253,
    "citationCount": 1049,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3422038",
        "name": "Su Lin Blodgett"
      },
      {
        "authorId": "2881033",
        "name": "Solon Barocas"
      },
      {
        "authorId": "2065041692",
        "name": "Hal Daum'e"
      },
      {
        "authorId": "1831395",
        "name": "Hanna M. Wallach"
      }
    ]
  },
  "13997424": {
    "paperId": "901335712430a194b6e15d817685e5ecc72a15c1",
    "externalIds": {
      "DBLP": "conf/ethnlp/Tatman17",
      "MAG": "2607719644",
      "ACL": "W17-1606",
      "DOI": "10.18653/v1/W17-1606",
      "CorpusId": 13997424
    },
    "publicationVenue": null,
    "title": "Gender and Dialect Bias in YouTube\u2019s Automatic Captions",
    "abstract": "This project evaluates the accuracy of YouTube\u2019s automatically-generated captions across two genders and five dialect groups. Speakers\u2019 dialect and gender was controlled for by using videos uploaded as part of the \u201caccent tag challenge\u201d, where speakers explicitly identify their language background. The results show robust differences in accuracy across both gender and dialect, with lower accuracy for 1) women and 2) speakers from Scotland. This finding builds on earlier research finding that speaker\u2019s sociolinguistic identity may negatively impact their ability to use automatic speech recognition, and demonstrates the need for sociolinguistically-stratified validation of systems.",
    "venue": "EthNLP@EACL",
    "year": 2017,
    "referenceCount": 46,
    "citationCount": 311,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3421442",
        "name": "Rachael Tatman"
      }
    ]
  },
  "29385817": {
    "paperId": "d8a6999ab3b0395477e34e7e2fdb10e122e1cc56",
    "externalIds": {
      "ACL": "P17-2009",
      "DBLP": "conf/acl/JurgensTJ17",
      "MAG": "2741636242",
      "DOI": "10.18653/v1/P17-2009",
      "CorpusId": 29385817
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Incorporating Dialectal Variability for Socially Equitable Language Identification",
    "abstract": "Language identification (LID) is a critical first step for processing multilingual text. Yet most LID systems are not designed to handle the linguistic diversity of global platforms like Twitter, where local dialects and rampant code-switching lead language classifiers to systematically miss minority dialect speakers and multilingual speakers. We propose a new dataset and a character-based sequence-to-sequence model for LID designed to support dialectal and multilingual language varieties. Our model achieves state-of-the-art performance on multiple LID benchmarks. Furthermore, in a case study using Twitter for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of \u201csocially inclusive\u201d NLP tools.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 49,
    "citationCount": 95,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3046220",
        "name": "David Jurgens"
      },
      {
        "authorId": "145317727",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      }
    ]
  },
  "184486914": {
    "paperId": "835ac3cbb41f2ec47718c5491211dd33b64f382b",
    "externalIds": {
      "DBLP": "conf/acl/ZmigrodMWC19",
      "MAG": "2949682057",
      "ArXiv": "1906.04571",
      "ACL": "P19-1161",
      "DOI": "10.18653/v1/P19-1161",
      "CorpusId": 184486914
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology",
    "abstract": "Gender stereotypes are manifest in most of the world\u2019s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 38,
    "citationCount": 257,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51044403",
        "name": "Ran Zmigrod"
      },
      {
        "authorId": "27689253",
        "name": "Sabrina J. Mielke"
      },
      {
        "authorId": "1831395",
        "name": "Hanna M. Wallach"
      },
      {
        "authorId": "1750769",
        "name": "Ryan Cotterell"
      }
    ]
  },
  "216868277": {
    "paperId": "3dbc85c3af12736a4c7f89a2204370b061e1d192",
    "externalIds": {
      "ArXiv": "2004.14870",
      "DBLP": "conf/emnlp/TanJVK20",
      "MAG": "3022031143",
      "ACL": "2020.emnlp-main.455",
      "DOI": "10.18653/v1/2020.emnlp-main.455",
      "CorpusId": 216868277
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding",
    "abstract": "Morphological inflection is a process of word formation where base words are modified to express different grammatical categories such as tense, case, voice, person, or number. World Englishes, such as Colloquial Singapore English (CSE) and African American Vernacular English (AAVE), differ from Standard English dialects in inflection use. Although comprehension by human readers is usually unimpaired by non-standard inflection use, NLP systems are not so robust. We introduce a new Base-Inflection Encoding of English text that is achieved by combining linguistic and statistical techniques. Fine-tuning pre-trained NLP models for downstream tasks under this novel encoding achieves robustness to non-standard inflection use while maintaining performance on Standard English examples. Models using this encoding also generalize better to non-standard dialects without explicit training. We suggest metrics to evaluate tokenizers and extensive model-independent analyses demonstrate the efficacy of the encoding when used together with data-driven subword tokenizers.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 73,
    "citationCount": 32,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145814654",
        "name": "Samson Tan"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      },
      {
        "authorId": "1697944",
        "name": "L. Varshney"
      },
      {
        "authorId": "37596605",
        "name": "Min-Yen Kan"
      }
    ]
  },
  "214630656": {
    "paperId": "219b7266ae848937da170c5510b2bfc66d17859a",
    "externalIds": {
      "DBLP": "journals/pnas/KoeneckeNLNQMTR20",
      "PubMedCentral": "7149386",
      "MAG": "3012624518",
      "DOI": "10.1073/pnas.1915768117",
      "CorpusId": 214630656,
      "PubMed": "32205437"
    },
    "publicationVenue": {
      "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
      "name": "Proceedings of the National Academy of Sciences of the United States of America",
      "type": "journal",
      "alternate_names": [
        "PNAS",
        "PNAS online",
        "Proceedings of the National Academy of Sciences of the United States of America.",
        "Proc National Acad Sci",
        "Proceedings of the National Academy of Sciences",
        "Proc National Acad Sci u s Am"
      ],
      "issn": "0027-8424",
      "alternate_issns": [
        "1091-6490"
      ],
      "url": "https://www.jstor.org/journal/procnatiacadscie",
      "alternate_urls": [
        "http://www.pnas.org/",
        "https://www.pnas.org/",
        "http://www.jstor.org/journals/00278424.html",
        "www.pnas.org/"
      ]
    },
    "title": "Racial disparities in automated speech recognition",
    "abstract": "Significance Automated speech recognition (ASR) systems are now used in a variety of applications to convert spoken language to text, from virtual assistants, to closed captioning, to hands-free computing. By analyzing a large corpus of sociolinguistic interviews with white and African American speakers, we demonstrate large racial disparities in the performance of five popular commercial ASR systems. Our results point to hurdles faced by African Americans in using increasingly widespread tools driven by speech recognition technology. More generally, our work illustrates the need to audit emerging machine-learning systems to ensure they are broadly inclusive. Automated speech recognition (ASR) systems, which use sophisticated machine-learning algorithms to convert spoken language to text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. Over the last several years, the quality of these systems has dramatically improved, due both to advances in deep learning and to the collection of large-scale datasets used to train the systems. There is concern, however, that these tools do not work equally well for all subgroups of the population. Here, we examine the ability of five state-of-the-art ASR systems\u2014developed by Amazon, Apple, Google, IBM, and Microsoft\u2014to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. In total, this corpus spans five US cities and consists of 19.8 h of audio matched on the age and gender of the speaker. We found that all five ASR systems exhibited substantial racial disparities, with an average word error rate (WER) of 0.35 for black speakers compared with 0.19 for white speakers. We trace these disparities to the underlying acoustic models used by the ASR systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus. We conclude by proposing strategies\u2014such as using more diverse training datasets that include African American Vernacular English\u2014to reduce these performance differences and ensure speech recognition technology is inclusive.",
    "venue": "Proceedings of the National Academy of Sciences of the United States of America",
    "year": 2020,
    "referenceCount": 39,
    "citationCount": 484,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "101883616",
        "name": "Allison Koenecke"
      },
      {
        "authorId": "41127369",
        "name": "A. Nam"
      },
      {
        "authorId": "145729503",
        "name": "Emily Lake"
      },
      {
        "authorId": "1589493589",
        "name": "Joe Nudell"
      },
      {
        "authorId": "113762868",
        "name": "Minnie Quartey"
      },
      {
        "authorId": "102794367",
        "name": "Zion Mengesha"
      },
      {
        "authorId": "1589437057",
        "name": "Connor Toups"
      },
      {
        "authorId": "2064764701",
        "name": "J. Rickford"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      },
      {
        "authorId": "143802734",
        "name": "Sharad Goel"
      }
    ]
  },
  "227231541": {
    "paperId": "763a2c057f529bfd3bf417fa60d93b3d5b219068",
    "externalIds": {
      "ACL": "2020.coling-main.313",
      "MAG": "3112849432",
      "DBLP": "conf/coling/Bird20",
      "DOI": "10.18653/V1/2020.COLING-MAIN.313",
      "CorpusId": 227231541
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Decolonising Speech and Language Technology",
    "abstract": "After generations of exploitation, Indigenous people often respond negatively to the idea that their languages are data ready for the taking. By treating Indigenous knowledge as a commodity, speech and language technologists risk disenfranchising local knowledge authorities, reenacting the causes of language endangerment. Scholars in related fields have responded to calls for decolonisation, and we in the speech and language technology community need to follow suit, and explore what this means for our practices that involve Indigenous languages and the communities who own them. This paper reviews colonising discourses in speech and language technology, and suggests new ways of working with Indigenous communities, and seeks to open a discussion of a postcolonial approach to computational methods for supporting language vitality.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2020,
    "referenceCount": 128,
    "citationCount": 139,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2066194290",
        "name": "Steven Bird"
      }
    ]
  },
  "225068033": {
    "paperId": "2e4ca3d95ffb83870661dd66deee143e782f0706",
    "externalIds": {
      "ArXiv": "2010.13588",
      "DBLP": "conf/coling/CaglayanMS20",
      "ACL": "2020.coling-main.210",
      "MAG": "3117667887",
      "DOI": "10.18653/V1/2020.COLING-MAIN.210",
      "CorpusId": 225068033
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale",
    "abstract": "Automatic evaluation of language generation systems is a well-studied problem in Natural Language Processing. While novel metrics are proposed every year, a few popular metrics remain as the de facto metrics to evaluate tasks such as image captioning and machine translation, despite their known limitations. This is partly due to ease of use, and partly because researchers expect to see them and know how to interpret them. In this paper, we urge the community for more careful consideration of how they automatically evaluate their models by demonstrating important failure cases on multiple datasets, language pairs and tasks. Our experiments show that metrics (i) usually prefer system outputs to human-authored texts, (ii) can be insensitive to correct translations of rare words, (iii) can yield surprisingly high scores when given a single sentence as system output for the entire test set.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2020,
    "referenceCount": 27,
    "citationCount": 33,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "10791325",
        "name": "Ozan Caglayan"
      },
      {
        "authorId": "3238408",
        "name": "P. Madhyastha"
      },
      {
        "authorId": "1702974",
        "name": "Lucia Specia"
      }
    ]
  },
  "235408131": {
    "paperId": "c00e103a1c2be6c396d88b59a367df05bff1248d",
    "externalIds": {
      "MAG": "3089525430",
      "ArXiv": "2009.13888",
      "ACL": "2020.emnlp-main.393",
      "DBLP": "conf/emnlp/EthayarajhJ20",
      "DOI": "10.18653/v1/2020.emnlp-main.393",
      "CorpusId": 235408131
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Utility Is in the Eye of the User: A Critique of NLP Leaderboard Design",
    "abstract": "Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards -- in their current form -- can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model's utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 56,
    "citationCount": 48,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Economics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "10324691",
        "name": "Kawin Ethayarajh"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      }
    ]
  },
  "233864598": {
    "paperId": "0f71a4fa9736ae916e6aef53045f6be4c901b0ff",
    "externalIds": {
      "DBLP": "journals/corr/abs-2105-02590",
      "ACL": "2021.acl-long.321",
      "ArXiv": "2105.02590",
      "DOI": "10.18653/v1/2021.acl-long.321",
      "CorpusId": 233864598
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Reliability Testing for Natural Language Processing Systems",
    "abstract": "Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing \u2014 with an emphasis on interdisciplinary collaboration \u2014 will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 129,
    "citationCount": 33,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145814654",
        "name": "Samson Tan"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      },
      {
        "authorId": "48162805",
        "name": "K. Baxter"
      },
      {
        "authorId": "3299973",
        "name": "Araz Taeihagh"
      },
      {
        "authorId": "32139163",
        "name": "G. Bennett"
      },
      {
        "authorId": "37596605",
        "name": "Min-Yen Kan"
      }
    ]
  },
  "174802812": {
    "paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea",
    "externalIds": {
      "ArXiv": "1906.02243",
      "ACL": "P19-1355",
      "MAG": "2963809228",
      "DBLP": "journals/corr/abs-1906-02243",
      "DOI": "10.18653/v1/P19-1355",
      "CorpusId": 174802812
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Energy and Policy Considerations for Deep Learning in NLP",
    "abstract": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 19,
    "citationCount": 2386,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Environmental Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Economics",
        "source": "s2-fos-model"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Engineering",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2268272",
        "name": "Emma Strubell"
      },
      {
        "authorId": "47079359",
        "name": "Ananya Ganesh"
      },
      {
        "authorId": "143753639",
        "name": "A. McCallum"
      }
    ]
  },
  "204823753": {
    "paperId": "2c24da194230a6ed0320d2107a01e3b82d9d7ed4",
    "externalIds": {
      "MAG": "2981494037",
      "DBLP": "journals/corr/abs-1910-10143",
      "ArXiv": "1910.10143",
      "DOI": "10.1088/2632-2153/ab7657",
      "CorpusId": 204823753
    },
    "publicationVenue": null,
    "title": "Establishing an evaluation metric to quantify climate change image realism",
    "abstract": "With success on controlled tasks, deep generative models are being increasingly applied to humanitarian applications (Nie et al 2017 Int. Conf. on Medical Image Computing and Computer-Assisted Intervention (Berlin: Springer) pp 417\u201325, Yanardag et al 2017 Deep Empathy). In this paper, we focus on the evaluation of a conditional generative model that illustrates the consequences of climate change-induced flooding to encourage public interest and awareness on the issue. Because metrics for comparing the realism of different modes in a conditional generative model do not exist, we propose several automated and human-based methods for evaluation. To do this, we adapt several existing metrics and assess the automated metrics against gold standard human evaluation. We find that using Fr\u00e9chet Inception Distance with embeddings from an intermediary Inception-v3 layer that precedes the auxiliary classifier produces results most correlated with human realism. While insufficient alone to establish a human-correlated automatic evaluation metric, we believe this work begins to bridge the gap between human and automated generative evaluation procedures, and to generate more realistic images of the future consequences of climate change.",
    "venue": "Machine Learning: Science and Technology",
    "year": 2019,
    "referenceCount": 31,
    "citationCount": 6,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Environmental Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2111056982",
        "name": "Sharon Zhou"
      },
      {
        "authorId": "2993731",
        "name": "A. Luccioni"
      },
      {
        "authorId": "1379506572",
        "name": "Gautier Cosne"
      },
      {
        "authorId": "145879842",
        "name": "Michael S. Bernstein"
      },
      {
        "authorId": "1751762",
        "name": "Yoshua Bengio"
      }
    ]
  },
  "211096620": {
    "paperId": "74b4f16c5ac91e3e7c88ae81cc8c91416b71d151",
    "externalIds": {
      "DBLP": "journals/corr/abs-2002-05651",
      "ArXiv": "2002.05651",
      "MAG": "3005957694",
      "CorpusId": 211096620
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning",
    "abstract": "Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 122,
    "citationCount": 378,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Environmental Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40068904",
        "name": "Peter Henderson"
      },
      {
        "authorId": "2143911040",
        "name": "Jie Hu"
      },
      {
        "authorId": "8365320",
        "name": "Joshua Romoff"
      },
      {
        "authorId": "2563117",
        "name": "E. Brunskill"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      },
      {
        "authorId": "145134886",
        "name": "Joelle Pineau"
      }
    ]
  },
  "198229505": {
    "paperId": "3c5f1ab37f70db503636075e15b3173f86eea00b",
    "externalIds": {
      "ArXiv": "1907.10597",
      "CorpusId": 198229505
    },
    "publicationVenue": null,
    "title": "Green AI",
    "abstract": "The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or\"price tag\"of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.",
    "venue": "",
    "year": 2019,
    "referenceCount": 48,
    "citationCount": 973,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Environmental Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "4671928",
        "name": "Roy Schwartz"
      },
      {
        "authorId": "34176020",
        "name": "Jesse Dodge"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      },
      {
        "authorId": "1741101",
        "name": "Oren Etzioni"
      }
    ]
  },
  "241583270": {
    "paperId": "1e7d821219bb4955e8609edaa46b4e20db6745ed",
    "externalIds": {
      "DBLP": "conf/emnlp/BannourGNL21",
      "ACL": "2021.sustainlp-1.2",
      "DOI": "10.18653/v1/2021.sustainlp-1.2",
      "CorpusId": 241583270
    },
    "publicationVenue": null,
    "title": "Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools",
    "abstract": "Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications. Due to the significant environmental impact of deep learning, cost-benefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment. In this paper, we review the tools that are available to measure energy use and CO2 emissions of NLP methods. We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational set-ups (local server vs. computing facility). Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments.",
    "venue": "SUSTAINLP",
    "year": 2021,
    "referenceCount": 53,
    "citationCount": 58,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Environmental Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "65849212",
        "name": "N. Bannour"
      },
      {
        "authorId": "3241499",
        "name": "Sahar Ghannay"
      },
      {
        "authorId": "1692256",
        "name": "Aur\u00e9lie N\u00e9v\u00e9ol"
      },
      {
        "authorId": "1769176",
        "name": "Anne-Laure Ligozat"
      }
    ]
  },
  "248780165": {
    "paperId": "d157131c50263a99d8722bec8ede8f67971041c6",
    "externalIds": {
      "DBLP": "conf/acl/PrzybylaS22",
      "ACL": "2022.findings-acl.304",
      "DOI": "10.18653/v1/2022.findings-acl.304",
      "CorpusId": 248780165
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Using NLP to quantify the environmental cost and diversity benefits of in-person NLP conferences",
    "abstract": "The environmental costs of research are progressively important to the NLP community and their associated challenges are increasingly debated. In this work, we analyse the carbon cost (measured as CO2-equivalent) associated with journeys made by researchers attending in-person NLP conferences. We obtain the necessary data by text-mining all publications from the ACL anthology available at the time of the study (n=60,572) and extracting information about an author\u2019s affiliation, including their address. This allows us to estimate the corresponding carbon cost and compare it to previously known values for training large models. Further, we look at the benefits of in-person conferences by demonstrating that they can increase participation diversity by encouraging attendance from the region surrounding the host country. We show how the trade-off between carbon cost and diversity of an event depends on its location and type. Our aim is to foster further discussion on the best way to address the joint issue of emissions and diversity in the future.",
    "venue": "Findings",
    "year": 2022,
    "referenceCount": 45,
    "citationCount": 5,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Environmental Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1984182",
        "name": "Piotr Przyby\u0142a"
      },
      {
        "authorId": "2895959",
        "name": "M. Shardlow"
      }
    ]
  },
  "15660866": {
    "paperId": "6d7edb69dd35d75d4e3797fbd36f78f56626c85f",
    "externalIds": {
      "MAG": "1498112155",
      "DOI": "10.4324/9781315259697-21",
      "CorpusId": 15660866
    },
    "publicationVenue": null,
    "title": "Do Artifacts Have Politics?",
    "abstract": "In controversies about technology and society, there is no idea more pro vocative than the notion that technical things have political qualities. At issue is the claim that the machines, structures, and systems of modern material culture can be accurately judged not only for their contributions of efficiency and pro ductivity, not merely for their positive and negative environmental side effects, but also for the ways in which they can embody specific forms of power and authority. Since ideas of this kind have a persistent and troubling presence in discussions about the meaning of technology, they deserve explicit attention.1 Writing in Technology and Culture almost two decades ago, Lewis Mumford gave classic statement to one version of the theme, arguing that \"from late neo lithic times in the Near East, right down to our own day, two technologies have recurrently existed side by side: one authoritarian, the other democratic, the first system-centered, immensely powerful, but inherently unstable, the other man-centered, relatively weak, but resourceful and durable.\"2 This thesis stands at the heart of Mumford's studies of the city, architecture, and the his tory of technics, and mirrors concerns voiced earlier in the works of Peter Kropotkin, William Morris, and other nineteenth century critics of industrial ism. More recently, antinuclear and prosolar energy movements in Europe and America have adopted a similar notion as a centerpiece in their arguments. Thus environmentalist Denis Hayes concludes, \"The increased deployment of nuclear power facilities must lead society toward authoritarianism. Indeed, safe reliance upon nuclear power as the principal source of energy may be possible only in a totalitarian state.\" Echoing the views of many proponents of appropri ate technology and the soft energy path, Hayes contends that \"dispersed solar sources are more compatible than centralized technologies with social equity, freedom and cultural pluralism.\"3 An eagerness to interpret technical artifacts in political language is by no means the exclusive property of critics of large-scale high-technology systems. A long lineage of boosters have insisted that the \"biggest and best\" that science and industry made available were the best guarantees of democracy, freedom, and social justice. The factory system, automobile, telephone, radio, television, the space program, and of course nuclear power itself have all at one time or another been described as democratizing, liberating forces. David Lilienthal, in T.V.A.: Democracy on the March, for example, found this promise in the phos 121",
    "venue": "Emerging Technologies: Ethics, Law and Governance",
    "year": 2017,
    "referenceCount": 23,
    "citationCount": 1526,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Political Science",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8074315",
        "name": "L. Winner"
      }
    ]
  },
  "1083991": {
    "paperId": "6a0388c46f2aff013343fdafaaffacf56a315915",
    "externalIds": {
      "ACL": "P16-2096",
      "MAG": "2511234952",
      "DBLP": "conf/acl/HovyS16",
      "DOI": "10.18653/v1/P16-2096",
      "CorpusId": 1083991
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "The Social Impact of Natural Language Processing",
    "abstract": "Medical sciences have long since established an ethics code for experiments, to minimize the risk of harm to subjects. Natural language processing (NLP) used to involve mostly anonymous corpora, with the goal of enriching linguistic analysis, and was therefore unlikely to raise ethical concerns. As NLP becomes increasingly wide-spread and uses more data from social media, however, the situation has changed: the outcome of NLP experiments and applications can now have a direct effect on individual users\u2019 lives. Until now, the discourse on this topic in the \ufb01eld has not followed the technological development, while public discourse was often focused on exaggerated dangers. This position paper tries to take back the initiative and start a discussion. We identify a number of social implications of NLP and discuss their ethical signi\ufb01cance, as well as ways to address them.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 75,
    "citationCount": 322,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2022288",
        "name": "Dirk Hovy"
      },
      {
        "authorId": "3416737",
        "name": "Shannon L. Spruit"
      }
    ]
  },
  "7434807": {
    "paperId": "9566afe3f27d7b81417d6b59e5aaedcf75c4079a",
    "externalIds": {
      "MAG": "2742139733",
      "DBLP": "conf/ethnlp/LeidnerP17",
      "ACL": "W17-1604",
      "DOI": "10.18653/v1/W17-1604",
      "CorpusId": 7434807
    },
    "publicationVenue": null,
    "title": "Ethical by Design: Ethics Best Practices for Natural Language Processing",
    "abstract": "Natural language processing (NLP) systems analyze and/or generate human language, typically on users\u2019 behalf. One natural and necessary question that needs to be addressed in this context, both in research projects and in production settings, is the question how ethical the work is, both regarding the process and its outcome. Towards this end, we articulate a set of issues, propose a set of best practices, notably a process featuring an ethics review board, and sketch and how they could be meaningfully applied. Our main argument is that ethical outcomes ought to be achieved by design, i.e. by following a process aligned by ethical values. We also offer some response options for those facing ethics issues. While a number of previous works exist that discuss ethical issues, in particular around big data and machine learning, to the authors\u2019 knowledge this is the first account of NLP and ethics from the perspective of a principled process.",
    "venue": "EthNLP@EACL",
    "year": 2017,
    "referenceCount": 55,
    "citationCount": 69,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2227049",
        "name": "Jochen L. Leidner"
      },
      {
        "authorId": "1798301",
        "name": "Vassilis Plachouras"
      }
    ]
  },
  "160011489": {
    "paperId": "34fa2515a259b4ff2aecb100a35e56019d62cd66",
    "externalIds": {
      "DBLP": "books/sp/19/FeurerH19",
      "DOI": "10.1007/978-3-030-05318-5_1",
      "CorpusId": 160011489
    },
    "publicationVenue": {
      "id": "a688800a-ab93-4531-9258-c10e1b6ddf68",
      "name": "Automated Machine Learning",
      "type": "journal",
      "alternate_names": [
        "Automation and Machine Learning",
        "Autom Mach Learn"
      ],
      "issn": "2520-131X",
      "alternate_issns": [
        "2516-5003"
      ]
    },
    "title": "Hyperparameter Optimization",
    "abstract": null,
    "venue": "Automated Machine Learning",
    "year": 2019,
    "referenceCount": 153,
    "citationCount": 99,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2868444",
        "name": "Matthias Feurer"
      },
      {
        "authorId": "144661829",
        "name": "F. Hutter"
      }
    ]
  },
  "207178262": {
    "paperId": "0023582fde36430c7e3ae81611a14e558c8f4bae",
    "externalIds": {
      "DBLP": "journals/fttcs/DworkR14",
      "MAG": "2027595342",
      "DOI": "10.1561/0400000042",
      "CorpusId": 207178262
    },
    "publicationVenue": {
      "id": "9cd88f1a-8917-4156-b5d6-fb0cef6af4f5",
      "name": "Foundations and Trends\u00ae in Theoretical Computer Science",
      "type": "journal",
      "alternate_names": [
        "Found Trends Theor Comput Sci",
        "Foundations and Trends in Theoretical Computer Science",
        "Found Trends\u00ae Theor Comput Sci"
      ],
      "issn": "1551-305X",
      "url": "http://nowpublishers.com/product.aspx?product=TCS",
      "alternate_urls": [
        "http://www.nowpublishers.com/tcs",
        "https://www.nowpublishers.com/tcs"
      ]
    },
    "title": "The Algorithmic Foundations of Differential Privacy",
    "abstract": "The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition.After motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some astonishingly powerful computational results, there are still fundamental limitations \u2014 not just on what can be achieved with differential privacy but on what can be achieved with any method that protects against a complete breakdown in privacy. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power. Certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed.We then turn from fundamentals to applications other than queryrelease, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams is discussed.Finally, we note that this work is meant as a thorough introduction to the problems and techniques of differential privacy, but is not intended to be an exhaustive survey \u2014 there is by now a vast amount of work in differential privacy, and we can cover only a small portion of it.",
    "venue": "Foundations and Trends\u00ae in Theoretical Computer Science",
    "year": 2014,
    "referenceCount": 96,
    "citationCount": 6916,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1781565",
        "name": "C. Dwork"
      },
      {
        "authorId": "1682008",
        "name": "Aaron Roth"
      }
    ]
  },
  "26783139": {
    "paperId": "573fd2ce97c70bb29097e8efb28a27af791225ca",
    "externalIds": {
      "DBLP": "journals/corr/abs-1708-06733",
      "MAG": "2748789698",
      "ArXiv": "1708.06733",
      "CorpusId": 26783139
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain",
    "abstract": "Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \\emph{BadNet}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of {25}\\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.",
    "venue": "arXiv.org",
    "year": 2017,
    "referenceCount": 52,
    "citationCount": 1550,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2367353",
        "name": "Tianyu Gu"
      },
      {
        "authorId": "1398683279",
        "name": "Brendan Dolan-Gavitt"
      },
      {
        "authorId": "1696125",
        "name": "S. Garg"
      }
    ]
  },
  "168170110": {
    "paperId": "f182ccbc90c1d20d358e3d197b340691f277428f",
    "externalIds": {
      "DBLP": "journals/corr/abs-1905-12457",
      "MAG": "2973217491",
      "ArXiv": "1905.12457",
      "DOI": "10.1109/ACCESS.2019.2941376",
      "CorpusId": 168170110
    },
    "publicationVenue": {
      "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
      "name": "IEEE Access",
      "type": "journal",
      "issn": "2169-3536",
      "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
      "alternate_urls": [
        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
      ]
    },
    "title": "A Backdoor Attack Against LSTM-Based Text Classification Systems",
    "abstract": "With the widespread use of deep learning system in many applications, the adversary has strong incentive to explore vulnerabilities of deep neural networks and manipulate them. Backdoor attacks against deep neural networks have been reported to be a new type of threat. In this attack, the adversary will inject backdoors into the model and then cause the misbehavior of the model through inputs including backdoor triggers. Existed research mainly focuses on backdoor attacks in image classification based on CNN, little attention has been paid to the backdoor attacks in RNN. In this paper, we implement a backdoor attack against LSTM-based text classification by data poisoning. After the backdoor is injected, the model will misclassify any text samples that contains a specific trigger sentence into the target category determined by the adversary. The backdoor attack is stealthy and the backdoor injected in the model has little impact on the performance of the model. We consider the backdoor attack in black-box setting, where the adversary has no knowledge of model structures or training algorithms except for a small amount of training data. We verify the attack through sentiment analysis experiment on the dataset of IMDB movie reviews. The experimental results indicate that our attack can achieve around 96% success rate with 1% poisoning rate.",
    "venue": "IEEE Access",
    "year": 2019,
    "referenceCount": 26,
    "citationCount": 269,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2774171",
        "name": "Jiazhu Dai"
      },
      {
        "authorId": "2109151603",
        "name": "Chuanshuai Chen"
      },
      {
        "authorId": "2110464101",
        "name": "Yufeng Li"
      }
    ]
  },
  "215754328": {
    "paperId": "0d360a1256ccdfca58cf98d12243df8407fd442d",
    "externalIds": {
      "MAG": "3035367371",
      "ACL": "2020.acl-main.249",
      "DBLP": "conf/acl/KuritaMN20",
      "ArXiv": "2004.06660",
      "DOI": "10.18653/v1/2020.acl-main.249",
      "CorpusId": 215754328
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Weight Poisoning Attacks on Pretrained Models",
    "abstract": "Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct \u201cweight poisoning\u201d attacks where pre-trained weights are injected with vulnerabilities that expose \u201cbackdoors\u201d after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 60,
    "citationCount": 371,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "147225682",
        "name": "Keita Kurita"
      },
      {
        "authorId": "144397625",
        "name": "Paul Michel"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  "53099247": {
    "paperId": "30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70",
    "externalIds": {
      "MAG": "2963456518",
      "DBLP": "conf/sp/MelisSCS19",
      "ArXiv": "1805.04049",
      "DOI": "10.1109/SP.2019.00029",
      "CorpusId": 53099247
    },
    "publicationVenue": {
      "id": "29b9c461-963e-4d11-b2ab-92c182243942",
      "name": "IEEE Symposium on Security and Privacy",
      "type": "conference",
      "alternate_names": [
        "S&P",
        "IEEE Symp Secur Priv"
      ],
      "url": "http://www.ieee-security.org/"
    },
    "title": "Exploiting Unintended Feature Leakage in Collaborative Learning",
    "abstract": "Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses.",
    "venue": "IEEE Symposium on Security and Privacy",
    "year": 2018,
    "referenceCount": 74,
    "citationCount": 1310,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145557680",
        "name": "Luca Melis"
      },
      {
        "authorId": "3469125",
        "name": "Congzheng Song"
      },
      {
        "authorId": "1728207",
        "name": "Emiliano De Cristofaro"
      },
      {
        "authorId": "1723945",
        "name": "Vitaly Shmatikov"
      }
    ]
  },
  "229156229": {
    "paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba",
    "externalIds": {
      "DBLP": "journals/corr/abs-2012-07805",
      "MAG": "3112689365",
      "ArXiv": "2012.07805",
      "CorpusId": 229156229
    },
    "publicationVenue": {
      "id": "54649c1d-6bcc-4232-9cd1-aa446867b8d0",
      "name": "USENIX Security Symposium",
      "type": "conference",
      "alternate_names": [
        "USENIX Secur Symp"
      ],
      "url": "http://www.usenix.org/events/bytopic/security.html"
    },
    "title": "Extracting Training Data from Large Language Models",
    "abstract": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
    "venue": "USENIX Security Symposium",
    "year": 2020,
    "referenceCount": 75,
    "citationCount": 1488,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2483738",
        "name": "Nicholas Carlini"
      },
      {
        "authorId": "2444919",
        "name": "Florian Tram\u00e8r"
      },
      {
        "authorId": "145217343",
        "name": "Eric Wallace"
      },
      {
        "authorId": "40844378",
        "name": "Matthew Jagielski"
      },
      {
        "authorId": "1404060687",
        "name": "Ariel Herbert-Voss"
      },
      {
        "authorId": "3844009",
        "name": "Katherine Lee"
      },
      {
        "authorId": "145625142",
        "name": "Adam Roberts"
      },
      {
        "authorId": "31035595",
        "name": "Tom B. Brown"
      },
      {
        "authorId": "143711382",
        "name": "D. Song"
      },
      {
        "authorId": "1758110",
        "name": "\u00da. Erlingsson"
      },
      {
        "authorId": "3046437",
        "name": "Alina Oprea"
      },
      {
        "authorId": "2402716",
        "name": "Colin Raffel"
      }
    ]
  },
  "253080602": {
    "paperId": "eb39dda2df56270599f2a28bc6433c84c1704949",
    "externalIds": {
      "ACL": "2022.emnlp-main.99",
      "ArXiv": "2210.11735",
      "DBLP": "journals/corr/abs-2210-11735",
      "DOI": "10.48550/arXiv.2210.11735",
      "CorpusId": 253080602
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Extracted BERT Model Leaks More Information than You Think!",
    "abstract": "The collection and availability of big data, combined with advances in pre-trained models (e.g. BERT), have revolutionized the predictive performance of natural language processing tasks. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. Due to significant commercial interest, there has been a surge of attempts to steal remote services via model extraction. Although previous works have made progress in defending against model extraction attacks, there has been little discussion on their performance in preventing privacy leakage. This work bridges this gap by launching an attribute inference attack against the extracted BERT model. Our extensive experiments reveal that model extraction can cause severe privacy leakage even when victim models are facilitated with state-of-the-art defensive strategies.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 29,
    "citationCount": 4,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2288269593",
        "name": "Xuanli He"
      },
      {
        "authorId": null,
        "name": "Chen Chen"
      },
      {
        "authorId": "3366777",
        "name": "L. Lyu"
      },
      {
        "authorId": "3101288",
        "name": "Qiongkai Xu"
      }
    ]
  },
  "216868525": {
    "paperId": "d73561ab8318ce343f5cb15f96c74f210b6b24fa",
    "externalIds": {
      "DBLP": "conf/emnlp/WallaceSS20",
      "MAG": "3099729825",
      "ACL": "2020.emnlp-main.446",
      "ArXiv": "2004.15015",
      "DOI": "10.18653/v1/2020.emnlp-main.446",
      "CorpusId": 216868525
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Imitation Attacks and Defenses for Black-box Machine Translation Systems",
    "abstract": "We consider an adversary looking to steal or attack a black-box machine translation (MT) system, either for financial gain or to exploit model errors. We first show that black-box MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their victims. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades imitation model BLEU and attack transfer rates at some cost in BLEU and inference speed.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 66,
    "citationCount": 107,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145217343",
        "name": "Eric Wallace"
      },
      {
        "authorId": "144872294",
        "name": "Mitchell Stern"
      },
      {
        "authorId": "143711382",
        "name": "D. Song"
      }
    ]
  },
  "252090014": {
    "paperId": "975e8d7065161d3dc0020ef343aa1db2a3db5a7b",
    "externalIds": {
      "ACL": "2022.coling-1.251",
      "DBLP": "conf/coling/XuHLQH22",
      "ArXiv": "2108.13873",
      "CorpusId": 252090014
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Student Surpasses Teacher: Imitation Attack for Black-Box NLP APIs",
    "abstract": "Machine-learning-as-a-service (MLaaS) has attracted millions of users to their splendid large-scale models. Although published as black-box APIs, the valuable models behind these services are still vulnerable to imitation attacks. Recently, a series of works have demonstrated that attackers manage to steal or extract the victim models. Nonetheless, none of the previous stolen models can outperform the original black-box APIs. In this work, we conduct unsupervised domain adaptation and multi-victim ensemble to showing that attackers could potentially surpass victims, which is beyond previous understanding of model extraction. Extensive experiments on both benchmark datasets and real-world APIs validate that the imitators can succeed in outperforming the original black-box models on transferred domains. We consider our work as a milestone in the research of imitation attack, especially on NLP APIs, as the superior performance could influence the defense or even publishing strategy of API providers.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2021,
    "referenceCount": 45,
    "citationCount": 19,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3101288",
        "name": "Qiongkai Xu"
      },
      {
        "authorId": "2288269593",
        "name": "Xuanli He"
      },
      {
        "authorId": "3366777",
        "name": "L. Lyu"
      },
      {
        "authorId": "153139892",
        "name": "Lizhen Qu"
      },
      {
        "authorId": "2561045",
        "name": "Gholamreza Haffari"
      }
    ]
  },
  "222134003": {
    "paperId": "8a027e49b3e961e1a9cd8e842281d112ab2698c9",
    "externalIds": {
      "ACL": "2020.findings-emnlp.213",
      "ArXiv": "2010.01285",
      "DBLP": "conf/emnlp/LyuHL20",
      "MAG": "3090414294",
      "DOI": "10.18653/v1/2020.findings-emnlp.213",
      "CorpusId": 222134003
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Differentially Private Representation for NLP: Formal Guarantee and An Empirical Study on Privacy and Fairness",
    "abstract": "It has been demonstrated that hidden representation learned by deep model can encode private information of the input, hence can be exploited to recover such information with reasonable accuracy. To address this issue, we propose a novel approach called Differentially Private Neural Representation (DPNR) to preserve privacy of the extracted representation from text. DPNR utilises Differential Privacy (DP) to provide formal privacy guarantee. Further, we show that masking words via dropout can further enhance privacy. To maintain utility of the learned representation, we integrate DP-noisy representation into a robust training process to derive a robust target model, which also helps for model fairness over various demographic variables. Experimental results on benchmark datasets under various parameter settings demonstrate that DPNR largely reduces privacy leakage without significantly sacrificing the main task performance.",
    "venue": "Findings",
    "year": 2020,
    "referenceCount": 42,
    "citationCount": 75,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3366777",
        "name": "L. Lyu"
      },
      {
        "authorId": "2288269593",
        "name": "Xuanli He"
      },
      {
        "authorId": "40609859",
        "name": "Yitong Li"
      }
    ]
  },
  "237353275": {
    "paperId": "bda3fe4ae1cb73ef99f48add40967179577d29e8",
    "externalIds": {
      "ACL": "2022.naacl-main.205",
      "DBLP": "journals/corr/abs-2108-12944",
      "ArXiv": "2108.12944",
      "DOI": "10.18653/v1/2022.naacl-main.205",
      "CorpusId": 237353275
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Selective Differential Privacy for Language Modeling",
    "abstract": "With the increasing applications of language models, it has become crucial to protect these models from leaking private information. Previous work has attempted to tackle this challenge by training RNN-based language models with differential privacy guarantees.However, applying classical differential privacy to language models leads to poor model performance as the underlying privacy notion is over-pessimistic and provides undifferentiated protection for all tokens in the data. Given that the private information in natural language is sparse (for example, the bulk of an email might not carry personally identifiable information), we propose a new privacy notion, selective differential privacy, to provide rigorous privacy guarantees on the sensitive portion of the data to improve model utility. To realize such a new notion, we develop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based language models. Besides language modeling, we also apply the method to a more concrete application \u2013 dialog systems. Experiments on both language modeling and dialog system building show that the proposed privacy-preserving mechanism achieves better utilities while remaining safe under various privacy attacks compared to the baselines. The data and code are released at https://github.com/wyshi/lm_privacy to facilitate future research.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 43,
    "citationCount": 57,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8299781",
        "name": "Weiyan Shi"
      },
      {
        "authorId": "2004851330",
        "name": "Aiqi Cui"
      },
      {
        "authorId": "2150320252",
        "name": "Evan Li"
      },
      {
        "authorId": "39823639",
        "name": "R. Jia"
      },
      {
        "authorId": "1564034697",
        "name": "Zhou Yu"
      }
    ]
  },
  "208909851": {
    "paperId": "8e58dc63817a2a26e5a2ddad38d8b1d19d1c3795",
    "externalIds": {
      "DBLP": "journals/corr/abs-1912-03817",
      "ArXiv": "1912.03817",
      "DOI": "10.1109/SP40001.2021.00019",
      "CorpusId": 208909851
    },
    "publicationVenue": {
      "id": "29b9c461-963e-4d11-b2ab-92c182243942",
      "name": "IEEE Symposium on Security and Privacy",
      "type": "conference",
      "alternate_names": [
        "S&P",
        "IEEE Symp Secur Priv"
      ],
      "url": "http://www.ieee-security.org/"
    },
    "title": "Machine Unlearning",
    "abstract": "Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult.We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning.Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63\u00d7, and 2.45\u00d7 for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36\u00d7 in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.",
    "venue": "IEEE Symposium on Security and Privacy",
    "year": 2019,
    "referenceCount": 68,
    "citationCount": 632,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1452678444",
        "name": "Lucas Bourtoule"
      },
      {
        "authorId": "143754359",
        "name": "Varun Chandrasekaran"
      },
      {
        "authorId": "1415982317",
        "name": "Christopher A. Choquette-Choo"
      },
      {
        "authorId": "120074583",
        "name": "Hengrui Jia"
      },
      {
        "authorId": "1452679273",
        "name": "Adelin Travers"
      },
      {
        "authorId": "23696685",
        "name": "Baiwu Zhang"
      },
      {
        "authorId": "47412202",
        "name": "D. Lie"
      },
      {
        "authorId": "1967156",
        "name": "Nicolas Papernot"
      }
    ]
  },
  "244909149": {
    "paperId": "2569a7309142e40815cf556b6417059df9abbda8",
    "externalIds": {
      "DBLP": "conf/aaai/HeXLWW22",
      "ArXiv": "2112.02701",
      "DOI": "10.1609/aaai.v36i10.21321",
      "CorpusId": 244909149
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark",
    "abstract": "Nowadays, due to the breakthrough in natural language generation (NLG), including machine translation, document summarization, image captioning, etc NLG models have been encapsulated in cloud APIs to serve over half a billion people worldwide and process over one hundred billion word generations per day. Thus, NLG APIs have already become essential profitable services in many commercial companies. Due to the substantial financial and intellectual investments, service providers adopt a pay-as-you-use policy to promote sustainable market growth. However, recent works have shown that cloud platforms suffer from financial losses imposed by model extraction attacks, which aim to imitate the functionality and utility of the victim services, thus violating the intellectual property (IP) of cloud APIs. This work targets at protecting IP of NLG APIs by identifying the attackers who have utilized watermarked responses from the victim NLG APIs. However, most existing watermarking techniques are not directly amenable for IP protection of NLG APIs. To bridge this gap, we first present a novel watermarking method for text generation APIs by conducting lexical modification to the original outputs. Compared with the competitive baselines, our watermark approach achieves better identifiable performance in terms of p-value, with fewer semantic losses. In addition, our watermarks are more understandable and intuitive to humans than the baselines. Finally, the empirical studies show our approach is also applicable to queries from different domains, and is effective on the attacker trained on a mixture of the corpus which includes less than 10% watermarked samples.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2021,
    "referenceCount": 53,
    "citationCount": 82,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Law",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2288269593",
        "name": "Xuanli He"
      },
      {
        "authorId": "3101288",
        "name": "Qiongkai Xu"
      },
      {
        "authorId": "3366777",
        "name": "L. Lyu"
      },
      {
        "authorId": "2397264",
        "name": "Fangzhao Wu"
      },
      {
        "authorId": "2143199909",
        "name": "Chenguang Wang"
      }
    ]
  },
  "232147529": {
    "paperId": "c4788d6d19c9c6555264f274d01fd0c34c22c674",
    "externalIds": {
      "DBLP": "journals/corr/abs-2103-04044",
      "ArXiv": "2103.04044",
      "ACL": "2021.hcinlp-1.8",
      "CorpusId": 232147529
    },
    "publicationVenue": null,
    "title": "Putting Humans in the Natural Language Processing Loop: A Survey",
    "abstract": "How can we design Natural Language Processing (NLP) systems that learn from human feedback? There is a growing research body of Human-in-the-loop (HITL) NLP frameworks that continuously integrate human feedback to improve the model itself. HITL NLP research is nascent but multifarious\u2014solving various NLP problems, collecting diverse feedback from different people, and applying different methods to learn from human feedback. We present a survey of HITL NLP work from both Machine Learning (ML) and Human-computer Interaction (HCI) communities that highlights its short yet inspiring history, and thoroughly summarize recent frameworks focusing on their tasks, goals, human interactions, and feedback learning methods. Finally, we discuss future studies for integrating human feedback in the NLP development loop.",
    "venue": "HCINLP",
    "year": 2021,
    "referenceCount": 32,
    "citationCount": 65,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1390877819",
        "name": "Zijie J. Wang"
      },
      {
        "authorId": "2026030439",
        "name": "Dongjin Choi"
      },
      {
        "authorId": "51132439",
        "name": "Shenyu Xu"
      },
      {
        "authorId": "2022168",
        "name": "Diyi Yang"
      }
    ]
  },
  "235694265": {
    "paperId": "a16ae67070de155789a871cb27ecbf9eaa98b379",
    "externalIds": {
      "ArXiv": "2107.00061",
      "ACL": "2021.acl-long.565",
      "DBLP": "journals/corr/abs-2107-00061",
      "DOI": "10.18653/v1/2021.acl-long.565",
      "CorpusId": 235694265
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "All That\u2019s \u2018Human\u2019 Is Not Gold: Evaluating Human Evaluation of Generated Text",
    "abstract": "Human evaluations are typically considered the gold standard in natural language generation, but as models\u2019 fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts\u2019 ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators\u2019 accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 45,
    "citationCount": 324,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40684993",
        "name": "Elizabeth Clark"
      },
      {
        "authorId": "50509991",
        "name": "Tal August"
      },
      {
        "authorId": "38618739",
        "name": "Sofia Serrano"
      },
      {
        "authorId": "3465456",
        "name": "Nikita Haduong"
      },
      {
        "authorId": "40895369",
        "name": "Suchin Gururangan"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      }
    ]
  },
  "218483124": {
    "paperId": "529025645c70a935221bd434484faee695ad0f25",
    "externalIds": {
      "MAG": "3032875465",
      "DBLP": "conf/chi/YangSRZ20",
      "DOI": "10.1145/3313831.3376301",
      "CorpusId": 218483124
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design",
    "abstract": "Artificial Intelligence (AI) plays an increasingly important role in improving HCI and user experience. Yet many challenges persist in designing and innovating valuable human-AI interactions. For example, AI systems can make unpredictable errors, and these errors damage UX and even lead to undesired societal impact. However, HCI routinely grapples with complex technologies and mitigates their unintended consequences. What makes AI different? What makes human-AI interaction appear particularly difficult to design? This paper investigates these questions. We synthesize prior research, our own design and research experience, and our observations when teaching human-AI interaction. We identify two sources of AI's distinctive design challenges: 1) uncertainty surrounding AI's capabilities, 2) AI's output complexity, spanning from simple to adaptive complex. We identify four levels of AI systems. On each level, designers encounter a different subset of the design challenges. We demonstrate how these findings reveal new insights for designers, researchers, and design tool makers in productively addressing the challenges of human-AI interaction going forward.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2020,
    "referenceCount": 68,
    "citationCount": 369,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2117860470",
        "name": "Qian Yang"
      },
      {
        "authorId": "1792714",
        "name": "Aaron Steinfeld"
      },
      {
        "authorId": "35959897",
        "name": "C. Ros\u00e9"
      },
      {
        "authorId": "145308025",
        "name": "J. Zimmerman"
      }
    ]
  },
  "220128138": {
    "paperId": "ebcbbb8fe297940d79b17aeb6d46bedff9db7fec",
    "externalIds": {
      "DBLP": "conf/chi/BansalWZFNKRW21",
      "ArXiv": "2006.14779",
      "MAG": "3037634279",
      "DOI": "10.1145/3411764.3445717",
      "CorpusId": 220128138
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance",
    "abstract": "Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI\u2019s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2020,
    "referenceCount": 101,
    "citationCount": 459,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "33340656",
        "name": "Gagan Bansal"
      },
      {
        "authorId": "35232494",
        "name": "Tongshuang Sherry Wu"
      },
      {
        "authorId": "153823289",
        "name": "Joyce Zhou"
      },
      {
        "authorId": "27083453",
        "name": "Raymond Fok"
      },
      {
        "authorId": "2571049",
        "name": "Besmira Nushi"
      },
      {
        "authorId": "1783184",
        "name": "Ece Kamar"
      },
      {
        "authorId": "78846919",
        "name": "Marco Tulio Ribeiro"
      },
      {
        "authorId": "1780531",
        "name": "Daniel S. Weld"
      }
    ]
  },
  "8943607": {
    "paperId": "5df85ae89af55c6d82a1a14836ea6bcfbfc2c0ec",
    "externalIds": {
      "MAG": "2059216172",
      "DBLP": "conf/chi/Horvitz99",
      "DOI": "10.1145/302979.303030",
      "CorpusId": 8943607
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "Principles of mixed-initiative user interfaces",
    "abstract": "Recent debate has centered on the relative promise of focusinguser-interface research on developing new metaphors and tools thatenhance users abilities to directly manipulate objects versusdirecting effort toward developing interface agents that provideautomation. In this paper, we review principles that show promisefor allowing engineers to enhance human-computer interactionthrough an elegant coupling of automated services with directmanipulation. Key ideas will be highlighted in terms of the Lookoutsystem for scheduling and meeting management.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 1999,
    "referenceCount": 24,
    "citationCount": 1327,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145479841",
        "name": "E. Horvitz"
      }
    ]
  },
  "86866942": {
    "paperId": "ad3cf68bae32d21f25ac142287d4a556155619d2",
    "externalIds": {
      "DBLP": "conf/chi/AmershiWVFNCSIB19",
      "MAG": "2916904544",
      "DOI": "10.1145/3290605.3300233",
      "CorpusId": 86866942
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "Guidelines for Human-AI Interaction",
    "abstract": "Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2019,
    "referenceCount": 45,
    "citationCount": 1090,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1719124",
        "name": "Saleema Amershi"
      },
      {
        "authorId": "1780531",
        "name": "Daniel S. Weld"
      },
      {
        "authorId": "3109339",
        "name": "Mihaela Vorvoreanu"
      },
      {
        "authorId": "3318905",
        "name": "Adam Fourney"
      },
      {
        "authorId": "2571049",
        "name": "Besmira Nushi"
      },
      {
        "authorId": "9703838",
        "name": "Penny Collisson"
      },
      {
        "authorId": "38972741",
        "name": "Jina Suh"
      },
      {
        "authorId": "1730570",
        "name": "Shamsi T. Iqbal"
      },
      {
        "authorId": "144609235",
        "name": "Paul N. Bennett"
      },
      {
        "authorId": "1781500",
        "name": "K. Quinn"
      },
      {
        "authorId": "144113253",
        "name": "J. Teevan"
      },
      {
        "authorId": "1405707881",
        "name": "Ruth Kikin-Gil"
      },
      {
        "authorId": "145479841",
        "name": "E. Horvitz"
      }
    ]
  },
  "246426909": {
    "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
    "externalIds": {
      "DBLP": "conf/nips/Ouyang0JAWMZASR22",
      "ArXiv": "2203.02155",
      "CorpusId": 246426909
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Training language models to follow instructions with human feedback",
    "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "referenceCount": 83,
    "citationCount": 9287,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "31793034",
        "name": "Long Ouyang"
      },
      {
        "authorId": "49387725",
        "name": "Jeff Wu"
      },
      {
        "authorId": "2115903168",
        "name": "Xu Jiang"
      },
      {
        "authorId": "2061137049",
        "name": "Diogo Almeida"
      },
      {
        "authorId": "2064084601",
        "name": "Carroll L. Wainwright"
      },
      {
        "authorId": "2051714782",
        "name": "Pamela Mishkin"
      },
      {
        "authorId": null,
        "name": "Chong Zhang"
      },
      {
        "authorId": "144517868",
        "name": "Sandhini Agarwal"
      },
      {
        "authorId": "2117680841",
        "name": "Katarina Slama"
      },
      {
        "authorId": "2064770039",
        "name": "Alex Ray"
      },
      {
        "authorId": "47971768",
        "name": "John Schulman"
      },
      {
        "authorId": "2052366271",
        "name": "Jacob Hilton"
      },
      {
        "authorId": "2151735262",
        "name": "Fraser Kelton"
      },
      {
        "authorId": "2142365973",
        "name": "Luke E. Miller"
      },
      {
        "authorId": "2151735251",
        "name": "Maddie Simens"
      },
      {
        "authorId": "119609682",
        "name": "Amanda Askell"
      },
      {
        "authorId": "2930640",
        "name": "P. Welinder"
      },
      {
        "authorId": "145791315",
        "name": "P. Christiano"
      },
      {
        "authorId": "2990741",
        "name": "J. Leike"
      },
      {
        "authorId": "49407415",
        "name": "Ryan J. Lowe"
      }
    ]
  },
  "221665105": {
    "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
    "externalIds": {
      "MAG": "3082115681",
      "DBLP": "journals/corr/abs-2009-01325",
      "ArXiv": "2009.01325",
      "CorpusId": 221665105
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Learning to summarize from human feedback",
    "abstract": "As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 84,
    "citationCount": 1554,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1387983862",
        "name": "Nisan Stiennon"
      },
      {
        "authorId": "31793034",
        "name": "Long Ouyang"
      },
      {
        "authorId": "49387725",
        "name": "Jeff Wu"
      },
      {
        "authorId": "2052152920",
        "name": "Daniel M. Ziegler"
      },
      {
        "authorId": "49407415",
        "name": "Ryan J. Lowe"
      },
      {
        "authorId": "153387869",
        "name": "Chelsea Voss"
      },
      {
        "authorId": "38909097",
        "name": "Alec Radford"
      },
      {
        "authorId": "2698777",
        "name": "Dario Amodei"
      },
      {
        "authorId": "145370786",
        "name": "Paul Christiano"
      }
    ]
  },
  "211126925": {
    "paperId": "7b4358d7692353003eae7e39cececa2c2c44c43a",
    "externalIds": {
      "ArXiv": "1911.01352",
      "MAG": "3000642987",
      "DBLP": "conf/iclr/WangQZ0YN0R20",
      "CorpusId": 211126925
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Learning from Explanations with Neural Execution Tree",
    "abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "referenceCount": 30,
    "citationCount": 38,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1390880371",
        "name": "Ziqi Wang"
      },
      {
        "authorId": "50625437",
        "name": "Yujia Qin"
      },
      {
        "authorId": "2203076",
        "name": "Wenxuan Zhou"
      },
      {
        "authorId": "49781448",
        "name": "Jun Yan"
      },
      {
        "authorId": "1557391091",
        "name": "Qinyuan Ye"
      },
      {
        "authorId": "152842060",
        "name": "Leonardo Neves"
      },
      {
        "authorId": "49293587",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "1384550891",
        "name": "Xiang Ren"
      }
    ]
  },
  "244708947": {
    "paperId": "605c32428861eb26b8631617b8f6c97a850d6a04",
    "externalIds": {
      "DBLP": "journals/corr/abs-2111-13440",
      "ArXiv": "2111.13440",
      "DOI": "10.1162/tacl_a_00485",
      "CorpusId": 244708947
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "True Few-Shot Learning with Prompts\u2014A Real-World Perspective",
    "abstract": "Abstract Prompt-based approaches excel at few-shot learning. However, Perez et al. (2021) recently cast doubt on their performance as they had difficulty getting good results in a \u201ctrue\u201d few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of Pet, a method that combines textual instructions with example-based finetuning. We show that, if correctly configured, Pet performs strongly in true few-shot settings without a dev set. Crucial for this strong performance is a number of design choices, including Pet\u2019s ability to intelligently handle multiple prompts. We put our findings to a real-world test by running Pet on RAFT, a benchmark of tasks taken from realistic NLP applications for which no labeled dev or test sets are available. Pet achieves a new state of the art on RAFT and performs close to non-expert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners can successfully be applied in true few-shot settings and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 93,
    "citationCount": 56,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32246932",
        "name": "Timo Schick"
      },
      {
        "authorId": "144418438",
        "name": "Hinrich Sch\u00fctze"
      }
    ]
  },
  "225062157": {
    "paperId": "ecb2b0859bab2761be397804516b8de3983366e8",
    "externalIds": {
      "ArXiv": "2010.11982",
      "MAG": "3093713068",
      "DBLP": "journals/corr/abs-2010-11982",
      "CorpusId": 225062157
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "The Turking Test: Can Language Models Understand Instructions?",
    "abstract": "Supervised machine learning provides the learner with a set of input-output examples of the target task. Humans, however, can also learn to perform new tasks from instructions in natural language. Can machines learn to understand instructions as well? We present the Turking Test, which examines a model's ability to follow natural language instructions of varying complexity. These range from simple tasks, like retrieving the nth word of a sentence, to ones that require creativity, such as generating examples for SNLI and SQuAD in place of human intelligence workers (\"turkers\"). Despite our lenient evaluation methodology, we observe that a large pretrained language model performs poorly across all tasks. Analyzing the model's error patterns reveals that the model tends to ignore explicit instructions and often generates outputs that cannot be construed as an attempt to solve the task. While it is not yet clear whether instruction understanding can be captured by traditional language models, the sheer expressivity of instruction understanding makes it an appealing alternative to the rising few-shot inference paradigm.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 12,
    "citationCount": 90,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1388010852",
        "name": "Avia Efrat"
      },
      {
        "authorId": "39455775",
        "name": "Omer Levy"
      }
    ]
  },
  "235358897": {
    "paperId": "f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d",
    "externalIds": {
      "ACL": "2021.findings-acl.368",
      "DBLP": "journals/corr/abs-2106-03427",
      "ArXiv": "2106.03427",
      "DOI": "10.18653/v1/2021.findings-acl.368",
      "CorpusId": 235358897
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring",
    "abstract": "Despite recent progress, learning new tasks through language instructions remains an extremely challenging problem. On the ALFRED benchmark for task learning, the published state-of-the-art system only achieves a task success rate of less than 10% in an unseen environment, compared to the human performance of over 90%. To address this issue, this paper takes a closer look at task learning. In a departure from a widely applied end-to-end architecture, we decomposed task learning into three sub-problems: sub-goal planning, scene navigation, and object manipulation; and developed a model HiTUT (stands for Hierarchical Tasks via Unified Transformers) that addresses each sub-problem in a unified manner to learn a hierarchical task structure. On the ALFRED benchmark, HiTUT has achieved the best performance with a remarkably higher generalization ability. In the unseen environment, HiTUT achieves over 160% performance gain in success rate compared to the previous state of the art. The explicit representation of task structures also enables an in-depth understanding of the nature of the problem and the ability of the agent, which provides insight for future benchmark development and evaluation.",
    "venue": "Findings",
    "year": 2021,
    "referenceCount": 46,
    "citationCount": 73,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46868553",
        "name": "Yichi Zhang"
      },
      {
        "authorId": "1707259",
        "name": "J. Chai"
      }
    ]
  },
  "237421373": {
    "paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c",
    "externalIds": {
      "ArXiv": "2104.08773",
      "ACL": "2022.acl-long.244",
      "DBLP": "conf/acl/MishraKBH22",
      "DOI": "10.18653/v1/2022.acl-long.244",
      "CorpusId": 237421373
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
    "abstract": "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 54,
    "citationCount": 637,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1817207",
        "name": "Swaroop Mishra"
      },
      {
        "authorId": "1783281",
        "name": "Daniel Khashabi"
      },
      {
        "authorId": "2064619864",
        "name": "Chitta Baral"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ]
  },
  "265659379": {
    "paperId": "a2b150e02306038389f5df683428f5a4659a468e",
    "externalIds": {
      "DBLP": "conf/iclr/Lou0XSAX0024",
      "ArXiv": "2312.02436",
      "DOI": "10.48550/arXiv.2312.02436",
      "CorpusId": 265659379
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following",
    "abstract": "In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. Experimental results across four zero-shot benchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes, reveal that LLMs, at various scales, trained on MUFFIN generally demonstrate superior instruction-following capabilities compared to those trained on the two aforementioned schemes.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 72,
    "citationCount": 18,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2118614649",
        "name": "Renze Lou"
      },
      {
        "authorId": "145086492",
        "name": "Kai Zhang"
      },
      {
        "authorId": "2153624353",
        "name": "Jian Xie"
      },
      {
        "authorId": "2253808926",
        "name": "Yuxuan Sun"
      },
      {
        "authorId": "2269759096",
        "name": "Janice Ahn"
      },
      {
        "authorId": "2143534669",
        "name": "Hanzi Xu"
      },
      {
        "authorId": "2254227752",
        "name": "Yu Su"
      },
      {
        "authorId": "2269736245",
        "name": "Wenpeng Yin"
      }
    ]
  },
  "53306064": {
    "paperId": "b0e716728e940eb2164892b7e284940157a2cebe",
    "externalIds": {
      "DBLP": "conf/aaai/YaoPWK0Y19",
      "MAG": "2952630347",
      "ArXiv": "1811.05701",
      "DOI": "10.1609/aaai.v33i01.33017378",
      "CorpusId": 53306064
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Plan-And-Write: Towards Better Automatic Storytelling",
    "abstract": "Automatic storytelling is challenging since it requires generating long, coherent natural language to describes a sensible sequence of events. Despite considerable efforts on automatic story generation in the past, prior work either is restricted in plot planning, or can only generate stories in a narrow domain. In this paper, we explore open-domain story generation that writes stories given a title (topic) as input. We propose a plan-and-write hierarchical generation framework that first plans a storyline, and then generates a story based on the storyline. We compare two planning strategies. The dynamic schema interweaves story planning and its surface realization in text, while the static schema plans out the entire storyline before generating stories. Experiments show that with explicit storyline planning, the generated stories are more diverse, coherent, and on topic than those generated without creating a full plan, according to both automatic and human evaluations.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2018,
    "referenceCount": 42,
    "citationCount": 374,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48028532",
        "name": "Lili Yao"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "1732071",
        "name": "R. Weischedel"
      },
      {
        "authorId": "152971314",
        "name": "Kevin Knight"
      },
      {
        "authorId": "144060462",
        "name": "Dongyan Zhao"
      },
      {
        "authorId": "144539156",
        "name": "Rui Yan"
      }
    ]
  },
  "118589015": {
    "paperId": "39d47ba0690cfac4c8efde6ecda104234c70ff86",
    "externalIds": {
      "MAG": "2951493682",
      "ACL": "N19-1172",
      "DBLP": "journals/corr/abs-1904-06828",
      "ArXiv": "1904.06828",
      "DOI": "10.18653/v1/N19-1172",
      "CorpusId": 118589015
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Pun Generation with Surprise",
    "abstract": "We tackle the problem of generating a pun sentence given a pair of homophones (e.g., \u201cdied\u201d and \u201cdyed\u201d). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a large corpus. In this paper, we propose an unsupervised approach to pun generation based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., \u201cdyed\u201d) and the distant context, but a strong association between the alternative word (e.g., \u201cdied\u201d) and the immediate context. We instantiate the surprisal principle in two ways: (i) as a measure based on the ratio of probabilities given by a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30% of the time, doubling the success rate of a neural generation baseline.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 32,
    "citationCount": 66,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2140062900",
        "name": "He He"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "145419642",
        "name": "Percy Liang"
      }
    ]
  },
  "248512713": {
    "paperId": "be9bb35bc3dc5630ff09895129d4d515aa94ed97",
    "externalIds": {
      "ACL": "2022.naacl-main.77",
      "DBLP": "journals/corr/abs-2205-01825",
      "ArXiv": "2205.01825",
      "DOI": "10.48550/arXiv.2205.01825",
      "CorpusId": 248512713
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "AmbiPun: Generating Humorous Puns with Ambiguous Context",
    "abstract": "In this paper, we propose a simple yet effective way to generate pun sentences that does not require any training on existing puns. Our approach is inspired by humor theories that ambiguity comes from the context rather than the pun word itself. Given a pair of definitions of a pun word, our model first produces a list of related concepts through a reverse dictionary. We then utilize one-shot GPT3 to generate context words and then generate puns incorporating context words from both concepts. Human evaluation shows that our method successfully generates pun 52% of the time, outperforming well-crafted baselines and the state-of-the-art models by a large margin.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 41,
    "citationCount": 21,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "26965176",
        "name": "Anirudh Mittal"
      },
      {
        "authorId": "48391421",
        "name": "Yufei Tian"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      }
    ]
  },
  "221818956": {
    "paperId": "12cfe5d242c940a0383b55978c7419ee6633ed85",
    "externalIds": {
      "DBLP": "journals/corr/abs-2009-09870",
      "ACL": "2020.emnlp-main.351",
      "ArXiv": "2009.09870",
      "MAG": "3087728281",
      "DOI": "10.18653/v1/2020.emnlp-main.351",
      "CorpusId": 221818956
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Content Planning for Neural Story Generation with Aristotelian Rescoring",
    "abstract": "Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 35,
    "citationCount": 119,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1405375772",
        "name": "Seraphina Goldfarb-Tarrant"
      },
      {
        "authorId": "51448832",
        "name": "Tuhin Chakrabarty"
      },
      {
        "authorId": "1732071",
        "name": "R. Weischedel"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      }
    ]
  },
  "221802643": {
    "paperId": "d45776f8d75b2505e5fd0349275896053f5a4ead",
    "externalIds": {
      "MAG": "3087142258",
      "DBLP": "journals/corr/abs-2009-08942",
      "CorpusId": 221802643
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Generating similes like a Pro: A Style Transfer Approach for Simile Generation",
    "abstract": "Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of simile generation. Generating a simile requires proper understanding for effective mapping of properties between two concepts. To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge. We then propose to fine-tune a pretrained sequence to sequence model, BART~\\cite{lewis2019bart}, on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence. Experiments show that our approach generates $88\\%$ novel similes that do not share properties with the training data. Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts \\textit{37\\%}\\footnote{We average 32.6\\% and 41.3\\% for 2 humans.} of the times, and three baseline systems including a recent metaphor generation model \\textit{71\\%}\\footnote{We average 82\\% ,63\\% and 68\\% for three baselines.} of the times when compared pairwise.\\footnote{The simile in the title is generated by our best model. Input: Generating similes effortlessly, output: Generating similes \\textit{like a Pro}.} We also show how replacing literal sentences with similes from our best model in machine generated stories improves evocativeness and leads to better acceptance by human judges.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 0,
    "citationCount": 2,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51448832",
        "name": "Tuhin Chakrabarty"
      },
      {
        "authorId": "2295928",
        "name": "S. Muresan"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      }
    ]
  },
  "232185202": {
    "paperId": "5678a6e9bd9b4c56f45219e9ccf1a5b6356963d4",
    "externalIds": {
      "DBLP": "journals/corr/abs-2103-06779",
      "ArXiv": "2103.06779",
      "MAG": "3168921656",
      "ACL": "2021.naacl-main.336",
      "DOI": "10.18653/V1/2021.NAACL-MAIN.336",
      "CorpusId": 232185202
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding",
    "abstract": "Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (CITATION) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66% of the time on average. A task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68% of the time compared to poems without metaphors.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 55,
    "citationCount": 65,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51448832",
        "name": "Tuhin Chakrabarty"
      },
      {
        "authorId": "2124921340",
        "name": "Xurui Zhang"
      },
      {
        "authorId": "2295928",
        "name": "S. Muresan"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      }
    ]
  },
  "236941431": {
    "paperId": "a0035379f93e0e95bdadd77a1d8eb27ba89dcf60",
    "externalIds": {
      "MAG": "3093524609",
      "ArXiv": "2010.01717",
      "DBLP": "conf/emnlp/AkouryWWHPI20",
      "ACL": "2020.emnlp-main.525",
      "DOI": "10.18653/v1/2020.emnlp-main.525",
      "CorpusId": 236941431
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation",
    "abstract": "Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 36,
    "citationCount": 84,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32362256",
        "name": "Nader Akoury"
      },
      {
        "authorId": "2051628335",
        "name": "Shufan Wang"
      },
      {
        "authorId": "1986610456",
        "name": "Josh Whiting"
      },
      {
        "authorId": "2073537368",
        "name": "Stephen Hood"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "2136562",
        "name": "Mohit Iyyer"
      }
    ]
  },
  "235294009": {
    "paperId": "432da56e21e0d4c85f7ea58159f363732c4ecdd4",
    "externalIds": {
      "DBLP": "journals/corr/abs-2106-01228",
      "ArXiv": "2106.01228",
      "ACL": "2021.acl-long.524",
      "DOI": "10.18653/v1/2021.acl-long.524",
      "CorpusId": 235294009
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Metaphor Generation with Conceptual Mappings",
    "abstract": "Generating metaphors is a difficult task as it requires understanding nuanced relationships between abstract concepts. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Guided by conceptual metaphor theory, we propose to control the generation process by encoding conceptual mappings between cognitive domains to generate meaningful metaphoric expressions. To achieve this, we develop two methods: 1) using FrameNet-based embeddings to learn mappings between domains and applying them at the lexical level (CM-Lex), and 2) deriving source/target pairs to train a controlled seq-to-seq generation model (CM-BART). We assess our methods through automatic and human evaluation for basic metaphoricity and conceptual metaphor presence. We show that the unsupervised CM-Lex model is competitive with recent deep learning metaphor generation systems, and CM-BART outperforms all other models both in automatic and human evaluations.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 52,
    "citationCount": 39,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "31600242",
        "name": "Kevin Stowe"
      },
      {
        "authorId": "51448832",
        "name": "Tuhin Chakrabarty"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "2295928",
        "name": "S. Muresan"
      },
      {
        "authorId": "1730400",
        "name": "Iryna Gurevych"
      }
    ]
  },
  "237491428": {
    "paperId": "bb583aef8ebaf53db6d01e39143c7998ec2bfb94",
    "externalIds": {
      "ArXiv": "2109.05097",
      "DBLP": "conf/emnlp/TianSP21",
      "DOI": "10.18653/v1/2021.findings-emnlp.136",
      "CorpusId": 237491428
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge",
    "abstract": "A hyperbole is an intentional and creative exaggeration not to be taken literally. Despite its ubiquity in daily life, the computational explorations of hyperboles are scarce. In this paper, we tackle the under-explored and challenging task: sentence-level hyperbole generation. We start with a representative syntactic pattern for intensification and systematically study the semantic (commonsense and counterfactual) relationships between each component in such hyperboles. Next, we leverage the COMeT and reverse COMeT models to do commonsense and counterfactual inference. We then generate multiple hyperbole candidates based on our findings from the pattern, and train neural classifiers to rank and select high-quality hyperboles. Automatic and human evaluations show that our generation method is able to generate hyperboles creatively with high success rate and intensity scores.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 29,
    "citationCount": 25,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48391421",
        "name": "Yufei Tian"
      },
      {
        "authorId": "2064327462",
        "name": "A. Sridhar"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      }
    ]
  },
  "248512960": {
    "paperId": "1ce77b594dac02d3d114fe2dba895852216825db",
    "externalIds": {
      "ACL": "2022.naacl-main.262",
      "DBLP": "conf/naacl/TianP22",
      "ArXiv": "2205.01821",
      "DOI": "10.48550/arXiv.2205.01821",
      "CorpusId": 248512960
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features",
    "abstract": "Poetry generation, and creative language generation in general, usually suffers from the lack of large training data. In this paper, we present a novel framework to generate sonnets that does not require training on poems. We design a hierarchical framework which plans the poem sketch before decoding. Specifically, a content planning module is trained on non-poetic texts to obtain discourse-level coherence; then a rhyme module generates rhyme words and a polishing module introduces imagery and similes for aesthetics purposes. Finally, we design a constrained decoding algorithm to impose the meter-and-rhyme constraint of the generated sonnets. Automatic and human evaluation show that our multi-stage approach without training on poem corpora generates more coherent, poetic, and creative sonnets than several strong baselines.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 35,
    "citationCount": 25,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Art",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48391421",
        "name": "Yufei Tian"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      }
    ]
  },
  "260484384": {
    "paperId": "1b69567bbde7aea5150bf48810b23b2c04512137",
    "externalIds": {
      "DBLP": "conf/naacl/Padmakumar022",
      "ArXiv": "2111.04193",
      "DOI": "10.18653/v1/2022.naacl-main.42",
      "CorpusId": 260484384
    },
    "publicationVenue": null,
    "title": "Machine-in-the-Loop Rewriting for Creative Image Captioning",
    "abstract": "Machine-in-the-loop writing aims to enable humans to collaborate with models to complete their writing tasks more effectively. Prior work has found that providing humans a machine-written draft or sentence-level continuations has limited success since the generated text tends to deviate from humans' intention. To allow the user to retain control over the content, we train a rewriting model that, when prompted, modifies specified spans of text within the user's original draft to introduce descriptive and figurative elements locally in the text. We evaluate the model on its ability to collaborate with humans on the task of creative image captioning. On a user study through Amazon Mechanical Turk, our model is rated to be more helpful than a baseline infilling language model. In addition, third-party evaluation shows that users write more descriptive and figurative captions when collaborating with our model compared to completing the task alone.",
    "venue": "NAACL-HLT",
    "year": 2021,
    "referenceCount": 34,
    "citationCount": 14,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Art",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2044959912",
        "name": "Vishakh Padmakumar"
      },
      {
        "authorId": "144533687",
        "name": "He He"
      }
    ]
  },
  "253107865": {
    "paperId": "77a94f6c91ee1590dd2c6fd80b4a6d8bffdb91ac",
    "externalIds": {
      "ArXiv": "2210.13669",
      "DBLP": "journals/corr/abs-2210-13669",
      "ACL": "2022.emnlp-main.460",
      "DOI": "10.48550/arXiv.2210.13669",
      "CorpusId": 253107865
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing",
    "abstract": "Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of large language models in the realm of computer assisted creativity, in this work, we present CoPoet, a collaborative poetry writing system, with the goal of to study if LLM\u2019s actually improve the quality of the generated content. In contrast to auto-completing a user\u2019s text, CoPoet is controlled by user instructions that specify the attributes of the desired text, such as Write a sentence about \u2018love\u2019 or Write a sentence ending in \u2018fly\u2019. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive to publicly available LLMs trained on instructions (InstructGPT), but also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from Monarchy to Climate change, which are preferred by third-party evaluators over poems written without the system.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 49,
    "citationCount": 58,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51448832",
        "name": "Tuhin Chakrabarty"
      },
      {
        "authorId": "2044959912",
        "name": "Vishakh Padmakumar"
      },
      {
        "authorId": "30835796",
        "name": "Hengxing He"
      }
    ]
  },
  "252873593": {
    "paperId": "2aab6ca1a8dae3f3db6d248231ac3fa4e222b30a",
    "externalIds": {
      "ACL": "2022.emnlp-main.296",
      "DBLP": "conf/emnlp/YangTPK22",
      "ArXiv": "2210.06774",
      "DOI": "10.48550/arXiv.2210.06774",
      "CorpusId": 252873593
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
    "abstract": "We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, long-range plot coherence and relevance are more central challenges here. We propose the Recursive Reprompting and Revision framework (Re3) to address these challenges by (a) prompting a general-purpose language model to construct a structured overarching plan, and (b) generating story passages by repeatedly injecting contextual information from both the plan and current story state into a language model prompt. We then revise by (c) reranking different continuations for plot coherence and premise relevance, and finally (d) editing the best continuation for factual consistency. Compared to similar-length stories generated directly from the same base model, human evaluators judged substantially more of Re3\u2019s stories as having a coherent overarching plot (by 14% absolute increase), and relevant to the given initial premise (by 20%).",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 0,
    "citationCount": 158,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1410652795",
        "name": "Kevin Yang"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "1932187449",
        "name": "Yuandong Tian"
      },
      {
        "authorId": "38666915",
        "name": "D. Klein"
      }
    ]
  },
  "8174610": {
    "paperId": "a3a8f179b8589ea51aa553722daf82f87210abc4",
    "externalIds": {
      "ACL": "D16-1126",
      "DBLP": "conf/emnlp/GhazvininejadSC16",
      "MAG": "2563845258",
      "DOI": "10.18653/v1/D16-1126",
      "CorpusId": 8174610
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Generating Topical Poetry",
    "abstract": "We describe Hafez, a program that generates any number of distinct poems on a user-supplied topic. Poems obey rhythmic and rhyme constraints. We describe the poetry-generation algorithm, give experimental data concerning its parameters, and show its generality with respect to language and poetic form.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2016,
    "referenceCount": 31,
    "citationCount": 147,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2320509",
        "name": "Marjan Ghazvininejad"
      },
      {
        "authorId": "143794227",
        "name": "Xing Shi"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      },
      {
        "authorId": "152971314",
        "name": "Kevin Knight"
      }
    ]
  },
  "44134226": {
    "paperId": "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6",
    "externalIds": {
      "MAG": "2798664956",
      "ArXiv": "1805.04833",
      "ACL": "P18-1082",
      "DBLP": "journals/corr/abs-1805-04833",
      "DOI": "10.18653/v1/P18-1082",
      "CorpusId": 44134226
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Hierarchical Neural Story Generation",
    "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 33,
    "citationCount": 1441,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144270981",
        "name": "Angela Fan"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "2921469",
        "name": "Yann Dauphin"
      }
    ]
  },
  "59599706": {
    "paperId": "2d6c0f7774d9d30d4972f5dba1d6e5389b3ddd2f",
    "externalIds": {
      "MAG": "2914855263",
      "DBLP": "journals/corr/abs-1902-01109",
      "ACL": "P19-1254",
      "ArXiv": "1902.01109",
      "DOI": "10.18653/v1/P19-1254",
      "CorpusId": 59599706
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Strategies for Structuring Story Generation",
    "abstract": "Writers often rely on plans or sketches to write long stories, but most current language models generate word by word from left to right. We explore coarse-to-fine models for creating narrative texts of several hundred words, and introduce new models which decompose stories by abstracting over actions and entities. The model first generates the predicate-argument structure of the text, where different mentions of the same entity are marked with placeholder tokens. It then generates a surface realization of the predicate-argument structure, and finally replaces the entity placeholders with context-sensitive names and references. Human judges prefer the stories from our models to a wide range of previous approaches to hierarchical text generation. Extensive analysis shows that our methods can help improve the diversity and coherence of events and entities in generated stories.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 48,
    "citationCount": 207,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144270981",
        "name": "Angela Fan"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "2921469",
        "name": "Yann Dauphin"
      }
    ]
  },
  "220046853": {
    "paperId": "f4b33eda909c873c519fce390c3fd66a568c2e27",
    "externalIds": {
      "ACL": "2020.acl-main.223",
      "MAG": "3034807061",
      "DBLP": "conf/acl/Cruys20",
      "DOI": "10.18653/v1/2020.acl-main.223",
      "CorpusId": 220046853
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Automatic Poetry Generation from Prosaic Text",
    "abstract": "In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language. In particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions. In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation. The system is exclusively trained on standard, non-poetic text, and its output is constrained in order to confer a poetic character to the generated verse. The framework is applied to the generation of poems in both English and French, and is equally evaluated for both languages. Even though it only uses standard, non-poetic text as input, the system yields state of the art results for poetry generation.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 30,
    "citationCount": 54,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1885890",
        "name": "T. V. D. Cruys"
      }
    ]
  },
  "5883024": {
    "paperId": "6ef6adf8777c50c4f4f0a7cc7804cf9d3317084b",
    "externalIds": {
      "MAG": "2170516265",
      "DBLP": "journals/corr/RiedlY14",
      "ArXiv": "1401.3841",
      "DOI": "10.1613/jair.2989",
      "CorpusId": 5883024
    },
    "publicationVenue": {
      "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
      "name": "Journal of Artificial Intelligence Research",
      "type": "journal",
      "alternate_names": [
        "JAIR",
        "J Artif Intell Res",
        "The Journal of Artificial Intelligence Research"
      ],
      "issn": "1076-9757",
      "url": "http://www.jair.org/"
    },
    "title": "Narrative Planning: Balancing Plot and Character",
    "abstract": "Narrative, and in particular storytelling, is an important part of the human experience. Consequently, computational systems that can reason about narrative can be more effective communicators, entertainers, educators, and trainers. One of the central challenges in computational narrative reasoning is narrative generation, the automated creation of meaningful event sequences. There are many factors - logical and aesthetic - that contribute to the success of a narrative artifact. Central to this success is its understandability. We argue that the following two attributes of narratives are universal: (a) the logical causal progression of plot, and (b) character believability. Character believability is the perception by the audience that the actions performed by characters do not negatively impact the audience's suspension of disbelief. Specifically, characters must be perceived by the audience to be intentional agents. In this article, we explore the use of refinement search as a technique for solving the narrative generation problem - to find a sound and believable sequence of character actions that transforms an initial world state into a world state in which goal propositions hold. We describe a novel refinement search planning algorithm - the Intent-based Partial Order Causal Link (IPOCL) planner - that, in addition to creating causally sound plot progression, reasons about character intentionality by identifying possible character goals that explain their actions and creating plan structures that explain why those characters commit to their goals. We present the results of an empirical evaluation that demonstrates that narrative plans generated by the IPOCL algorithm support audience comprehension of character intentions better than plans generated by conventional partial-order planners.",
    "venue": "Journal of Artificial Intelligence Research",
    "year": 2010,
    "referenceCount": 95,
    "citationCount": 558,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2757194",
        "name": "Mark O. Riedl"
      },
      {
        "authorId": "145513579",
        "name": "R. Young"
      }
    ]
  },
  "233204282": {
    "paperId": "89ffb2586b23561676e821187aa8fb3bab73bb7f",
    "externalIds": {
      "ACL": "2021.nuse-1.7",
      "DBLP": "journals/corr/abs-2104-04039",
      "MAG": "3168256335",
      "DOI": "10.18653/V1/2021.NUSE-1.7",
      "CorpusId": 233204282
    },
    "publicationVenue": null,
    "title": "Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes",
    "abstract": "We describe a Plug-and-Play controllable language generation framework, Plug-and-Blend, that allows a human user to input multiple control codes (topics). In the context of automated story generation, this allows a human user lose or fine grained control of the topics that will appear in the generated story, and can even allow for overlapping, blended topics. We show that our framework, working with different generation models, controls the generation towards given continuous-weighted control codes while keeping the generated sentences fluent, demonstrating strong blending capability.",
    "venue": "NUSE",
    "year": 2021,
    "referenceCount": 26,
    "citationCount": 24,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2108651163",
        "name": "Zhiyu Lin"
      },
      {
        "authorId": "2065904932",
        "name": "Mark O. Riedl"
      }
    ]
  },
  "222341855": {
    "paperId": "63c1abce10c354c64b9d521ee959f9d3dd409169",
    "externalIds": {
      "DBLP": "conf/emnlp/BrahmanC20",
      "MAG": "3092724964",
      "ArXiv": "2010.06822",
      "ACL": "2020.emnlp-main.426",
      "DOI": "10.18653/v1/2020.emnlp-main.426",
      "CorpusId": 222341855
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Modeling Protagonist Emotions for Emotion-Aware Storytelling",
    "abstract": "Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist. Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning. Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 91,
    "citationCount": 48,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "9252833",
        "name": "Faeze Brahman"
      },
      {
        "authorId": "37202877",
        "name": "Snigdha Chaturvedi"
      }
    ]
  },
  "252596159": {
    "paperId": "f9b16559282e5bea0bb072f9e260a4f0af697f4a",
    "externalIds": {
      "DBLP": "conf/chi/MirowskiMPE23",
      "ArXiv": "2209.14958",
      "DOI": "10.1145/3544548.3581225",
      "CorpusId": 252596159
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals",
    "abstract": "Language models are increasingly attracting interest from writers. However, such models lack long-range semantic coherence, limiting their usefulness for longform creative writing. We address this limitation by applying language models hierarchically, in a system we call Dramatron. By building structural context via prompt chaining, Dramatron can generate coherent scripts and screenplays complete with title, characters, story beats, location descriptions, and dialogue. We illustrate Dramatron\u2019s usefulness as an interactive co-creative system with a user study of 15 theatre and film industry professionals. Participants co-wrote theatre scripts and screenplays with Dramatron and engaged in open-ended interviews. We report reflections both from our interviewees and from independent reviewers who critiqued performances of several of the scripts to illustrate how both Dramatron and hierarchical text generation could be useful for human-machine co-creativity. Finally, we discuss the suitability of Dramatron for co-creativity, ethical considerations\u2014including plagiarism and bias\u2014and participatory models for the design and deployment of such tools.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2022,
    "referenceCount": 142,
    "citationCount": 176,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Art",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144705062",
        "name": "Piotr Wojciech Mirowski"
      },
      {
        "authorId": "2034344309",
        "name": "K. Mathewson"
      },
      {
        "authorId": "2053696095",
        "name": "Jaylen Pittman"
      },
      {
        "authorId": "2186406258",
        "name": "Richard Evans"
      }
    ]
  },
  "219955663": {
    "paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a",
    "externalIds": {
      "MAG": "3100572490",
      "DBLP": "conf/nips/HoJA20",
      "ArXiv": "2006.11239",
      "CorpusId": 219955663
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Denoising Diffusion Probabilistic Models",
    "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "referenceCount": 73,
    "citationCount": 11926,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Physics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2126278",
        "name": "Jonathan Ho"
      },
      {
        "authorId": "1623995772",
        "name": "Ajay Jain"
      },
      {
        "authorId": "1689992",
        "name": "P. Abbeel"
      }
    ]
  },
  "247748611": {
    "paperId": "a747e8f2659df479c0092301b9658fc582423df1",
    "externalIds": {
      "DBLP": "conf/acl/AjiWKCRMKMPBLR22",
      "ACL": "2022.acl-long.500",
      "ArXiv": "2203.13357",
      "DOI": "10.48550/arXiv.2203.13357",
      "CorpusId": 247748611
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia",
    "abstract": "NLP research is impeded by a lack of resources and awareness of the challenges presented by underrepresented languages and dialects. Focusing on the languages spoken in Indonesia, the second most linguistically diverse and the fourth most populous nation of the world, we provide an overview of the current state of NLP research for Indonesia\u2019s 700+ languages. We highlight challenges in Indonesian NLP and how these affect the performance of current NLP systems. Finally, we provide general recommendations to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 213,
    "citationCount": 75,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8129718",
        "name": "Alham Fikri Aji"
      },
      {
        "authorId": "9162688",
        "name": "Genta Indra Winata"
      },
      {
        "authorId": "2789148",
        "name": "Fajri Koto"
      },
      {
        "authorId": "66986482",
        "name": "Samuel Cahyawijaya"
      },
      {
        "authorId": "2279712392",
        "name": "Ade Romadhony"
      },
      {
        "authorId": "1935324",
        "name": "Rahmad Mahendra"
      },
      {
        "authorId": "46199596",
        "name": "Kemal Kurniawan"
      },
      {
        "authorId": "35722593",
        "name": "David Moeljadi"
      },
      {
        "authorId": "2368148",
        "name": "Radityo Eko Prasojo"
      },
      {
        "authorId": "145465286",
        "name": "Timothy Baldwin"
      },
      {
        "authorId": "1800564",
        "name": "Jey Han Lau"
      },
      {
        "authorId": "2884561",
        "name": "Sebastian Ruder"
      }
    ]
  },
  "236460241": {
    "paperId": "ee6d66efc86746d42ace14db30fcbaf9d3380e25",
    "externalIds": {
      "ArXiv": "2301.01967",
      "DBLP": "conf/acl/DogruozSBT20",
      "ACL": "2021.acl-long.131",
      "DOI": "10.18653/v1/2021.acl-long.131",
      "CorpusId": 236460241
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies",
    "abstract": "The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. So far, much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in linguistics. To fill this gap, we offer a survey of code-switching (C-S) covering the literature in linguistics with a reflection on the key issues in language technologies. From the linguistic perspective, we provide an overview of structural and functional patterns of C-S focusing on the literature from European and Indian contexts as highly multilingual areas. From the language technologies perspective, we discuss how massive language models fail to represent diverse C-S types due to lack of appropriate training data, lack of robust evaluation benchmarks for C-S (across multilingual situations and types of C-S) and lack of end-to- end systems that cover sociolinguistic aspects of C-S as well. Our survey will be a step to- wards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and C-S.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 116,
    "citationCount": 64,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1904399",
        "name": "A. Seza Do\u011fru\u00f6z"
      },
      {
        "authorId": "3010457",
        "name": "Sunayana Sitaram"
      },
      {
        "authorId": "32383682",
        "name": "Barbara E. Bullock"
      },
      {
        "authorId": "8770367",
        "name": "Almeida Jacqueline Toribio"
      }
    ]
  },
  "252184818": {
    "paperId": "b592a963ca5383fbb8aa4e9db3ae82e298f7fff1",
    "externalIds": {
      "ACL": "2022.sigul-1.12",
      "CorpusId": 252184818
    },
    "publicationVenue": null,
    "title": "Language Technologies for Low Resource Languages: Sociolinguistic and Multilingual Insights",
    "abstract": "There is a growing interest in building language technologies (LTs) for low resource languages (LRLs). However, there are flaws in the planning, data collection and development phases mostly due to the assumption that LRLs are similar to High Resource Languages (HRLs) but only smaller in size. In our paper, we first provide examples of failed LTs for LRLs and provide the reasons for these failures. Second, we discuss the problematic issues with the data for LRLs. Finally, we provide recommendations for building better LTs for LRLs through insights from sociolinguistics and multilingualism. Our goal is not to solve all problems around LTs for LRLs but to raise awareness about the existing issues, provide recommendations toward possible solutions and encourage collaboration across academic disciplines for developing LTs that actually serve the needs and preferences of the LRL communities.",
    "venue": "SIGUL",
    "year": 2022,
    "referenceCount": 31,
    "citationCount": 11,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1904399",
        "name": "A. Seza Do\u011fru\u00f6z"
      },
      {
        "authorId": "3010457",
        "name": "Sunayana Sitaram"
      }
    ]
  },
  "252624636": {
    "paperId": "7625e596f3c6fba28e57afabf70dcf6e6bed7718",
    "externalIds": {
      "ACL": "2022.lrec-1.697",
      "DBLP": "conf/lrec/ArreerardMP22",
      "CorpusId": 252624636
    },
    "publicationVenue": {
      "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
      "name": "International Conference on Language Resources and Evaluation",
      "type": "conference",
      "alternate_names": [
        "LREC",
        "Int Conf Lang Resour Evaluation"
      ],
      "url": "http://www.lrec-conf.org/"
    },
    "title": "Survey on Thai NLP Language Resources and Tools",
    "abstract": "Over the past decades, Natural Language Processing (NLP) research has been expanding to cover more languages. Recently particularly, NLP community has paid increasing attention to under-resourced languages. However, there are still many languages for which NLP research is limited in terms of both language resources and software tools. Thai language is one of the under-resourced languages in the NLP domain, although it is spoken by nearly 70 million people globally. In this paper, we report on our survey on the past development of Thai NLP research to help understand its current state and future research directions. Our survey shows that, although Thai NLP community has achieved a significant achievement over the past three decades, particularly on NLP upstream tasks such as tokenisation, research on downstream tasks such as syntactic parsing and semantic analysis is still limited. But we foresee that Thai NLP research will advance rapidly as richer Thai language resources and more robust NLP techniques become available.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2022,
    "referenceCount": 51,
    "citationCount": 10,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51913103",
        "name": "Ratchakrit Arreerard"
      },
      {
        "authorId": "2186552673",
        "name": "Stephen Mander"
      },
      {
        "authorId": "2068413302",
        "name": "S. Piao"
      }
    ]
  },
  "45262369": {
    "paperId": "61fa36ae4d2f4608c8c2facd4a8f88b052e36d31",
    "externalIds": {
      "MAG": "2153088403",
      "DOI": "10.1146/ANNUREV.ANTHRO.34.081804.120406",
      "CorpusId": 45262369
    },
    "publicationVenue": null,
    "title": "AREAL LINGUISTICS AND MAINLAND SOUTHEAST ASIA",
    "abstract": "AbstractMainland Southeast Asia provides a dramatic demonstration of the areal phenomenon in linguistics: When languages are spoken historically in the same location they often show significant parallels in the organization of a wide range of structural domains, whether the languages descend from the same historical source. The effects of areal diffusion raise fundamental questions for the traditional essentialist vision of languages as entities with offspring that diverge, with shared innovations marking divergent branches and internal processes of evolution accounting for diversity among modern languages. Recent theoretical and empirical research on linguistic diversity, language change, and social diffusion of innovation argues for a unit-based approach to language change and relatedness, where the units of analysis are individual speakers and individual linguistic items. This review begins with discussion of the language situation in Mainland Southeast Asia, where the language \u201cgenealogies\u201d have been ...",
    "venue": "",
    "year": 2005,
    "referenceCount": 111,
    "citationCount": 171,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Sociology",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3247678",
        "name": "N. Enfield"
      }
    ]
  },
  "250425961": {
    "paperId": "e19b54ad4c1c8af045069e9cac350ffc2ce60e1a",
    "externalIds": {
      "ArXiv": "2207.04672",
      "DBLP": "journals/corr/abs-2207-04672",
      "DOI": "10.48550/arXiv.2207.04672",
      "CorpusId": 250425961
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "No Language Left Behind: Scaling Human-Centered Machine Translation",
    "abstract": "Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 0,
    "citationCount": 927,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2175650427",
        "name": "Nllb team"
      },
      {
        "authorId": "1398996347",
        "name": "M. Costa-juss\u00e0"
      },
      {
        "authorId": "2059363961",
        "name": "James Cross"
      },
      {
        "authorId": "2166310112",
        "name": "Onur cCelebi"
      },
      {
        "authorId": "46183659",
        "name": "Maha Elbayad"
      },
      {
        "authorId": "1702066",
        "name": "Kenneth Heafield"
      },
      {
        "authorId": "47926975",
        "name": "Kevin Heffernan"
      },
      {
        "authorId": "2175649909",
        "name": "Elahe Kalbassi"
      },
      {
        "authorId": "82469889",
        "name": "Janice Lam"
      },
      {
        "authorId": "2082021589",
        "name": "Daniel Licht"
      },
      {
        "authorId": "40148380",
        "name": "Jean Maillard"
      },
      {
        "authorId": "2091912142",
        "name": "Anna Sun"
      },
      {
        "authorId": "2175975228",
        "name": "Skyler Wang"
      },
      {
        "authorId": "2293203",
        "name": "Guillaume Wenzek"
      },
      {
        "authorId": "118325632",
        "name": "Alison Youngblood"
      },
      {
        "authorId": "2175649907",
        "name": "Bapi Akula"
      },
      {
        "authorId": "2934336",
        "name": "Lo\u00efc Barrault"
      },
      {
        "authorId": "2175804876",
        "name": "Gabriel Mejia Gonzalez"
      },
      {
        "authorId": "2175651102",
        "name": "Prangthip Hansanti"
      },
      {
        "authorId": "2055094870",
        "name": "John Hoffman"
      },
      {
        "authorId": "2175650054",
        "name": "Semarley Jarrett"
      },
      {
        "authorId": "1914544232",
        "name": "Kaushik Ram Sadagopan"
      },
      {
        "authorId": "2175650209",
        "name": "Dirk Rowe"
      },
      {
        "authorId": "3416737",
        "name": "Shannon L. Spruit"
      },
      {
        "authorId": "33806547",
        "name": "C. Tran"
      },
      {
        "authorId": "5657660",
        "name": "Pierre Yves Andrews"
      },
      {
        "authorId": "1769060",
        "name": "Necip Fazil Ayan"
      },
      {
        "authorId": "2116473",
        "name": "Shruti Bhosale"
      },
      {
        "authorId": "2068070",
        "name": "Sergey Edunov"
      },
      {
        "authorId": "144270981",
        "name": "Angela Fan"
      },
      {
        "authorId": "2107063269",
        "name": "Cynthia Gao"
      },
      {
        "authorId": "28554843",
        "name": "Vedanuj Goswami"
      },
      {
        "authorId": "2061585840",
        "name": "Francisco Guzm'an"
      },
      {
        "authorId": "1755162",
        "name": "Philipp Koehn"
      },
      {
        "authorId": "2175651049",
        "name": "Alexandre Mourachko"
      },
      {
        "authorId": "146424711",
        "name": "C. Ropers"
      },
      {
        "authorId": "2475227",
        "name": "Safiyyah Saleem"
      },
      {
        "authorId": "144518416",
        "name": "Holger Schwenk"
      },
      {
        "authorId": "2155451431",
        "name": "Jeff Wang"
      }
    ]
  },
  "3004919": {
    "paperId": "f25baf281c13207c2459b0264aa3fa30212ab5e8",
    "externalIds": {
      "MAG": "2791522043",
      "ACL": "W16-3708",
      "DBLP": "conf/wssanlp/LapitanBA16",
      "CorpusId": 3004919
    },
    "publicationVenue": null,
    "title": "Crowdsourcing-based Annotation of Emotions in Filipino and English Tweets",
    "abstract": "The automatic analysis of emotions conveyed in social media content, e.g., tweets, has many beneficial applications. In the Philippines, one of the most disaster-prone countries in the world, such methods could potentially enable first responders to make timely decisions despite the risk of data deluge. However, recognising emotions expressed in Philippine-generated tweets, which are mostly written in Filipino, English or a mix of both, is a non-trivial task. In order to facilitate the development of natural language processing (NLP) methods that will automate such type of analysis, we have built a corpus of tweets whose predominant emotions have been manually annotated by means of crowdsourcing. Defining measures ensuring that only high-quality annotations were retained, we have produced a gold standard corpus of 1,146 emotion-labelled Filipino and English tweets. We validate the value of this manually produced resource by demonstrating that an automatic emotion-prediction method based on the use of a publicly available word-emotion association lexicon was unable to reproduce the labels assigned via crowdsourcing. While we are planning to make a few extensions to the corpus in the near future, its current version has been made publicly available in order to foster the development of emotion analysis methods based on advanced Filipino and English NLP.",
    "venue": "WSSANLP@COLING",
    "year": 2016,
    "referenceCount": 14,
    "citationCount": 16,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35645750",
        "name": "F. Lapitan"
      },
      {
        "authorId": "1400900759",
        "name": "R. Batista-Navarro"
      },
      {
        "authorId": "2674576",
        "name": "E. Albacea"
      }
    ]
  },
  "5631708": {
    "paperId": "f28cb37e0f1a225f0d4f27f43ef4e05eee8b321c",
    "externalIds": {
      "MAG": "197277712",
      "DBLP": "conf/interspeech/LyuTCL10",
      "DOI": "10.21437/Interspeech.2010-563",
      "CorpusId": 5631708
    },
    "publicationVenue": {
      "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
      "name": "Interspeech",
      "type": "conference",
      "alternate_names": [
        "Conf Int Speech Commun Assoc",
        "INTERSPEECH",
        "Conference of the International Speech Communication Association"
      ],
      "issn": "2308-457X",
      "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
      "alternate_urls": [
        "http://www.isca-speech.org/"
      ]
    },
    "title": "SEAME: a Mandarin-English code-switching speech corpus in south-east asia",
    "abstract": "In Singapore and Malaysia, people often speak a mixture of Mandarin and English within a single sentence. We call such sentences intra-sentential code-switch sentences. In this paper, we report on the development of a Mandarin-English codeswitching spontaneous speech corpus: SEAME. The corpus is developed as part of a multilingual speech recognition project and will be used to examine how Mandarin-English codeswitch speech occurs in the spoken language in South-East Asia. Additionally, it can provide insights into the development of large vocabulary continuous speech recognition (LVCSR) for code-switching speech. The corpus collected consists of intra-sentential code-switching utterances that are recorded under both interview and conversational settings. This paper describes the corpus design and the analysis of collected corpus.",
    "venue": "Interspeech",
    "year": 2010,
    "referenceCount": 13,
    "citationCount": 131,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1719058",
        "name": "Dau-Cheng Lyu"
      },
      {
        "authorId": "1805622",
        "name": "T. Tan"
      },
      {
        "authorId": "1742722",
        "name": "Chng Eng Siong"
      },
      {
        "authorId": "1711271",
        "name": "Haizhou Li"
      }
    ]
  },
  "249209909": {
    "paperId": "11f64ec047782cada21d50efea1e0dc5843675f6",
    "externalIds": {
      "ACL": "2023.eacl-main.57",
      "DBLP": "journals/corr/abs-2205-15960",
      "ArXiv": "2205.15960",
      "DOI": "10.48550/arXiv.2205.15960",
      "CorpusId": 249209909
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages",
    "abstract": "Natural language processing (NLP) has a significant impact on society via technologies such as machine translation and search engines. Despite its success, NLP technology is only widely available for high-resource languages such as English and Chinese, while it remains inaccessible to many languages due to the unavailability of data resources and benchmarks. In this work, we focus on developing resources for languages in Indonesia. Despite being the second most linguistically diverse country, most languages in Indonesia are categorized as endangered and some are even extinct. We develop the first-ever parallel resource for 10 low-resource languages in Indonesia. Our resource includes sentiment and machine translation datasets, and bilingual lexicons. We provide extensive analyses and describe challenges for creating such resources. We hope this work can spark NLP research on Indonesian and other underrepresented languages.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 85,
    "citationCount": 66,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "9162688",
        "name": "Genta Indra Winata"
      },
      {
        "authorId": "8129718",
        "name": "Alham Fikri Aji"
      },
      {
        "authorId": "66986482",
        "name": "Samuel Cahyawijaya"
      },
      {
        "authorId": "1935324",
        "name": "Rahmad Mahendra"
      },
      {
        "authorId": "2789148",
        "name": "Fajri Koto"
      },
      {
        "authorId": "2279712392",
        "name": "Ade Romadhony"
      },
      {
        "authorId": "46199596",
        "name": "Kemal Kurniawan"
      },
      {
        "authorId": "35722593",
        "name": "David Moeljadi"
      },
      {
        "authorId": "2368148",
        "name": "Radityo Eko Prasojo"
      },
      {
        "authorId": "40539650",
        "name": "Pascale Fung"
      },
      {
        "authorId": "145465286",
        "name": "Timothy Baldwin"
      },
      {
        "authorId": "1800564",
        "name": "Jey Han Lau"
      },
      {
        "authorId": "2082372",
        "name": "Rico Sennrich"
      },
      {
        "authorId": "2884561",
        "name": "Sebastian Ruder"
      }
    ]
  },
  "253762058": {
    "paperId": "940fc621079ea349109202c7d705461b50d541d8",
    "externalIds": {
      "DBLP": "conf/ijcnlp/WinataWKSP22",
      "ACL": "2022.aacl-main.59",
      "CorpusId": 253762058
    },
    "publicationVenue": null,
    "title": "Cross-lingual Few-Shot Learning on Unseen Languages",
    "abstract": "Large pre-trained language models (LMs) have demonstrated the ability to obtain good performance on downstream tasks with limited examples in cross-lingual settings. However, this was mostly studied for relatively resource-rich languages, where at least enough unlabeled data is available to be included in pre-training a multilingual language model. In this paper, we explore the problem of cross-lingual transfer in unseen languages, where no unlabeled data is available for pre-training a model. We use a downstream sentiment analysis task across 12 languages, including 8 unseen languages, to analyze the effectiveness of several few-shot learning strategies across the three major types of model architectures and their learning dynamics. We also compare strategies for selecting languages for transfer and contrast findings across languages seen in pre-training compared to those that are not. Our findings contribute to the body of knowledge on cross-lingual models for low-resource settings that is paramount to increasing coverage, diversity, and equity in access to NLP technology. We show that, in few-shot learning, linguistically similar and geographically similar languages are useful for cross-lingual adaptation, but taking the context from a mixture of random source languages is surprisingly more effective. We also compare different model architectures and show that the encoder-only model, XLM-R, gives the best downstream task performance.",
    "venue": "AACL",
    "year": 2022,
    "referenceCount": 57,
    "citationCount": 30,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "9162688",
        "name": "Genta Indra Winata"
      },
      {
        "authorId": "50425845",
        "name": "Shijie Wu"
      },
      {
        "authorId": "145304589",
        "name": "Mayank Kulkarni"
      },
      {
        "authorId": "1794626",
        "name": "T. Solorio"
      },
      {
        "authorId": "1398830377",
        "name": "Daniel Preotiuc-Pietro"
      }
    ]
  },
  "254853901": {
    "paperId": "03c19ddaa26068f23e27ba94b10f08160e87f668",
    "externalIds": {
      "ArXiv": "2212.09648",
      "DBLP": "journals/corr/abs-2212-09648",
      "DOI": "10.48550/arXiv.2212.09648",
      "CorpusId": 254853901
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "NusaCrowd: Open Source Initiative for Indonesian NLP Resources",
    "abstract": "We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments. NusaCrowd's data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 200,
    "citationCount": 34,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "66986482",
        "name": "Samuel Cahyawijaya"
      },
      {
        "authorId": "116344405",
        "name": "Holy Lovenia"
      },
      {
        "authorId": "8129718",
        "name": "Alham Fikri Aji"
      },
      {
        "authorId": "9162688",
        "name": "Genta Indra Winata"
      },
      {
        "authorId": "150048491",
        "name": "Bryan Wilie"
      },
      {
        "authorId": "1935324",
        "name": "Rahmad Mahendra"
      },
      {
        "authorId": "104768157",
        "name": "C. Wibisono"
      },
      {
        "authorId": "2279712392",
        "name": "Ade Romadhony"
      },
      {
        "authorId": "1939999507",
        "name": "Karissa Vincentio"
      },
      {
        "authorId": "2789148",
        "name": "Fajri Koto"
      },
      {
        "authorId": "117696399",
        "name": "Jennifer Santoso"
      },
      {
        "authorId": "35722593",
        "name": "David Moeljadi"
      },
      {
        "authorId": "2197090678",
        "name": "Cahya Wirawan"
      },
      {
        "authorId": "2197090652",
        "name": "Frederikus Hudi"
      },
      {
        "authorId": "134112343",
        "name": "Ivan Halim Parmonangan"
      },
      {
        "authorId": "2683858",
        "name": "Ika Alfina"
      },
      {
        "authorId": "2196919922",
        "name": "Muhammad Satrio Wicaksono"
      },
      {
        "authorId": "1943296899",
        "name": "Ilham Firdausi Putra"
      },
      {
        "authorId": "2197090705",
        "name": "Samsul Rahmadani"
      },
      {
        "authorId": "9128778",
        "name": "Yulianti Oenang"
      },
      {
        "authorId": "22171680",
        "name": "Ali Akbar Septiandri"
      },
      {
        "authorId": "2197071075",
        "name": "James Jaya"
      },
      {
        "authorId": "4834571",
        "name": "Kaustubh D. Dhole"
      },
      {
        "authorId": "9366773",
        "name": "Arie A. Suryani"
      },
      {
        "authorId": "9358635",
        "name": "Rifki Afina Putri"
      },
      {
        "authorId": "144610224",
        "name": "Dan Su"
      },
      {
        "authorId": "144077726",
        "name": "K. Stevens"
      },
      {
        "authorId": "66436856",
        "name": "Made Nindyatama Nityasya"
      },
      {
        "authorId": "2191731497",
        "name": "Muhammad Farid Adilazuarda"
      },
      {
        "authorId": "2197071063",
        "name": "Ryan Ignatius"
      },
      {
        "authorId": "2197070752",
        "name": "Ryandito Diandaru"
      },
      {
        "authorId": "1660855299",
        "name": "Tiezheng Yu"
      },
      {
        "authorId": "2197070698",
        "name": "Vito Ghifari"
      },
      {
        "authorId": "47653392",
        "name": "Wenliang Dai"
      },
      {
        "authorId": "98271906",
        "name": "Yan Xu"
      },
      {
        "authorId": "2197071047",
        "name": "Dyah Damapuspita"
      },
      {
        "authorId": "120064613",
        "name": "C. Tho"
      },
      {
        "authorId": "18159304",
        "name": "I. M. K. Karo"
      },
      {
        "authorId": "36045311",
        "name": "Tirana Noor Fatyanosa"
      },
      {
        "authorId": "3391272",
        "name": "Ziwei Ji"
      },
      {
        "authorId": "2057151752",
        "name": "Pascale Fung"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "145465286",
        "name": "Timothy Baldwin"
      },
      {
        "authorId": "2884561",
        "name": "Sebastian Ruder"
      },
      {
        "authorId": "2085419515",
        "name": "Herry Sujaini"
      },
      {
        "authorId": "1783949",
        "name": "S. Sakti"
      },
      {
        "authorId": "1962263",
        "name": "A. Purwarianti"
      }
    ]
  },
  "252819207": {
    "paperId": "2abfa04644b89342ddde7a40068b673d8b23bd13",
    "externalIds": {
      "DBLP": "conf/coling/RomadhonaLLT22",
      "ACL": "2022.coling-1.389",
      "CorpusId": 252819207
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "BRCC and SentiBahasaRojak: The First Bahasa Rojak Corpus for Pretraining and Sentiment Analysis Dataset",
    "abstract": "Code-mixing refers to the mixed use of multiple languages. It is prevalent in multilingual societies and is also one of the most challenging natural language processing tasks. In this paper, we study Bahasa Rojak, a dialect popular in Malaysia that consists of English, Malay, and Chinese. Aiming to establish a model to deal with the code-mixing phenomena of Bahasa Rojak, we use data augmentation to automatically construct the first Bahasa Rojak corpus for pre-training language models, which we name the Bahasa Rojak Crawled Corpus (BRCC). We also develop a new pre-trained model called \u201cMixed XLM\u201d. The model can tag the language of the input token automatically to process code-mixing input. Finally, to test the effectiveness of the Mixed XLM model pre-trained on BRCC for social media scenarios where code-mixing is found frequently, we compile a new Bahasa Rojak sentiment analysis dataset, SentiBahasaRojak, with a Kappa value of 0.77.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2022,
    "referenceCount": 39,
    "citationCount": 4,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2187454749",
        "name": "Nanda Putri Romadhona"
      },
      {
        "authorId": "2135796743",
        "name": "Sin-En Lu"
      },
      {
        "authorId": "2187456195",
        "name": "Bo-Han Lu"
      },
      {
        "authorId": "1724351",
        "name": "Richard Tzong-Han Tsai"
      }
    ]
  },
  "226226744": {
    "paperId": "1109d62ebd2b29a7dc148bc30dd6cfc803a63dec",
    "externalIds": {
      "DBLP": "journals/corr/abs-2011-00677",
      "MAG": "3110149585",
      "ACL": "2020.coling-main.66",
      "ArXiv": "2011.00677",
      "DOI": "10.18653/V1/2020.COLING-MAIN.66",
      "CorpusId": 226226744
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP",
    "abstract": "Although the Indonesian language is spoken by almost 200 million people and the 10th most spoken language in the world, it is under-represented in NLP research. Previous work on Indonesian has been hampered by a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In this work, we release the IndoLEM dataset comprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. We additionally release IndoBERT, a new pre-trained language model for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it against existing resources. Our experiments show that IndoBERT achieves state-of-the-art performance over most of the tasks in IndoLEM.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2020,
    "referenceCount": 66,
    "citationCount": 194,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2789148",
        "name": "Fajri Koto"
      },
      {
        "authorId": "2953039",
        "name": "Afshin Rahimi"
      },
      {
        "authorId": "1800564",
        "name": "Jey Han Lau"
      },
      {
        "authorId": "145465286",
        "name": "Timothy Baldwin"
      }
    ]
  },
  "211677475": {
    "paperId": "a622332550eaf535cf0f0f6c3a3f3ba197c39cac",
    "externalIds": {
      "MAG": "3007955273",
      "ArXiv": "2003.00744",
      "DBLP": "journals/corr/abs-2003-00744",
      "ACL": "2020.findings-emnlp.92",
      "DOI": "10.18653/v1/2020.findings-emnlp.92",
      "CorpusId": 211677475
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "PhoBERT: Pre-trained language models for Vietnamese",
    "abstract": "We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at https://github.com/VinAIResearch/PhoBERT",
    "venue": "Findings",
    "year": 2020,
    "referenceCount": 39,
    "citationCount": 305,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34691913",
        "name": "Dat Quoc Nguyen"
      },
      {
        "authorId": "1398541475",
        "name": "A. Nguyen"
      }
    ]
  },
  "231698737": {
    "paperId": "86b369759bff13ec23b45ca23cc5461292a75415",
    "externalIds": {
      "ArXiv": "2101.09635",
      "DBLP": "journals/corr/abs-2101-09635",
      "CorpusId": 231698737
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "WangchanBERTa: Pretraining transformer-based Thai Language Models",
    "abstract": "Transformer-based language models, more specifically BERT-based architectures have achieved state-of-the-art performance in many downstream tasks. However, for a relatively low-resource language such as Thai, the choices of models are limited to training a BERT-based model based on a much smaller dataset or finetuning multi-lingual models, both of which yield suboptimal downstream performance. Moreover, large-scale multi-lingual pretraining does not take into account language-specific features for Thai. To overcome these limitations, we pretrain a language model based on RoBERTa-base architecture on a large, deduplicated, cleaned training set (78GB in total size), curated from diverse domains of social media posts, news articles and other publicly available datasets. We apply text processing rules that are specific to Thai most importantly preserving spaces, which are important chunk and sentence boundaries in Thai before subword tokenization. We also experiment with word-level, syllable-level and SentencePiece tokenization with a smaller dataset to explore the effects on tokenization on downstream performance. Our model wangchanberta-base-att-spm-uncased trained on the 78.5GB dataset outperforms strong baselines (NBSVM, CRF and ULMFit) and multi-lingual models (XLMR and mBERT) on both sequence classification and token classification tasks in human-annotated, mono-lingual contexts.",
    "venue": "arXiv.org",
    "year": 2021,
    "referenceCount": 42,
    "citationCount": 65,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1796312599",
        "name": "Lalita Lowphansirikul"
      },
      {
        "authorId": "122556419",
        "name": "Charin Polpanumas"
      },
      {
        "authorId": "2047307090",
        "name": "Nawat Jantrakulchai"
      },
      {
        "authorId": "2304090",
        "name": "Sarana Nutanong"
      }
    ]
  },
  "256657381": {
    "paperId": "aefc100870ab0adea73105b7a616472c204f81e4",
    "externalIds": {
      "DBLP": "conf/skima/BornVK22",
      "DOI": "10.1109/SKIMA57145.2022.10029532",
      "CorpusId": 256657381
    },
    "publicationVenue": {
      "id": "ffa77b48-003c-4377-b323-1e4308d73011",
      "name": "International Conference on Software, Knowledge, Information Management and Applications",
      "type": "conference",
      "alternate_names": [
        "SKIMA",
        "Int Conf Softw Knowl Inf Manag Appl"
      ]
    },
    "title": "Encoder-Decoder Language Model for Khmer Handwritten Text Recognition in Historical Documents",
    "abstract": "Correcting spelling errors in texts extracted from Khmer palm leaf manuscripts by handwritten text recognition (HTR) systems can be very challenging. A Khmer Language Model developed in this study aims to facilitate the task mentioned above. The proposed model utilizes long short-term memory (LSTM) modules applicable for improving the performance of text recognition which is to predict a sequence of characters as output. The architecture of the language model is based on an encoder-decoder mechanism which is composed of two parts: an encoder to capture the context of the input erroneous word and a decoder to decode and predict the correctly spelt output word. Experimental evaluations are conducted on a text corpus consisting of Khmer words extracted from Sleuk-Rith set.",
    "venue": "International Conference on Software, Knowledge, Information Management and Applications",
    "year": 2022,
    "referenceCount": 19,
    "citationCount": 1,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2204866832",
        "name": "Seanghort Born"
      },
      {
        "authorId": "8700325",
        "name": "Dona Valy"
      },
      {
        "authorId": "38591458",
        "name": "Phutphalla Kong"
      }
    ]
  },
  "238634516": {
    "paperId": "01a207b8f352f1971f04cd2a28b8859c4cde3746",
    "externalIds": {
      "ACL": "2022.lrec-1.698",
      "DBLP": "conf/lrec/LinFCYJ22",
      "ArXiv": "2110.05896",
      "CorpusId": 238634516
    },
    "publicationVenue": {
      "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
      "name": "International Conference on Language Resources and Evaluation",
      "type": "conference",
      "alternate_names": [
        "LREC",
        "Int Conf Lang Resour Evaluation"
      ],
      "url": "http://www.lrec-conf.org/"
    },
    "title": "LaoPLM: Pre-trained Language Models for Lao",
    "abstract": "Trained on the large corpus, pre-trained language models (PLMs) can capture different levels of concepts in context and hence generate universal language representations. They can benefit from multiple downstream natural language processing (NLP) tasks. Although PTMs have been widely used in most NLP applications, especially for high-resource languages such as English, it is under-represented in Lao NLP research. Previous work on Lao has been hampered by the lack of annotated datasets and the sparsity of language resources. In this work, we construct a text classification dataset to alleviate the resource-scarce situation of the Lao language. In addition, we present the first transformer-based PTMs for Lao with four versions: BERT-Small , BERT-Base , ELECTRA-Small , and ELECTRA-Base . Furthermore, we evaluate them on two downstream tasks: part-of-speech (POS) tagging and text classification. Experiments demonstrate the effectiveness of our Lao models. We release our models and datasets to the community, hoping to facilitate the future development of Lao NLP applications.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2021,
    "referenceCount": 32,
    "citationCount": 3,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "67285699",
        "name": "Nankai Lin"
      },
      {
        "authorId": "2110659921",
        "name": "Yingwen Fu"
      },
      {
        "authorId": "2128681494",
        "name": "Chuwei Chen"
      },
      {
        "authorId": null,
        "name": "Ziyu Yang"
      },
      {
        "authorId": "2130537542",
        "name": "Shengyi Jiang"
      }
    ]
  },
  "243986012": {
    "paperId": "a25a5210ee6c2721ab53b44c62b3a8eb66cf22dc",
    "externalIds": {
      "ACL": "2022.lrec-1.703",
      "DBLP": "conf/lrec/CruzC22",
      "ArXiv": "2111.06053",
      "CorpusId": 243986012
    },
    "publicationVenue": {
      "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
      "name": "International Conference on Language Resources and Evaluation",
      "type": "conference",
      "alternate_names": [
        "LREC",
        "Int Conf Lang Resour Evaluation"
      ],
      "url": "http://www.lrec-conf.org/"
    },
    "title": "Improving Large-scale Language Models and Resources for Filipino",
    "abstract": "In this paper, we improve on existing language resources for the low-resource Filipino language in two ways. First, we outline the construction of the TLUnified dataset, a large-scale pretraining corpus that serves as an improvement over smaller existing pretraining datasets for the language in terms of scale and topic variety. Second, we pretrain new Transformer language models following the RoBERTa pretraining technique to supplant existing models trained with small corpora. Our new RoBERTa models show significant improvements over existing Filipino models in three benchmark datasets with an average gain of 4.47% test accuracy across three classification tasks with varying difficulty.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2021,
    "referenceCount": 21,
    "citationCount": 21,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51017310",
        "name": "Jan Christian Blaise Cruz"
      },
      {
        "authorId": "1973047",
        "name": "C. Cheng"
      }
    ]
  },
  "256390185": {
    "paperId": "54772ffae642a87b9a6122a6f1bae76b926a7230",
    "externalIds": {
      "DBLP": "conf/eacl/PhanDTTPCL23",
      "ArXiv": "2210.05598",
      "ACL": "2023.eacl-main.228",
      "DOI": "10.18653/v1/2023.eacl-main.228",
      "CorpusId": 256390185
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "Enriching Biomedical Knowledge for Low-resource Language Through Large-scale Translation",
    "abstract": "Biomedical data and benchmarks are highly valuable yet very limited in low-resource languages other than English, such as Vietnamese. In this paper, we use a state-of-the-art translation model in English-Vietnamese to translate and produce both pretrained and supervised data in the biomedical domains. Thanks to such large-scale translation, we introduce ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus. ViPubMedT5 demonstrates state-of-the-art results on two different biomedical benchmarks in summarization and acronym disambiguation. Further, we release ViMedNLI - a new NLP task in Vietnamese translated from MedNLI using the recently public En-vi translation model and carefully refined by human experts, with evaluations of existing methods against ViPubmedT5.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 54,
    "citationCount": 5,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2066589762",
        "name": "Long Phan"
      },
      {
        "authorId": "2187429046",
        "name": "Tai Dang"
      },
      {
        "authorId": "2057078797",
        "name": "H. Tran"
      },
      {
        "authorId": "40895509",
        "name": "Trieu H. Trinh"
      },
      {
        "authorId": "123111662",
        "name": "Vy Phan"
      },
      {
        "authorId": "2187429646",
        "name": "Lam D. Chau"
      },
      {
        "authorId": "1707242",
        "name": "Minh-Thang Luong"
      }
    ]
  },
  "25269943": {
    "paperId": "2c8449865745c63c8541c5321fbce741b646b665",
    "externalIds": {
      "MAG": "1963898561",
      "DBLP": "journals/ijdet/HuCZHFW08",
      "DOI": "10.4018/JDET.2008070101",
      "CorpusId": 25269943
    },
    "publicationVenue": {
      "id": "8bdd7e9d-b07a-45be-93e9-fc789485c2b9",
      "name": "International Journal of Distance Education Technologies",
      "type": "journal",
      "alternate_names": [
        "Int J Distance Educ Technol"
      ],
      "issn": "1539-3100",
      "url": "http://www.idea-group.com/journals/details.asp?id=498",
      "alternate_urls": [
        "https://www.igi-global.com/journal/international-journal-distance-education-technologies/1078",
        "http://www.idea-group.com/journals/tocVolumes.asp?id=498"
      ]
    },
    "title": "Using a User-Interactive QA System for Personalized E-Learning",
    "abstract": "A personalized e-learning framework based on a user-interactive question-answering (QA) system is proposed, in which a user-modeling approach is used to capture personal information of students and a personalized answer extraction algorithm is proposed for personalized automatic answering. In our approach, a topic ontology (or concept hierarchy) of course content defined by an instructor is used for the system to generate the corresponding structure of boards for holding relevant questions. Students can interactively post questions, and also browse, select, and answer others\u2019 questions in their interested boards. A knowledge base is accumulated using historical question/answer (Q/A) pairs for knowledge reuse. The students\u2019 log data are used to build an association space to compute the interest and authority of the students for each board and each topic. The personal information of students can help instructors design suitable teaching materials to enhance instruction efficiency, be used to implement the personalized automatic answering and distribute unsolved questions to relevant students to enhance the learning efficiency. The experiment results show the efficacy of our user-modeling approach.",
    "venue": "International Journal of Distance Education Technologies",
    "year": 2008,
    "referenceCount": 26,
    "citationCount": 15,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "34205677",
        "name": "Dawei Hu"
      },
      {
        "authorId": "145582188",
        "name": "Wei Chen"
      },
      {
        "authorId": "1986297",
        "name": "Q. Zeng"
      },
      {
        "authorId": "144824187",
        "name": "Tianyong Hao"
      },
      {
        "authorId": "145612971",
        "name": "Min Feng"
      },
      {
        "authorId": "120640002",
        "name": "Wenyin Liu"
      }
    ]
  },
  "258841788": {
    "paperId": "ca1e3ef6211f2d867d9b9b7055c17734ac5b431d",
    "externalIds": {
      "ACL": "2024.eacl-long.68",
      "ArXiv": "2305.14195",
      "DBLP": "conf/eacl/SiciliaGA24",
      "CorpusId": 258841788
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors of Language Models in Human-Machine Conversations",
    "abstract": "While demographic factors like age and gender change the way people talk, and in particular, the way people talk to machines, there is little investigation into how large pre-trained language models (LMs) can adapt to these changes. To remedy this gap, we consider how demographic factors in LM language skills can be measured to determine compatibility with a target demographic. We suggest clinical techniques from Speech Language Pathology, which has norms for acquisition of language skills in humans. We conduct evaluation with a domain expert (i.e., a clinically licensed speech language pathologist), and also propose automated techniques to complement clinical evaluation at scale. Empirically, we focus on age, finding LM capability varies widely depending on task: GPT-3.5 mimics the ability of humans ranging from age 6-15 at tasks requiring inference, and simultaneously, outperforms a typical 21 year old at memorization. GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills. Findings affirm the importance of considering demographic alignment and conversational goals when using LMs as public-facing tools. Code, data, and a package will be available.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 55,
    "citationCount": 2,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51137683",
        "name": "Anthony Sicilia"
      },
      {
        "authorId": "2157150046",
        "name": "Jennifer C. Gates"
      },
      {
        "authorId": "2715920",
        "name": "Malihe Alikhani"
      }
    ]
  },
  "247594214": {
    "paperId": "94272b4b82df42004d095391ec8f34047bd915dc",
    "externalIds": {
      "ACL": "2022.findings-acl.228",
      "DBLP": "journals/corr/abs-2203-09679",
      "ArXiv": "2203.09679",
      "DOI": "10.18653/v1/2022.findings-acl.228",
      "CorpusId": 247594214
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Modeling Intensification for Sign Language Generation: A Computational Approach",
    "abstract": "End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state-of-the-art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model.",
    "venue": "Findings",
    "year": 2022,
    "referenceCount": 50,
    "citationCount": 13,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "91268193",
        "name": "Mert Inan"
      },
      {
        "authorId": "2112888634",
        "name": "Yang Zhong"
      },
      {
        "authorId": "67114070",
        "name": "Sabit Hassan"
      },
      {
        "authorId": "3476301",
        "name": "Lorna C. Quandt"
      },
      {
        "authorId": "2715920",
        "name": "Malihe Alikhani"
      }
    ]
  },
  "221006691": {
    "paperId": "082d70e93af82d3ad289795312c717e7f1858e5f",
    "externalIds": {
      "MAG": "3034251282",
      "DBLP": "journals/corr/abs-2004-11829",
      "ArXiv": "2004.11829",
      "CorpusId": 221006691
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A survey on domain adaptation theory",
    "abstract": "All famous machine learning algorithms that comprise both supervised and semi-supervised learning work well only under a common assumption: the training and test data follow the same distribution. When the distribution changes, most statistical models must be reconstructed from newly collected data, which for some applications can be costly or impossible to obtain. Therefore, it has become necessary to develop approaches that reduce the need and the effort to obtain new labeled samples by exploiting data that are available in related areas, and using these further across similar fields. This has given rise to a new machine learning framework known as transfer learning: a learning setting inspired by the capability of a human being to extrapolate knowledge across tasks to learn more efficiently. Despite a large amount of different transfer learning scenarios, the main objective of this survey is to provide an overview of the state-of-the-art theoretical results in a specific, and arguably the most popular, sub-field of transfer learning, called domain adaptation. In this sub-field, the data distribution is assumed to change across the training and the test data, while the learning task remains the same. We provide a first up-to-date description of existing results related to domain adaptation problem that cover learning bounds based on different statistical learning frameworks.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 96,
    "citationCount": 122,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145898069",
        "name": "I. Redko"
      },
      {
        "authorId": "1890744",
        "name": "Emilie Morvant"
      },
      {
        "authorId": "1749327",
        "name": "Amaury Habrard"
      },
      {
        "authorId": "1738336",
        "name": "M. Sebban"
      },
      {
        "authorId": "1734694",
        "name": "Youn\u00e8s Bennani"
      }
    ]
  },
  "8577357": {
    "paperId": "66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde",
    "externalIds": {
      "DBLP": "journals/ml/Ben-DavidBCKPV10",
      "MAG": "2104094955",
      "DOI": "10.1007/s10994-009-5152-4",
      "CorpusId": 8577357
    },
    "publicationVenue": {
      "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
      "name": "Machine-mediated learning",
      "type": "journal",
      "alternate_names": [
        "Mach learn",
        "Machine Learning",
        "Mach Learn"
      ],
      "issn": "0732-6718",
      "alternate_issns": [
        "0885-6125"
      ],
      "url": "http://www.springer.com/computer/artificial/journal/10994",
      "alternate_urls": [
        "https://link.springer.com/journal/10994",
        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
      ]
    },
    "title": "A theory of learning from different domains",
    "abstract": null,
    "venue": "Machine-mediated learning",
    "year": 2010,
    "referenceCount": 35,
    "citationCount": 3240,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1401829700",
        "name": "Shai Ben-David"
      },
      {
        "authorId": "2116927",
        "name": "John Blitzer"
      },
      {
        "authorId": "1693407",
        "name": "K. Crammer"
      },
      {
        "authorId": "145500336",
        "name": "Alex Kulesza"
      },
      {
        "authorId": "145366908",
        "name": "Fernando C Pereira"
      },
      {
        "authorId": "4006636",
        "name": "Jennifer Wortman Vaughan"
      }
    ]
  },
  "250451458": {
    "paperId": "726377ee402a032ff06eaf8fa9d6ef6f39858860",
    "externalIds": {
      "DBLP": "journals/corr/abs-2207-05685",
      "ArXiv": "2207.05685",
      "DOI": "10.48550/arXiv.2207.05685",
      "CorpusId": 250451458
    },
    "publicationVenue": {
      "id": "f9af8000-42f8-410d-a622-e8811e41660a",
      "name": "Conference on Uncertainty in Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Uncertainty in Artificial Intelligence",
        "UAI",
        "Conf Uncertain Artif Intell",
        "Uncertain Artif Intell"
      ],
      "url": "http://www.auai.org/"
    },
    "title": "PAC-Bayesian Domain Adaptation Bounds for Multiclass Learners",
    "abstract": "Multiclass neural networks are a common tool in modern unsupervised domain adaptation, yet an appropriate theoretical description for their non-uniform sample complexity is lacking in the adaptation literature. To fill this gap, we propose the first PAC-Bayesian adaptation bounds for multiclass learners. We facilitate practical use of our bounds by also proposing the first approximation techniques for the multiclass distribution divergences we consider. For divergences dependent on a Gibbs predictor, we propose additional PAC-Bayesian adaptation bounds which remove the need for inefficient Monte-Carlo estimation. Empirically, we test the efficacy of our proposed approximation techniques as well as some novel design-concepts which we include in our bounds. Finally, we apply our bounds to analyze a common adaptation algorithm that uses neural networks.",
    "venue": "Conference on Uncertainty in Artificial Intelligence",
    "year": 2022,
    "referenceCount": 93,
    "citationCount": 8,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51137683",
        "name": "Anthony Sicilia"
      },
      {
        "authorId": "2115500561",
        "name": "Katherine Atwell"
      },
      {
        "authorId": "2715920",
        "name": "Malihe Alikhani"
      },
      {
        "authorId": "3367790",
        "name": "Seong Jae Hwang"
      }
    ]
  },
  "261295214": {
    "paperId": "20f63033e8775cbab0692aed92d38da7e725d64e",
    "externalIds": {
      "DBLP": "books/daglib/0033642",
      "CorpusId": 261295214
    },
    "publicationVenue": null,
    "title": "Understanding Machine Learning - From Theory to Algorithms",
    "abstract": "Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability ; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning ; and emerging theoretical concepts such as the PACBayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and nonexpert readers in statistics, computer science, mathematics, and engineering.",
    "venue": "",
    "year": 2014,
    "referenceCount": 0,
    "citationCount": 2760,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1389955537",
        "name": "Shai Shalev-Shwartz"
      },
      {
        "authorId": "1401829700",
        "name": "Shai Ben-David"
      }
    ]
  },
  "233210028": {
    "paperId": "5694cdabf62b4e605e181c27819a24ee33c69ca4",
    "externalIds": {
      "DBLP": "journals/corr/abs-2104-05600",
      "ArXiv": "2104.05600",
      "DOI": "10.1007/978-3-030-87199-4_53",
      "CorpusId": 233210028,
      "PubMed": "34957473"
    },
    "publicationVenue": {
      "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
      "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "type": "conference",
      "alternate_names": [
        "Medical Image Computing and Computer-Assisted Intervention",
        "MICCAI",
        "Med Image Comput Comput Interv",
        "Int Conf Med Image Comput Comput Interv"
      ],
      "url": "http://www.miccai.org/"
    },
    "title": "PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in Medical Imaging",
    "abstract": null,
    "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
    "year": 2021,
    "referenceCount": 57,
    "citationCount": 4,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51137683",
        "name": "Anthony Sicilia"
      },
      {
        "authorId": "121809235",
        "name": "Xingchen Zhao"
      },
      {
        "authorId": "2072734675",
        "name": "Anastasia Sosnovskikh"
      },
      {
        "authorId": "3367790",
        "name": "Seong Jae Hwang"
      }
    ]
  },
  "252907611": {
    "paperId": "016cbbfeb1803134eea5b18c0fe7d8c448e53449",
    "externalIds": {
      "DBLP": "conf/ijcnlp/SiciliaA22",
      "ArXiv": "2210.07777",
      "DOI": "10.48550/arXiv.2210.07777",
      "CorpusId": 252907611
    },
    "publicationVenue": null,
    "title": "LEATHER: A Framework for Learning to Generate Human-like Text in Dialogue",
    "abstract": "Algorithms for text-generation in dialogue can be misguided. For example, in task-oriented settings, reinforcement learning that optimizes only task-success can lead to abysmal lexical diversity. We hypothesize this is due to poor theoretical understanding of the objectives in text-generation and their relation to the learning process (i.e., model training). To this end, we propose a new theoretical framework for learning to generate text in dialogue. Compared to existing theories of learning, our framework allows for analysis of the multi-faceted goals inherent to text-generation. We use our framework to develop theoretical guarantees for learners that adapt to unseen data. As an example, we apply our theory to study data-shift within a cooperative learning algorithm proposed for the GuessWhat?! visual dialogue game. From this insight, we propose a new algorithm, and empirically, we demonstrate our proposal improves both task-success and human-likeness of the generated text. Finally, we show statistics from our theory are empirically predictive of multiple qualities of the generated dialogue, suggesting our theory is useful for model-selection when human evaluations are not available.",
    "venue": "AACL/IJCNLP",
    "year": 2022,
    "referenceCount": 37,
    "citationCount": 4,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51137683",
        "name": "Anthony Sicilia"
      },
      {
        "authorId": "2715920",
        "name": "Malihe Alikhani"
      }
    ]
  },
  "2871880": {
    "paperId": "1d5972b32a9b5a455a6eef389de5b7fca25771ad",
    "externalIds": {
      "DBLP": "journals/corr/GaninUAGLLML15",
      "ArXiv": "1505.07818",
      "MAG": "2998115938",
      "DOI": "10.1007/978-3-319-58347-1_10",
      "CorpusId": 2871880
    },
    "publicationVenue": {
      "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
      "name": "Journal of machine learning research",
      "type": "journal",
      "alternate_names": [
        "Journal of Machine Learning Research",
        "J mach learn res",
        "J Mach Learn Res"
      ],
      "issn": "1532-4435",
      "alternate_issns": [
        "1533-7928"
      ],
      "url": "http://www.ai.mit.edu/projects/jmlr/",
      "alternate_urls": [
        "http://jmlr.csail.mit.edu/",
        "http://www.jmlr.org/",
        "http://portal.acm.org/affiliated/jmlr"
      ]
    },
    "title": "Domain-Adversarial Training of Neural Networks",
    "abstract": null,
    "venue": "Journal of machine learning research",
    "year": 2015,
    "referenceCount": 67,
    "citationCount": 8533,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2825246",
        "name": "Yaroslav Ganin"
      },
      {
        "authorId": "145754674",
        "name": "E. Ustinova"
      },
      {
        "authorId": "2780075",
        "name": "Hana Ajakan"
      },
      {
        "authorId": "31580144",
        "name": "Pascal Germain"
      },
      {
        "authorId": "1777528",
        "name": "H. Larochelle"
      },
      {
        "authorId": "2482151",
        "name": "Fran\u00e7ois Laviolette"
      },
      {
        "authorId": "143858557",
        "name": "M. Marchand"
      },
      {
        "authorId": "1740145",
        "name": "V. Lempitsky"
      }
    ]
  },
  "196170882": {
    "paperId": "7babff88d77beb97e38eb067d097d1522b3dfc49",
    "externalIds": {
      "ACL": "P19-1211",
      "MAG": "2950881821",
      "DBLP": "conf/acl/ChenC19",
      "DOI": "10.18653/v1/P19-1211",
      "CorpusId": 196170882
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Adversarial Domain Adaptation Using Artificial Titles for Abstractive Title Generation",
    "abstract": "A common issue in training a deep learning, abstractive summarization model is lack of a large set of training summaries. This paper examines techniques for adapting from a labeled source domain to an unlabeled target domain in the context of an encoder-decoder model for text generation. In addition to adversarial domain adaptation (ADA), we introduce the use of artificial titles and sequential training to capture the grammatical style of the unlabeled target domain. Evaluation on adapting to/from news articles and Stack Exchange posts indicates that the use of these techniques can boost performance for both unsupervised adaptation as well as fine-tuning with limited target data.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 19,
    "citationCount": 5,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "27375808",
        "name": "Francine Chen"
      },
      {
        "authorId": "35081710",
        "name": "Yan-Ying Chen"
      }
    ]
  },
  "258947837": {
    "paperId": "c00f716e5a852c144cb1be6f6b27ea8d4e1fe9bb",
    "externalIds": {
      "ACL": "2023.acl-long.492",
      "ArXiv": "2211.16550",
      "DBLP": "conf/acl/StefanikKS23",
      "DOI": "10.18653/v1/2023.acl-long.492",
      "CorpusId": 258947837
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Soft Alignment Objectives for Robust Adaptation of Language Generation",
    "abstract": "Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application.However, the traditional adaptation by further training on in-domain data rapidly weakens the model\u2019s ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors.This work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference.Our results show that (1) avoiding the common assumption of a single correct prediction by constructing the training target from tokens\u2019 semantic similarity can largely mitigate catastrophic forgetting of adaptation, while (2) preserving the adaptation in-domain quality, (3) with negligible additions to compute costs.In the broader context, the objectives grounded in a continuous token similarity pioneer the exploration of the middle ground between the efficient but naive exact-match token-level objectives and expressive but computationally- and resource-intensive sequential objectives.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 55,
    "citationCount": 2,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2106506537",
        "name": "Michal vStef'anik"
      },
      {
        "authorId": "2194666242",
        "name": "Marek Kadlc\u00edk"
      },
      {
        "authorId": "1980208",
        "name": "Petr Sojka"
      }
    ]
  },
  "245218833": {
    "paperId": "5b0cfef3ebb8f709a4cf2e4718e8440cc5890bab",
    "externalIds": {
      "ACL": "2022.naacl-main.96",
      "ArXiv": "2112.08786",
      "DBLP": "conf/naacl/ChronopoulouPD22",
      "DOI": "10.18653/v1/2022.naacl-main.96",
      "CorpusId": 245218833
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Efficient Hierarchical Domain Adaptation for Pretrained Language Models",
    "abstract": "The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 52,
    "citationCount": 38,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3379701",
        "name": "Alexandra Chronopoulou"
      },
      {
        "authorId": "39139825",
        "name": "Matthew E. Peters"
      },
      {
        "authorId": "34176020",
        "name": "Jesse Dodge"
      }
    ]
  },
  "256846453": {
    "paperId": "629bc57782bb4326a3eb5f89314e350729c5f417",
    "externalIds": {
      "DBLP": "conf/eacl/ChronopoulouPFD23",
      "ArXiv": "2302.07027",
      "ACL": "2023.findings-eacl.153",
      "DOI": "10.48550/arXiv.2302.07027",
      "CorpusId": 256846453
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models",
    "abstract": "Pretrained language models (PLMs) are trained on massive corpora, but often need to specialize to specific domains. A parameter-efficient adaptation method suggests training an adapter for each domain on the task of language modeling. This leads to good in-domain scores but can be impractical for domain- or resource-restricted settings. A solution is to use a related-domain adapter for the novel domain at test time. In this paper, we introduce AdapterSoup, an approach that performs weight-space averaging of adapters trained on different domains. Our approach is embarrassingly parallel: first, we train a set of domain-specific adapters; then, for each novel domain, we determine which adapters should be averaged at test time. We present extensive experiments showing that AdapterSoup consistently improves performance to new domains without extra training. We also explore weight averaging of adapters trained on the same domain with different hyper-parameters, and show that it preserves the performance of a PLM on new domains while obtaining strong in-domain results. We explore various approaches for choosing which adapters to combine, such as text clustering and semantic similarity. We find that using clustering leads to the most competitive results on novel domains.",
    "venue": "Findings",
    "year": 2023,
    "referenceCount": 40,
    "citationCount": 52,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3379701",
        "name": "Alexandra Chronopoulou"
      },
      {
        "authorId": "39139825",
        "name": "Matthew E. Peters"
      },
      {
        "authorId": "2277248",
        "name": "Alexander M. Fraser"
      },
      {
        "authorId": "34176020",
        "name": "Jesse Dodge"
      }
    ]
  },
  "15164488": {
    "paperId": "807e421679d4a9d629d2fad1f60f28787dca60e7",
    "externalIds": {
      "MAG": "2963938442",
      "DBLP": "journals/corr/YangHSC17",
      "ArXiv": "1702.02206",
      "ACL": "P17-1096",
      "DOI": "10.18653/v1/P17-1096",
      "CorpusId": 15164488
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Semi-Supervised QA with Generative Domain-Adaptive Nets",
    "abstract": "We study the problem of semi-supervised question answering\u2014utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 48,
    "citationCount": 151,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2109512754",
        "name": "Zhilin Yang"
      },
      {
        "authorId": "145919378",
        "name": "Junjie Hu"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      },
      {
        "authorId": "50056360",
        "name": "William W. Cohen"
      }
    ]
  },
  "222291004": {
    "paperId": "9d8cd6c25d28026d41b2b07be06810c4dabf4715",
    "externalIds": {
      "ACL": "2020.insights-1.6",
      "ArXiv": "2010.04826",
      "MAG": "3091939303",
      "DBLP": "journals/corr/abs-2010-04826",
      "DOI": "10.18653/v1/2020.insights-1.6",
      "CorpusId": 222291004
    },
    "publicationVenue": {
      "id": "30e39c3f-406d-4192-8192-e8d75001c38a",
      "name": "First Workshop on Insights from Negative Results in NLP",
      "type": "conference",
      "alternate_names": [
        "Insights",
        "First Workshop Insight Negat Result NLP"
      ],
      "url": "https://www.aclweb.org/anthology/venues/insights/"
    },
    "title": "On Task-Level Dialogue Composition of Generative Transformer Model",
    "abstract": "Task-oriented dialogue systems help users accomplish tasks such as booking a movie ticket and ordering food via conversation. Generative models parameterized by a deep neural network are widely used for next turn response generation in such systems. It is natural for users of the system to want to accomplish multiple tasks within the same conversation, but the ability of generative models to compose multiple tasks is not well studied. In this work, we begin by studying the effect of training human-human task-oriented dialogues towards improving the ability to compose multiple tasks on Transformer generative models. To that end, we propose and explore two solutions: (1) creating synthetic multiple task dialogue data for training from human-human single task dialogue and (2) forcing the encoder representation to be invariant to single and multiple task dialogues using an auxiliary loss. The results from our experiments highlight the difficulty of even the sophisticated variant of transformer model in learning to compose multiple tasks from single task dialogues.",
    "venue": "First Workshop on Insights from Negative Results in NLP",
    "year": 2020,
    "referenceCount": 27,
    "citationCount": 2,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32899078",
        "name": "Prasanna Parthasarathi"
      },
      {
        "authorId": "2072676",
        "name": "Arvind Neelakantan"
      },
      {
        "authorId": "46617804",
        "name": "Sharan Narang"
      }
    ]
  },
  "220444793": {
    "paperId": "33c7da49a60255677ab0f4568c27dec7a8257d5e",
    "externalIds": {
      "DBLP": "conf/sigdial/ShenWMP20",
      "ACL": "2020.sigdial-1.2",
      "MAG": "3034595127",
      "DOI": "10.18653/v1/2020.sigdial-1.2",
      "CorpusId": 220444793
    },
    "publicationVenue": {
      "id": "6a470734-72c6-4809-a07d-d34dee0df4a1",
      "name": "SIGDIAL Conferences",
      "type": "conference",
      "alternate_names": [
        "SIGDIAL",
        "SIGDIAL Conf",
        "Annu Meet Sp\u00e9c Interest Group Discourse Dialogue",
        "Annual Meeting of the Special Interest Group on Discourse and Dialogue"
      ]
    },
    "title": "Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context",
    "abstract": "We introduce a counseling dialogue system that seeks to assist counselors while they are learning and refining their counseling skills. The system generates counselors\u2019reflections \u2013 i.e., responses that reflect back on what the client has said given the dialogue history. Our method builds upon the new generative pretrained transformer architecture and enhances it with context augmentation techniques inspired by traditional strategies used during counselor training. Through a set of comparative experiments, we show that the system that incorporates these strategies performs better in the reflection generation task than a system that is just fine-tuned with counseling conversations. To confirm our findings, we present a human evaluation study that shows that our system generates naturally-looking reflections that are also stylistically and grammatically correct.",
    "venue": "SIGDIAL Conferences",
    "year": 2020,
    "referenceCount": 36,
    "citationCount": 45,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2072820796",
        "name": "Siqi Shen"
      },
      {
        "authorId": "145645240",
        "name": "Charles F Welch"
      },
      {
        "authorId": "2105984203",
        "name": "Rada Mihalcea"
      },
      {
        "authorId": "1396239754",
        "name": "Ver\u00f3nica P\u00e9rez-Rosas"
      }
    ]
  },
  "1054586": {
    "paperId": "c80725ad0c0cd06416f3c01a78b7c419359d3fe2",
    "externalIds": {
      "MAG": "2756978580",
      "DBLP": "conf/emnlp/WangULCS17",
      "ACL": "D17-1155",
      "DOI": "10.18653/v1/D17-1155",
      "CorpusId": 1054586
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Instance Weighting for Neural Machine Translation Domain Adaptation",
    "abstract": "Instance weighting has been widely applied to phrase-based machine translation domain adaptation. However, it is challenging to be applied to Neural Machine Translation (NMT) directly, because NMT is not a linear model. In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2017,
    "referenceCount": 34,
    "citationCount": 130,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "108085542",
        "name": "Rui Wang"
      },
      {
        "authorId": "1802277",
        "name": "M. Utiyama"
      },
      {
        "authorId": "2978364",
        "name": "Lemao Liu"
      },
      {
        "authorId": "2849740",
        "name": "Kehai Chen"
      },
      {
        "authorId": "1698363",
        "name": "E. Sumita"
      }
    ]
  },
  "248779887": {
    "paperId": "55739eaba9f3f954996bd72a27ac5ee0cfec8520",
    "externalIds": {
      "ACL": "2022.acl-long.122",
      "DBLP": "conf/acl/WelchGKPM22",
      "DOI": "10.18653/v1/2022.acl-long.122",
      "CorpusId": 248779887
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Leveraging Similar Users for Personalized Language Modeling with Limited Data",
    "abstract": "Personalized language models are designed and trained to capture language patterns specific to individual users. This makes them more accurate at predicting what a user will write. However, when a new user joins a platform and not enough text is available, it is harder to build effective personalized language models. We propose a solution for this problem, using a model trained on users that are similar to a new user. In this paper, we explore strategies for finding the similarity between new users and existing ones and methods for using the data from existing users who are a good match. We further explore the trade-off between available data for new users and how well their language can be modeled.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 33,
    "citationCount": 27,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145645240",
        "name": "Charles F Welch"
      },
      {
        "authorId": "2053399628",
        "name": "Chenxi Gu"
      },
      {
        "authorId": "1727211",
        "name": "Jonathan K. Kummerfeld"
      },
      {
        "authorId": "1396239754",
        "name": "Ver\u00f3nica P\u00e9rez-Rosas"
      },
      {
        "authorId": "2105984203",
        "name": "Rada Mihalcea"
      }
    ]
  },
  "259370819": {
    "paperId": "847e106814753600f5029b8dbaa4255bf1f716e2",
    "externalIds": {
      "ACL": "2023.acl-long.853",
      "DBLP": "conf/acl/AragonMGLM23",
      "DOI": "10.18653/v1/2023.acl-long.853",
      "CorpusId": 259370819
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media",
    "abstract": "Mental disorders affect millions of people worldwide and cause interference with their thinking and behavior. Through the past years, awareness created by health campaigns and other sources motivated the study of these disorders using information extracted from social media platforms. In this work, we aim to contribute to the study of these disorders and to the understanding of how mental problems reflect on social media. To achieve this goal, we propose a double-domain adaptation of a language model. First, we adapted the model to social media language, and then, we adapted it to the mental health domain. In both steps, we incorporated a lexical resource to guide the masking process of the language model and, therefore, to help it in paying more attention to words related to mental disorders. We have evaluated our model in the detection of signs of three major mental disorders: Anorexia, Self-harm, and Depression. Results are encouraging as they show that the proposed adaptation enhances the classification performance and yields competitive results against state-of-the-art methods.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 46,
    "citationCount": 17,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47829199",
        "name": "Mario Ezra Arag\u00f3n"
      },
      {
        "authorId": "1400883876",
        "name": "Adrian Pastor Lopez-Monroy"
      },
      {
        "authorId": "2177617077",
        "name": "Luis C. Gonz\u00e1lez"
      },
      {
        "authorId": "2356644",
        "name": "D. Losada"
      },
      {
        "authorId": "2068628554",
        "name": "M. Montes"
      }
    ]
  },
  "259833896": {
    "paperId": "690b43567b5b60d5edc708458dd142603d438b23",
    "externalIds": {
      "DBLP": "conf/acl-clinicalnlp/LuPLSQ0NCL23",
      "ACL": "2023.clinicalnlp-1.30",
      "DOI": "10.18653/v1/2023.clinicalnlp-1.30",
      "CorpusId": 259833896
    },
    "publicationVenue": {
      "id": "a631cd93-9863-44c8-8dff-9eb3fbf4f83e",
      "name": "Clinical Natural Language Processing Workshop",
      "type": "conference",
      "alternate_names": [
        "ClinicalNLP",
        "Clin Nat Lang Process Workshop"
      ]
    },
    "title": "Prompt Discriminative Language Models for Domain Adaptation",
    "abstract": "Prompt tuning offers an efficient approach to domain adaptation for pretrained language models, which predominantly focus on masked language modeling or generative objectives. However, the potential of discriminative language models in biomedical tasks remains underexplored.To bridge this gap, we develop BioDLM, a method tailored for biomedical domain adaptation of discriminative language models that incorporates prompt-based continual pretraining and prompt tuning for downstream tasks. BioDLM aims to maximize the potential of discriminative language models in low-resource scenarios by reformulating these tasks as span-level corruption detection, thereby enhancing performance on domain-specific tasks and improving the efficiency of continual pertaining.In this way, BioDLM provides a data-efficient domain adaptation method for discriminative language models, effectively enhancing performance on discriminative tasks within the biomedical domain.",
    "venue": "Clinical Natural Language Processing Workshop",
    "year": 2023,
    "referenceCount": 47,
    "citationCount": 5,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1515662094",
        "name": "K. Lu"
      },
      {
        "authorId": "2039635",
        "name": "P. Potash"
      },
      {
        "authorId": "29457904",
        "name": "Xihui Lin"
      },
      {
        "authorId": "2109026528",
        "name": "Yuwen Sun"
      },
      {
        "authorId": "2210751970",
        "name": "Zihan Qian"
      },
      {
        "authorId": "2112340945",
        "name": "Zheng Yuan"
      },
      {
        "authorId": "40466858",
        "name": "Tristan Naumann"
      },
      {
        "authorId": "2114620790",
        "name": "Tianxi Cai"
      },
      {
        "authorId": "2218113443",
        "name": "Junwei Lu"
      }
    ]
  },
  "216553205": {
    "paperId": "6b3e271e60df9825542374ee5e31e4a1ec2ee1bc",
    "externalIds": {
      "DBLP": "journals/corr/abs-2004-12332",
      "MAG": "3034422725",
      "ArXiv": "2004.12332",
      "ACL": "2020.acl-main.262",
      "DOI": "10.18653/v1/2020.acl-main.262",
      "CorpusId": 216553205
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds",
    "abstract": "Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protected attribute is slow and expensive. Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias? While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias. In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval. We provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias. In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases. For example, consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences \u2013 to claim it is biased with 95% confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 14,
    "citationCount": 24,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "10324691",
        "name": "Kawin Ethayarajh"
      }
    ]
  },
  "14688775": {
    "paperId": "d895647b4a80861703851ef55930a2627fe19492",
    "externalIds": {
      "ACL": "P07-1056",
      "MAG": "2163302275",
      "DBLP": "conf/acl/BlitzerDP07",
      "CorpusId": 14688775
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification",
    "abstract": "Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2007,
    "referenceCount": 12,
    "citationCount": 2328,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Business",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2116927",
        "name": "John Blitzer"
      },
      {
        "authorId": "1782853",
        "name": "Mark Dredze"
      },
      {
        "authorId": "145366908",
        "name": "Fernando C Pereira"
      }
    ]
  },
  "247596807": {
    "paperId": "a243f87588a2381257d325c40c9709b0e4d03ada",
    "externalIds": {
      "ArXiv": "2203.11317",
      "ACL": "2022.findings-acl.68",
      "DBLP": "conf/acl/AtwellSHA22",
      "DOI": "10.48550/arXiv.2203.11317",
      "CorpusId": 247596807
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error",
    "abstract": "Discourse analysis allows us to attain inferences of a text document that extend beyond the sentence-level. The current performance of discourse models is very low on texts outside of the training distribution\u2019s coverage, diminishing the practical utility of existing models. There is need for a measure that can inform us to what extent our model generalizes from the training to the test sample when these samples may be drawn from distinct distributions. While this can be estimated via distribution shift, we argue that this does not directly correlate with change in the observed error of a classifier (i.e. error-gap). Thus, we propose to use a statistic from the theoretical domain adaptation literature which can be directly tied to error-gap. We study the bias of this statistic as an estimator of error-gap both theoretically and through a large-scale empirical study of over 2400 experiments on 6 discourse datasets from domains including, but not limited to: news, biomedical texts, TED talks, Reddit posts, and fiction. Our results not only motivate our proposal and help us to understand its limitations, but also provide insight on the properties of discourse models and datasets which improve performance in domain adaptation. For instance, we find that non-news datasets are slightly easier to transfer to than news datasets when the training and test sets are very different. Our code and an associated Python package are available to allow practitioners to make more informed model and dataset choices.",
    "venue": "Findings",
    "year": 2022,
    "referenceCount": 71,
    "citationCount": 14,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2115500561",
        "name": "Katherine Atwell"
      },
      {
        "authorId": "51137683",
        "name": "Anthony Sicilia"
      },
      {
        "authorId": "3367790",
        "name": "Seong Jae Hwang"
      },
      {
        "authorId": "2715920",
        "name": "Malihe Alikhani"
      }
    ]
  },
  "115146939": {
    "paperId": "e0ea4598f2c44a00784a7952ed11eae670faacc9",
    "externalIds": {
      "MAG": "2955608071",
      "DBLP": "conf/naacl/HaoP19",
      "ACL": "N19-1158",
      "DOI": "10.18653/v1/N19-1158",
      "CorpusId": 115146939
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Analyzing Bayesian Crosslingual Transfer in Topic Models",
    "abstract": "We introduce a theoretical analysis of crosslingual transfer in probabilistic topic models. By formulating posterior inference through Gibbs sampling as a process of language transfer, we propose a new measure that quantifies the loss of knowledge across languages during this process. This measure enables us to derive a PAC-Bayesian bound that elucidates the factors affecting model quality, both during training and in downstream applications. We provide experimental validation of the analysis on a diverse set of five languages, and discuss best practices for data collection and model design based on our analysis.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 28,
    "citationCount": 2,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145105573",
        "name": "Shudong Hao"
      },
      {
        "authorId": "143946641",
        "name": "Michael J. Paul"
      }
    ]
  },
  "207984514": {
    "paperId": "024841e69db9274e708731d9b3e97040a1bac773",
    "externalIds": {
      "MAG": "2985449513",
      "ACL": "D19-1222",
      "DBLP": "conf/emnlp/ElSaharG19",
      "DOI": "10.18653/v1/D19-1222",
      "CorpusId": 207984514
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "To Annotate or Not? Predicting Performance Drop under Domain Shift",
    "abstract": "Performance drop due to domain-shift is an endemic problem for NLP models in production. This problem creates an urge to continuously annotate evaluation datasets to measure the expected drop in the model performance which can be prohibitively expensive and slow. In this paper, we study the problem of predicting the performance drop of modern NLP models under domain-shift, in the absence of any target domain labels. We investigate three families of methods (\\mathcal{H}-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging respectively.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "referenceCount": 48,
    "citationCount": 89,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2218938",
        "name": "Hady ElSahar"
      },
      {
        "authorId": "2907260",
        "name": "Matthias Gall\u00e9"
      }
    ]
  },
  "199379496": {
    "paperId": "f8a13e94260373d2904f32616decce817b388990",
    "externalIds": {
      "DBLP": "conf/bea/MayfieldMPGMDB19",
      "MAG": "2971062439",
      "ACL": "W19-4446",
      "DOI": "10.18653/v1/W19-4446",
      "CorpusId": 199379496
    },
    "publicationVenue": null,
    "title": "Equity Beyond Bias in Language Technologies for Education",
    "abstract": "There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, identifying future work on equity in NLP. We present case studies in a range of topics like intelligent tutoring systems, computer-assisted language learning, automated essay scoring, and sentiment analysis in classrooms, and provide an actionable agenda for research.",
    "venue": "BEA@ACL",
    "year": 2019,
    "referenceCount": 206,
    "citationCount": 41,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3309954",
        "name": "Elijah Mayfield"
      },
      {
        "authorId": "38113700",
        "name": "Michael A. Madaio"
      },
      {
        "authorId": "9358910",
        "name": "Shrimai Prabhumoye"
      },
      {
        "authorId": "2630367",
        "name": "David B. Gerritsen"
      },
      {
        "authorId": "2064450880",
        "name": "Brittany McLaughlin"
      },
      {
        "authorId": "1403727025",
        "name": "Ezekiel Dixon-Rom\u00e1n"
      },
      {
        "authorId": "1690706",
        "name": "A. Black"
      }
    ]
  },
  "258217665": {
    "paperId": "efa6a639cb13b924dd008728db8755ae40065127",
    "externalIds": {
      "DBLP": "conf/chi/HarringtonE23",
      "DOI": "10.1145/3544548.3580719",
      "CorpusId": 258217665
    },
    "publicationVenue": {
      "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
      "name": "International Conference on Human Factors in Computing Systems",
      "type": "conference",
      "alternate_names": [
        "CHI",
        "Int Conf Hum Factor Comput Syst",
        "Human Factors in Computing Systems",
        "Conference on Human Interface",
        "Conf Hum Interface",
        "Hum Factor Comput Syst"
      ],
      "url": "http://www.acm.org/sigchi/"
    },
    "title": "Trust, Comfort and Relatability: Understanding Black Older Adults\u2019 Perceptions of Chatbot Design for Health Information Seeking",
    "abstract": "Conversational agents such as chatbots have emerged as a useful resource to access real-time health information online. Perceptions of trust and credibility among chatbots have been attributed to the anthropomorphism and humanness of the chatbot design, with gender and race influencing their reception. Few existing studies have looked specifically at the diversity of chatbot avatar design related to both race, age, and gender, which may have particular significance for racially minoritized users like Black older adults. In this paper, we explored perceptions of chatbots with varying identities for health information seeking in a diary and interview study with 30 Black older adults. Our findings suggest that while racial and age likeness influence feelings of trust and comfort with chatbots, constructs such as professionalism and likeability and overall familiarity also influence reception. Based on these findings, we provide implications for designing text-based chatbots that consider Black older adults.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2023,
    "referenceCount": 86,
    "citationCount": 12,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46848591",
        "name": "Christina N. Harrington"
      },
      {
        "authorId": "2137967742",
        "name": "Lisa Egede"
      }
    ]
  },
  "252818942": {
    "paperId": "85f40f11e6dea79210aabc579feaf7b20b4bc92f",
    "externalIds": {
      "ACL": "2022.coling-1.112",
      "DBLP": "conf/coling/DasB22",
      "CorpusId": 252818942
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Quantifying Bias from Decoding Techniques in Natural Language Generation",
    "abstract": "Natural language generation (NLG) models can propagate social bias towards particular demography. Though several studies investigated bias from data and model, NLG task distinctively uses stochastic decoder that can positively or negatively impact the bias-sensitive tokens initially predicted by the model. To address this gap in research, we present an extensive analysis of bias from decoding techniques for open-domain language generation considering the entire decoding space. We analyze to what extent bias metrics like toxicity and sentiment are impacted by the individual components of decoder algorithms. To this extent, we also analyze the trade-off between bias scores and human-annotated generation quality throughout the decoder space. Together, these methods reveal the imperative of testing inference time bias and provide evidence on the usefulness of inspecting the entire decoding spectrum.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2022,
    "referenceCount": 64,
    "citationCount": 1,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "50378124",
        "name": "M. Das"
      },
      {
        "authorId": "1720266",
        "name": "Wolf-Tilo Balke"
      }
    ]
  },
  "259370533": {
    "paperId": "2386c6a7c40b5129960f2eb3c6be27db65b04c1f",
    "externalIds": {
      "ACL": "2023.acl-long.163",
      "ArXiv": "2307.04303",
      "DBLP": "journals/corr/abs-2307-04303",
      "DOI": "10.48550/arXiv.2307.04303",
      "CorpusId": 259370533
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Learning to Generate Equitable Text in Dialogue from Biased Training Data",
    "abstract": "The ingrained principles of fairness in a dialogue system\u2019s decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. Yet, there is no comprehensive study of equitable text generation in dialogue. Aptly, in this work, we use theories of computational learning to study this problem. We provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data). With this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable text without any modifications to the biased training data on which they learn. To exemplify our theory in practice, we look at a group of algorithms for the GuessWhat?! visual dialogue game and, using this example, test our theory empirically. Our theory accurately predicts relative-performance of multiple algorithms in generating equitable text as measured by both human and automated evaluation.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 40,
    "citationCount": 14,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51137683",
        "name": "Anthony Sicilia"
      },
      {
        "authorId": "2715920",
        "name": "Malihe Alikhani"
      }
    ]
  },
  "221819206": {
    "paperId": "56fdad6964cf6da35d836f9b4777ad0f94c2b487",
    "externalIds": {
      "MAG": "3101934021",
      "ACL": "2020.emnlp-main.232",
      "DBLP": "conf/emnlp/VargasC20",
      "ArXiv": "2009.09435",
      "DOI": "10.18653/v1/2020.emnlp-main.232",
      "CorpusId": 221819206
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation",
    "abstract": "Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias sub-space is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 27,
    "citationCount": 24,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2055384350",
        "name": "Francisco Vargas"
      },
      {
        "authorId": "1750769",
        "name": "Ryan Cotterell"
      }
    ]
  },
  "259859034": {
    "paperId": "c373c792bcf5d0add8de812425d384ff101ef070",
    "externalIds": {
      "DBLP": "conf/acl/YuJKYJ23",
      "DOI": "10.18653/v1/2023.findings-acl.375",
      "CorpusId": 259859034
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Unlearning Bias in Language Models by Partitioning Gradients",
    "abstract": ",",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 51,
    "citationCount": 64,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2110963190",
        "name": "Charles Yu"
      },
      {
        "authorId": "2165226931",
        "name": "Sullam Jeoung"
      },
      {
        "authorId": "1581343094",
        "name": "Anish Kasi"
      },
      {
        "authorId": "144808890",
        "name": "Pengfei Yu"
      },
      {
        "authorId": "2072975661",
        "name": "Heng Ji"
      }
    ]
  },
  "256827819": {
    "paperId": "fe7a0612b24b48fe09304af39cdb27d8e33697c6",
    "externalIds": {
      "DBLP": "conf/eacl/KumarLZCESR23",
      "ACL": "2023.eacl-main.201",
      "ArXiv": "2302.06321",
      "DOI": "10.48550/arXiv.2302.06321",
      "CorpusId": 256827819
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "Parameter-efficient Modularised Bias Mitigation via AdapterFusion",
    "abstract": "Large pre-trained language models contain societal biases and carry along these biases to downstream tasks. Current in-processing bias mitigation approaches (like adversarial training) impose debiasing by updating a model\u2019s parameters, effectively transferring the model to a new, irreversible debiased state. In this work, we propose a novel approach to develop stand-alone debiasing functionalities separate from the model, which can be integrated into the model on-demand, while keeping the core model untouched. Drawing from the concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing with Adapter Modules) \u2013 a debiasing approach to first encapsulate arbitrary bias mitigation functionalities into separate adapters, and then add them to the model on-demand in order to deliver fairness qualities. We conduct a large set of experiments on three classification tasks with gender, race, and age as protected attributes. Our results show that DAM improves or maintains the effectiveness of bias mitigation, avoids catastrophic forgetting in a multi-attribute scenario, and maintains on-par task performance, while granting parameter-efficiency and easy switching between the original and debiased models.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 58,
    "citationCount": 23,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2116415074",
        "name": "Deepak Kumar"
      },
      {
        "authorId": "2053814964",
        "name": "Oleg Lesota"
      },
      {
        "authorId": "30647302",
        "name": "George Zerveas"
      },
      {
        "authorId": "144011619",
        "name": "Daniel Cohen"
      },
      {
        "authorId": "30044743",
        "name": "Carsten Eickhoff"
      },
      {
        "authorId": "144125621",
        "name": "M. Schedl"
      },
      {
        "authorId": "2844293",
        "name": "Navid Rekabsaz"
      }
    ]
  },
  "195316733": {
    "paperId": "493fac37cea49afb98c52c2f5dd75c303a325b25",
    "externalIds": {
      "MAG": "2951864292",
      "DBLP": "journals/corr/abs-1906-08976",
      "ACL": "P19-1159",
      "ArXiv": "1906.08976",
      "DOI": "10.18653/v1/P19-1159",
      "CorpusId": 195316733
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Mitigating Gender Bias in Natural Language Processing: Literature Review",
    "abstract": "As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 63,
    "citationCount": 503,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1516120843",
        "name": "Tony Sun"
      },
      {
        "authorId": "146072982",
        "name": "Andrew Gaut"
      },
      {
        "authorId": "148149462",
        "name": "Shirlyn Tang"
      },
      {
        "authorId": "2154731574",
        "name": "Yuxin Huang"
      },
      {
        "authorId": "2165346",
        "name": "Mai Elsherief"
      },
      {
        "authorId": "33524946",
        "name": "Jieyu Zhao"
      },
      {
        "authorId": "1705929",
        "name": "Diba Mirza"
      },
      {
        "authorId": "1397933253",
        "name": "E. Belding-Royer"
      },
      {
        "authorId": "2782886",
        "name": "Kai-Wei Chang"
      },
      {
        "authorId": "1682479",
        "name": "William Yang Wang"
      }
    ]
  },
  "259095603": {
    "paperId": "f891e9eeedbf20cdc54429ffcc0402a10f48494e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-04597",
      "ArXiv": "2306.04597",
      "ACL": "2023.acl-short.30",
      "DOI": "10.48550/arXiv.2306.04597",
      "CorpusId": 259095603
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
    "abstract": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 31,
    "citationCount": 20,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2221493995",
        "name": "Himanshu Thakur"
      },
      {
        "authorId": "1819271266",
        "name": "Atishay Jain"
      },
      {
        "authorId": "2127734657",
        "name": "Praneetha Vaddamanu"
      },
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "49933077",
        "name": "Louis-philippe Morency"
      }
    ]
  },
  "250562745": {
    "paperId": "2a83a92b08e0f3873d07162c73c67e533321112e",
    "externalIds": {
      "DBLP": "conf/naacl/LiuZFV22",
      "DOI": "10.18653/v1/2022.findings-naacl.18",
      "CorpusId": 250562745
    },
    "publicationVenue": null,
    "title": "Aligning Generative Language Models with Human Values",
    "abstract": ",",
    "venue": "NAACL-HLT",
    "year": 2022,
    "referenceCount": 55,
    "citationCount": 43,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "7247867",
        "name": "Ruibo Liu"
      },
      {
        "authorId": "2143853895",
        "name": "Ge Zhang"
      },
      {
        "authorId": "2109365500",
        "name": "Xinyu Feng"
      },
      {
        "authorId": "1918441",
        "name": "Soroush Vosoughi"
      }
    ]
  },
  "258378162": {
    "paperId": "f6ee3544b8c53ef2f1015e5c29269ae36c428352",
    "externalIds": {
      "ACL": "2023.eacl-main.189",
      "DBLP": "conf/eacl/MadanagopalC23",
      "DOI": "10.18653/v1/2023.eacl-main.189",
      "CorpusId": 258378162
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "Reinforced Sequence Training based Subjective Bias Correction",
    "abstract": "Subjective bias is ubiquitous on news sites, social media, and knowledge resources like Wikipedia. Many existing methods for subjective bias correction have typically focused on making one-word edits and have been trained over a single (often, noisy) domain. In contrast, we propose a novel reinforced sequence training approach for robust subjective bias correction. Three of the unique characteristics of the approach are: (i) it balances bias neutralization with fluency and semantics preservation through reinforcement learning, to broaden the scope to bias beyond a single word; (ii) it is cross-trained over multiple sources of bias to be more robust to new styles of biased writing that are not seen in the training data for a single domain; and (iii) it is used to fine-tune a large pre-trained transformer model to yield state-of-the-art performance in bias text correction task. Extensive experiments show that the proposed approach results in significant improvements in subjective bias correction versus alternatives.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 56,
    "citationCount": 1,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2914608",
        "name": "K. Madanagopal"
      },
      {
        "authorId": "1697232",
        "name": "James Caverlee"
      }
    ]
  },
  "258833129": {
    "paperId": "f5c73d9e6641b018b633690102121f5605d34fb0",
    "externalIds": {
      "DBLP": "conf/emnlp/YaoWT0LDC023",
      "ArXiv": "2305.13172",
      "DOI": "10.48550/arXiv.2305.13172",
      "CorpusId": 258833129
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
    "abstract": "Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context. Code and datasets are available at https://github.com/zjunlp/EasyEdit.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 91,
    "citationCount": 201,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "4841460",
        "name": "Yunzhi Yao"
      },
      {
        "authorId": "144282672",
        "name": "Peng Wang"
      },
      {
        "authorId": "2064522174",
        "name": "Bo Tian"
      },
      {
        "authorId": "46378881",
        "name": "Siyuan Cheng"
      },
      {
        "authorId": "9956037",
        "name": "Zhoubo Li"
      },
      {
        "authorId": "152931849",
        "name": "Shumin Deng"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      },
      {
        "authorId": "2608639",
        "name": "Ningyu Zhang"
      }
    ]
  },
  "249642147": {
    "paperId": "1d650f1afd45c59ff907396fe8b678595dcb85ea",
    "externalIds": {
      "DBLP": "conf/icml/MitchellLBMF22",
      "ArXiv": "2206.06520",
      "CorpusId": 249642147
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Memory-Based Model Editing at Scale",
    "abstract": "Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit's intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model's predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.",
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "referenceCount": 50,
    "citationCount": 256,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49688913",
        "name": "E. Mitchell"
      },
      {
        "authorId": "2116721670",
        "name": "Charles Lin"
      },
      {
        "authorId": "2691021",
        "name": "Antoine Bosselut"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      },
      {
        "authorId": "46881670",
        "name": "Chelsea Finn"
      }
    ]
  },
  "252762125": {
    "paperId": "7471cb40a33e9d971a922b5dff5ca9b4a73ca609",
    "externalIds": {
      "DBLP": "journals/corr/abs-2210-03329",
      "ArXiv": "2210.03329",
      "DOI": "10.48550/arXiv.2210.03329",
      "CorpusId": 252762125
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Calibrating Factual Knowledge in Pretrained Language Models",
    "abstract": "Previous literature has proved that Pretrained Language Models (PLMs) can store factual knowledge. However, we find that facts stored in the PLMs are not always correct. It motivates us to explore a fundamental question: How do we calibrate factual knowledge in PLMs without re-training from scratch? In this work, we propose a simple and lightweight method CaliNet to achieve this goal. To be specific, we first detect whether PLMs can learn the right facts via a contrastive score between right and fake facts. If not, we then use a lightweight method to add and adapt new parameters to specific factual texts. Experiments on the knowledge probing task show the calibration effectiveness and efficiency. In addition, through closed-book question answering, we find that the calibrated PLM possesses knowledge generalization ability after fine-tuning. Beyond the calibration performance, we further investigate and visualize the knowledge calibration mechanism.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 26,
    "citationCount": 72,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2047143813",
        "name": "Qingxiu Dong"
      },
      {
        "authorId": "10780897",
        "name": "Damai Dai"
      },
      {
        "authorId": "2183730942",
        "name": "Yifan Song"
      },
      {
        "authorId": "47883405",
        "name": "Jingjing Xu"
      },
      {
        "authorId": "3335836",
        "name": "Zhifang Sui"
      },
      {
        "authorId": "143900005",
        "name": "Lei Li"
      }
    ]
  },
  "256194369": {
    "paperId": "a9be51698e7c2247853b7b6f1f70fc4d6d7ef605",
    "externalIds": {
      "DBLP": "journals/corr/abs-2301-09785",
      "ArXiv": "2301.09785",
      "DOI": "10.48550/arXiv.2301.09785",
      "CorpusId": 256194369
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Transformer-Patcher: One Mistake worth One Neuron",
    "abstract": "Large Transformer-based Pretrained Language Models (PLMs) dominate almost all Natural Language Processing (NLP) tasks. Nevertheless, they still make mistakes from time to time. For a model deployed in an industrial environment, fixing these mistakes quickly and robustly is vital to improve user experiences. Previous works formalize such problems as Model Editing (ME) and mostly focus on fixing one mistake. However, the one-mistake-fixing scenario is not an accurate abstraction of the real-world challenge. In the deployment of AI services, there are ever-emerging mistakes, and the same mistake may recur if not corrected in time. Thus a preferable solution is to rectify the mistakes as soon as they appear nonstop. Therefore, we extend the existing ME into Sequential Model Editing (SME) to help develop more practical editing methods. Our study shows that most current ME methods could yield unsatisfying results in this scenario. We then introduce Transformer-Patcher, a novel model editor that can shift the behavior of transformer-based models by simply adding and training a few neurons in the last Feed-Forward Network layer. Experimental results on both classification and generation tasks show that Transformer-Patcher can successively correct up to thousands of errors (Reliability) and generalize to their equivalent inputs (Generality) while retaining the model's accuracy on irrelevant inputs (Locality). Our method outperforms previous fine-tuning and HyperNetwork-based methods and achieves state-of-the-art performance for Sequential Model Editing (SME). The code is available at https://github.com/ZeroYuHuang/Transformer-Patcher.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 33,
    "citationCount": 126,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2109583210",
        "name": "Zeyu Huang"
      },
      {
        "authorId": "2714199",
        "name": "Yikang Shen"
      },
      {
        "authorId": "2144555913",
        "name": "Xiaofeng Zhang"
      },
      {
        "authorId": "49178343",
        "name": "Jie Zhou"
      },
      {
        "authorId": "21505283",
        "name": "Wenge Rong"
      },
      {
        "authorId": "2091444262",
        "name": "Zhang Xiong"
      }
    ]
  },
  "258832407": {
    "paperId": "ff2a0fb125e7f03428420230c6ecbeafd4cf07a8",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-12740",
      "ArXiv": "2305.12740",
      "DOI": "10.48550/arXiv.2305.12740",
      "CorpusId": 258832407
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Can We Edit Factual Knowledge by In-Context Learning?",
    "abstract": "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/Zce1112zslx/IKE.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 44,
    "citationCount": 134,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2113919886",
        "name": "Ce Zheng"
      },
      {
        "authorId": "49192881",
        "name": "Lei Li"
      },
      {
        "authorId": "2047143813",
        "name": "Qingxiu Dong"
      },
      {
        "authorId": "2118167265",
        "name": "Yuxuan Fan"
      },
      {
        "authorId": "150358371",
        "name": "Zhiyong Wu"
      },
      {
        "authorId": "47883405",
        "name": "Jingjing Xu"
      },
      {
        "authorId": "7267809",
        "name": "Baobao Chang"
      }
    ]
  },
  "233289412": {
    "paperId": "240b0caabb415578bdea4da7d0a32bdff2e8163f",
    "externalIds": {
      "ArXiv": "2104.08164",
      "DBLP": "journals/corr/abs-2104-08164",
      "ACL": "2021.emnlp-main.522",
      "DOI": "10.18653/v1/2021.emnlp-main.522",
      "CorpusId": 233289412
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Editing Factual Knowledge in Language Models",
    "abstract": "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix \u2018bugs\u2019 or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor\u2019s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a \u2018probe\u2019 revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 63,
    "citationCount": 407,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "41019080",
        "name": "Nicola De Cao"
      },
      {
        "authorId": "2782694",
        "name": "Wilker Aziz"
      },
      {
        "authorId": "144889265",
        "name": "Ivan Titov"
      }
    ]
  },
  "239050360": {
    "paperId": "9286ac6e9b1aacd7d93496eb4615ae7678876d2a",
    "externalIds": {
      "DBLP": "journals/corr/abs-2110-11309",
      "ArXiv": "2110.11309",
      "CorpusId": 239050360
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Fast Model Editing at Scale",
    "abstract": "While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks using Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model's behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code and data available at https://sites.google.com/view/mend-editing.",
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "referenceCount": 56,
    "citationCount": 282,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49688913",
        "name": "E. Mitchell"
      },
      {
        "authorId": "2116721670",
        "name": "Charles Lin"
      },
      {
        "authorId": "2691021",
        "name": "Antoine Bosselut"
      },
      {
        "authorId": "46881670",
        "name": "Chelsea Finn"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      }
    ]
  },
  "233296761": {
    "paperId": "2c871df72c52b58f05447fcb3afc838168d94505",
    "externalIds": {
      "ArXiv": "2104.08696",
      "DBLP": "journals/corr/abs-2104-08696",
      "ACL": "2022.acl-long.581",
      "DOI": "10.18653/v1/2022.acl-long.581",
      "CorpusId": 233296761
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Knowledge Neurons in Pretrained Transformers",
    "abstract": "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 38,
    "citationCount": 338,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "10780897",
        "name": "Damai Dai"
      },
      {
        "authorId": "145307652",
        "name": "Li Dong"
      },
      {
        "authorId": "34128716",
        "name": "Y. Hao"
      },
      {
        "authorId": "3335836",
        "name": "Zhifang Sui"
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei"
      }
    ]
  },
  "255825985": {
    "paperId": "996445d847f06e99b0bd259345408a0cf1bce87e",
    "externalIds": {
      "DBLP": "conf/nips/MengBAB22",
      "ArXiv": "2202.05262",
      "CorpusId": 255825985
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Locating and Editing Factual Associations in GPT",
    "abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "referenceCount": 59,
    "citationCount": 879,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "153615419",
        "name": "Kevin Meng"
      },
      {
        "authorId": "144159726",
        "name": "David Bau"
      },
      {
        "authorId": "50112310",
        "name": "A. Andonian"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      }
    ]
  },
  "252873467": {
    "paperId": "2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413",
    "externalIds": {
      "DBLP": "conf/iclr/MengSABB23",
      "ArXiv": "2210.07229",
      "DOI": "10.48550/arXiv.2210.07229",
      "CorpusId": 252873467
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Mass-Editing Memory in a Transformer",
    "abstract": "Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info.",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "referenceCount": 58,
    "citationCount": 391,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "153615419",
        "name": "Kevin Meng"
      },
      {
        "authorId": "1429844787",
        "name": "Arnab Sen Sharma"
      },
      {
        "authorId": "50112310",
        "name": "A. Andonian"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "144159726",
        "name": "David Bau"
      }
    ]
  },
  "258865984": {
    "paperId": "56e952fd463accff09cf2e35432aaabd7c7c57f3",
    "externalIds": {
      "DBLP": "conf/emnlp/ZhongWMPC23",
      "ArXiv": "2305.14795",
      "DOI": "10.48550/arXiv.2305.14795",
      "CorpusId": 258865984
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
    "abstract": "The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark, MQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (e.g., OpenAI GPT-3.5-turbo) and outperforms previous model editors by a large margin.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 35,
    "citationCount": 142,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49164966",
        "name": "Zexuan Zhong"
      },
      {
        "authorId": "47039337",
        "name": "Zhengxuan Wu"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      },
      {
        "authorId": "144922861",
        "name": "Christopher Potts"
      },
      {
        "authorId": "50536468",
        "name": "Danqi Chen"
      }
    ]
  },
  "258437155": {
    "paperId": "56da914761e445a24481629cfc116336a0aec978",
    "externalIds": {
      "ACL": "2023.acl-long.300",
      "DBLP": "journals/corr/abs-2305-01651",
      "ArXiv": "2305.01651",
      "DOI": "10.48550/arXiv.2305.01651",
      "CorpusId": 258437155
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
    "abstract": "Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs\u2019 abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These methods improve performance on cloze instances only when there is lexical overlap between injected facts and target inferences. Yet, prepending entity definitions in an LM\u2019s context improves performance across all settings, suggesting that there is substantial headroom for parameter-updating approaches for knowledge injection.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 30,
    "citationCount": 60,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "115412405",
        "name": "Yasumasa Onoe"
      },
      {
        "authorId": "2129403254",
        "name": "Michael J.Q. Zhang"
      },
      {
        "authorId": "2204461780",
        "name": "Shankar Padmanabhan"
      },
      {
        "authorId": "1814094",
        "name": "Greg Durrett"
      },
      {
        "authorId": "2890423",
        "name": "Eunsol Choi"
      }
    ]
  },
  "258960406": {
    "paperId": "cc57a02307b77585f69779cca2937dedc69006d6",
    "externalIds": {
      "ArXiv": "2305.17553",
      "DBLP": "journals/corr/abs-2305-17553",
      "DOI": "10.48550/arXiv.2305.17553",
      "CorpusId": 258960406
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
    "abstract": "Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing CounterFact benchmark to include a dynamic component and dub our benchmark CounterFact+. Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric. We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity. Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 19,
    "citationCount": 49,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1388392024",
        "name": "J. Hoelscher-Obermaier"
      },
      {
        "authorId": "2218886081",
        "name": "Julia Persson"
      },
      {
        "authorId": "2005663935",
        "name": "Esben Kran"
      },
      {
        "authorId": "2621022",
        "name": "Ioannis Konstas"
      },
      {
        "authorId": "2143198655",
        "name": "Fazl Barez"
      }
    ]
  },
  "258865393": {
    "paperId": "2b72888cc3ff048038f6011b8e3d89ba106540b6",
    "externalIds": {
      "ArXiv": "2305.14956",
      "DBLP": "conf/emnlp/GuptaMS00WT23",
      "DOI": "10.18653/v1/2023.emnlp-main.511",
      "CorpusId": 258865393
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Editing Common Sense in Transformers",
    "abstract": "Editing model parameters directly in Transformers makes updating open-source transformer-based models possible without re-training (Meng et al., 2023). However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers' reliability and usefulness. In this paper, we investigate whether commonsense judgments are causally associated with localized, editable parameters in Transformers, and we provide an affirmative answer. We find that directly applying the MEMIT editing algorithm results in sub-par performance and improve it for the commonsense domain by varying edit tokens and improving the layer selection strategy, i.e., $MEMIT_{CSK}$. GPT-2 Large and XL models edited using $MEMIT_{CSK}$ outperform best-fine-tuned baselines by 10.97% and 10.73% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novel evaluation dataset, PROBE SET, that contains unaffected and affected neighborhoods, affected paraphrases, and affected reasoning challenges. $MEMIT_{CSK}$ performs well across the metrics while fine-tuning baselines show significant trade-offs between unaffected and affected metrics. These results suggest a compelling future direction for incorporating feedback about common sense into Transformers through direct model editing.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 45,
    "citationCount": 12,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2200083553",
        "name": "Anshita Gupta"
      },
      {
        "authorId": "2261673039",
        "name": "Debanjan Mondal"
      },
      {
        "authorId": "2046876495",
        "name": "Akshay Krishna Sheshadri"
      },
      {
        "authorId": "50771250",
        "name": "Wenlong Zhao"
      },
      {
        "authorId": "1737850",
        "name": "Xiang Lorraine Li"
      },
      {
        "authorId": "35823986",
        "name": "Sarah Wiegreffe"
      },
      {
        "authorId": "1721168",
        "name": "Niket Tandon"
      }
    ]
  },
  "250637571": {
    "paperId": "7958647ee241185ab253cdaa63466033e37e78ca",
    "externalIds": {
      "DBLP": "conf/ijcai/GuTL22",
      "DOI": "10.24963/ijcai.2022/768",
      "CorpusId": 250637571
    },
    "publicationVenue": {
      "id": "67f7f831-711a-43c8-8785-1e09005359b5",
      "name": "International Joint Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Int Jt Conf Artif Intell",
        "IJCAI"
      ],
      "url": "http://www.ijcai.org/"
    },
    "title": "Who Says What to Whom: A Survey of Multi-Party Conversations",
    "abstract": "Multi-party conversations (MPCs) are a more practical and challenging scenario involving more than two interlocutors. This research topic has drawn significant attention from both academia and industry, and it is nowadays counted as one of the most promising research areas in the field of dialogue systems. In general, MPC algorithms aim at addressing the issues of Who says What to Whom, specifically, who speaks, say what, and address whom. The complicated interactions between interlocutors, between utterances, and between interlocutors and utterances develop many variant tasks of MPCs worth investigation. In this paper, we present a comprehensive survey of recent advances in text-based MPCs. In particular, we first summarize recent advances on the research of MPC context modeling including dialogue discourse parsing, dialogue flow modeling and self-supervised training for MPCs. Then we review the state-of-the-art models categorized by Who says What to Whom in MPCs. Finally, we highlight the challenges which are not yet well addressed in MPCs and present future research directions.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2022,
    "referenceCount": 79,
    "citationCount": 26,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3028818",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "8801869",
        "name": "Chongyang Tao"
      },
      {
        "authorId": "1749989",
        "name": "Zhenhua Ling"
      }
    ]
  },
  "232110776": {
    "paperId": "281b4a7e7fb057d8266ec0610888905c46fd715d",
    "externalIds": {
      "ArXiv": "2110.04984",
      "DBLP": "journals/corr/abs-2110-04984",
      "CorpusId": 232110776
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Advances in Multi-turn Dialogue Comprehension: A Survey",
    "abstract": "Training machines to understand natural language and interact with humans is an elusive and essential task of artificial intelligence. A diversity of dialogue systems has been designed with the rapid development of deep learning techniques, especially the recent pre-trained language models (PrLMs). Among these studies, the fundamental yet challenging type of task is dialogue comprehension whose role is to teach the machines to read and comprehend the dialogue context before responding. In this paper, we review the previous methods from the technical perspective of dialogue modeling for the dialogue comprehension task. We summarize the characteristics and challenges of dialogue comprehension in contrast to plain-text reading comprehension. Then, we discuss three typical patterns of dialogue modeling. In addition, we categorize dialogue-related pre-training techniques which are employed to enhance PrLMs in dialogue scenarios. Finally, we highlight the technical advances in recent years and point out the lessons from the empirical analysis and the prospects towards a new frontier of researches.",
    "venue": "arXiv.org",
    "year": 2021,
    "referenceCount": 119,
    "citationCount": 19,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3322871",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "47941144",
        "name": "Hai Zhao"
      }
    ]
  },
  "16537814": {
    "paperId": "811e014002d1e4d1e185fc236cf9e3fafe2aade5",
    "externalIds": {
      "ACL": "D16-1231",
      "DBLP": "conf/emnlp/OuchiT16",
      "MAG": "2561158378",
      "DOI": "10.18653/v1/D16-1231",
      "CorpusId": 16537814
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Addressee and Response Selection for Multi-Party Conversation",
    "abstract": "To create conversational systems working in actual situations, it is crucial to assume that they interact with multiple agents. In this work, we tackle addressee and response selection for multi-party conversation, in which systems are expected to select whom they address as well as what they say. The key challenge of this task is to jointly model who is talking about what in a previous context. For the joint modeling, we propose two modeling frameworks: 1) static modeling and 2) dynamic modeling. To show benchmark results of our frameworks, we created a multi-party conversation corpus. Our experiments on the dataset show that the recurrent neural network based models of our frameworks robustly predict addressees and responses in conversations with a large number of agents.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2016,
    "referenceCount": 34,
    "citationCount": 60,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "33516663",
        "name": "Hiroki Ouchi"
      },
      {
        "authorId": "3229899",
        "name": "Yuta Tsuboi"
      }
    ]
  },
  "173188574": {
    "paperId": "5c0909aab443692887b25261da3af74d570c07cd",
    "externalIds": {
      "MAG": "2966126777",
      "DBLP": "conf/ijcai/HuCL0MY19",
      "ArXiv": "1905.13637",
      "DOI": "10.24963/ijcai.2019/696",
      "CorpusId": 173188574
    },
    "publicationVenue": {
      "id": "67f7f831-711a-43c8-8785-1e09005359b5",
      "name": "International Joint Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "Int Jt Conf Artif Intell",
        "IJCAI"
      ],
      "url": "http://www.ijcai.org/"
    },
    "title": "GSN: A Graph-Structured Network for Multi-Party Dialogues",
    "abstract": "Existing neural models for dialogue response generation assume that utterances are sequentially organized. However, many real-world dialogues involve multiple interlocutors (i.e., multi-party dialogues), where the assumption does not hold as utterances from different interlocutors can occur ``in parallel.'' This paper generalizes existing sequence-based models to a Graph-Structured neural Network (GSN) for dialogue modeling. The core of GSN is a graph-based encoder that can model the information flow along the graph-structured dialogues (two-party sequential dialogues are a special case). Experimental results show that GSN significantly outperforms existing sequence-based models.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2019,
    "referenceCount": 34,
    "citationCount": 68,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "7849217",
        "name": "Wenpeng Hu"
      },
      {
        "authorId": "51177175",
        "name": "Zhangming Chan"
      },
      {
        "authorId": "47655430",
        "name": "Bing Liu"
      },
      {
        "authorId": "144060462",
        "name": "Dongyan Zhao"
      },
      {
        "authorId": "1685259",
        "name": "Jinwen Ma"
      },
      {
        "authorId": "144539156",
        "name": "Rui Yan"
      }
    ]
  },
  "196192979": {
    "paperId": "c59d36e79d573cc4a2440cb2a7154eada5c0ead2",
    "externalIds": {
      "MAG": "2949600515",
      "DBLP": "conf/acl/KummerfeldGPAGG19",
      "ACL": "P19-1374",
      "DOI": "10.18653/v1/P19-1374",
      "CorpusId": 196192979
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Large-Scale Corpus for Conversation Disentanglement",
    "abstract": "Disentangling conversations mixed together in a single stream of messages is a difficult task, made harder by the lack of large manually annotated datasets. We created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure. Our data is 16 times larger than all previously released datasets combined, the first to include adjudication of annotation disagreements, and the first to include context. We use our data to re-examine prior work, in particular, finding that 89% of conversations in a widely used dialogue corpus are either missing messages or contain extra messages. Our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement, which will help advance dialogue research.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 41,
    "citationCount": 95,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1727211",
        "name": "Jonathan K. Kummerfeld"
      },
      {
        "authorId": "1905888",
        "name": "S. R. Gouravajhala"
      },
      {
        "authorId": "79548673",
        "name": "Joseph Peper"
      },
      {
        "authorId": "81176329",
        "name": "V. Athreya"
      },
      {
        "authorId": "144543562",
        "name": "R. Chulaka Gunasekara"
      },
      {
        "authorId": "2504586",
        "name": "Jatin Ganhotra"
      },
      {
        "authorId": "80836534",
        "name": "S. Patel"
      },
      {
        "authorId": "1725498",
        "name": "L. Polymenakos"
      },
      {
        "authorId": "2598433",
        "name": "Walter S. Lasecki"
      }
    ]
  },
  "235313361": {
    "paperId": "51b9f8aef39de4b6db820b5c4b5bca14fc32aa4d",
    "externalIds": {
      "ArXiv": "2106.01541",
      "ACL": "2021.acl-long.285",
      "DBLP": "journals/corr/abs-2106-01541",
      "DOI": "10.18653/v1/2021.acl-long.285",
      "CorpusId": 235313361
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding",
    "abstract": "Recently, various neural models for multi-party conversation (MPC) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction. However, these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process. To this end, we present MPC-BERT, a pre-trained model for MPC understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks. Particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection. We evaluate MPC-BERT on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 26,
    "citationCount": 49,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3028818",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "8801869",
        "name": "Chongyang Tao"
      },
      {
        "authorId": "1749989",
        "name": "Zhenhua Ling"
      },
      {
        "authorId": "2110091832",
        "name": "Can Xu"
      },
      {
        "authorId": "2442662",
        "name": "Xiubo Geng"
      },
      {
        "authorId": "71790825",
        "name": "Daxin Jiang"
      }
    ]
  },
  "247451284": {
    "paperId": "23bb7ac9d1164b0b429e59eb012584c1c1c64e73",
    "externalIds": {
      "DBLP": "conf/acl/Ma0Z22",
      "ArXiv": "2110.08018",
      "ACL": "2022.acl-long.23",
      "DOI": "10.18653/v1/2022.acl-long.23",
      "CorpusId": 247451284
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Structural Characterization for Dialogue Disentanglement",
    "abstract": "Tangled multi-party dialogue contexts lead to challenges for dialogue reading comprehension, where multiple dialogue threads flow simultaneously within a common dialogue record, increasing difficulties in understanding the dialogue history for both human and machine. Previous studies mainly focus on utterance encoding methods with carefully designed features but pay inadequate attention to characteristic features of the structure of dialogues. We specially take structure factors into account and design a novel model for dialogue disentangling. Based on the fact that dialogues are constructed on successive participation and interactions between speakers, we model structural information of dialogues in two aspects: 1)speaker property that indicates whom a message is from, and 2) reference dependency that shows whom a message may refer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC benchmark dataset and contributes to dialogue-related comprehension.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 66,
    "citationCount": 15,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2141114505",
        "name": "Xinbei Ma"
      },
      {
        "authorId": "3322871",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "47941144",
        "name": "Hai Zhao"
      }
    ]
  },
  "247476252": {
    "paperId": "daadd8b4af33abc89f1148ee1685b8a3099759ed",
    "externalIds": {
      "ACL": "2022.acl-long.349",
      "DBLP": "journals/corr/abs-2203-08500",
      "ArXiv": "2203.08500",
      "DOI": "10.48550/arXiv.2203.08500",
      "CorpusId": 247476252
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations",
    "abstract": "Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated. Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is more challenging, since there exist complicated context structures and the generated responses heavily rely on both interlocutors (i.e., speaker and addressee) and history utterances. To address these challenges, we present HeterMPC, a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph. Besides, we also design six types of meta relations with node-edge-type-dependent parameters to characterize the heterogeneous interactions within the graph. Through multi-hop updating, HeterMPC can adequately utilize the structural knowledge of conversations for response generation. Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 46,
    "citationCount": 23,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3028818",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "2111728713",
        "name": "Chao-Hong Tan"
      },
      {
        "authorId": "8801869",
        "name": "Chongyang Tao"
      },
      {
        "authorId": "2072392338",
        "name": "Zhen-Hua Ling"
      },
      {
        "authorId": "2144026961",
        "name": "Huang Hu"
      },
      {
        "authorId": "2442662",
        "name": "Xiubo Geng"
      },
      {
        "authorId": "2086994543",
        "name": "Daxin Jiang"
      }
    ]
  },
  "258833159": {
    "paperId": "13068f5f0f2ab8a50ee0c43a9362e351d2019377",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-12412",
      "ArXiv": "2305.12412",
      "ACL": "2023.acl-long.7",
      "DOI": "10.48550/arXiv.2305.12412",
      "CorpusId": 258833159
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "EM Pre-training for Multi-party Dialogue Response Generation",
    "abstract": "Dialogue response generation requires an agent to generate a response according to the current dialogue history, in terms of which two-party dialogues have been well studied, but leaving a great gap for multi-party dialogues at the same time. Different from two-party dialogues where each response is a direct reply to its previous utterance, the addressee of a response utterance should be specified before it is generated in the multi-party scenario. Thanks to the huge amount of two-party conversational data, various pre-trained language models for two-party dialogue response generation have been proposed. However, due to the lack of annotated addressee labels in multi-party dialogue datasets, it is hard to use them to pre-train a response generation model for multi-party dialogues. To tackle this obstacle, we propose an Expectation-Maximization (EM) approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Theoretical analyses and extensive experiments have justified the feasibility and effectiveness of our proposed method. The official implementation of this paper is available at https://github.com/EricLee8/MPDRG.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 29,
    "citationCount": 8,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2110418724",
        "name": "Yiyang Li"
      },
      {
        "authorId": "2146232510",
        "name": "Hai Zhao"
      }
    ]
  },
  "258715296": {
    "paperId": "9c2d502097be3a364d51315123c248282d6d534e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-09360",
      "ACL": "2023.acl-long.651",
      "ArXiv": "2305.09360",
      "DOI": "10.48550/arXiv.2305.09360",
      "CorpusId": 258715296
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding",
    "abstract": "Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the performance on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that GIFT can significantly improve the performance of three PLMs on three downstream tasks and two benchmarks with only 4 additional parameters per encoding layer, achieving new state-of-the-art performance on MPC understanding.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 35,
    "citationCount": 6,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3028818",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "2072392338",
        "name": "Zhen-Hua Ling"
      },
      {
        "authorId": "145014498",
        "name": "QUAN LIU"
      },
      {
        "authorId": "2155398718",
        "name": "Cong Liu"
      },
      {
        "authorId": "2090465180",
        "name": "Guoping Hu"
      }
    ]
  },
  "153312685": {
    "paperId": "b4693a93b033d6ec6c5c98a22797e43928ac7470",
    "externalIds": {
      "MAG": "3090350559",
      "DBLP": "journals/csur/DabreCK20",
      "DOI": "10.1145/3406095",
      "CorpusId": 153312685
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "A Survey of Multilingual Neural Machine Translation",
    "abstract": "We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in recent years. MNMT has been useful in improving translation quality as a result of translation knowledge transfer (transfer learning). MNMT is more promising and interesting than its statistical machine translation counterpart, because end-to-end modeling and distributed representations open new avenues for research on machine translation. Many approaches have been proposed to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and, hence, deserve further exploration. In this article, we present an in-depth survey of existing literature on MNMT. We first categorize various approaches based on their central use-case and then further categorize them based on resource scenarios, underlying modeling principles, core-issues, and challenges. Wherever possible, we address the strengths and weaknesses of several techniques by comparing them with each other. We also discuss the future directions for MNMT. This article is aimed towards both beginners and experts in NMT. We hope this article will serve as a starting point as well as a source of new ideas for researchers and engineers interested in MNMT.",
    "venue": "ACM Computing Surveys",
    "year": 2019,
    "referenceCount": 181,
    "citationCount": 244,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3209719",
        "name": "Raj Dabre"
      },
      {
        "authorId": "2427516",
        "name": "Chenhui Chu"
      },
      {
        "authorId": "1711973",
        "name": "Anoop Kunchukuttan"
      }
    ]
  },
  "140928479": {
    "paperId": "5a05cd1f253baaa1b67c55d22335403a6251094c",
    "externalIds": {
      "MAG": "39149507",
      "DOI": "10.1515/9783110252903.109",
      "CorpusId": 140928479
    },
    "publicationVenue": null,
    "title": "How anger rose: Hypothesis testing in diachronic semantics",
    "abstract": null,
    "venue": "",
    "year": 2011,
    "referenceCount": 0,
    "citationCount": 18,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1796288",
        "name": "D. Geeraerts"
      },
      {
        "authorId": "145914884",
        "name": "C. Gevaert"
      },
      {
        "authorId": "1754574",
        "name": "D. Speelman"
      }
    ]
  },
  "47019063": {
    "paperId": "a41cf8154a64fe95f9c362e5664aebb02b18ee85",
    "externalIds": {
      "ACL": "C18-1117",
      "DBLP": "conf/coling/KutuzovOSV18",
      "ArXiv": "1806.03537",
      "MAG": "2807650837",
      "CorpusId": 47019063
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Diachronic word embeddings and semantic shifts: a survey",
    "abstract": "Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of NLP, as well as prospects and possible applications.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2018,
    "referenceCount": 74,
    "citationCount": 291,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2689095",
        "name": "Andrey Kutuzov"
      },
      {
        "authorId": "2732223",
        "name": "Lilja \u00d8vrelid"
      },
      {
        "authorId": "3461918",
        "name": "Terrence Szymanski"
      },
      {
        "authorId": "2027091",
        "name": "Erik Velldal"
      }
    ]
  },
  "76666453": {
    "paperId": "755cda23e5b90d3c3f2aa027cb00dd071ee97386",
    "externalIds": {
      "MAG": "2921267133",
      "ArXiv": "1811.06278",
      "CorpusId": 76666453
    },
    "publicationVenue": null,
    "title": "Survey of Computational Approaches to Lexical Semantic Change",
    "abstract": "Our languages are in constant flux driven by external factors such as cultural, societal and technological changes, as well as by only partially understood internal motivations. Words acquire new meanings and lose old senses, new words are coined or borrowed from other languages and obsolete words slide into obscurity. Understanding the characteristics of shifts in the meaning and in the use of words is useful for those who work with the content of historical texts, the interested general public, but also in and of itself. The findings from automatic lexical semantic change detection, and the models of diachronic conceptual change are currently being incorporated in approaches for measuring document across-time similarity, information retrieval from long-term document archives, the design of OCR algorithms, and so on. In recent years we have seen a surge in interest in the academic community in computational methods and tools supporting inquiry into diachronic conceptual change and lexical replacement. This article is an extract of a survey of recent computational techniques to tackle lexical semantic change currently under review. In this article we focus on diachronic conceptual change as an extension of semantic change.",
    "venue": "",
    "year": 2018,
    "referenceCount": 252,
    "citationCount": 156,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1731960",
        "name": "Nina Tahmasebi"
      },
      {
        "authorId": "143739029",
        "name": "L. Borin"
      },
      {
        "authorId": "1774986",
        "name": "A. Jatowt"
      }
    ]
  },
  "257921251": {
    "paperId": "ab08495f575c7616b316b86a5e4cdbe84fb5d0e6",
    "externalIds": {
      "ArXiv": "2304.01666",
      "DBLP": "journals/csur/PeritiM24",
      "DOI": "10.1145/3672393",
      "CorpusId": 257921251
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "Lexical Semantic Change through Large Language Models: a Survey",
    "abstract": "Lexical Semantic Change (LSC) is the task of identifying, interpreting, and assessing the possible change over time in the meanings of a target word. Traditionally, LSC has been addressed by linguists and social scientists through manual and time-consuming analyses, which have thus been limited in terms of the volume, genres, and time-frame that can be considered. In recent years, computational approaches based on Natural Language Processing have gained increasing attention to automate LSC as much as possible. Significant advancements have been made by relying on Large Language Models (LLMs), which can handle the multiple usages of the words and better capture the related semantic change. In this article, we survey the approaches based on LLMs for LSC and we propose a classification framework characterized by three dimensions: meaning representation, time-awareness, and learning modality. The framework is exploited to i) review the measures for change assessment, ii) compare the approaches on performance, and iii) discuss the current issues in terms of scalability, interpretability, and robustness. Open challenges and future research directions about the use of LLMs for LSC are finally outlined.",
    "venue": "ACM Computing Surveys",
    "year": 2023,
    "referenceCount": 116,
    "citationCount": 18,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2136116967",
        "name": "Francesco Periti"
      },
      {
        "authorId": "1732265",
        "name": "S. Montanelli"
      }
    ]
  },
  "220686630": {
    "paperId": "a0895e9555527e30b82a8c66b6993683c6cabe14",
    "externalIds": {
      "MAG": "3045400194",
      "DBLP": "conf/semeval/SchlechtwegMHDT20",
      "ACL": "2020.semeval-1.1",
      "ArXiv": "2007.11464",
      "DOI": "10.18653/v1/2020.semeval-1.1",
      "CorpusId": 220686630
    },
    "publicationVenue": {
      "id": "70713d09-6e4b-4554-9d3f-94d08aba320c",
      "name": "International Workshop on Semantic Evaluation",
      "type": "conference",
      "alternate_names": [
        "SemEval ",
        "Int Workshop Semantic Evaluation"
      ]
    },
    "title": "SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection",
    "abstract": "Lexical Semantic Change detection, i.e., the task of identifying words that change meaning over time, is a very active research area, with applications in NLP, lexicography, and linguistics. Evaluation is currently the most pressing problem in Lexical Semantic Change detection, as no gold standards are available to the community, which hinders progress. We present the results of the first shared task that addresses this gap by providing researchers with an evaluation framework and manually annotated, high-quality datasets for English, German, Latin, and Swedish. 33 teams submitted 186 systems, which were evaluated on two subtasks.",
    "venue": "International Workshop on Semantic Evaluation",
    "year": 2020,
    "referenceCount": 103,
    "citationCount": 219,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3449121",
        "name": "Dominik Schlechtweg"
      },
      {
        "authorId": "144463772",
        "name": "Barbara McGillivray"
      },
      {
        "authorId": "3422512",
        "name": "Simon Hengchen"
      },
      {
        "authorId": "2026652",
        "name": "Haim Dubossarsky"
      },
      {
        "authorId": "1731960",
        "name": "Nina Tahmasebi"
      }
    ]
  },
  "229292864": {
    "paperId": "16af57bf06ebcba08fb50fd0582df93cab5ada89",
    "externalIds": {
      "MAG": "3115844705",
      "DBLP": "conf/evalita/BasileCCCV20",
      "DOI": "10.4000/BOOKS.AACCADEMIA.7613",
      "CorpusId": 229292864
    },
    "publicationVenue": {
      "id": "7100af1a-3160-4b14-9ad1-7bdd7dc4b314",
      "name": "International Workshop on Evaluation of Natural Language and Speech Tools for Italian",
      "type": "conference",
      "alternate_names": [
        "EVALITA",
        "Int Workshop Evaluation Nat Lang Speech Tool Ital"
      ]
    },
    "title": "DIACR-Ita @ EVALITA2020: Overview of the EVALITA2020 Diachronic Lexical Semantics (DIACR-Ita) Task",
    "abstract": "This paper describes the first edition of the \u201cDiachronic Lexical Seman-tics\u201d (DIACR-Ita) task at the EVALITA2020 campaign. The task challenges participants to develop systems that can automatically detect if a given word has changed its meaning over time, given con-textual information from corpora.The task, at its first edition, attracted 9 participant teams and collected a total of 36 sub-mission runs",
    "venue": "International Workshop on Evaluation of Natural Language and Speech Tools for Italian",
    "year": 2020,
    "referenceCount": 32,
    "citationCount": 43,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1731651",
        "name": "Pierpaolo Basile"
      },
      {
        "authorId": "25392172",
        "name": "A. Caputo"
      },
      {
        "authorId": "1864635",
        "name": "Tommaso Caselli"
      },
      {
        "authorId": "29931342",
        "name": "Pierluigi Cassotti"
      },
      {
        "authorId": "72137436",
        "name": "Rossella Varvara"
      }
    ]
  },
  "240007326": {
    "paperId": "e4ed8bf2155d35ee83a769b9c8f48fb299afc1d8",
    "externalIds": {
      "DOI": "10.28995/2075-7182-2021-20-533-545",
      "CorpusId": 240007326
    },
    "publicationVenue": null,
    "title": "RuShiftEval: a shared task on semantic shift detection for Russian",
    "abstract": "We present the \ufb01rst shared task on diachronic word meaning change detection for the Russian. The participating systems were provided with three sub-corpora of the Russian National Corpus \u2014 corresponding to pre-Soviet, Soviet and post-Soviet periods respectively \u2014 and a set of approximately one hundred Russian nouns. The task was to rank those nouns according to the degrees of their meaning change between periods. Although RuShiftEval is in many respects similar to the previous tasks organized for other languages",
    "venue": "",
    "year": 2021,
    "referenceCount": 26,
    "citationCount": 38,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145579909",
        "name": "Lidia Pivovarova"
      },
      {
        "authorId": "2689095",
        "name": "Andrey Kutuzov"
      }
    ]
  },
  "5480561": {
    "paperId": "7ee0a337faec1d87bbb15d84856a43a4aa64ac65",
    "externalIds": {
      "ArXiv": "1605.09096",
      "DBLP": "conf/acl/HamiltonLJ16",
      "ACL": "P16-1141",
      "MAG": "2416513196",
      "DOI": "10.18653/v1/P16-1141",
      "CorpusId": 5480561
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change",
    "abstract": "Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 72,
    "citationCount": 884,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49437682",
        "name": "William L. Hamilton"
      },
      {
        "authorId": "1702139",
        "name": "J. Leskovec"
      },
      {
        "authorId": "1746807",
        "name": "Dan Jurafsky"
      }
    ]
  },
  "36748720": {
    "paperId": "ffa952b637e03bcae13b38e13ce5cf73c6f24e59",
    "externalIds": {
      "DBLP": "conf/wsdm/YaoSDRX18",
      "MAG": "2782822144",
      "ArXiv": "1703.00607",
      "DOI": "10.1145/3159652.3159703",
      "CorpusId": 36748720
    },
    "publicationVenue": {
      "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
      "name": "Web Search and Data Mining",
      "type": "conference",
      "alternate_names": [
        "Web Search Data Min",
        "WSDM"
      ],
      "url": "http://www.wikicfp.com/cfp/program?id=3158"
    },
    "title": "Dynamic Word Embeddings for Evolving Semantic Discovery",
    "abstract": "Word evolution refers to the changing meanings and associations of words throughout time, as a byproduct of human language evolution. By studying word evolution, we can infer social trends and language constructs over different periods of human history. However, traditional techniques such as word representation learning do not adequately capture the evolving language structure and vocabulary. In this paper, we develop a dynamic statistical model to learn time-aware word vector representation. We propose a model that simultaneously learns time-aware embeddings and solves the resulting alignment problem. This model is trained on a crawled NYTimes dataset. Additionally, we develop multiple intuitive evaluation strategies of temporal word embeddings. Our qualitative and quantitative tests indicate that our method not only reliably captures this evolution over time, but also consistently outperforms state-of-the-art temporal embedding approaches on both semantic accuracy and alignment quality.",
    "venue": "Web Search and Data Mining",
    "year": 2017,
    "referenceCount": 45,
    "citationCount": 207,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2117803201",
        "name": "Zijun Yao"
      },
      {
        "authorId": "2213925",
        "name": "Yifan Sun"
      },
      {
        "authorId": "2365722",
        "name": "Weicong Ding"
      },
      {
        "authorId": "145850291",
        "name": "Nikhil S. Rao"
      },
      {
        "authorId": "144467554",
        "name": "Hui Xiong"
      }
    ]
  },
  "258833586": {
    "paperId": "a2fb308508afbe00aab0709f6563719bd86e256a",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-11993",
      "ACL": "2023.acl-long.176",
      "ArXiv": "2305.11993",
      "DOI": "10.48550/arXiv.2305.11993",
      "CorpusId": 258833586
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis",
    "abstract": "We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations.Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is generated for each usage with a specialised Flan-T5 language model, and the most prototypical definition in a usage cluster is chosen as the sense label. We demonstrate how the resulting sense labels can make existing approaches to semantic change analysis more interpretable, and how they can allow users \u2014 historical linguists, lexicographers, or social scientists \u2014 to explore and intuitively explain diachronic trajectories of word meaning. Semantic change analysis is only one of many possible applications of the \u2018definitions as representations\u2019 paradigm. Beyond being human-readable, contextualised definitions also outperform token or usage sentence embeddings in word-in-context semantic similarity judgements, making them a new promising type of lexical representation for NLP.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 40,
    "citationCount": 18,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "24068173",
        "name": "Mario Giulianelli"
      },
      {
        "authorId": "2218113101",
        "name": "Iris Luden"
      },
      {
        "authorId": "2147411708",
        "name": "Raquel Fern\u00e1ndez"
      },
      {
        "authorId": "2689095",
        "name": "Andrey Kutuzov"
      }
    ]
  },
  "259370555": {
    "paperId": "6632dfd527f6161feed7fc6ab3970b53ffbbffcc",
    "externalIds": {
      "DBLP": "conf/acl/CassottiSGSB23",
      "ACL": "2023.acl-short.135",
      "DOI": "10.18653/v1/2023.acl-short.135",
      "CorpusId": 259370555
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "XL-LEXEME: WiC Pretrained Model for Cross-Lingual LEXical sEMantic changE",
    "abstract": "The recent introduction of large-scale datasets for the WiC (Word in Context) task enables the creation of more reliable and meaningful contextualized word embeddings.However, most of the approaches to the WiC task use cross-encoders, which prevent the possibility of deriving comparable word embeddings.In this work, we introduce XL-LEXEME, a Lexical Semantic Change Detection model.XL-LEXEME extends SBERT, highlighting the target word in the sentence.We evaluate XL-LEXEME on the multilingual benchmarks for SemEval-2020 Task 1 - Lexical Semantic Change (LSC) Detection and the RuShiftEval shared task involving five languages: English, German, Swedish, Latin, and Russian.XL-LEXEME outperforms the state-of-the-art in English, German and Swedish with statistically significant differences from the baseline results and obtains state-of-the-art performance in the RuShiftEval shared task.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 37,
    "citationCount": 27,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "29931342",
        "name": "Pierluigi Cassotti"
      },
      {
        "authorId": "40989465",
        "name": "Lucia Siciliani"
      },
      {
        "authorId": "1873109",
        "name": "M. Degemmis"
      },
      {
        "authorId": "145467353",
        "name": "G. Semeraro"
      },
      {
        "authorId": "1731651",
        "name": "Pierpaolo Basile"
      }
    ]
  },
  "5445756": {
    "paperId": "3f7983818b76a5f1b5daf9b605877ed401c8e73c",
    "externalIds": {
      "MAG": "1975147762",
      "DBLP": "journals/jcisd/Weininger88",
      "DOI": "10.1021/ci00057a005",
      "CorpusId": 5445756
    },
    "publicationVenue": {
      "id": "7270e188-d479-4233-b457-1d07288baaef",
      "name": "Journal of chemical information and computer sciences",
      "type": "journal",
      "alternate_names": [
        "J Chem Inf Comput Sci",
        "J chem inf comput sci",
        "Journal of Chemical Information and Computer Sciences"
      ],
      "issn": "0095-2338",
      "url": "https://pubs.acs.org/journal/jcisd8",
      "alternate_urls": [
        "http://pubs.acs.org/journals/jcisd8/index.html"
      ]
    },
    "title": "SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules",
    "abstract": "18-24.",
    "venue": "Journal of chemical information and computer sciences",
    "year": 1988,
    "referenceCount": 18,
    "citationCount": 5391,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2339223",
        "name": "D. Weininger"
      }
    ]
  },
  "212415210": {
    "paperId": "8338a903d8078481ff8af777475f7394d00e9d57",
    "externalIds": {
      "DBLP": "journals/mlst/KrennHNFA20",
      "MAG": "3045928028",
      "DOI": "10.1088/2632-2153/aba947",
      "CorpusId": 212415210
    },
    "publicationVenue": null,
    "title": "Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation",
    "abstract": "The discovery of novel materials and functional molecules can help to solve some of society\u2019s most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering\u2013generally denoted as inverse design\u2013was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model\u2019s internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.",
    "venue": "Machine Learning: Science and Technology",
    "year": 2019,
    "referenceCount": 65,
    "citationCount": 569,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Materials Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "5906965",
        "name": "Mario Krenn"
      },
      {
        "authorId": "122433803",
        "name": "Florian Hase"
      },
      {
        "authorId": "133638577",
        "name": "AkshatKumar Nigam"
      },
      {
        "authorId": "35323511",
        "name": "Pascal Friederich"
      },
      {
        "authorId": "1380248954",
        "name": "Al\u00e1n Aspuru-Guzik"
      }
    ]
  },
  "254017556": {
    "paperId": "710e16ceb9f98d4f4864afaf0aa4bbbe529edc44",
    "externalIds": {
      "ArXiv": "2211.13322",
      "DBLP": "journals/corr/abs-2211-13322",
      "DOI": "10.1039/D3DD00012E",
      "CorpusId": 254017556
    },
    "publicationVenue": {
      "id": "738bbcb3-7641-4f2a-8b05-79ca86a3681b",
      "name": "Digital Discovery",
      "alternate_names": [
        "Digit Discov"
      ],
      "issn": "2635-098X",
      "url": "https://www.rsc.org/journals-books-databases/about-journals/digital-discovery"
    },
    "title": "Group SELFIES: A Robust Fragment-Based Molecular String Representation",
    "abstract": "We introduce Group SELFIES, a molecular string representation that leverages group tokens to represent functional groups or entire substructures while maintaining chemical robustness guarantees. Molecular string representations, such as SMILES...",
    "venue": "Digital Discovery",
    "year": 2022,
    "referenceCount": 56,
    "citationCount": 25,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Materials Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "80658777",
        "name": "Austin H. Cheng"
      },
      {
        "authorId": "2192514753",
        "name": "Andy Cai"
      },
      {
        "authorId": "51895312",
        "name": "Santiago Miret"
      },
      {
        "authorId": "2594102",
        "name": "Gustavo Malkomes"
      },
      {
        "authorId": "2482400",
        "name": "Mariano Phielipp"
      },
      {
        "authorId": "1380248954",
        "name": "Al\u00e1n Aspuru-Guzik"
      }
    ]
  },
  "224803102": {
    "paperId": "95ce6f77e26b496ffb705a0a3b54f2fb7a6d2452",
    "externalIds": {
      "DBLP": "journals/corr/abs-2010-09885",
      "ArXiv": "2010.09885",
      "MAG": "3093934881",
      "CorpusId": 224803102
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
    "abstract": "GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 36,
    "citationCount": 320,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Biology",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1999563324",
        "name": "Seyone Chithrananda"
      },
      {
        "authorId": "35748708",
        "name": "Gabriel Grand"
      },
      {
        "authorId": "2378027",
        "name": "Bharath Ramsundar"
      }
    ]
  },
  "252089745": {
    "paperId": "299e281cb6e2bfae6e8459d2ac354e11e40931be",
    "externalIds": {
      "DBLP": "journals/corr/abs-2209-01712",
      "ArXiv": "2209.01712",
      "DOI": "10.48550/arXiv.2209.01712",
      "CorpusId": 252089745
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "ChemBERTa-2: Towards Chemical Foundation Models",
    "abstract": "Large pretrained models such as GPT-3 have had tremendous impact on modern natural language processing by leveraging self-supervised learning to learn salient representations that can be used to readily finetune on a wide variety of downstream tasks. We investigate the possibility of transferring such advances to molecular machine learning by building a chemical foundation model, ChemBERTa-2, using the language of SMILES. While labeled data for molecular prediction tasks is typically scarce, libraries of SMILES strings are readily available. In this work, we build upon ChemBERTa by optimizing the pretraining process. We compare multi-task and self-supervised pretraining by varying hyperparameters and pretraining dataset size, up to 77M compounds from PubChem. To our knowledge, the 77M set constitutes one of the largest datasets used for molecular pretraining to date. We find that with these pretraining improvements, we are competitive with existing state-of-the-art architectures on the MoleculeNet benchmark suite. We analyze the degree to which improvements in pretraining translate to improvement on downstream tasks.",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 21,
    "citationCount": 94,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Biology",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145390565",
        "name": "Walid Ahmad"
      },
      {
        "authorId": "2084632592",
        "name": "Elana Simon"
      },
      {
        "authorId": "1999563324",
        "name": "Seyone Chithrananda"
      },
      {
        "authorId": "35748708",
        "name": "Gabriel Grand"
      },
      {
        "authorId": "2378027",
        "name": "Bharath Ramsundar"
      }
    ]
  },
  "257506709": {
    "paperId": "b935cf6d1c18f82baac148d748d531449ce810d9",
    "externalIds": {
      "DBLP": "journals/jcisd/TysingerRS23",
      "DOI": "10.1021/acs.jcim.2c01618",
      "CorpusId": 257506709,
      "PubMed": "36914216"
    },
    "publicationVenue": {
      "id": "3f16aef5-6b9f-4f87-baca-cbf8147e352f",
      "name": "Journal of Chemical Information and Modeling",
      "type": "journal",
      "alternate_names": [
        "J Chem Inf Model"
      ],
      "issn": "1549-9596",
      "url": "http://pubs.acs.org/jcim",
      "alternate_urls": [
        "http://pubs.acs.org/journals/jcisd8/index.html",
        "https://pubs.acs.org/journal/jcisd8"
      ]
    },
    "title": "Can We Quickly Learn to \"Translate\" Bioactive Molecules with Transformer Models?",
    "abstract": "Meaningful exploration of the chemical space of druglike molecules in drug design is a highly challenging task due to a combinatorial explosion of possible modifications of molecules. In this work, we address this problem with transformer models, a type of machine learning (ML) model originally developed for machine translation. By training transformer models on pairs of similar bioactive molecules from the public ChEMBL data set, we enable them to learn medicinal-chemistry-meaningful, context-dependent transformations of molecules, including those absent from the training set. By retrospective analysis on the performance of transformer models on ChEMBL subsets of ligands binding to COX2, DRD2, or HERG protein targets, we demonstrate that the models can generate structures identical or highly similar to most active ligands, despite the models having not seen any ligands active against the corresponding protein target during training. Our work demonstrates that human experts working on hit expansion in drug design can easily and quickly employ transformer models, originally developed to translate texts from one natural language to another, to \"translate\" from known molecules active against a given protein target to novel molecules active against the same target.",
    "venue": "Journal of Chemical Information and Modeling",
    "year": 2023,
    "referenceCount": 39,
    "citationCount": 13,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2211488055",
        "name": "Emma P Tysinger"
      },
      {
        "authorId": "39340849",
        "name": "B. Rai"
      },
      {
        "authorId": "4975845",
        "name": "Anton V. Sinitskiy"
      }
    ]
  },
  "243865204": {
    "paperId": "57651d65078818821234d13544ac1f29858dcd67",
    "externalIds": {
      "DBLP": "conf/emnlp/EdwardsZJ21",
      "ACL": "2021.emnlp-main.47",
      "DOI": "10.18653/v1/2021.emnlp-main.47",
      "CorpusId": 243865204
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries",
    "abstract": "We propose a new task, Text2Mol, to retrieve molecules using natural language descriptions as queries. Natural language and molecules encode information in very different ways, which leads to the exciting but challenging problem of integrating these two very different modalities. Although some work has been done on text-based retrieval and structure-based retrieval, this new task requires integrating molecules and natural language more directly. Moreover, this can be viewed as an especially challenging cross-lingual retrieval problem by considering the molecules as a language with a very unique grammar. We construct a paired dataset of molecules and their corresponding text descriptions, which we use to learn an aligned common semantic embedding space for retrieval. We extend this to create a cross-modal attention-based model for explainability and reranking by interpreting the attentions as association rules. We also employ an ensemble approach to integrate our different architectures, which significantly improves results from 0.372 to 0.499 MRR. This new multimodal approach opens a new perspective on solving problems in chemistry literature understanding and molecular machine learning.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 70,
    "citationCount": 100,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48870109",
        "name": "Carl N. Edwards"
      },
      {
        "authorId": "143869012",
        "name": "Chengxiang Zhai"
      },
      {
        "authorId": "2113323573",
        "name": "Heng Ji"
      }
    ]
  },
  "257435961": {
    "paperId": "a9e916f8bbb6a08793e949eee8b5a06c74b17f36",
    "externalIds": {
      "DBLP": "journals/tai/ZhaoZCZC24",
      "DOI": "10.1109/TAI.2023.3254518",
      "CorpusId": 257435961
    },
    "publicationVenue": {
      "id": "3c27e831-750f-45bc-9914-2148a5259eba",
      "name": "IEEE Transactions on Artificial Intelligence",
      "type": "journal",
      "alternate_names": [
        "IEEE Trans Artif Intell"
      ],
      "issn": "2691-4581",
      "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688"
    },
    "title": "Adversarial Modality Alignment Network for Cross-Modal Molecule Retrieval",
    "abstract": "The cross-modal molecule retrieval (Text2Mol) task aims to bridge the semantic gap between molecules and natural language descriptions. A solution to this nontrivial problem relies on a graph convolutional network (GCN) and cross-modal attention with contrastive learning for reasonable results. However, there exist the following issues. First, the cross-modal attention mechanism is only in favor of text representations and cannot provide helpful information for molecule representations. Second, the GCN-based molecule encoder ignores edge features and the importance of various substructures of a molecule. Finally, the retrieval learning loss function is rather simplistic. This article further investigates the Text2Mol problem and proposes a novel adversarial modality alignment network (AMAN) based method to sufficiently learn both description and molecule information. Our method utilizes a SciBERT as a text encoder and a graph transformer network as a molecule encoder to generate multimodal representations. Then, an adversarial network is used to align these modalities interactively. Meanwhile, a triplet loss function is leveraged to perform retrieval learning and further enhance the modality alignment. Experiments on the ChEBI-20 dataset show the effectiveness of our AMAN method compared with baselines.",
    "venue": "IEEE Transactions on Artificial Intelligence",
    "year": 2024,
    "referenceCount": 56,
    "citationCount": 13,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2118225075",
        "name": "Wenyu Zhao"
      },
      {
        "authorId": "144066008",
        "name": "Dong Zhou"
      },
      {
        "authorId": "1767063",
        "name": "Buqing Cao"
      },
      {
        "authorId": "2153281856",
        "name": "Kai Zhang"
      },
      {
        "authorId": "2108742496",
        "name": "Jinjun Chen"
      }
    ]
  },
  "246815222": {
    "paperId": "6958612fea7f220757b4165b8e12d4b62b4baa80",
    "externalIds": {
      "PubMedCentral": "8844428",
      "DOI": "10.1038/s41467-022-28494-3",
      "CorpusId": 246815222,
      "PubMed": "35165275"
    },
    "publicationVenue": {
      "id": "43b3f0f9-489a-4566-8164-02fafde3cd98",
      "name": "Nature Communications",
      "type": "journal",
      "alternate_names": [
        "Nat Commun"
      ],
      "issn": "2041-1723",
      "url": "https://www.nature.com/ncomms/",
      "alternate_urls": [
        "http://www.nature.com/ncomms/about/index.html",
        "http://www.nature.com/ncomms/index.html"
      ]
    },
    "title": "A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals",
    "abstract": null,
    "venue": "Nature Communications",
    "year": 2022,
    "referenceCount": 75,
    "citationCount": 101,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      },
      {
        "category": "Biology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1633538428",
        "name": "Zheni Zeng"
      },
      {
        "authorId": "1390925224",
        "name": "Yuan Yao"
      },
      {
        "authorId": "2141313179",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "1753344",
        "name": "Maosong Sun"
      }
    ]
  },
  "248376906": {
    "paperId": "3b9b1aba877ecd3f7e508cbc78a41b623349902b",
    "externalIds": {
      "ACL": "2022.emnlp-main.26",
      "DBLP": "journals/corr/abs-2204-11817",
      "ArXiv": "2204.11817",
      "DOI": "10.48550/arXiv.2204.11817",
      "CorpusId": 248376906
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Translation between Molecules and Natural Language",
    "abstract": "We present MolT5 - a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings. MolT5 allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo molecule generation (altogether: translation between molecules and language), which we explore for the first time. Since MolT5 pretrains models on single-modal data, it helps overcome the chemistry domain shortcoming of data scarcity. Furthermore, we consider several metrics, including a new cross-modal embedding-based metric, to evaluate the tasks of molecule captioning and text-based molecule generation. Our results show that MolT5-based models are able to generate outputs, both molecules and captions, which in many cases are high quality.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 89,
    "citationCount": 126,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48870109",
        "name": "Carl N. Edwards"
      },
      {
        "authorId": "145242558",
        "name": "T. Lai"
      },
      {
        "authorId": "79769007",
        "name": "Kevin Ros"
      },
      {
        "authorId": "3359924",
        "name": "Garrett Honke"
      },
      {
        "authorId": "2072975661",
        "name": "Heng Ji"
      }
    ]
  },
  "259077070": {
    "paperId": "119a3ed0898499fce0ce6af6958d566d82390ba5",
    "externalIds": {
      "ArXiv": "2306.13089",
      "DBLP": "conf/nips/ZhaoLMXFDKL23",
      "DOI": "10.1101/2023.05.30.542904",
      "CorpusId": 259077070
    },
    "publicationVenue": {
      "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
      "name": "bioRxiv",
      "type": "journal",
      "url": "http://biorxiv.org/"
    },
    "title": "GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning",
    "abstract": "Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset consisting of more than two thousand molecule tasks with corresponding instructions derived from task descriptions. We pretrain GIMLET on the molecule tasks along with instructions, enabling the model to transfer effectively to a broad range of tasks. Experimental results demonstrate that GIMLET significantly outperforms molecule-text baselines in instruction-based zero-shot learning, even achieving closed results to supervised GNN models on tasks such as toxcast and muv.1",
    "venue": "bioRxiv",
    "year": 2023,
    "referenceCount": 89,
    "citationCount": 44,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Biology",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2146233407",
        "name": "Haiteng Zhao"
      },
      {
        "authorId": "1563693999",
        "name": "Shengchao Liu"
      },
      {
        "authorId": "2149147043",
        "name": "Chang Ma"
      },
      {
        "authorId": "2143535070",
        "name": "Hannan Xu"
      },
      {
        "authorId": "49252800",
        "name": "Jie Fu"
      },
      {
        "authorId": "123580511",
        "name": "Zhihong Deng"
      },
      {
        "authorId": "47648549",
        "name": "Lingpeng Kong"
      },
      {
        "authorId": "11149237",
        "name": "Qi Liu"
      }
    ]
  },
  "252212175": {
    "paperId": "1c7a4e8d9f4fcf19a5d1caa078c66ca39cb75dd2",
    "externalIds": {
      "ArXiv": "2209.05481",
      "DBLP": "journals/corr/abs-2209-05481",
      "DOI": "10.48550/arXiv.2209.05481",
      "CorpusId": 252212175
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language",
    "abstract": "Although artificial intelligence (AI) has made significant progress in understanding molecules in a wide range of fields, existing models generally acquire the single cognitive ability from the single molecular modality. Since the hierarchy of molecular knowledge is profound, even humans learn from different modalities including both intuitive diagrams and professional texts to assist their understanding. Inspired by this, we propose a molecular multimodal foundation model which is pretrained from molecular graphs and their semantically related textual data (crawled from published Scientific Citation Index papers) via contrastive learning. This AI model represents a critical attempt that directly bridges molecular graphs and natural language. Importantly, through capturing the specific and complementary information of the two modalities, our proposed model can better grasp molecular expertise. Experimental results show that our model not only exhibits promising performance in cross-modal tasks such as cross-modal retrieval and molecule caption, but also enhances molecular property prediction and possesses capability to generate meaningful molecular graphs from natural language descriptions. We believe that our model would have a broad impact on AI-empowered fields across disciplines such as biology, chemistry, materials, environment, and medicine, among others.",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 80,
    "citationCount": 90,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2100573353",
        "name": "Bing Su"
      },
      {
        "authorId": "2155890724",
        "name": "Dazhao Du"
      },
      {
        "authorId": "2717881",
        "name": "Zhao-Qing Yang"
      },
      {
        "authorId": "2110334261",
        "name": "Yujie Zhou"
      },
      {
        "authorId": "2118506408",
        "name": "Jiangmeng Li"
      },
      {
        "authorId": "36290866",
        "name": "Anyi Rao"
      },
      {
        "authorId": "2037778",
        "name": "Haoran Sun"
      },
      {
        "authorId": "1776220",
        "name": "Zhiwu Lu"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ]
  },
  "254926709": {
    "paperId": "958bb3831589246fe5b6b58cf99e3b65c58d027f",
    "externalIds": {
      "DBLP": "journals/natmi/LiuNWLQLTXA23",
      "ArXiv": "2212.10789",
      "DOI": "10.48550/arXiv.2212.10789",
      "CorpusId": 254926709
    },
    "publicationVenue": null,
    "title": "Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing",
    "abstract": "There is increasing adoption of artificial intelligence in drug discovery. However, existing studies use machine learning to mainly utilize the chemical structures of molecules but ignore the vast textual knowledge available in chemistry. Incorporating textual knowledge enables us to realize new drug design objectives, adapt to text-based instructions and predict complex biological activities. Here we present a multi-modal molecule structure-text model, MoleculeSTM, by jointly learning molecules' chemical structures and textual descriptions via a contrastive learning strategy. To train MoleculeSTM, we construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000 chemical structure-text pairs. To demonstrate the effectiveness and utility of MoleculeSTM, we design two challenging zero-shot tasks based on text instructions, including structure-text retrieval and molecule editing. MoleculeSTM has two main properties: open vocabulary and compositionality via natural language. In experiments, MoleculeSTM obtains the state-of-the-art generalization ability to novel biochemical concepts across various benchmarks.",
    "venue": "Nat. Mac. Intell.",
    "year": 2022,
    "referenceCount": 84,
    "citationCount": 104,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Biology",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1563693999",
        "name": "Shengchao Liu"
      },
      {
        "authorId": "2066304514",
        "name": "Weili Nie"
      },
      {
        "authorId": "50097250",
        "name": "Chengpeng Wang"
      },
      {
        "authorId": "2121696601",
        "name": "Jiarui Lu"
      },
      {
        "authorId": "88820329",
        "name": "Zhuoran Qiao"
      },
      {
        "authorId": "2146021664",
        "name": "Ling Liu"
      },
      {
        "authorId": "2115855484",
        "name": "Jian Tang"
      },
      {
        "authorId": "2723309",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "2047844",
        "name": "Anima Anandkumar"
      }
    ]
  },
  "258762343": {
    "paperId": "ed1353d705eeabc0e916caba5fbae890eefe4f84",
    "externalIds": {
      "DBLP": "conf/acl/LiuZXW0QZL23",
      "ArXiv": "2305.10688",
      "ACL": "2023.acl-short.138",
      "DOI": "10.48550/arXiv.2305.10688",
      "CorpusId": 258762343
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
    "abstract": "Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 48,
    "citationCount": 54,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2109372136",
        "name": "Zequn Liu"
      },
      {
        "authorId": "143715293",
        "name": "W. Zhang"
      },
      {
        "authorId": "2111056280",
        "name": "Yingce Xia"
      },
      {
        "authorId": "47767550",
        "name": "Lijun Wu"
      },
      {
        "authorId": "1889683",
        "name": "Shufang Xie"
      },
      {
        "authorId": "143826491",
        "name": "Tao Qin"
      },
      {
        "authorId": "2215481115",
        "name": "M. Zhang"
      },
      {
        "authorId": "2110264337",
        "name": "Tie-Yan Liu"
      }
    ]
  },
  "256701737": {
    "paperId": "ce6f2d68b1a4029ff4a838fcf12d5ad1d47f0e68",
    "externalIds": {
      "PubMedCentral": "9911740",
      "DOI": "10.1038/s41467-023-36476-2",
      "CorpusId": 256701737,
      "PubMed": "36759510"
    },
    "publicationVenue": {
      "id": "43b3f0f9-489a-4566-8164-02fafde3cd98",
      "name": "Nature Communications",
      "type": "journal",
      "alternate_names": [
        "Nat Commun"
      ],
      "issn": "2041-1723",
      "url": "https://www.nature.com/ncomms/",
      "alternate_urls": [
        "http://www.nature.com/ncomms/about/index.html",
        "http://www.nature.com/ncomms/index.html"
      ]
    },
    "title": "Multilingual translation for zero-shot biomedical classification using BioTranslator",
    "abstract": null,
    "venue": "Nature Communications",
    "year": 2023,
    "referenceCount": 108,
    "citationCount": 18,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Biology",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2117998083",
        "name": "Hanwen Xu"
      },
      {
        "authorId": "2170076404",
        "name": "Addie Woicik"
      },
      {
        "authorId": "1759772",
        "name": "Hoifung Poon"
      },
      {
        "authorId": "2200563989",
        "name": "R. Altman"
      },
      {
        "authorId": "2151484530",
        "name": "Sheng Wang"
      }
    ]
  },
  "258960560": {
    "paperId": "9e8af0791e8c87452c8cff25dab5448a29c218d4",
    "externalIds": {
      "ArXiv": "2305.18090",
      "DBLP": "journals/corr/abs-2305-18090",
      "DOI": "10.48550/arXiv.2305.18090",
      "CorpusId": 258960560
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback",
    "abstract": "Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reaction and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. To bridge this gap, we propose ChatDrug, a framework to facilitate the systematic investigation of drug editing using LLMs. ChatDrug jointly leverages a prompt module, a retrieval and domain feedback (ReDF) module, and a conversation module to streamline effective drug editing. We empirically show that ChatDrug reaches the best performance on 33 out of 39 drug editing tasks, encompassing small molecules, peptides, and proteins. We further demonstrate, through 10 case studies, that ChatDrug can successfully identify the key substructures (e.g., the molecule functional groups, peptide motifs, and protein structures) for manipulation, generating diverse and valid suggestions for drug editing. Promisingly, we also show that ChatDrug can offer insightful explanations from a domain-specific perspective, enhancing interpretability and enabling informed decision-making. This research sheds light on the potential of ChatGPT and conversational LLMs for drug editing. It paves the way for a more efficient and collaborative drug discovery pipeline, contributing to the advancement of pharmaceutical research and development.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 97,
    "citationCount": 31,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Biology",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1563693999",
        "name": "Shengchao Liu"
      },
      {
        "authorId": "2110170885",
        "name": "Jiong Wang"
      },
      {
        "authorId": "2108568154",
        "name": "Yijin Yang"
      },
      {
        "authorId": "50097250",
        "name": "Chengpeng Wang"
      },
      {
        "authorId": "2146021664",
        "name": "Ling Liu"
      },
      {
        "authorId": "1694050",
        "name": "Hongyu Guo"
      },
      {
        "authorId": "2723309",
        "name": "Chaowei Xiao"
      }
    ]
  },
  "259982545": {
    "paperId": "7c8254f6d95863fffeef2ba3d0d07f42b0f72e21",
    "externalIds": {
      "ArXiv": "2307.09484",
      "DBLP": "journals/corr/abs-2307-09484",
      "DOI": "10.48550/arXiv.2307.09484",
      "CorpusId": 259982545
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "MolFM: A Multimodal Molecular Foundation Model",
    "abstract": "Molecular knowledge resides within three different modalities of information sources: molecular structures, biomedical documents, and knowledge bases. Effective incorporation of molecular knowledge from these modalities holds paramount significance in facilitating biomedical research. However, existing multimodal molecular foundation models exhibit limitations in capturing intricate connections between molecular structures and texts, and more importantly, none of them attempt to leverage a wealth of molecular expertise derived from knowledge graphs. In this study, we introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention between atoms of molecular structures, neighbors of molecule entities and semantically related texts to facilitate cross-modal comprehension. We provide theoretical analysis that our cross-modal pre-training captures local and global molecular knowledge by minimizing the distance in the feature space between different modalities of the same molecule, as well as molecules sharing similar structures or functions. MolFM achieves state-of-the-art performance on various downstream tasks. On cross-modal retrieval, MolFM outperforms existing models with 12.13% and 5.04% absolute gains under the zero-shot and fine-tuning settings, respectively. Furthermore, qualitative analysis showcases MolFM's implicit ability to provide grounding from molecular substructures and knowledge graphs. Code and models are available on https://github.com/BioFM/OpenBioMed.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 64,
    "citationCount": 29,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Biology",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2154365142",
        "name": "Yi Luo"
      },
      {
        "authorId": "2118047940",
        "name": "Kai Yang"
      },
      {
        "authorId": "2215865552",
        "name": "Massimo Hong"
      },
      {
        "authorId": "2146036843",
        "name": "Xingyi Liu"
      },
      {
        "authorId": "38301933",
        "name": "Zaiqing Nie"
      }
    ]
  },
  "18304214": {
    "paperId": "150f2f0acae29be50001ed666fdf75fe4eb8b5d5",
    "externalIds": {
      "CorpusId": 18304214
    },
    "publicationVenue": null,
    "title": "Drug Discovery Chemistry: a Primer for the Non-specialist",
    "abstract": "REVIEWS This review seeks to explain some of the common terminology used by medicinal and synthetic chemists. Aimed at the non-specialist, its intent is to help facilitate discussions between chemists and their counterparts from other disciplines. Like all scientific disciplines, drug discovery chemistry is rife with terminology and methodology that can seem intractable to those outside the sphere of synthetic chemistry. Derived from a successful in-house workshop, this Foundation Review aims to demystify some of this inherent terminology, providing the non-specialist with a general insight into the nomenclature, terminology and workflow of medicinal chemists within the pharmaceutical industry. Owing to its multidisciplinary nature, those working within drug discovery are exposed to a considerable quantity of terminology, drawn from a wide variety of specialisms. From analysts to computational scientists, toxicologists and pharmacologists, each scientific area tends to develop its own dialogue and vocabulary that, to the outsider, can be complex and sometimes overwhelming when trying to collaborate and communicate across disciplines and projects. The synthetic chemist is by no means exempt from this endemic use of jargon. Aside from terminology for the specific chemical entities produced in the laboratory and the functionality these entities contain, chemists make frequent reference to the names of the reactions, techniques and methodology used to assemble them. Though second nature to the practicing chemist, this terminology is frequently referred to with little or no explanation or clarification to those outside the chemistry community. Within Vernalis, a series of informal discussions clearly highlighted the ways in which different scientists visualise, and thus describe, key candidate compounds. For example, crystallographers would refer to electron densities, while modellers would discuss compounds in terms of their intermolecular interactions with their desired targets. These and other colleagues outside chemistry would often despair as the project chemists discussed seemingly endless lists of functional groups, core ring systems and reaction types. It quickly became apparent that while chemists had, as part of their training, often picked up sufficient biology to allow them to at least partly follow the biological discussions within project meetings, the non-chemists often found chemical discussions considerably more difficult to follow, despite their best efforts. From this starting point, we developed, implemented and evolved an in-house workshop, which we loosely entitled 'chemistry for non-chemists'. This allowed those interested parties to understand a little better the mindset of the synthetic chemist, their terminology, nomenclature and the 'toolbox' of reactions commonly used \u2026",
    "venue": "",
    "year": null,
    "referenceCount": 41,
    "citationCount": 33,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2250261755",
        "name": "Allan M Jordan"
      },
      {
        "authorId": "3320479",
        "name": "S. Roughley"
      },
      {
        "authorId": "2250261755",
        "name": "Allan M Jordan"
      },
      {
        "authorId": "2250118148",
        "name": "Jordan"
      }
    ]
  },
  "9364240": {
    "paperId": "3ef2d7122dcc5ce32f072e97498628380f0d691d",
    "externalIds": {
      "MAG": "2162306460",
      "DOI": "10.1158/1535-7163.MCT-13-0791",
      "CorpusId": 9364240,
      "PubMed": "24435445"
    },
    "publicationVenue": {
      "id": "05590b5e-de5a-404d-bec3-bfe1e9d96382",
      "name": "Molecular Cancer Therapeutics",
      "type": "journal",
      "alternate_names": [
        "Mol Cancer Ther"
      ],
      "issn": "1535-7163",
      "url": "https://mct.aacrjournals.org/"
    },
    "title": "Targeting Microtubules by Natural Agents for Cancer Therapy",
    "abstract": "Natural compounds that target microtubules and disrupt the normal function of the mitotic spindle have proven to be one of the best classes of cancer chemotherapeutic drugs available in clinics to date. There is increasing evidence showing that even minor alteration of microtubule dynamics can engage the spindle checkpoint, arresting cell-cycle progression at mitosis and subsequently leading to cell death. Our improved understanding of tumor biology and our continued appreciation for what the microtubule targeting agents (MTAs) can do have helped pave the way for a new era in the treatment of cancer. The effectiveness of these agents for cancer therapy has been impaired, however, by various side effects and drug resistance. Several new MTAs have shown potent activity against the proliferation of various cancer cells, including resistance to the existing MTAs. Sustained investigation of the mechanisms of action of MTAs, development and discovery of new drugs, and exploring new treatment strategies that reduce side effects and circumvent drug resistance could provide more effective therapeutic options for patients with cancer. This review focuses on the successful cancer chemotherapy from natural compounds in clinical settings and the challenges that may abort their usefulness. Mol Cancer Ther; 13(2); 275\u201384. \u00a92014 AACR.",
    "venue": "Molecular Cancer Therapeutics",
    "year": 2014,
    "referenceCount": 83,
    "citationCount": 417,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Biology",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "11531590",
        "name": "Eiman Mukhtar"
      },
      {
        "authorId": "4435551",
        "name": "V. Adhami"
      },
      {
        "authorId": "2009590",
        "name": "H. Mukhtar"
      }
    ]
  },
  "12624880": {
    "paperId": "b0d6e53049bf5e885f68074588dd138e45947664",
    "externalIds": {
      "MAG": "2295234361",
      "DOI": "10.1364/opn.18.10.000026",
      "CorpusId": 12624880
    },
    "publicationVenue": {
      "id": "bc96f9bd-89da-4c9a-ab24-27749617f6ff",
      "name": "Conference on Lasers and Electro-Optics",
      "type": "conference",
      "alternate_names": [
        "CLEO",
        "Conf Laser Electro-optics"
      ]
    },
    "title": "Organic Photovoltaics",
    "abstract": "Solid-state organic photovoltaic technologies are emerging and maturing with reports of power conversion efficiencies close to 5%. This tutorial will provide an overview of the chemistry, the physics and engineering of solar cells based on organic materials.",
    "venue": "Conference on Lasers and Electro-Optics",
    "year": 2007,
    "referenceCount": 0,
    "citationCount": 473,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "s2-fos-model"
      },
      {
        "category": "Materials Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Physics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46529529",
        "name": "B. Kippelen"
      }
    ]
  },
  "258059651": {
    "paperId": "ae6a4cd221684be6ca3082b6f526a7901281490b",
    "externalIds": {
      "ArXiv": "2304.05332",
      "DBLP": "journals/corr/abs-2304-05332",
      "DOI": "10.48550/arXiv.2304.05332",
      "CorpusId": 258059651
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Emergent autonomous scientific research capabilities of large language models",
    "abstract": "Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 14,
    "citationCount": 96,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Biology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "50771006",
        "name": "Daniil A. Boiko"
      },
      {
        "authorId": "152587056",
        "name": "R. MacKnight"
      },
      {
        "authorId": "2181059489",
        "name": "Gabe Gomes"
      }
    ]
  },
  "257581728": {
    "paperId": "23d01461a54505705649c1f5378f81dcd524d46c",
    "externalIds": {
      "DBLP": "journals/jcisd/NascimentoP23",
      "DOI": "10.1021/acs.jcim.3c00285",
      "CorpusId": 257581728,
      "PubMed": "36926868"
    },
    "publicationVenue": {
      "id": "3f16aef5-6b9f-4f87-baca-cbf8147e352f",
      "name": "Journal of Chemical Information and Modeling",
      "type": "journal",
      "alternate_names": [
        "J Chem Inf Model"
      ],
      "issn": "1549-9596",
      "url": "http://pubs.acs.org/jcim",
      "alternate_urls": [
        "http://pubs.acs.org/journals/jcisd8/index.html",
        "https://pubs.acs.org/journal/jcisd8"
      ]
    },
    "title": "Do Large Language Models Understand Chemistry? A Conversation with ChatGPT",
    "abstract": "Large language models (LLMs) have promised a revolution in answering complex questions using the ChatGPT model. Its application in chemistry is still in its infancy. This viewpoint addresses the question of how well ChatGPT understands chemistry by posing five simple tasks in different subareas of chemistry.",
    "venue": "Journal of Chemical Information and Modeling",
    "year": 2023,
    "referenceCount": 44,
    "citationCount": 83,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2215408868",
        "name": "Cayque Monteiro Castro Nascimento"
      },
      {
        "authorId": "145647498",
        "name": "A. S. Pimentel"
      }
    ]
  },
  "256317153": {
    "paperId": "fb408bafec0110964d06d742ef67e0aa4ebe81e1",
    "externalIds": {
      "PubMedCentral": "10087057",
      "DOI": "10.1039/d2dd00087c",
      "CorpusId": 256317153,
      "PubMed": "37065678"
    },
    "publicationVenue": {
      "id": "738bbcb3-7641-4f2a-8b05-79ca86a3681b",
      "name": "Digital Discovery",
      "alternate_names": [
        "Digit Discov"
      ],
      "issn": "2635-098X",
      "url": "https://www.rsc.org/journals-books-databases/about-journals/digital-discovery"
    },
    "title": "Assessment of chemistry knowledge in large language models that generate code",
    "abstract": "In this work, we investigate the question: do code-generating large language models know chemistry? Our results indicate, mostly yes. To evaluate this, we introduce an expandable framework for evaluating chemistry knowledge in these models, through prompting models to solve chemistry problems posed as coding tasks. To do so, we produce a benchmark set of problems, and evaluate these models based on correctness of code by automated testing and evaluation by experts. We find that recent LLMs are able to write correct code across a variety of topics in chemistry and their accuracy can be increased by 30 percentage points via prompt engineering strategies, like putting copyright notices at the top of files. Our dataset and evaluation tools are open source which can be contributed to or built upon by future researchers, and will serve as a community resource for evaluating the performance of new models as they emerge. We also describe some good practices for employing LLMs in chemistry. The general success of these models demonstrates that their impact on chemistry teaching and research is poised to be enormous.",
    "venue": "Digital Discovery",
    "year": 2023,
    "referenceCount": 11,
    "citationCount": 47,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2257535",
        "name": "A. White"
      },
      {
        "authorId": "6409967",
        "name": "Glen M. Hocky"
      },
      {
        "authorId": "117316043",
        "name": "Mehrad Ansari"
      },
      {
        "authorId": "95666896",
        "name": "Heta A. Gandhi"
      },
      {
        "authorId": "2161337138",
        "name": "Sam Cox"
      },
      {
        "authorId": "1805407482",
        "name": "Geemi P Wellawatte"
      },
      {
        "authorId": "2199740349",
        "name": "Subarna Sasmal"
      },
      {
        "authorId": "2143504196",
        "name": "Ziyue Yang"
      },
      {
        "authorId": "2203420643",
        "name": "Kangxin Liu"
      },
      {
        "authorId": "2203062396",
        "name": "Yuvraj Singh"
      },
      {
        "authorId": "2135922773",
        "name": "Willmor J Pe\u00f1a Ccoa"
      }
    ]
  },
  "259937507": {
    "paperId": "186ca9802a449d622bb2ab4e0228c364496f5961",
    "externalIds": {
      "DBLP": "journals/corr/abs-2307-08423",
      "ArXiv": "2307.08423",
      "DOI": "10.48550/arXiv.2307.08423",
      "CorpusId": 259937507
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems",
    "abstract": "Advances in artificial intelligence (AI) are fueling a new paradigm of discoveries in natural sciences. Today, AI has started to advance natural sciences by improving, accelerating, and enabling our understanding of natural phenomena at a wide range of spatial and temporal scales, giving rise to a new area of research known as AI for science (AI4Science). Being an emerging research paradigm, AI4Science is unique in that it is an enormous and highly interdisciplinary area. Thus, a unified and technical treatment of this field is needed yet challenging. This work aims to provide a technically thorough account of a subarea of AI4Science; namely, AI for quantum, atomistic, and continuum systems. These areas aim at understanding the physical world from the subatomic (wavefunctions and electron density), atomic (molecules, proteins, materials, and interactions), to macro (fluids, climate, and subsurface) scales and form an important subarea of AI4Science. A unique advantage of focusing on these areas is that they largely share a common set of challenges, thereby allowing a unified and foundational treatment. A key common challenge is how to capture physics first principles, especially symmetries, in natural systems by deep learning methods. We provide an in-depth yet intuitive account of techniques to achieve equivariance to symmetry transformations. We also discuss other common technical challenges, including explainability, out-of-distribution generalization, knowledge transfer with foundation and large language models, and uncertainty quantification. To facilitate learning and education, we provide categorized lists of resources that we found to be useful. We strive to be thorough and unified and hope this initial effort may trigger more community interests and efforts to further advance AI4Science.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 0,
    "citationCount": 78,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2155975186",
        "name": "Xuan Zhang"
      },
      {
        "authorId": "2109120459",
        "name": "Limei Wang"
      },
      {
        "authorId": "2211730572",
        "name": "Jacob Helwig"
      },
      {
        "authorId": "2004524780",
        "name": "Youzhi Luo"
      },
      {
        "authorId": "2084647086",
        "name": "Cong Fu"
      },
      {
        "authorId": "14629242",
        "name": "Yaochen Xie"
      },
      {
        "authorId": "38813990",
        "name": "Meng Liu"
      },
      {
        "authorId": "2107966079",
        "name": "Yu-Ching Lin"
      },
      {
        "authorId": "2115510017",
        "name": "Zhao Xu"
      },
      {
        "authorId": "1879114760",
        "name": "Keqiang Yan"
      },
      {
        "authorId": "95261912",
        "name": "Keir Adams"
      },
      {
        "authorId": "47927975",
        "name": "Maurice Weiler"
      },
      {
        "authorId": "2118053386",
        "name": "Xiner Li"
      },
      {
        "authorId": "2427076",
        "name": "Tianfan Fu"
      },
      {
        "authorId": "2204444421",
        "name": "Yucheng Wang"
      },
      {
        "authorId": "2119316118",
        "name": "Haiyang Yu"
      },
      {
        "authorId": "2118597356",
        "name": "Yuqing Xie"
      },
      {
        "authorId": "2119032930",
        "name": "Xiang Fu"
      },
      {
        "authorId": "103618574",
        "name": "A. Strasser"
      },
      {
        "authorId": "50433504",
        "name": "Shenglong Xu"
      },
      {
        "authorId": "2153630672",
        "name": "Yi Liu"
      },
      {
        "authorId": "93584228",
        "name": "Yuanqi Du"
      },
      {
        "authorId": "2223648674",
        "name": "Alexandra Saxton"
      },
      {
        "authorId": "2055943899",
        "name": "Hongyi Ling"
      },
      {
        "authorId": "46877850",
        "name": "Hannah Lawrence"
      },
      {
        "authorId": "2124211700",
        "name": "Hannes St\u00e4rk"
      },
      {
        "authorId": "1914700964",
        "name": "Shurui Gui"
      },
      {
        "authorId": "48870109",
        "name": "Carl N. Edwards"
      },
      {
        "authorId": "2068203542",
        "name": "Nicholas Gao"
      },
      {
        "authorId": "2036602766",
        "name": "A. Ladera"
      },
      {
        "authorId": "3716141",
        "name": "Tailin Wu"
      },
      {
        "authorId": "146613894",
        "name": "E. Hofgard"
      },
      {
        "authorId": "90258499",
        "name": "A. M. Tehrani"
      },
      {
        "authorId": "2151036085",
        "name": "Rui Wang"
      },
      {
        "authorId": "102488653",
        "name": "Ameya Daigavane"
      },
      {
        "authorId": "2223648322",
        "name": "Montgomery Bohde"
      },
      {
        "authorId": "2179190141",
        "name": "Jerry Kurtin"
      },
      {
        "authorId": "2111287017",
        "name": "Qiang Huang"
      },
      {
        "authorId": "2223648653",
        "name": "Tuong Phung"
      },
      {
        "authorId": "1490886784",
        "name": "Minkai Xu"
      },
      {
        "authorId": "38009979",
        "name": "Chaitanya K. Joshi"
      },
      {
        "authorId": "2054235048",
        "name": "Simon V. Mathis"
      },
      {
        "authorId": "3371922",
        "name": "K. Azizzadenesheli"
      },
      {
        "authorId": "2030142647",
        "name": "Ada Fang"
      },
      {
        "authorId": "1422193589",
        "name": "A. Aspuru\u2010Guzik"
      },
      {
        "authorId": "2231179",
        "name": "E. Bekkers"
      },
      {
        "authorId": "2149583375",
        "name": "Michael M. Bronstein"
      },
      {
        "authorId": "2095762",
        "name": "M. Zitnik"
      },
      {
        "authorId": "2047844",
        "name": "Anima Anandkumar"
      },
      {
        "authorId": "2490652",
        "name": "Stefano Ermon"
      },
      {
        "authorId": "2075355155",
        "name": "Pietro Lio'"
      },
      {
        "authorId": "2151886670",
        "name": "Rose Yu"
      },
      {
        "authorId": "51249380",
        "name": "Stephan Gunnemann"
      },
      {
        "authorId": "1702139",
        "name": "J. Leskovec"
      },
      {
        "authorId": "144016781",
        "name": "Heng Ji"
      },
      {
        "authorId": "49991208",
        "name": "Jimeng Sun"
      },
      {
        "authorId": "1741283",
        "name": "R. Barzilay"
      },
      {
        "authorId": "35132120",
        "name": "T. Jaakkola"
      },
      {
        "authorId": "13027820",
        "name": "Connor W. Coley"
      },
      {
        "authorId": "2067730514",
        "name": "Xiaoning Qian"
      },
      {
        "authorId": "2198715748",
        "name": "Xiaofeng Qian"
      },
      {
        "authorId": "5485763",
        "name": "T. Smidt"
      },
      {
        "authorId": "1743600",
        "name": "Shuiwang Ji"
      }
    ]
  },
  "236976388": {
    "paperId": "eadb1e7da375939e25083ae3936c4f4ef1f2a719",
    "externalIds": {
      "DBLP": "journals/csur/MadsenRC23",
      "ArXiv": "2108.04840",
      "DOI": "10.1145/3546577",
      "CorpusId": 236976388
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "Post-hoc Interpretability for Neural NLP: A Survey",
    "abstract": "Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.",
    "venue": "ACM Computing Surveys",
    "year": 2021,
    "referenceCount": 150,
    "citationCount": 186,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "152446182",
        "name": "Andreas Madsen"
      },
      {
        "authorId": "145732771",
        "name": "Siva Reddy"
      },
      {
        "authorId": "144631588",
        "name": "A. Chandar"
      }
    ]
  },
  "251104722": {
    "paperId": "2c709ef6186bd607494a3344c903552ea500e449",
    "externalIds": {
      "ArXiv": "2207.13243",
      "DBLP": "journals/corr/abs-2207-13243",
      "DOI": "10.1109/SaTML54575.2023.00039",
      "CorpusId": 251104722
    },
    "publicationVenue": null,
    "title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks",
    "abstract": "The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, \u201cinner\u201d interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.",
    "venue": "2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
    "year": 2022,
    "referenceCount": 335,
    "citationCount": 106,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2179318557",
        "name": "Tilman Raukur"
      },
      {
        "authorId": "120892153",
        "name": "A. Ho"
      },
      {
        "authorId": "2103487700",
        "name": "Stephen Casper"
      },
      {
        "authorId": "1397904824",
        "name": "Dylan Hadfield-Menell"
      }
    ]
  },
  "257900969": {
    "paperId": "0b3904d0e229796aff0bda43bb386513353bc992",
    "externalIds": {
      "DBLP": "journals/corr/abs-2303-18223",
      "ArXiv": "2303.18223",
      "DOI": "10.48550/arXiv.2303.18223",
      "CorpusId": 257900969
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Survey of Large Language Models",
    "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 417,
    "citationCount": 1783,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2542603",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "1423651904",
        "name": "Kun Zhou"
      },
      {
        "authorId": "2018027",
        "name": "Junyi Li"
      },
      {
        "authorId": "1997234792",
        "name": "Tianyi Tang"
      },
      {
        "authorId": "72541556",
        "name": "Xiaolei Wang"
      },
      {
        "authorId": "151472453",
        "name": "Yupeng Hou"
      },
      {
        "authorId": "2007666579",
        "name": "Yingqian Min"
      },
      {
        "authorId": "2107926615",
        "name": "Beichen Zhang"
      },
      {
        "authorId": "2155570461",
        "name": "Junjie Zhang"
      },
      {
        "authorId": "2198280871",
        "name": "Zican Dong"
      },
      {
        "authorId": "2111895473",
        "name": "Yifan Du"
      },
      {
        "authorId": "2181967397",
        "name": "Chen Yang"
      },
      {
        "authorId": "2109315001",
        "name": "Yushuo Chen"
      },
      {
        "authorId": "46842323",
        "name": "Z. Chen"
      },
      {
        "authorId": "2118240359",
        "name": "Jinhao Jiang"
      },
      {
        "authorId": "1708171825",
        "name": "Ruiyang Ren"
      },
      {
        "authorId": "2209136299",
        "name": "Yifan Li"
      },
      {
        "authorId": "2109887979",
        "name": "Xinyu Tang"
      },
      {
        "authorId": "2119618242",
        "name": "Zikang Liu"
      },
      {
        "authorId": "2108129670",
        "name": "Peiyu Liu"
      },
      {
        "authorId": "50204644",
        "name": "J. Nie"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ]
  },
  "260357841": {
    "paperId": "06d8562831c32844285a691c5250d04726df3c61",
    "externalIds": {
      "DBLP": "journals/corr/abs-2307-12980",
      "DOI": "10.48550/arXiv.2307.12980",
      "CorpusId": 260357841
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models",
    "abstract": "\u2014Prompt engineering is a technique that involves augmenting a large pre-trained model with task-speci\ufb01c hints, known as prompts, to adapt the model to new tasks. Prompts can be created manually as natural language instructions or generated automatically as either natural language instructions or vector representations. Prompt engineering enables the ability to perform predictions based solely on prompts without updating model parameters, and the easier application of large pre-trained models in real-world tasks. In past years, Prompt engineering has been well-studied in natural language processing. Recently, it has also been intensively studied in vision-language modeling. However, there is currently a lack of a systematic overview of prompt engineering on pre-trained vision-language models. This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models ( e.g., Flamingo), image-text matching models ( e.g., CLIP), and text-to-image generation models ( e.g., Stable Diffusion). For each type of model, a brief model summary, prompting methods, prompting-based applications, and the corresponding responsibility and integrity issues are summarized and discussed. Furthermore, the commonalities and differences between prompting on vision-language models, language models, and vision models are also discussed. The challenges, future directions, and research opportunities are summarized to foster future research on this topic.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 216,
    "citationCount": 98,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "52203056",
        "name": "Jindong Gu"
      },
      {
        "authorId": "2223193538",
        "name": "Zhen Han"
      },
      {
        "authorId": "2116572341",
        "name": "Shuo Chen"
      },
      {
        "authorId": "1791052",
        "name": "Ahmad Beirami"
      },
      {
        "authorId": "2147293727",
        "name": "Bailan He"
      },
      {
        "authorId": "2143853643",
        "name": "Gengyuan Zhang"
      },
      {
        "authorId": "2072387342",
        "name": "Ruotong Liao"
      },
      {
        "authorId": "2219078907",
        "name": "Yao Qin"
      },
      {
        "authorId": "1742501819",
        "name": "Volker Tresp"
      },
      {
        "authorId": "143635540",
        "name": "Philip H. S. Torr"
      }
    ]
  },
  "263886074": {
    "paperId": "8aa98fbfb6f1e979dead13ce24075503fe47658e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2301-00234",
      "DOI": "10.48550/arXiv.2301.00234",
      "CorpusId": 263886074
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Survey for In-context Learning",
    "abstract": "With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few training examples. It has been a new trend exploring ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress, challenges, and future work in ICL. We \ufb01rst present a formal de\ufb01nition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques of ICL, including training strategies, prompting strategies, and so on. Finally, we present the challenges of ICL and provide potential directions for further research. We hope our work can encourage more research on uncovering how ICL works and improving ICL in future work. 1",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 63,
    "citationCount": 329,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2047143813",
        "name": "Qingxiu Dong"
      },
      {
        "authorId": "49192881",
        "name": "Lei Li"
      },
      {
        "authorId": "10780897",
        "name": "Damai Dai"
      },
      {
        "authorId": "2113919886",
        "name": "Ce Zheng"
      },
      {
        "authorId": "2267004431",
        "name": "Zhiyong Wu"
      },
      {
        "authorId": "7267809",
        "name": "Baobao Chang"
      },
      {
        "authorId": "2116530295",
        "name": "Xu Sun"
      },
      {
        "authorId": "2257464374",
        "name": "Jingjing Xu"
      },
      {
        "authorId": "2257344719",
        "name": "Lei Li"
      },
      {
        "authorId": "3335836",
        "name": "Zhifang Sui"
      }
    ]
  },
  "253553585": {
    "paperId": "ce913026f693101e54d3ab9152e107034d81fce1",
    "externalIds": {
      "DBLP": "journals/tmlr/LiangBLTSYZNWKN23",
      "DOI": "10.1111/nyas.15007",
      "CorpusId": 253553585,
      "PubMed": "37230490"
    },
    "publicationVenue": null,
    "title": "Holistic Evaluation of Language Models",
    "abstract": "Language models (LMs) like GPT\u20103, PaLM, and ChatGPT are the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of LMs. LMs can serve many purposes and their behavior should satisfy many desiderata. To navigate the vast space of potential scenarios and metrics, we taxonomize the space and select representative subsets. We evaluate models on 16 core scenarios and 7 metrics, exposing important trade\u2010offs. We supplement our core evaluation with seven targeted evaluations to deeply analyze specific aspects (including world knowledge, reasoning, regurgitation of copyrighted content, and generation of disinformation). We benchmark 30 LMs, from OpenAI, Microsoft, Google, Meta, Cohere, AI21 Labs, and others. Prior to HELM, models were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: all 30 models are now benchmarked under the same standardized conditions. Our evaluation surfaces 25 top\u2010level findings. For full transparency, we release all raw model prompts and completions publicly. HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2023,
    "referenceCount": 68,
    "citationCount": 726,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145419642",
        "name": "Percy Liang"
      },
      {
        "authorId": "150272855",
        "name": "Rishi Bommasani"
      },
      {
        "authorId": "2110585783",
        "name": "Tony Lee"
      },
      {
        "authorId": "2754804",
        "name": "Dimitris Tsipras"
      },
      {
        "authorId": "1914569491",
        "name": "Dilara Soylu"
      },
      {
        "authorId": "19168196",
        "name": "Michihiro Yasunaga"
      },
      {
        "authorId": "9227100",
        "name": "Yian Zhang"
      },
      {
        "authorId": "22252150",
        "name": "D. Narayanan"
      },
      {
        "authorId": "3374063",
        "name": "Yuhuai Wu"
      },
      {
        "authorId": "32423266",
        "name": "Ananya Kumar"
      },
      {
        "authorId": "51149693",
        "name": "Benjamin Newman"
      },
      {
        "authorId": "2833699",
        "name": "Binhang Yuan"
      },
      {
        "authorId": "1748871792",
        "name": "Bobby Yan"
      },
      {
        "authorId": "2146064162",
        "name": "Ce Zhang"
      },
      {
        "authorId": "133749287",
        "name": "Christian Cosgrove"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      },
      {
        "authorId": "2061444681",
        "name": "Christopher R'e"
      },
      {
        "authorId": "1413421064",
        "name": "Diana Acosta-Navas"
      },
      {
        "authorId": "152951058",
        "name": "Drew A. Hudson"
      },
      {
        "authorId": "49456763",
        "name": "E. Zelikman"
      },
      {
        "authorId": "41152329",
        "name": "Esin Durmus"
      },
      {
        "authorId": "8759332",
        "name": "Faisal Ladhak"
      },
      {
        "authorId": "2047004093",
        "name": "Frieda Rong"
      },
      {
        "authorId": "40046694",
        "name": "Hongyu Ren"
      },
      {
        "authorId": "18307037",
        "name": "Huaxiu Yao"
      },
      {
        "authorId": "39597242",
        "name": "Jue Wang"
      },
      {
        "authorId": "50818255",
        "name": "Keshav Santhanam"
      },
      {
        "authorId": "4773175",
        "name": "Laurel J. Orr"
      },
      {
        "authorId": "2118604716",
        "name": "Lucia Zheng"
      },
      {
        "authorId": "2186981598",
        "name": "Mert Yuksekgonul"
      },
      {
        "authorId": "51903517",
        "name": "Mirac Suzgun"
      },
      {
        "authorId": "2182172863",
        "name": "Nathan S. Kim"
      },
      {
        "authorId": "2820009",
        "name": "Neel Guha"
      },
      {
        "authorId": "22193324",
        "name": "Niladri S. Chatterji"
      },
      {
        "authorId": "144112155",
        "name": "O. Khattab"
      },
      {
        "authorId": "2071773966",
        "name": "Peter Henderson"
      },
      {
        "authorId": "144862341",
        "name": "Qian Huang"
      },
      {
        "authorId": "2121293578",
        "name": "Ryan Chi"
      },
      {
        "authorId": "46215055",
        "name": "Sang Michael Xie"
      },
      {
        "authorId": "2852106",
        "name": "Shibani Santurkar"
      },
      {
        "authorId": "25769960",
        "name": "S. Ganguli"
      },
      {
        "authorId": "2117567142",
        "name": "Tatsunori Hashimoto"
      },
      {
        "authorId": "8938047",
        "name": "Thomas F. Icard"
      },
      {
        "authorId": "123437034",
        "name": "Tianyi Zhang"
      },
      {
        "authorId": "113810201",
        "name": "Vishrav Chaudhary"
      },
      {
        "authorId": "2127971344",
        "name": "William Wang"
      },
      {
        "authorId": "2145429039",
        "name": "Xuechen Li"
      },
      {
        "authorId": "2054708905",
        "name": "Yifan Mai"
      },
      {
        "authorId": "49889860",
        "name": "Yuhui Zhang"
      },
      {
        "authorId": "2740047",
        "name": "Yuta Koreeda"
      }
    ]
  },
  "257532815": {
    "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
    "externalIds": {
      "ArXiv": "2303.08774",
      "CorpusId": 257532815
    },
    "publicationVenue": null,
    "title": "GPT-4 Technical Report",
    "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
    "venue": "",
    "year": 2023,
    "referenceCount": 0,
    "citationCount": 8391,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2275249853",
        "name": "OpenAI Josh Achiam"
      },
      {
        "authorId": "2275250875",
        "name": "Steven Adler"
      },
      {
        "authorId": "144517868",
        "name": "Sandhini Agarwal"
      },
      {
        "authorId": "2274773568",
        "name": "Lama Ahmad"
      },
      {
        "authorId": "2258629",
        "name": "Ilge Akkaya"
      },
      {
        "authorId": "2275244794",
        "name": "Florencia Leoni Aleman"
      },
      {
        "authorId": "2275252021",
        "name": "Diogo Almeida"
      },
      {
        "authorId": "2275252424",
        "name": "Janko Altenschmidt"
      },
      {
        "authorId": "2275245579",
        "name": "Sam Altman"
      },
      {
        "authorId": "2275246437",
        "name": "Shyamal Anadkat"
      },
      {
        "authorId": "2275139370",
        "name": "Red Avila"
      },
      {
        "authorId": "2256699302",
        "name": "Igor Babuschkin"
      },
      {
        "authorId": "2054519183",
        "name": "S. Balaji"
      },
      {
        "authorId": "2275251659",
        "name": "Valerie Balcom"
      },
      {
        "authorId": "47626612",
        "name": "Paul Baltescu"
      },
      {
        "authorId": "2275198557",
        "name": "Haim-ing Bao"
      },
      {
        "authorId": "2275251620",
        "name": "Mo Bavarian"
      },
      {
        "authorId": "2275245092",
        "name": "Jeff Belgum"
      },
      {
        "authorId": "4689792",
        "name": "Irwan Bello"
      },
      {
        "authorId": "2275245414",
        "name": "Jake Berdine"
      },
      {
        "authorId": "2275245581",
        "name": "Gabriel Bernadett-Shapiro"
      },
      {
        "authorId": "133740015",
        "name": "Christopher Berner"
      },
      {
        "authorId": "2275251674",
        "name": "Lenny Bogdonoff"
      },
      {
        "authorId": "2275246071",
        "name": "Oleg Boiko"
      },
      {
        "authorId": "2275248137",
        "name": "Madelaine Boyd"
      },
      {
        "authorId": "2275245419",
        "name": "Anna-Luisa Brakman"
      },
      {
        "authorId": "2065151121",
        "name": "G. Brockman"
      },
      {
        "authorId": "2275219628",
        "name": "Tim Brooks"
      },
      {
        "authorId": "35167962",
        "name": "Miles Brundage"
      },
      {
        "authorId": "2146257251",
        "name": "Kevin Button"
      },
      {
        "authorId": "2275157286",
        "name": "Trevor Cai"
      },
      {
        "authorId": "2274782053",
        "name": "Rosie Campbell"
      },
      {
        "authorId": "2275245404",
        "name": "Andrew Cann"
      },
      {
        "authorId": "2275246368",
        "name": "Brittany Carey"
      },
      {
        "authorId": "2275120298",
        "name": "Chelsea Carlson"
      },
      {
        "authorId": "144114446",
        "name": "Rory Carmichael"
      },
      {
        "authorId": "1466431052",
        "name": "Brooke Chan"
      },
      {
        "authorId": "2275545855",
        "name": "Che Chang"
      },
      {
        "authorId": "2057091285",
        "name": "Fotis Chantzis"
      },
      {
        "authorId": "2253841704",
        "name": "Derek Chen"
      },
      {
        "authorId": "2275188918",
        "name": "Sully Chen"
      },
      {
        "authorId": "2275179180",
        "name": "Ruby Chen"
      },
      {
        "authorId": "2275289833",
        "name": "Jason Chen"
      },
      {
        "authorId": "2108828435",
        "name": "Mark Chen"
      },
      {
        "authorId": "1490681878",
        "name": "B. Chess"
      },
      {
        "authorId": "2275251158",
        "name": "Chester Cho"
      },
      {
        "authorId": "2276186593",
        "name": "Casey Chu"
      },
      {
        "authorId": "2275839391",
        "name": "Hyung Won Chung"
      },
      {
        "authorId": "2275231534",
        "name": "Dave Cummings"
      },
      {
        "authorId": "49645091",
        "name": "Jeremiah Currier"
      },
      {
        "authorId": "2276187456",
        "name": "Yunxing Dai"
      },
      {
        "authorId": "2275251205",
        "name": "Cory Decareaux"
      },
      {
        "authorId": "2275244920",
        "name": "Thomas Degry"
      },
      {
        "authorId": "2275247090",
        "name": "Noah Deutsch"
      },
      {
        "authorId": "2275251200",
        "name": "Damien Deville"
      },
      {
        "authorId": "2275244298",
        "name": "Arka Dhar"
      },
      {
        "authorId": "35363891",
        "name": "David Dohan"
      },
      {
        "authorId": "2275252295",
        "name": "Steve Dowling"
      },
      {
        "authorId": "2275245491",
        "name": "Sheila Dunning"
      },
      {
        "authorId": "66821245",
        "name": "Adrien Ecoffet"
      },
      {
        "authorId": "2275245457",
        "name": "Atty Eleti"
      },
      {
        "authorId": "2146257131",
        "name": "Tyna Eloundou"
      },
      {
        "authorId": "2065430571",
        "name": "David Farhi"
      },
      {
        "authorId": "2096916416",
        "name": "L. Fedus"
      },
      {
        "authorId": "2275249996",
        "name": "Niko Felix"
      },
      {
        "authorId": "2275245820",
        "name": "Sim'on Posada Fishman"
      },
      {
        "authorId": "2275244914",
        "name": "Juston Forte"
      },
      {
        "authorId": "2275251173",
        "name": "Is-abella Fulford"
      },
      {
        "authorId": "2027599537",
        "name": "Leo Gao"
      },
      {
        "authorId": "2275200811",
        "name": "Elie Georges"
      },
      {
        "authorId": "2275254804",
        "name": "C. Gibson"
      },
      {
        "authorId": "2275144649",
        "name": "Vik Goel"
      },
      {
        "authorId": "2325028819",
        "name": "Tarun Gogineni"
      },
      {
        "authorId": "2261041177",
        "name": "Gabriel Goh"
      },
      {
        "authorId": "2158366935",
        "name": "Raphael Gontijo-Lopes"
      },
      {
        "authorId": "2265066144",
        "name": "Jonathan Gordon"
      },
      {
        "authorId": "2275250003",
        "name": "Morgan Grafstein"
      },
      {
        "authorId": "145565184",
        "name": "Scott Gray"
      },
      {
        "authorId": "2275247307",
        "name": "Ryan Greene"
      },
      {
        "authorId": "2275137274",
        "name": "Joshua Gross"
      },
      {
        "authorId": "2253699903",
        "name": "S. Gu"
      },
      {
        "authorId": "2276101257",
        "name": "Yufei Guo"
      },
      {
        "authorId": "2004021329",
        "name": "Chris Hallacy"
      },
      {
        "authorId": "2275540338",
        "name": "Jesse Han"
      },
      {
        "authorId": "2275295848",
        "name": "Jeff Harris"
      },
      {
        "authorId": "2275226809",
        "name": "Yuchen He"
      },
      {
        "authorId": "2275245527",
        "name": "Mike Heaton"
      },
      {
        "authorId": "2151087994",
        "name": "Johannes Heidecke"
      },
      {
        "authorId": "2242286342",
        "name": "Chris Hesse"
      },
      {
        "authorId": "2226452668",
        "name": "Alan Hickey"
      },
      {
        "authorId": "2275246148",
        "name": "Wade Hickey"
      },
      {
        "authorId": "2275245339",
        "name": "Peter Hoeschele"
      },
      {
        "authorId": "103681415",
        "name": "Brandon Houghton"
      },
      {
        "authorId": "2275214107",
        "name": "Kenny Hsu"
      },
      {
        "authorId": "2275210604",
        "name": "Shengli Hu"
      },
      {
        "authorId": "2275777049",
        "name": "Xin Hu"
      },
      {
        "authorId": "39378983",
        "name": "Joost Huizinga"
      },
      {
        "authorId": "2276187117",
        "name": "Shantanu Jain"
      },
      {
        "authorId": "2171110177",
        "name": "Shawn Jain"
      },
      {
        "authorId": "2151094350",
        "name": "Joanne Jang"
      },
      {
        "authorId": "2253471334",
        "name": "Angela Jiang"
      },
      {
        "authorId": "2275172062",
        "name": "Roger Jiang"
      },
      {
        "authorId": "2275752035",
        "name": "Haozhun Jin"
      },
      {
        "authorId": "2275203081",
        "name": "Denny Jin"
      },
      {
        "authorId": "2275250083",
        "name": "Shino Jomoto"
      },
      {
        "authorId": "2275247096",
        "name": "B. Jonn"
      },
      {
        "authorId": "35450887",
        "name": "Heewoo Jun"
      },
      {
        "authorId": "2403754",
        "name": "Tomer Kaftan"
      },
      {
        "authorId": "2275230678",
        "name": "Lukasz Kaiser"
      },
      {
        "authorId": "2275169038",
        "name": "Ali Kamali"
      },
      {
        "authorId": "3151440",
        "name": "I. Kanitscheider"
      },
      {
        "authorId": "2844898",
        "name": "N. Keskar"
      },
      {
        "authorId": "2152264064",
        "name": "Tabarak Khan"
      },
      {
        "authorId": "2275246102",
        "name": "Logan Kilpatrick"
      },
      {
        "authorId": "2260346092",
        "name": "Jong Wook Kim"
      },
      {
        "authorId": "2149054292",
        "name": "Christina Kim"
      },
      {
        "authorId": "2275296777",
        "name": "Yongjik Kim"
      },
      {
        "authorId": "2275112980",
        "name": "Hendrik Kirchner"
      },
      {
        "authorId": "51131802",
        "name": "J. Kiros"
      },
      {
        "authorId": "2146257375",
        "name": "Matthew Knight"
      },
      {
        "authorId": "1485556711",
        "name": "Daniel Kokotajlo"
      },
      {
        "authorId": "2275246094",
        "name": "Lukasz Kondraciuk"
      },
      {
        "authorId": "1666171360",
        "name": "A. Kondrich"
      },
      {
        "authorId": "2275252322",
        "name": "Aris Konstantinidis"
      },
      {
        "authorId": "2275245594",
        "name": "Kyle Kosic"
      },
      {
        "authorId": "2064404342",
        "name": "Gretchen Krueger"
      },
      {
        "authorId": "2275229877",
        "name": "Vishal Kuo"
      },
      {
        "authorId": "2275247085",
        "name": "Michael Lampe"
      },
      {
        "authorId": "2275246287",
        "name": "Ikai Lan"
      },
      {
        "authorId": "2274915115",
        "name": "Teddy Lee"
      },
      {
        "authorId": "2990741",
        "name": "J. Leike"
      },
      {
        "authorId": "52152632",
        "name": "Jade Leung"
      },
      {
        "authorId": "2275256930",
        "name": "Daniel Levy"
      },
      {
        "authorId": "2275285124",
        "name": "C. Li"
      },
      {
        "authorId": "2275176375",
        "name": "Rachel Lim"
      },
      {
        "authorId": "2275759230",
        "name": "Molly Lin"
      },
      {
        "authorId": "2253840098",
        "name": "Stephanie Lin"
      },
      {
        "authorId": "1380985420",
        "name": "Ma-teusz Litwin"
      },
      {
        "authorId": "2275248327",
        "name": "Theresa Lopez"
      },
      {
        "authorId": "2257272397",
        "name": "Ryan Lowe"
      },
      {
        "authorId": "2275245628",
        "name": "Patricia Lue"
      },
      {
        "authorId": "119341078",
        "name": "A. Makanju"
      },
      {
        "authorId": "2275245649",
        "name": "Kim Malfacini"
      },
      {
        "authorId": "46430291",
        "name": "Sam Manning"
      },
      {
        "authorId": "14113256",
        "name": "Todor Markov"
      },
      {
        "authorId": "2275245336",
        "name": "Yaniv Markovski"
      },
      {
        "authorId": "2114362965",
        "name": "Bianca Martin"
      },
      {
        "authorId": "2275231822",
        "name": "Katie Mayer"
      },
      {
        "authorId": "2275247045",
        "name": "Andrew Mayne"
      },
      {
        "authorId": "39593364",
        "name": "Bob McGrew"
      },
      {
        "authorId": "2047820455",
        "name": "S. McKinney"
      },
      {
        "authorId": "3028785",
        "name": "C. McLeavey"
      },
      {
        "authorId": "2274772421",
        "name": "Paul McMillan"
      },
      {
        "authorId": "2275234856",
        "name": "Jake McNeil"
      },
      {
        "authorId": "2275210659",
        "name": "David Medina"
      },
      {
        "authorId": "2275132306",
        "name": "Aalok Mehta"
      },
      {
        "authorId": "10698483",
        "name": "Jacob Menick"
      },
      {
        "authorId": "2275246330",
        "name": "Luke Metz"
      },
      {
        "authorId": "2275252694",
        "name": "Andrey Mishchenko"
      },
      {
        "authorId": "2051714782",
        "name": "Pamela Mishkin"
      },
      {
        "authorId": "2275245453",
        "name": "Vinnie Monaco"
      },
      {
        "authorId": "1404556973",
        "name": "Evan Morikawa"
      },
      {
        "authorId": "3407880",
        "name": "Daniel P. Mossing"
      },
      {
        "authorId": "2275154456",
        "name": "Tong Mu"
      },
      {
        "authorId": "2117715631",
        "name": "Mira Murati"
      },
      {
        "authorId": "147746767",
        "name": "O. Murk"
      },
      {
        "authorId": "2275246116",
        "name": "David M'ely"
      },
      {
        "authorId": "3422774",
        "name": "Ashvin Nair"
      },
      {
        "authorId": "7406311",
        "name": "Reiichiro Nakano"
      },
      {
        "authorId": "2057426488",
        "name": "Rajeev Nayak"
      },
      {
        "authorId": "2072676",
        "name": "Arvind Neelakantan"
      },
      {
        "authorId": "2273886618",
        "name": "Richard Ngo"
      },
      {
        "authorId": "2275115983",
        "name": "Hyeonwoo Noh"
      },
      {
        "authorId": "2228518120",
        "name": "Ouyang Long"
      },
      {
        "authorId": "1435765036",
        "name": "Cullen O'Keefe"
      },
      {
        "authorId": "2713380",
        "name": "J. Pachocki"
      },
      {
        "authorId": "34800652",
        "name": "Alex Paino"
      },
      {
        "authorId": "2275244652",
        "name": "Joe Palermo"
      },
      {
        "authorId": "2275246178",
        "name": "Ashley Pantuliano"
      },
      {
        "authorId": "50213542",
        "name": "Giambattista Parascandolo"
      },
      {
        "authorId": "2275245818",
        "name": "Joel Parish"
      },
      {
        "authorId": "2275245435",
        "name": "Emy Parparita"
      },
      {
        "authorId": "2274774915",
        "name": "Alexandre Passos"
      },
      {
        "authorId": "2068123790",
        "name": "Mikhail Pavlov"
      },
      {
        "authorId": "2275125663",
        "name": "Andrew Peng"
      },
      {
        "authorId": "2275245529",
        "name": "Adam Perelman"
      },
      {
        "authorId": "2275250075",
        "name": "Filipe de Avila Belbute Peres"
      },
      {
        "authorId": "2136008481",
        "name": "Michael Petrov"
      },
      {
        "authorId": "1463773776",
        "name": "Henrique Pond\u00e9 de Oliveira Pinto"
      },
      {
        "authorId": "2275246346",
        "name": "Michael Pokorny"
      },
      {
        "authorId": "2275246814",
        "name": "Michelle Pokrass"
      },
      {
        "authorId": "144401061",
        "name": "Vitchyr H. Pong"
      },
      {
        "authorId": "2275150061",
        "name": "Tolly Powell"
      },
      {
        "authorId": "146162186",
        "name": "Alethea Power"
      },
      {
        "authorId": "2151088845",
        "name": "Boris Power"
      },
      {
        "authorId": "2275243930",
        "name": "Elizabeth Proehl"
      },
      {
        "authorId": "2285654208",
        "name": "Raul Puri"
      },
      {
        "authorId": "38909097",
        "name": "Alec Radford"
      },
      {
        "authorId": "2275178294",
        "name": "Jack W. Rae"
      },
      {
        "authorId": "2261024614",
        "name": "Aditya Ramesh"
      },
      {
        "authorId": "2275225165",
        "name": "Cameron Raymond"
      },
      {
        "authorId": "2275252438",
        "name": "Francis Real"
      },
      {
        "authorId": "2275252095",
        "name": "Kendra Rimbach"
      },
      {
        "authorId": "2275207240",
        "name": "Carl Ross"
      },
      {
        "authorId": "11150265",
        "name": "Bob Rotsted"
      },
      {
        "authorId": "2275250007",
        "name": "Henri Roussez"
      },
      {
        "authorId": "2260406867",
        "name": "Nick Ryder"
      },
      {
        "authorId": "47204843",
        "name": "M. Saltarelli"
      },
      {
        "authorId": "2275246803",
        "name": "Ted Sanders"
      },
      {
        "authorId": "2852106",
        "name": "Shibani Santurkar"
      },
      {
        "authorId": "144864359",
        "name": "Girish Sastry"
      },
      {
        "authorId": "2275265666",
        "name": "Heather Schmidt"
      },
      {
        "authorId": "2252874293",
        "name": "David Schnurr"
      },
      {
        "authorId": "47971768",
        "name": "John Schulman"
      },
      {
        "authorId": "2196579",
        "name": "Daniel Selsam"
      },
      {
        "authorId": "2275244711",
        "name": "Kyla Sheppard"
      },
      {
        "authorId": "102475503",
        "name": "Toki Sherbakov"
      },
      {
        "authorId": "2275246834",
        "name": "Jessica Shieh"
      },
      {
        "authorId": "118335789",
        "name": "Sarah Shoker"
      },
      {
        "authorId": "67311962",
        "name": "Pranav Shyam"
      },
      {
        "authorId": "2700360",
        "name": "Szymon Sidor"
      },
      {
        "authorId": "2064673055",
        "name": "Eric Sigler"
      },
      {
        "authorId": "2151735251",
        "name": "Maddie Simens"
      },
      {
        "authorId": "2275252299",
        "name": "Jordan Sitkin"
      },
      {
        "authorId": "2117680841",
        "name": "Katarina Slama"
      },
      {
        "authorId": "103422608",
        "name": "Ian Sohl"
      },
      {
        "authorId": "2901424",
        "name": "Benjamin D. Sokolowsky"
      },
      {
        "authorId": "2307592658",
        "name": "Yang Song"
      },
      {
        "authorId": "2275245668",
        "name": "Natalie Staudacher"
      },
      {
        "authorId": "9927844",
        "name": "F. Such"
      },
      {
        "authorId": "2275252251",
        "name": "Natalie Summers"
      },
      {
        "authorId": "1701686",
        "name": "I. Sutskever"
      },
      {
        "authorId": "2275750817",
        "name": "Jie Tang"
      },
      {
        "authorId": "145950540",
        "name": "N. Tezak"
      },
      {
        "authorId": "2151289331",
        "name": "Madeleine Thompson"
      },
      {
        "authorId": "2275252092",
        "name": "Phil Tillet"
      },
      {
        "authorId": "2267339677",
        "name": "Amin Tootoonchian"
      },
      {
        "authorId": "2275249879",
        "name": "Elizabeth Tseng"
      },
      {
        "authorId": "2275249709",
        "name": "Preston Tuggle"
      },
      {
        "authorId": "2275244171",
        "name": "Nick Turley"
      },
      {
        "authorId": "2065005836",
        "name": "Jerry Tworek"
      },
      {
        "authorId": "2275203310",
        "name": "Juan Felipe Cer'on Uribe"
      },
      {
        "authorId": "2275244586",
        "name": "Andrea Vallone"
      },
      {
        "authorId": "2275245661",
        "name": "Arun Vijayvergiya"
      },
      {
        "authorId": "153387869",
        "name": "Chelsea Voss"
      },
      {
        "authorId": "2275245962",
        "name": "Carroll L. Wainwright"
      },
      {
        "authorId": "2275528432",
        "name": "Justin Jay Wang"
      },
      {
        "authorId": "2275540420",
        "name": "Alvin Wang"
      },
      {
        "authorId": "2275189326",
        "name": "Ben Wang"
      },
      {
        "authorId": "2170081200",
        "name": "Jonathan Ward"
      },
      {
        "authorId": "2253952872",
        "name": "Jason Wei"
      },
      {
        "authorId": "2275244218",
        "name": "CJ Weinmann"
      },
      {
        "authorId": "2275245663",
        "name": "Akila Welihinda"
      },
      {
        "authorId": "2930640",
        "name": "P. Welinder"
      },
      {
        "authorId": "2275139180",
        "name": "Jiayi Weng"
      },
      {
        "authorId": "2065741038",
        "name": "Lilian Weng"
      },
      {
        "authorId": "2275252154",
        "name": "Matt Wiethoff"
      },
      {
        "authorId": "2275249733",
        "name": "Dave Willner"
      },
      {
        "authorId": "2059411355",
        "name": "Clemens Winter"
      },
      {
        "authorId": "2275244177",
        "name": "Samuel Wolrich"
      },
      {
        "authorId": "2275225207",
        "name": "Hannah Wong"
      },
      {
        "authorId": "2275245771",
        "name": "Lauren Workman"
      },
      {
        "authorId": "2275299848",
        "name": "Sherwin Wu"
      },
      {
        "authorId": "2274911253",
        "name": "Jeff Wu"
      },
      {
        "authorId": "2307456650",
        "name": "Michael Wu"
      },
      {
        "authorId": "2275190169",
        "name": "Kai Xiao"
      },
      {
        "authorId": "2275452480",
        "name": "Tao Xu"
      },
      {
        "authorId": "2275310096",
        "name": "Sarah Yoo"
      },
      {
        "authorId": "2275593618",
        "name": "Kevin Yu"
      },
      {
        "authorId": "2275194186",
        "name": "Qim-ing Yuan"
      },
      {
        "authorId": "2563432",
        "name": "Wojciech Zaremba"
      },
      {
        "authorId": "49629836",
        "name": "Rowan Zellers"
      },
      {
        "authorId": "2262080679",
        "name": "Chong Zhang"
      },
      {
        "authorId": "2275288889",
        "name": "Marvin Zhang"
      },
      {
        "authorId": "2275545682",
        "name": "Shengjia Zhao"
      },
      {
        "authorId": "2275257857",
        "name": "Tianhao Zheng"
      },
      {
        "authorId": "2275201537",
        "name": "Juntang Zhuang"
      },
      {
        "authorId": "2275245715",
        "name": "William Zhuk"
      },
      {
        "authorId": "2368067",
        "name": "Barret Zoph"
      }
    ]
  },
  "248476411": {
    "paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
    "externalIds": {
      "ArXiv": "2204.14198",
      "DBLP": "journals/corr/abs-2204-14198",
      "CorpusId": 248476411
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
    "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "referenceCount": 182,
    "citationCount": 2489,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2285263",
        "name": "Jean-Baptiste Alayrac"
      },
      {
        "authorId": "7408951",
        "name": "Jeff Donahue"
      },
      {
        "authorId": "152831141",
        "name": "Pauline Luc"
      },
      {
        "authorId": "19200186",
        "name": "Antoine Miech"
      },
      {
        "authorId": "2159207795",
        "name": "Iain Barr"
      },
      {
        "authorId": "66535271",
        "name": "Yana Hasson"
      },
      {
        "authorId": "3257286",
        "name": "Karel Lenc"
      },
      {
        "authorId": "1697879",
        "name": "A. Mensch"
      },
      {
        "authorId": "2143434227",
        "name": "Katie Millican"
      },
      {
        "authorId": "47447264",
        "name": "Malcolm Reynolds"
      },
      {
        "authorId": "81387328",
        "name": "Roman Ring"
      },
      {
        "authorId": "2143538252",
        "name": "Eliza Rutherford"
      },
      {
        "authorId": "12159303",
        "name": "Serkan Cabi"
      },
      {
        "authorId": "22237490",
        "name": "Tengda Han"
      },
      {
        "authorId": "48398849",
        "name": "Zhitao Gong"
      },
      {
        "authorId": "2412073",
        "name": "Sina Samangooei"
      },
      {
        "authorId": "49601928",
        "name": "Marianne Monteiro"
      },
      {
        "authorId": "10698483",
        "name": "Jacob Menick"
      },
      {
        "authorId": "148016269",
        "name": "Sebastian Borgeaud"
      },
      {
        "authorId": "2065040422",
        "name": "Andy Brock"
      },
      {
        "authorId": "3208081",
        "name": "Aida Nematzadeh"
      },
      {
        "authorId": "7782886",
        "name": "Sahand Sharifzadeh"
      },
      {
        "authorId": "9961753",
        "name": "Mikolaj Binkowski"
      },
      {
        "authorId": "2026369796",
        "name": "Ricardo Barreira"
      },
      {
        "authorId": "1689108",
        "name": "O. Vinyals"
      },
      {
        "authorId": "1688869",
        "name": "Andrew Zisserman"
      },
      {
        "authorId": "34838386",
        "name": "K. Simonyan"
      }
    ]
  },
  "256390509": {
    "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
    "externalIds": {
      "DBLP": "journals/corr/abs-2301-12597",
      "ArXiv": "2301.12597",
      "DOI": "10.48550/arXiv.2301.12597",
      "CorpusId": 256390509
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
    "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "referenceCount": 48,
    "citationCount": 2899,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49299019",
        "name": "Junnan Li"
      },
      {
        "authorId": "2981509",
        "name": "Dongxu Li"
      },
      {
        "authorId": "1702137",
        "name": "S. Savarese"
      },
      {
        "authorId": "2184854289",
        "name": "Steven C. H. Hoi"
      }
    ]
  },
  "258179774": {
    "paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
    "externalIds": {
      "DBLP": "journals/corr/abs-2304-08485",
      "ArXiv": "2304.08485",
      "DOI": "10.48550/arXiv.2304.08485",
      "CorpusId": 258179774
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Visual Instruction Tuning",
    "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 63,
    "citationCount": 2513,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2143856368",
        "name": "Haotian Liu"
      },
      {
        "authorId": "2109737569",
        "name": "Chunyuan Li"
      },
      {
        "authorId": "31060482",
        "name": "Qingyang Wu"
      },
      {
        "authorId": "144756076",
        "name": "Yong Jae Lee"
      }
    ]
  },
  "257404891": {
    "paperId": "af997821231898a5f8d0fd78dad4eec526acabe5",
    "externalIds": {
      "DBLP": "journals/corr/abs-2303-04671",
      "ArXiv": "2303.04671",
      "DOI": "10.48550/arXiv.2303.04671",
      "CorpusId": 257404891
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
    "abstract": "ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \\textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \\url{https://github.com/microsoft/visual-chatgpt}.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 61,
    "citationCount": 523,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2151101534",
        "name": "Chenfei Wu"
      },
      {
        "authorId": "2333305",
        "name": "Sheng-Kai Yin"
      },
      {
        "authorId": "15629561",
        "name": "Weizhen Qi"
      },
      {
        "authorId": "2108428177",
        "name": "Xiaodong Wang"
      },
      {
        "authorId": "1576234850",
        "name": "Zecheng Tang"
      },
      {
        "authorId": "2072609829",
        "name": "Nan Duan"
      }
    ]
  },
  "257833781": {
    "paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",
    "externalIds": {
      "ArXiv": "2303.17580",
      "DBLP": "journals/corr/abs-2303-17580",
      "DOI": "10.48550/arXiv.2303.17580",
      "CorpusId": 257833781
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
    "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 44,
    "citationCount": 661,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1471660296",
        "name": "Yongliang Shen"
      },
      {
        "authorId": "50982078",
        "name": "Kaitao Song"
      },
      {
        "authorId": "48391466",
        "name": "Xu Tan"
      },
      {
        "authorId": "2118015967",
        "name": "D. Li"
      },
      {
        "authorId": "1776903",
        "name": "Weiming Lu"
      },
      {
        "authorId": "2056432541",
        "name": "Y. Zhuang"
      }
    ]
  },
  "258822817": {
    "paperId": "9f411fda2ad5b141a3115f707bcf5ee865b3fb94",
    "externalIds": {
      "DBLP": "conf/nips/TangYZ0B23",
      "ArXiv": "2305.11846",
      "DOI": "10.48550/arXiv.2305.11846",
      "CorpusId": 258822817
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Any-to-Any Generation via Composable Diffusion",
    "abstract": "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 59,
    "citationCount": 120,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "151270642",
        "name": "Zineng Tang"
      },
      {
        "authorId": "2155459391",
        "name": "Ziyi Yang"
      },
      {
        "authorId": "8652308",
        "name": "Chenguang Zhu"
      },
      {
        "authorId": "48262024",
        "name": "Michael Zeng"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      }
    ]
  },
  "258564264": {
    "paperId": "7dc6da87eaa6f830354feb2db14023cab8678c91",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-05665",
      "ArXiv": "2305.05665",
      "DOI": "10.1109/CVPR52729.2023.01457",
      "CorpusId": 258564264
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "ImageBind One Embedding Space to Bind Them All",
    "abstract": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications \u2018out-of-the-box\u2019 including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "referenceCount": 87,
    "citationCount": 611,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3102850",
        "name": "Rohit Girdhar"
      },
      {
        "authorId": "1388811741",
        "name": "Alaaeldin El-Nouby"
      },
      {
        "authorId": "2109168016",
        "name": "Zhuang Liu"
      },
      {
        "authorId": "152964870",
        "name": "Mannat Singh"
      },
      {
        "authorId": "3085301",
        "name": "Kalyan Vasudev Alwala"
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin"
      },
      {
        "authorId": "1806773",
        "name": "Ishan Misra"
      }
    ]
  },
  "261696650": {
    "paperId": "fa75a55760e6ea49b39b83cb85c99a22e1088254",
    "externalIds": {
      "ArXiv": "2309.05519",
      "DBLP": "journals/corr/abs-2309-05519",
      "DOI": "10.48550/arXiv.2309.05519",
      "CorpusId": 261696650
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "NExT-GPT: Any-to-Any Multimodal LLM",
    "abstract": "While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "referenceCount": 136,
    "citationCount": 285,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1957924118",
        "name": "Shengqiong Wu"
      },
      {
        "authorId": "46959445",
        "name": "Hao Fei"
      },
      {
        "authorId": "1990265392",
        "name": "Leigang Qu"
      },
      {
        "authorId": "144540018",
        "name": "Wei Ji"
      },
      {
        "authorId": "144078686",
        "name": "Tat-Seng Chua"
      }
    ]
  },
  "263137930": {
    "paperId": "f2f9c02a7eb484dd7b7ac46892856e3f278eed77",
    "externalIds": {
      "ArXiv": "2309.16058",
      "DBLP": "journals/corr/abs-2309-16058",
      "DOI": "10.48550/arXiv.2309.16058",
      "CorpusId": 263137930
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
    "abstract": "We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM's capabilities, we fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs. We conduct comprehensive empirical analysis comprising both human and automatic evaluations, and demonstrate state-of-the-art performance on various multimodal tasks.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 66,
    "citationCount": 74,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2256132624",
        "name": "Seungwhan Moon"
      },
      {
        "authorId": "2111680936",
        "name": "Andrea Madotto"
      },
      {
        "authorId": "2146396528",
        "name": "Zhaojiang Lin"
      },
      {
        "authorId": "2248184174",
        "name": "Tushar Nagarajan"
      },
      {
        "authorId": "2249027674",
        "name": "Matt Smith"
      },
      {
        "authorId": "2249741629",
        "name": "Shashank Jain"
      },
      {
        "authorId": "2248041474",
        "name": "Chun-Fu Yeh"
      },
      {
        "authorId": "2248184913",
        "name": "Prakash Murugesan"
      },
      {
        "authorId": "2248176677",
        "name": "Peyman Heidari"
      },
      {
        "authorId": "2247965931",
        "name": "Yue Liu"
      },
      {
        "authorId": "27693639",
        "name": "Kavya Srinet"
      },
      {
        "authorId": "3057557",
        "name": "Babak Damavandi"
      },
      {
        "authorId": "2247977368",
        "name": "Anuj Kumar"
      }
    ]
  },
  "261076491": {
    "paperId": "1245ef1926416d649b62323975c6fa22dfb885ee",
    "externalIds": {
      "DBLP": "conf/iclr/Hu0WWPCYWZZ0LXL24",
      "ArXiv": "2308.12038",
      "DOI": "10.48550/arXiv.2308.12038",
      "CorpusId": 261076491
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages",
    "abstract": "Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in non-English languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a (quasi)-zero-shot manner, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in image-to-text and text-to-image generation, which achieve state-of-the-art (open-source) performance in Chinese. To facilitate future research, we open-source codes and model weights at https://github.com/OpenBMB/VisCPM.git.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 94,
    "citationCount": 33,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "92837695",
        "name": "Jinyi Hu"
      },
      {
        "authorId": "1390925224",
        "name": "Yuan Yao"
      },
      {
        "authorId": "2146309007",
        "name": "Chong Wang"
      },
      {
        "authorId": "2205366189",
        "name": "Shanonan Wang"
      },
      {
        "authorId": "47304253",
        "name": "Yinxu Pan"
      },
      {
        "authorId": "2157954216",
        "name": "Qi-An Chen"
      },
      {
        "authorId": "2117902355",
        "name": "Tianyu Yu"
      },
      {
        "authorId": "2180430182",
        "name": "Han Wu"
      },
      {
        "authorId": null,
        "name": "Yue Zhao"
      },
      {
        "authorId": "2233320353",
        "name": "Haoye Zhang"
      },
      {
        "authorId": "48506411",
        "name": "Xu Han"
      },
      {
        "authorId": "2427350",
        "name": "Yankai Lin"
      },
      {
        "authorId": "2233089123",
        "name": "Jiao Xue"
      },
      {
        "authorId": "2144118403",
        "name": "Dahai Li"
      },
      {
        "authorId": "2141313179",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "1753344",
        "name": "Maosong Sun"
      }
    ]
  },
  "263333963": {
    "paperId": "07d639b011f48615c1154cb6cdbc067bfe331348",
    "externalIds": {
      "ArXiv": "2310.00653",
      "DBLP": "journals/corr/abs-2310-00653",
      "DOI": "10.48550/arXiv.2310.00653",
      "CorpusId": 263333963
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants",
    "abstract": "Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 35,
    "citationCount": 13,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2117902355",
        "name": "Tianyu Yu"
      },
      {
        "authorId": "92837695",
        "name": "Jinyi Hu"
      },
      {
        "authorId": "1390925224",
        "name": "Yuan Yao"
      },
      {
        "authorId": "2233320353",
        "name": "Haoye Zhang"
      },
      {
        "authorId": "2249846477",
        "name": "Yue Zhao"
      },
      {
        "authorId": "2249899670",
        "name": "Chongyi Wang"
      },
      {
        "authorId": "2205366189",
        "name": "Shanonan Wang"
      },
      {
        "authorId": "2249962257",
        "name": "Yinxv Pan"
      },
      {
        "authorId": "2233089123",
        "name": "Jiao Xue"
      },
      {
        "authorId": "2144118403",
        "name": "Dahai Li"
      },
      {
        "authorId": "2141313179",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2242734235",
        "name": "Hai-Tao Zheng"
      },
      {
        "authorId": "1753344",
        "name": "Maosong Sun"
      }
    ]
  },
  "261101015": {
    "paperId": "fc6a2f7478f68adefd69e2071f27e38aa1647f2f",
    "externalIds": {
      "ArXiv": "2308.12966",
      "CorpusId": 261101015
    },
    "publicationVenue": null,
    "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
    "abstract": "In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",
    "venue": "",
    "year": 2023,
    "referenceCount": 86,
    "citationCount": 409,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "41211611",
        "name": "Jinze Bai"
      },
      {
        "authorId": "3768186",
        "name": "Shuai Bai"
      },
      {
        "authorId": null,
        "name": "Shusheng Yang"
      },
      {
        "authorId": "2217429986",
        "name": "Shijie Wang"
      },
      {
        "authorId": "2110171536",
        "name": "Sinan Tan"
      },
      {
        "authorId": "2155302144",
        "name": "Peng Wang"
      },
      {
        "authorId": "35996608",
        "name": "Junyang Lin"
      },
      {
        "authorId": "2192678144",
        "name": "Chang Zhou"
      },
      {
        "authorId": "1709595",
        "name": "Jingren Zhou"
      }
    ]
  },
  "259262263": {
    "paperId": "3b6179c293df29e31d31cea46476f104ab6950f2",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-14824",
      "ArXiv": "2306.14824",
      "DOI": "10.48550/arXiv.2306.14824",
      "CorpusId": 259262263
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
    "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 68,
    "citationCount": 496,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2087004998",
        "name": "Zhiliang Peng"
      },
      {
        "authorId": "51456429",
        "name": "Wenhui Wang"
      },
      {
        "authorId": "145307652",
        "name": "Li Dong"
      },
      {
        "authorId": "34128716",
        "name": "Y. Hao"
      },
      {
        "authorId": "3110003",
        "name": "Shaohan Huang"
      },
      {
        "authorId": "2118866998",
        "name": "Shuming Ma"
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei"
      }
    ]
  },
  "258291930": {
    "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b",
    "externalIds": {
      "ArXiv": "2304.10592",
      "DBLP": "conf/iclr/Zhu0SLE24",
      "DOI": "10.48550/arXiv.2304.10592",
      "CorpusId": 258291930
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
    "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 62,
    "citationCount": 1330,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1388731230",
        "name": "Deyao Zhu"
      },
      {
        "authorId": "2153417252",
        "name": "Jun Chen"
      },
      {
        "authorId": "2151708219",
        "name": "Xiaoqian Shen"
      },
      {
        "authorId": "2144440192",
        "name": "Xiang Li"
      },
      {
        "authorId": "1712479",
        "name": "Mohamed Elhoseiny"
      }
    ]
  },
  "259251834": {
    "paperId": "c7a7104df3db13737a865ede2be8146990fa4026",
    "externalIds": {
      "ArXiv": "2306.14565",
      "DBLP": "conf/iclr/LiuLLWYW24",
      "CorpusId": 259251834
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
    "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public datasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data are available at https://github.com/FuxiaoLiu/LRV-Instruction.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 49,
    "citationCount": 146,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "52220309",
        "name": "Fuxiao Liu"
      },
      {
        "authorId": "143786724",
        "name": "Kevin Lin"
      },
      {
        "authorId": "50703697",
        "name": "Linjie Li"
      },
      {
        "authorId": "2124948371",
        "name": "Jianfeng Wang"
      },
      {
        "authorId": "1964574",
        "name": "Y. Yacoob"
      },
      {
        "authorId": "29957038",
        "name": "Lijuan Wang"
      }
    ]
  },
  "258418343": {
    "paperId": "570079bbdd8758dfe865097e05719313c9c1301a",
    "externalIds": {
      "ArXiv": "2304.15010",
      "DBLP": "journals/corr/abs-2304-15010",
      "DOI": "10.48550/arXiv.2304.15010",
      "CorpusId": 258418343
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
    "abstract": "How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMA-Adapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instruction-following ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This strategy effectively alleviates the interference between the two tasks of image-text alignment and instruction following and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset. During inference, we incorporate additional expert models (e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image understanding capability without incurring training costs. Compared to the original LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal instructions by merely introducing 14M parameters over LLaMA. The newly designed framework also exhibits stronger language-only instruction-following capabilities and even excels in chat interactions. Our code and models are available at https://github.com/ZrrSkywalker/LLaMA-Adapter.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 79,
    "citationCount": 457,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144740494",
        "name": "Peng Gao"
      },
      {
        "authorId": "150147382",
        "name": "Jiaming Han"
      },
      {
        "authorId": "2115713503",
        "name": "Renrui Zhang"
      },
      {
        "authorId": "2112305433",
        "name": "Ziyi Lin"
      },
      {
        "authorId": "1947101",
        "name": "Shijie Geng"
      },
      {
        "authorId": "9548994",
        "name": "Aojun Zhou"
      },
      {
        "authorId": "143715293",
        "name": "W. Zhang"
      },
      {
        "authorId": "2887562",
        "name": "Pan Lu"
      },
      {
        "authorId": "3486481",
        "name": "Conghui He"
      },
      {
        "authorId": "27577617",
        "name": "Xiangyu Yue"
      },
      {
        "authorId": "47893312",
        "name": "Hongsheng Li"
      },
      {
        "authorId": "2059129841",
        "name": "Y. Qiao"
      }
    ]
  },
  "259501644": {
    "paperId": "451a3f03aca4aa87b93981364842137417549e58",
    "externalIds": {
      "ArXiv": "2307.04087",
      "DBLP": "journals/corr/abs-2307-04087",
      "DOI": "10.48550/arXiv.2307.04087",
      "CorpusId": 259501644
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "SVIT: Scaling up Visual Instruction Tuning",
    "abstract": "Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 55,
    "citationCount": 93,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "143946810",
        "name": "Bo Zhao"
      },
      {
        "authorId": "1773326",
        "name": "Boya Wu"
      },
      {
        "authorId": "34097174",
        "name": "Tiejun Huang"
      }
    ]
  },
  "258352455": {
    "paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3",
    "externalIds": {
      "DBLP": "journals/corr/abs-2304-14178",
      "ArXiv": "2304.14178",
      "DOI": "10.48550/arXiv.2304.14178",
      "CorpusId": 258352455
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
    "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 36,
    "citationCount": 713,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2199011713",
        "name": "Qinghao Ye"
      },
      {
        "authorId": "153194420",
        "name": "Haiyang Xu"
      },
      {
        "authorId": "2115723816",
        "name": "Guohai Xu"
      },
      {
        "authorId": "2153258288",
        "name": "Jiabo Ye"
      },
      {
        "authorId": "2047087220",
        "name": "Ming Yan"
      },
      {
        "authorId": "2118764703",
        "name": "Yi Zhou"
      },
      {
        "authorId": "2110125710",
        "name": "Junyan Wang"
      },
      {
        "authorId": "120897486",
        "name": "Anwen Hu"
      },
      {
        "authorId": "2055357477",
        "name": "Pengcheng Shi"
      },
      {
        "authorId": "37198550",
        "name": "Yaya Shi"
      },
      {
        "authorId": "143971529",
        "name": "Chenliang Li"
      },
      {
        "authorId": "2110355824",
        "name": "Yuanhong Xu"
      },
      {
        "authorId": "123655156",
        "name": "Hehong Chen"
      },
      {
        "authorId": "2122989639",
        "name": "Junfeng Tian"
      },
      {
        "authorId": "50480206",
        "name": "Qiang Qi"
      },
      {
        "authorId": "2116921824",
        "name": "Ji Zhang"
      },
      {
        "authorId": "2194508991",
        "name": "Feiyan Huang"
      }
    ]
  },
  "256504063": {
    "paperId": "780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050",
    "externalIds": {
      "DBLP": "journals/corr/abs-2302-00923",
      "ArXiv": "2302.00923",
      "DOI": "10.48550/arXiv.2302.00923",
      "CorpusId": 256504063
    },
    "publicationVenue": null,
    "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
    "abstract": "Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2023,
    "referenceCount": 63,
    "citationCount": 284,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3322871",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "2085709",
        "name": "Aston Zhang"
      },
      {
        "authorId": "1701799",
        "name": "Mu Li"
      },
      {
        "authorId": "2146232510",
        "name": "Hai Zhao"
      },
      {
        "authorId": "50877490",
        "name": "G. Karypis"
      },
      {
        "authorId": "78088877",
        "name": "Alexander J. Smola"
      }
    ]
  },
  "261823391": {
    "paperId": "3803d1f291e162bdaa4678a2c5a2bbcf63c050f4",
    "externalIds": {
      "DBLP": "conf/iclr/ZhaoCSMA0LWHC24",
      "ArXiv": "2309.07915",
      "DOI": "10.48550/arXiv.2309.07915",
      "CorpusId": 261823391
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning",
    "abstract": "Since the resurgence of deep learning, vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks. In this paper, we address the limitation above by 1) introducing vision-language Model with Multi-Modal In-Context Learning(MMICL), a new approach to allow the VLM to deal with multi-modal inputs efficiently; 2) proposing a novel context scheme to augment the in-context learning ability of the VLM; 3) constructing the Multi-modal In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to understand complex multi-modal prompts. Our experiments confirm that MMICL achieves new state-of-the-art zero-shot performance on a wide range of general vision-language tasks, especially for complex benchmarks, including MME and MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge of complex multi-modal prompt understanding and emerges the impressive ICL ability. Furthermore, we observe that MMICL successfully alleviates language bias in VLMs, a common issue for VLMs that often leads to hallucination when faced with extensive textual context. Our code, dataset, dataset tool, and model are available at https://github.com/PKUnlp-icler/MIC",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 140,
    "citationCount": 107,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2112675144",
        "name": "Haozhe Zhao"
      },
      {
        "authorId": "2117632647",
        "name": "Zefan Cai"
      },
      {
        "authorId": "2053739525",
        "name": "Shuzheng Si"
      },
      {
        "authorId": "2241105586",
        "name": "Xiaojian Ma"
      },
      {
        "authorId": "2240549884",
        "name": "Kaikai An"
      },
      {
        "authorId": "2146034504",
        "name": "Liang Chen"
      },
      {
        "authorId": "46271003",
        "name": "Zixuan Liu"
      },
      {
        "authorId": "47673176",
        "name": "Sheng Wang"
      },
      {
        "authorId": "2219388940",
        "name": "Wenjuan Han"
      },
      {
        "authorId": "39488576",
        "name": "Baobao Chang"
      }
    ]
  },
  "258212542": {
    "paperId": "170c97c7215f42edfb20c2248f954879e91ef86e",
    "externalIds": {
      "ArXiv": "2304.09842",
      "DBLP": "conf/nips/LuPCGCWZG23",
      "DOI": "10.48550/arXiv.2304.09842",
      "CorpusId": 258212542
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
    "abstract": "Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at https://chameleon-llm.github.io.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 71,
    "citationCount": 223,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2887562",
        "name": "Pan Lu"
      },
      {
        "authorId": "1780690",
        "name": "Baolin Peng"
      },
      {
        "authorId": "47413820",
        "name": "Hao Cheng"
      },
      {
        "authorId": "1947267",
        "name": "Michel Galley"
      },
      {
        "authorId": "2782886",
        "name": "Kai-Wei Chang"
      },
      {
        "authorId": "39092098",
        "name": "Y. Wu"
      },
      {
        "authorId": "145380991",
        "name": "Song-Chun Zhu"
      },
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      }
    ]
  },
  "262053313": {
    "paperId": "6ab33b17cd45e7cbd2cb9b0c5a2d56e5eac1c814",
    "externalIds": {
      "ArXiv": "2309.11436",
      "DBLP": "journals/corr/abs-2309-11436",
      "DOI": "10.48550/arXiv.2309.11436",
      "CorpusId": 262053313
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "You Only Look at Screens: Multimodal Chain-of-Action Agents",
    "abstract": "Autonomous graphical user interface (GUI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, most existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-GUI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -- leveraging a series of intermediate previous action histories and future action plans -- to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30$K$ unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-GUI achieves state-of-the-art performance with an action type prediction accuracy of 90\\% and an overall action success rate of 74\\%. Code is publicly available at https://github.com/cooelf/Auto-GUI.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 59,
    "citationCount": 57,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3322871",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "2244790002",
        "name": "Aston Zhang"
      }
    ]
  },
  "235458009": {
    "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
    "externalIds": {
      "DBLP": "conf/iclr/HuSWALWWC22",
      "ArXiv": "2106.09685",
      "CorpusId": 235458009
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "referenceCount": 65,
    "citationCount": 6448,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2157840220",
        "name": "J. E. Hu"
      },
      {
        "authorId": "1752875",
        "name": "Yelong Shen"
      },
      {
        "authorId": "104100507",
        "name": "Phillip Wallis"
      },
      {
        "authorId": "1388725932",
        "name": "Zeyuan Allen-Zhu"
      },
      {
        "authorId": "2110486765",
        "name": "Yuanzhi Li"
      },
      {
        "authorId": "2135571585",
        "name": "Shean Wang"
      },
      {
        "authorId": "2109136147",
        "name": "Weizhu Chen"
      }
    ]
  },
  "258841328": {
    "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
    "externalIds": {
      "ArXiv": "2305.14314",
      "DBLP": "conf/nips/DettmersPHZ23",
      "DOI": "10.48550/arXiv.2305.14314",
      "CorpusId": 258841328
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 72,
    "citationCount": 1591,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3239480",
        "name": "Tim Dettmers"
      },
      {
        "authorId": "51152502",
        "name": "Artidoro Pagnoni"
      },
      {
        "authorId": "14487640",
        "name": "Ari Holtzman"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ]
  },
  "258865326": {
    "paperId": "9c3a9b4821daa03cb5369041d59d2714329a3811",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-15023",
      "ArXiv": "2305.15023",
      "DOI": "10.48550/arXiv.2305.15023",
      "CorpusId": 258865326
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models",
    "abstract": "Recently, growing interest has been aroused in extending the multimodal capability of large language models (LLMs), e.g., vision-language (VL) learning, which is regarded as the next milestone of artificial general intelligence. However, existing solutions are prohibitively expensive, which not only need to optimize excessive parameters, but also require another large-scale pre-training before VL instruction tuning. In this paper, we propose a novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA). Instead of using large neural networks to connect the image encoder and LLM, MMA adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models. Meanwhile, MMA is also equipped with a routing algorithm to help LLMs achieve an automatic shift between single- and multi-modal instructions without compromising their ability of natural language understanding. To validate MMA, we apply it to a recent LLM called LLaMA and term this formed large vision-language instructed model as LaVIN. To validate MMA and LaVIN, we conduct extensive experiments under two setups, namely multimodal science question answering and multimodal dialogue. The experimental results not only demonstrate the competitive performance and the superior training efficiency of LaVIN than existing multimodal LLMs, but also confirm its great potential as a general-purpose chatbot. More importantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4 training hours with 3.8M trainable parameters, greatly confirming the effectiveness of MMA. Our project is released at https://luogen1996.github.io/lavin.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 53,
    "citationCount": 66,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2056100172",
        "name": "Gen Luo"
      },
      {
        "authorId": "2110191063",
        "name": "Yiyi Zhou"
      },
      {
        "authorId": "2143150727",
        "name": "Tianhe Ren"
      },
      {
        "authorId": "2118529267",
        "name": "Shen Chen"
      },
      {
        "authorId": "1759841",
        "name": "Xiaoshuai Sun"
      },
      {
        "authorId": "1572139630",
        "name": "Rongrong Ji"
      }
    ]
  },
  "258436945": {
    "paperId": "0046306876ff2d5600699327e52bc29fa5e9ec91",
    "externalIds": {
      "ArXiv": "2305.01278",
      "DBLP": "conf/nips/Zhang0Y00LC23",
      "DOI": "10.48550/arXiv.2305.01278",
      "CorpusId": 258436945
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Transfer Visual Prompt Generator across LLMs",
    "abstract": "While developing a new multimodal LLM (MLLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG part of the MLLM still suffers from indispensable computational costs, i.e., requiring thousands of GPU hours and millions of training data. One alternative solution is to transfer an existing VPG from any existing MLLMs for the target MLLM. In this work, we for the first time investigate the VPG transferability across LLMs, and explore a solution to reduce the cost of VPG transfer. We first study the VPG transfer across different LLM sizes (e.g., small-to-large), and across different LLM types, through which we diagnose the key factors to maximize the transfer efficiency. Based on our observation, we design a two-stage transfer framework named VPGTrans, which is simple yet highly effective. Through extensive experiments, we demonstrate that VPGTrans helps significantly speed up the transfer learning process without compromising performance. Remarkably, it helps achieve the VPG transfer from BLIP-2 OPT$_\\text{2.7B}$ to BLIP-2 OPT$_\\text{6.7B}$ with over 10 times speed-up and 10.7% training data compared with connecting a VPG to OPT$_\\text{6.7B}$ from scratch. Further, a series of intriguing findings and potential rationales behind them are provided and discussed. Finally, we showcase the practical value of our VPGTrans approach, by customizing two novel MLLMs, including VL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 61,
    "citationCount": 78,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2153656874",
        "name": "Ao Zhang"
      },
      {
        "authorId": "46959445",
        "name": "Hao Fei"
      },
      {
        "authorId": "1390925224",
        "name": "Yuan Yao"
      },
      {
        "authorId": "2072613978",
        "name": "Wei Ji"
      },
      {
        "authorId": "2156060734",
        "name": "Li Li"
      },
      {
        "authorId": null,
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "143779329",
        "name": "Tat-seng Chua"
      }
    ]
  },
  "221191589": {
    "paperId": "22c39a725b020a57e4c152333ea702a342eee46c",
    "externalIds": {
      "MAG": "3080685974",
      "DBLP": "conf/kdd/0001S20",
      "DOI": "10.1145/3394486.3406465",
      "CorpusId": 221191589
    },
    "publicationVenue": {
      "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
      "name": "Knowledge Discovery and Data Mining",
      "type": "conference",
      "alternate_names": [
        "KDD",
        "Knowl Discov Data Min"
      ],
      "url": "http://www.acm.org/sigkdd/"
    },
    "title": "Scientific Text Mining and Knowledge Graphs",
    "abstract": "Unstructured scientific text, in various forms of textual artifacts, including manuscripts, publications, patents, and proposals, is used to store the tremendous wealth of knowledge discovered after weeks, months, and years, developing hypotheses, working in the lab or clinic, and analyzing results. A grand challenge on data mining research is to develop effective methods for transforming the scientific text into well-structured forms (e.g., ontology, taxonomy, knowledge graphs), so that machine intelligent systems can build on them for hypothesis generation and validation. In this tutorial, we provide a comprehensive overview on recent research and development in this direction. First, we introduce a series of text mining methods that extract phrases, entities, scientific concepts, relations, claims, and experimental evidence. Then we discuss methods that construct and learn from scientific knowledge graphs for accurate search, document classification, and exploratory analysis. Specifically, we focus on scalable, effective, weakly supervised methods that work on text in sciences (e.g., chemistry, biology).",
    "venue": "Knowledge Discovery and Data Mining",
    "year": 2020,
    "referenceCount": 33,
    "citationCount": 4,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144812586",
        "name": "Meng Jiang"
      },
      {
        "authorId": "2884976",
        "name": "Jingbo Shang"
      }
    ]
  },
  "250390715": {
    "paperId": "8255ab7a62c31c9a426ecad73143883d47f33f4e",
    "externalIds": {
      "ACL": "2022.naacl-tutorials.3",
      "DOI": "10.18653/v1/2022.naacl-tutorials.3",
      "CorpusId": 250390715
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "New Frontiers of Information Extraction",
    "abstract": "This tutorial targets researchers and practitioners who are interested in AI and ML technologies for structural information extraction (IE) from unstructured textual sources. Particularly, this tutorial will provide audience with a systematic introduction to recent advances of IE, by answering several important research questions. These questions include (i) how to develop an robust IE system from noisy, insufficient training data, while ensuring the reliability of its prediction? (ii) how to foster the generalizability of IE through enhancing the system\u2019s cross-lingual, cross-domain, cross-task and cross-modal transferability? (iii) how to precisely support extracting structural information with extremely fine-grained, diverse and boundless labels? (iv) how to further improve IE by leveraging indirect supervision from other NLP tasks, such as NLI, QA or summarization, and pre-trained language models? (v) how to acquire knowledge to guide the inference of IE systems? We will discuss several lines of frontier research that tackle those challenges, and will conclude the tutorial by outlining directions for further investigation.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 106,
    "citationCount": 10,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      },
      {
        "authorId": "34170717",
        "name": "Lifu Huang"
      },
      {
        "authorId": "2118482058",
        "name": "Manling Li"
      },
      {
        "authorId": "2108536188",
        "name": "Ben Zhou"
      },
      {
        "authorId": "2113323573",
        "name": "Heng Ji"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "263866951": {
    "paperId": "277dd00ab02f122133bf56b485dfb7c730acdcde",
    "externalIds": {
      "DBLP": "conf/acm/AsaiMZC23",
      "ACL": "2023.acl-tutorials.6",
      "DOI": "10.18653/v1/2023.acl-tutorials.6",
      "CorpusId": 263866951
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Retrieval-based Language Models and Applications",
    "abstract": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 39,
    "citationCount": 60,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2290402940",
        "name": "Akari Asai"
      },
      {
        "authorId": "48872685",
        "name": "Sewon Min"
      },
      {
        "authorId": "49164966",
        "name": "Zexuan Zhong"
      },
      {
        "authorId": "2286629648",
        "name": "Danqi Chen"
      }
    ]
  },
  "236456742": {
    "paperId": "f2cbbbbbca2a8b9636eca890dc1d14a9ac50b7a0",
    "externalIds": {
      "DBLP": "journals/aim/Gil21",
      "DOI": "10.1609/aimag.v42i4.18149",
      "CorpusId": 236456742
    },
    "publicationVenue": {
      "id": "6fedff74-7525-4b7f-bbb4-4df4e23948e4",
      "name": "The AI Magazine",
      "type": "journal",
      "alternate_names": [
        "AI Mag",
        "Ai Mag",
        "Ai Magazine"
      ],
      "issn": "0738-4602",
      "url": "https://www.aaai.org/Library/Magazine/magazine-library.php",
      "alternate_urls": [
        "https://www.aaai.org/ojs/index.php/aimagazine/",
        "https://www.aaai.org/Magazine/magazine.php"
      ]
    },
    "title": "Will AI Write Scientific Papers in the Future?",
    "abstract": "In this presidential address, I would like to start with a personal reflection on the field and then share with you the research directions I am pursuing and my excitement about the future of AI. In my personal research to advance AI while advancing scientific discoveries, one question that I have been pondering for some years now is whether AI will write scientific papers in the future. I want to reflect on this question, and look back at the many accomplishments in our field that can make us very hopeful that the answer will be yes, and that it may happen sooner than we might expect.",
    "venue": "The AI Magazine",
    "year": 2022,
    "referenceCount": 26,
    "citationCount": 18,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46701545",
        "name": "Y. Gil"
      }
    ]
  },
  "231740610": {
    "paperId": "cefd3993db4d065b95ab8f105452fb728c02b60e",
    "externalIds": {
      "DBLP": "journals/jair/YuanLN22",
      "ArXiv": "2102.00176",
      "DOI": "10.1613/jair.1.12862",
      "CorpusId": 231740610
    },
    "publicationVenue": {
      "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
      "name": "Journal of Artificial Intelligence Research",
      "type": "journal",
      "alternate_names": [
        "JAIR",
        "J Artif Intell Res",
        "The Journal of Artificial Intelligence Research"
      ],
      "issn": "1076-9757",
      "url": "http://www.jair.org/"
    },
    "title": "Can We Automate Scientific Reviewing?",
    "abstract": "The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question \u201ccan we automate scientific reviewing? \u201d, discussing the possibility of using natural language processing (NLP) models to generate peer reviews for scientific papers. Because it is non-trivial to define what a \u201cgood\u201d review is in the first place, we first discuss possible evaluation metrics that could be used to judge success in this task. We then focus on the machine learning domain and collect a dataset of papers in the domain, annotate them with different aspects of content covered in each review, and train targeted summarization models that take in papers as input and generate reviews as output. Comprehensive experimental results on the test set show that while system-generated reviews are comprehensive, touching upon more aspects of the paper than human-written reviews, the generated texts are less constructive and less factual than human-written reviews for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. Given these results, we pose eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research in this direction.\nWe make relevant resource publicly available for use by future research: https://github. com/neulab/ReviewAdvisor. In addition, while our conclusion is that the technology is not yet ready for use in high-stakes review settings we provide a system demo, ReviewAdvisor (http://review.nlpedia.ai/), showing the current capabilities and failings of state-of-the-art NLP models at this task (see demo screenshot in A.2). A review of this paper written by the system proposed in this paper can be found in A.1.",
    "venue": "Journal of Artificial Intelligence Research",
    "year": 2021,
    "referenceCount": 85,
    "citationCount": 71,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "30300197",
        "name": "Weizhe Yuan"
      },
      {
        "authorId": "144118452",
        "name": "Pengfei Liu"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  "258361324": {
    "paperId": "aa92dc559b8845bf134f3bfad4fc188615453dfb",
    "externalIds": {
      "DOI": "10.1038/s42254-023-00581-4",
      "CorpusId": 258361324
    },
    "publicationVenue": {
      "id": "3639d55b-36ef-4fa6-97fd-1bdc155f9081",
      "name": "Nature Reviews Physics",
      "alternate_names": [
        "Nat Rev Phys"
      ],
      "issn": "2522-5820",
      "url": "https://www.nature.com/natrevphys/"
    },
    "title": "Science in the age of large language models",
    "abstract": null,
    "venue": "Nature Reviews Physics",
    "year": 2023,
    "referenceCount": 17,
    "citationCount": 131,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Physics",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8318698",
        "name": "Abeba Birhane"
      },
      {
        "authorId": "51880633",
        "name": "Atoosa Kasirzadeh"
      },
      {
        "authorId": "145664726",
        "name": "David Leslie"
      },
      {
        "authorId": "12806133",
        "name": "Sandra Wachter"
      }
    ]
  },
  "257463753": {
    "paperId": "da9683e826c37a6383c124b5c6cddefcb35ee8fd",
    "externalIds": {
      "DBLP": "journals/corr/abs-2303-13367",
      "ArXiv": "2303.13367",
      "DOI": "10.1002/asi.24750",
      "CorpusId": 257463753
    },
    "publicationVenue": null,
    "title": "ChatGPT and a new academic reality: Artificial Intelligence\u2010written research papers and the ethics of the large language models in scholarly publishing",
    "abstract": "This article discusses OpenAI's ChatGPT, a generative pre\u2010trained transformer, which uses natural language processing to fulfill text\u2010based user requests (i.e., a \u201cchatbot\u201d). The history and principles behind ChatGPT and similar models are discussed. This technology is then discussed in relation to its potential impact on academia and scholarly research and publishing. ChatGPT is seen as a potential model for the automated preparation of essays and other types of scholarly manuscripts. Potential ethical issues that could arise with the emergence of large language models like GPT\u20103, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.",
    "venue": "J. Assoc. Inf. Sci. Technol.",
    "year": 2023,
    "referenceCount": 107,
    "citationCount": 358,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2000639769",
        "name": "Brady D. Lund"
      },
      {
        "authorId": "2155389734",
        "name": "Ting Wang"
      },
      {
        "authorId": "2212887866",
        "name": "Nishith Reddy Mannuru"
      },
      {
        "authorId": "2058824340",
        "name": "Bing Nie"
      },
      {
        "authorId": "98041740",
        "name": "S. Shimray"
      },
      {
        "authorId": "2141037440",
        "name": "Ziang Wang"
      }
    ]
  },
  "245769589": {
    "paperId": "8dc1e4bac2d0403ba4bec7bcb8abb7534c53ab1f",
    "externalIds": {
      "DBLP": "journals/corr/abs-2201-01880",
      "ArXiv": "2201.01880",
      "CorpusId": 245769589
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Automatic Related Work Generation: A Meta Study",
    "abstract": "Academic research is an exploration activity to solve problems that have never been resolved before. By this nature, each academic research work is required to perform a literature review to distinguish its novelties that have not been addressed by prior works. In natural language processing, this literature review is usually conducted under the\"Related Work\"section. The task of automatic related work generation aims to automatically generate the\"Related Work\"section given the rest of the research paper and a list of cited papers. Although this task was proposed over 10 years ago, it received little attention until very recently, when it was cast as a variant of the scientific multi-document summarization problem. However, even today, the problems of automatic related work and citation text generation are not yet standardized. In this survey, we conduct a meta-study to compare the existing literature on related work generation from the perspectives of problem formulation, dataset collection, methodological approach, performance evaluation, and future prospects to provide the reader insight into the progress of the state-of-the-art studies, as well as and how future studies can be conducted. We also survey relevant fields of study that we suggest future work to consider integrating.",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 124,
    "citationCount": 9,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "89919188",
        "name": "Xiangci Li"
      },
      {
        "authorId": "2112778",
        "name": "Jessica Ouyang"
      }
    ]
  },
  "258947504": {
    "paperId": "0133c1128f2036ecb6b65ab15c562b71bf4f18a0",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-16859",
      "ArXiv": "2305.16859",
      "DOI": "10.48550/arXiv.2305.16859",
      "CorpusId": 258947504
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Scientific Fact-Checking: A Survey of Resources and Approaches",
    "abstract": "The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant attention due to the growing importance of scientific and health discussions on online platforms. Automated scientific fact-checking methods based on NLP can help combat the spread of misinformation, assist researchers in knowledge discovery, and help individuals understand new scientific breakthroughs. In this paper, we present a comprehensive survey of existing research in this emerging field and its related tasks. We provide a task description, discuss the construction process of existing datasets, and analyze proposed models and approaches. Based on our findings, we identify intriguing challenges and outline potential future directions to advance the field.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 92,
    "citationCount": 34,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2066962303",
        "name": "Juraj Vladika"
      },
      {
        "authorId": "2522197",
        "name": "F. Matthes"
      }
    ]
  },
  "248512482": {
    "paperId": "6c5c6f883604a3abaa829b83d2958de8c343beeb",
    "externalIds": {
      "ArXiv": "2205.02007",
      "DBLP": "journals/corr/abs-2205-02007",
      "DOI": "10.1145/3576896",
      "CorpusId": 248512482
    },
    "publicationVenue": {
      "id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
      "name": "Communications of the ACM",
      "type": "journal",
      "alternate_names": [
        "Commun ACM",
        "Communications of The ACM"
      ],
      "issn": "0001-0782",
      "url": "http://www.acm.org/pubs/cacm/",
      "alternate_urls": [
        "http://portal.acm.org/cacm",
        "http://www.acm.org/pubs/contents/journals/cacm/",
        "https://cacm.acm.org/"
      ]
    },
    "title": "A Computational Inflection for Scientific Discovery",
    "abstract": "Enabling researchers to leverage systems to overcome the limits of human cognitive capacity.",
    "venue": "Communications of the ACM",
    "year": 2022,
    "referenceCount": 73,
    "citationCount": 25,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2041698667",
        "name": "Tom Hope"
      },
      {
        "authorId": "145612610",
        "name": "Doug Downey"
      },
      {
        "authorId": "1741101",
        "name": "Oren Etzioni"
      },
      {
        "authorId": "1780531",
        "name": "Daniel S. Weld"
      },
      {
        "authorId": "145479841",
        "name": "E. Horvitz"
      }
    ]
  },
  "52118895": {
    "paperId": "b21b927c251c415b601b6d7f785a42cc5c292635",
    "externalIds": {
      "DBLP": "journals/corr/abs-1808-09602",
      "ArXiv": "1808.09602",
      "MAG": "2950620630",
      "ACL": "D18-1360",
      "DOI": "10.18653/v1/D18-1360",
      "CorpusId": 52118895
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",
    "abstract": "We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 52,
    "citationCount": 623,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145081697",
        "name": "Yi Luan"
      },
      {
        "authorId": "2265599",
        "name": "Luheng He"
      },
      {
        "authorId": "144339506",
        "name": "Mari Ostendorf"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ]
  },
  "218470122": {
    "paperId": "e99a259299d4d555ee4c354f2095ab4401369c82",
    "externalIds": {
      "DBLP": "journals/corr/abs-2005-00512",
      "ACL": "2020.acl-main.670",
      "MAG": "3035372073",
      "ArXiv": "2005.00512",
      "DOI": "10.18653/v1/2020.acl-main.670",
      "CorpusId": 218470122
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "SciREX: A Challenge Dataset for Document-Level Information Extraction",
    "abstract": "Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 30,
    "citationCount": 145,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49837811",
        "name": "Sarthak Jain"
      },
      {
        "authorId": "15292561",
        "name": "Madeleine van Zuylen"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      },
      {
        "authorId": "46181066",
        "name": "Iz Beltagy"
      }
    ]
  },
  "245704273": {
    "paperId": "f95620883ce631dcca296d6301ab094555a9b1c4",
    "externalIds": {
      "ACL": "2022.tacl-1.22",
      "ArXiv": "2106.00676",
      "DBLP": "journals/tacl/ShenLWKWD22",
      "DOI": "10.1162/tacl_a_00466",
      "CorpusId": 245704273
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "VILA: Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups",
    "abstract": "Accurately extracting structured content from PDFs is a critical first step for NLP over scientific papers. Recent work has improved extraction accuracy by incorporating elementary layout information, for example, each token\u2019s 2D position on the page, into language model pretraining. We introduce new methods that explicitly model VIsual LAyout (VILA) groups, that is, text lines or text blocks, to further improve performance. In our I-VILA approach, we show that simply inserting special tokens denoting layout group boundaries into model inputs can lead to a 1.9% Macro F1 improvement in token classification. In the H-VILA approach, we show that hierarchical encoding of layout-groups can result in up to 47% inference time reduction with less than 0.8% Macro F1 loss. Unlike prior layout-aware approaches, our methods do not require expensive additional pretraining, only fine-tuning, which we show can reduce training cost by up to 95%. Experiments are conducted on a newly curated evaluation suite, S2-VLUE, that unifies existing automatically labeled datasets and includes a new dataset of manual annotations covering diverse papers from 19 scientific disciplines. Pre-trained weights, benchmark datasets, and source code are available at https://github.com/allenai/VILA.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 44,
    "citationCount": 33,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "101568984",
        "name": "Zejiang Shen"
      },
      {
        "authorId": "46258841",
        "name": "Kyle Lo"
      },
      {
        "authorId": "31860505",
        "name": "Lucy Lu Wang"
      },
      {
        "authorId": "2003338023",
        "name": "Bailey Kuehl"
      },
      {
        "authorId": "1780531",
        "name": "Daniel S. Weld"
      },
      {
        "authorId": "145612610",
        "name": "Doug Downey"
      }
    ]
  },
  "258685532": {
    "paperId": "049288e68caeadf7842df6977e140b47a8a2f89d",
    "externalIds": {
      "ArXiv": "2305.08264",
      "ACL": "2023.acl-long.201",
      "DBLP": "journals/corr/abs-2305-08264",
      "DOI": "10.48550/arXiv.2305.08264",
      "CorpusId": 258685532
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling",
    "abstract": "We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP tasks like named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various BERT-based models pretrained on different scientific text corpora on MatSci-NLP to understand the impact of pretraining strategies on understanding materials science text. Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks.Our experiments in this low-resource training setting show that language models pretrained on scientific text outperform BERT trained on general text. MatBERT, a model pretrained specifically on materials science journals, generally performs best for most tasks. Moreover, we propose a unified text-to-schema for multitask learning on {pasted macro \u2018BENCHMARK\u2019} and compare its performance with traditional fine-tuning methods. In our analysis of different training methods, we find that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask NLP fine-tuning methods. The code and datasets are publicly available https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 56,
    "citationCount": 20,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Materials Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2152602955",
        "name": "Yurun Song"
      },
      {
        "authorId": "51895312",
        "name": "Santiago Miret"
      },
      {
        "authorId": "2116441692",
        "name": "Bang Liu"
      }
    ]
  },
  "220056972": {
    "paperId": "824636e935807dab178a30622383647686f98085",
    "externalIds": {
      "DBLP": "conf/acl/WangGLCJLLSCPH20",
      "MAG": "3038057640",
      "ACL": "2020.acl-demos.8",
      "DOI": "10.18653/v1/2020.acl-demos.8",
      "CorpusId": 220056972
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "EVIDENCEMINER: Textual Evidence Discovery for Life Sciences",
    "abstract": "Traditional search engines for life sciences (e.g., PubMed) are designed for document retrieval and do not allow direct retrieval of specific statements. Some of these statements may serve as textual evidence that is key to tasks such as hypothesis generation and new finding validation. We present EVIDENCEMINER, a web-based system that lets users query a natural language statement and automatically retrieves textual evidence from a background corpora for life sciences. EVIDENCEMINER is constructed in a completely automated way without any human effort for training data annotation. It is supported by novel data-driven methods for distantly supervised named entity recognition and open information extraction. The entities and patterns are pre-computed and indexed offline to support fast online evidence retrieval. The annotation results are also highlighted in the original document for better visualization. EVIDENCEMINER also includes analytic functionalities such as the most frequent entity and relation summarization. EVIDENCEMINER can help scientists uncover important research issues, leading to more effective research and more in-depth quantitative analysis. The system of EVIDENCEMINER is available at https://evidenceminer.firebaseapp.com/.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 24,
    "citationCount": 13,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Biology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2154990549",
        "name": "Xuan Wang"
      },
      {
        "authorId": "2069571239",
        "name": "Yingjun Guan"
      },
      {
        "authorId": "2109300810",
        "name": "Weili Liu"
      },
      {
        "authorId": "72446317",
        "name": "Aabhas Chauhan"
      },
      {
        "authorId": "1488691379",
        "name": "Enyi Jiang"
      },
      {
        "authorId": "37696683",
        "name": "Qi Li"
      },
      {
        "authorId": "72861332",
        "name": "D. Liem"
      },
      {
        "authorId": "41130227",
        "name": "Dibakar Sigdel"
      },
      {
        "authorId": "145710797",
        "name": "J. Caufield"
      },
      {
        "authorId": "3023770",
        "name": "P. Ping"
      },
      {
        "authorId": "153034701",
        "name": "Jiawei Han"
      }
    ]
  },
  "250286985": {
    "paperId": "24a95ff7a4f37d7d05f30e60dae40a576f49eeda",
    "externalIds": {
      "DBLP": "conf/aaai/Yuan022",
      "DOI": "10.1609/aaai.v36i10.21418",
      "CorpusId": 250286985
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "KID-Review: Knowledge-Guided Scientific Review Generation with Oracle Pre-training",
    "abstract": "The surge in the number of scientific submissions has brought challenges to the work of peer review. In this paper, as a first step, we explore the possibility of designing an automated system, which is not meant to replace humans, but rather providing a first-pass draft for a machine-assisted human review process. Specifically, we present an end-to-end knowledge-guided review generation framework for scientific papers grounded in cognitive psychology research that a better understanding of text requires different types of knowledge. In practice, we found that this seemingly intuitive idea suffered from training difficulties. In order to solve this problem, we put forward an oracle pre-training strategy, which can not only make the Kid-Review better educated but also make the generated review cover more aspects. Experimentally, we perform a comprehensive evaluation (human and automatic) from different perspectives. Empirical results have shown the effectiveness of different types of knowledge as well as oracle pre-training. We make all code, relevant dataset available: https://github.com/Anonymous4nlp233/KIDReview as well as the Kid-Review system: http://nlpeer.reviews.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2022,
    "referenceCount": 51,
    "citationCount": 5,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "30300197",
        "name": "Weizhe Yuan"
      },
      {
        "authorId": "144118452",
        "name": "Pengfei Liu"
      }
    ]
  },
  "252682946": {
    "paperId": "07dc375b95aaeb748d7b0560bfa7d81f1bddc8b2",
    "externalIds": {
      "ArXiv": "2210.00881",
      "DBLP": "journals/natmi/KrennBCEFGLLMSSTVXYK23",
      "DOI": "10.1038/s42256-023-00735-0",
      "CorpusId": 252682946
    },
    "publicationVenue": {
      "id": "6457124b-39bf-4d02-bff4-73752ff21562",
      "name": "Nature Machine Intelligence",
      "type": "journal",
      "alternate_names": [
        "Nat Mach Intell"
      ],
      "issn": "2522-5839",
      "url": "https://www.nature.com/natmachintell/"
    },
    "title": "Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network",
    "abstract": null,
    "venue": "Nature Machine Intelligence",
    "year": 2022,
    "referenceCount": 101,
    "citationCount": 30,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "5906965",
        "name": "Mario Krenn"
      },
      {
        "authorId": "102683668",
        "name": "L. Buffoni"
      },
      {
        "authorId": "145360457",
        "name": "B. Coutinho"
      },
      {
        "authorId": "2981096",
        "name": "S. Eppel"
      },
      {
        "authorId": "40104995",
        "name": "J. Foster"
      },
      {
        "authorId": "2299002545",
        "name": "Andrew Gritsevskiy"
      },
      {
        "authorId": "72152328",
        "name": "Harlin Lee"
      },
      {
        "authorId": "2141583641",
        "name": "Yichao Lu"
      },
      {
        "authorId": "2095423656",
        "name": "Jo\u00e3o P. Moutinho"
      },
      {
        "authorId": "84710505",
        "name": "Nima Sanjabi"
      },
      {
        "authorId": "51129341",
        "name": "Rishi Sonthalia"
      },
      {
        "authorId": "2150443015",
        "name": "Ngoc M. Tran"
      },
      {
        "authorId": "2149984387",
        "name": "Francisco Valente"
      },
      {
        "authorId": "2154871103",
        "name": "Yangxinyu Xie"
      },
      {
        "authorId": "2151886670",
        "name": "Rose Yu"
      },
      {
        "authorId": "2058236577",
        "name": "Michael Kopp"
      }
    ]
  },
  "232126179": {
    "paperId": "278ecb42fc5ba89d710055a7056384c22886c883",
    "externalIds": {
      "DBLP": "conf/wsdm/WangX0ZZ21",
      "DOI": "10.1145/3437963.3441739",
      "CorpusId": 232126179
    },
    "publicationVenue": {
      "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
      "name": "Web Search and Data Mining",
      "type": "conference",
      "alternate_names": [
        "Web Search Data Min",
        "WSDM"
      ],
      "url": "http://www.wikicfp.com/cfp/program?id=3158"
    },
    "title": "AutoCite: Multi-Modal Representation Fusion for Contextual Citation Generation",
    "abstract": "Citing comprehensive and correct related work is crucial in academic writing. It can not only support the author's claims but also help readers trace other related research papers. Nowadays, with the rapid increase in the number of scientific literatures, it has become increasingly challenging to search for high-quality citations and write the manuscript. In this paper, we present an automatic writing assistant model, AutoCite, which not only infers potentially related work but also automatically generates the citation context at the same time. Specifically, AutoCite involves a novel multi-modal encoder and a multi-task decoder architecture. Based on the multi-modal inputs, the encoder in AutoCite learns paper representations with both citation network structure and textual contexts. The multi-task decoder in AutoCite couples and jointly learns citation prediction and context generation in a unified manner. To effectively join the encoder and decoder, we introduce a novel representation fusion component, i.e., gated neural fusion, which feeds the multi-modal representation inputs from the encoder and creates outputs for the downstream multi-task decoder adaptively. Extensive experiments on five real-world citation network datasets validate the effectiveness of our model.",
    "venue": "Web Search and Data Mining",
    "year": 2021,
    "referenceCount": 34,
    "citationCount": 8,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35832075",
        "name": "Qingqin Wang"
      },
      {
        "authorId": "33629364",
        "name": "Yun Xiong"
      },
      {
        "authorId": "49889358",
        "name": "Yao Zhang"
      },
      {
        "authorId": "1718428",
        "name": "Jiawei Zhang"
      },
      {
        "authorId": "8247706",
        "name": "Yangyong Zhu"
      }
    ]
  },
  "261530162": {
    "paperId": "d00735241af700d21762d2f3ca00d920241a15a4",
    "externalIds": {
      "ArXiv": "2309.01219",
      "DBLP": "journals/corr/abs-2309-01219",
      "DOI": "10.48550/arXiv.2309.01219",
      "CorpusId": 261530162
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
    "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 215,
    "citationCount": 348,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1895977079",
        "name": "Yue Zhang"
      },
      {
        "authorId": "2110450452",
        "name": "Yafu Li"
      },
      {
        "authorId": "152496687",
        "name": "Leyang Cui"
      },
      {
        "authorId": "1724421",
        "name": "Deng Cai"
      },
      {
        "authorId": "2978364",
        "name": "Lemao Liu"
      },
      {
        "authorId": "2156525869",
        "name": "Tingchen Fu"
      },
      {
        "authorId": "14799547",
        "name": "Xinting Huang"
      },
      {
        "authorId": "2065703096",
        "name": "Enbo Zhao"
      },
      {
        "authorId": "2257439415",
        "name": "Yu Zhang"
      },
      {
        "authorId": "2109404730",
        "name": "Yulong Chen"
      },
      {
        "authorId": "1800190",
        "name": "Longyue Wang"
      },
      {
        "authorId": "1755919",
        "name": "A. Luu"
      },
      {
        "authorId": "2237804371",
        "name": "Wei Bi"
      },
      {
        "authorId": "8815141",
        "name": "Freda Shi"
      },
      {
        "authorId": "34720053",
        "name": "Shuming Shi"
      }
    ]
  },
  "261705916": {
    "paperId": "396305230ddcf915b19a19683a89e34d76321a33",
    "externalIds": {
      "DBLP": "journals/corr/abs-2309-06794",
      "ArXiv": "2309.06794",
      "DOI": "10.48550/arXiv.2309.06794",
      "CorpusId": 261705916
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models",
    "abstract": "As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 175,
    "citationCount": 58,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2239197934",
        "name": "Hongbin Ye"
      },
      {
        "authorId": "2239249506",
        "name": "Tong Liu"
      },
      {
        "authorId": "2239587085",
        "name": "Aijia Zhang"
      },
      {
        "authorId": "2239199462",
        "name": "Wei Hua"
      },
      {
        "authorId": "2239200814",
        "name": "Weiqiang Jia"
      }
    ]
  },
  "261696947": {
    "paperId": "71bc0c97c20fffce796a355b16bd202987260029",
    "externalIds": {
      "ArXiv": "2309.05922",
      "DBLP": "journals/corr/abs-2309-05922",
      "DOI": "10.48550/arXiv.2309.05922",
      "CorpusId": 261696947
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Survey of Hallucination in Large Foundation Models",
    "abstract": "Hallucination in a foundation model (FM) refers to the generation of content that strays from factual reality or includes fabricated information. This survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``Large'' Foundation Models (LFMs). The paper classifies various types of hallucination phenomena that are specific to LFMs and establishes evaluation criteria for assessing the extent of hallucination. It also examines existing strategies for mitigating hallucination in LFMs and discusses potential directions for future research in this area. Essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in LFMs.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 40,
    "citationCount": 253,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "9460529",
        "name": "Vipula Rawte"
      },
      {
        "authorId": "144463965",
        "name": "A. Sheth"
      },
      {
        "authorId": "48806891",
        "name": "Amitava Das"
      }
    ]
  },
  "219178913": {
    "paperId": "77b101d2c0f3d2842edb4acdbca0c4e859cda4d5",
    "externalIds": {
      "DBLP": "journals/inffus/MaNXC20",
      "MAG": "3037611961",
      "DOI": "10.1016/j.inffus.2020.06.011",
      "CorpusId": 219178913
    },
    "publicationVenue": {
      "id": "06afdd0b-0d85-413f-af8a-c3045c12c561",
      "name": "Information Fusion",
      "type": "journal",
      "alternate_names": [
        "Inf Fusion"
      ],
      "issn": "1566-2535",
      "url": "https://www.journals.elsevier.com/information-fusion",
      "alternate_urls": [
        "http://www.sciencedirect.com/science/journal/15662535"
      ]
    },
    "title": "A survey on empathetic dialogue systems",
    "abstract": null,
    "venue": "Information Fusion",
    "year": 2020,
    "referenceCount": 152,
    "citationCount": 182,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145921076",
        "name": "Yukun Ma"
      },
      {
        "authorId": "2055542232",
        "name": "Khanh Linh Nguyen"
      },
      {
        "authorId": "121112586",
        "name": "Frank Xing"
      },
      {
        "authorId": "49943757",
        "name": "E. Cambria"
      }
    ]
  },
  "216641884": {
    "paperId": "35763b04ca7ec8d1cbdacb3be5635015d2f7ad9b",
    "externalIds": {
      "MAG": "3021583105",
      "ArXiv": "2004.13818",
      "DBLP": "journals/corr/abs-2004-13818",
      "CorpusId": 216641884
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Survey of Document Grounded Dialogue Systems (DGDS)",
    "abstract": "Dialogue system (DS) attracts great attention from industry and academia because of its wide application prospects. Researchers usually divide the DS according to the function. However, many conversations require the DS to switch between different functions. For example, movie discussion can change from chit-chat to QA, the conversational recommendation can transform from chit-chat to recommendation, etc. Therefore, classification according to functions may not be enough to help us appreciate the current development trend. We classify the DS based on background knowledge. Specifically, study the latest DS based on the unstructured document(s). We define Document Grounded Dialogue System (DGDS) as the DS that the dialogues are centering on the given document(s). The DGDS can be used in scenarios such as talking over merchandise against product Manual, commenting on news reports, etc. We believe that extracting unstructured document(s) information is the future trend of the DS because a great amount of human knowledge lies in these document(s). The research of the DGDS not only possesses a broad application prospect but also facilitates AI to better understand human knowledge and natural language. We analyze the classification, architecture, datasets, models, and future development trends of the DGDS, hoping to help researchers in this field.",
    "venue": "arXiv.org",
    "year": 2020,
    "referenceCount": 144,
    "citationCount": 18,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "153132928",
        "name": "Longxuan Ma"
      },
      {
        "authorId": "1806419",
        "name": "Weinan Zhang"
      },
      {
        "authorId": null,
        "name": "Mingda Li"
      },
      {
        "authorId": "40282288",
        "name": "Ting Liu"
      }
    ]
  },
  "248780269": {
    "paperId": "04aa1605c650bee77e09ad61c4e894ecb9f543a8",
    "externalIds": {
      "DBLP": "conf/acl/ShenPWPM22",
      "ACL": "2022.acl-long.221",
      "DOI": "10.18653/v1/2022.acl-long.221",
      "CorpusId": 248780269
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Knowledge Enhanced Reflection Generation for Counseling Dialogues",
    "abstract": "In this paper, we study the effect of commonsense and domain knowledge while generating responses in counseling conversations using retrieval and generative methods for knowledge integration. We propose a pipeline that collects domain knowledge through web mining, and show that retrieval from both domain-specific and commonsense knowledge bases improves the quality of generated responses. We also present a model that incorporates knowledge generated by COMET using soft positional encoding and masked self-attention.We show that both retrieved and COMET-generated knowledge improve the system\u2019s performance as measured by automatic metrics and also by human evaluation. Lastly, we present a comparative study on the types of knowledge encoded by our system showing that causal and intentional relationships benefit the generation task more than other types of commonsense relations.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 41,
    "citationCount": 19,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2072820796",
        "name": "Siqi Shen"
      },
      {
        "authorId": "1396239754",
        "name": "Ver\u00f3nica P\u00e9rez-Rosas"
      },
      {
        "authorId": "145645240",
        "name": "Charles F Welch"
      },
      {
        "authorId": "1746416",
        "name": "Soujanya Poria"
      },
      {
        "authorId": "2105984203",
        "name": "Rada Mihalcea"
      }
    ]
  },
  "248266574": {
    "paperId": "3d6b094f439ceae770ad1ca5cb322421debf3ba8",
    "externalIds": {
      "DBLP": "conf/naacl/RonyU022",
      "ArXiv": "2204.09149",
      "DOI": "10.48550/arXiv.2204.09149",
      "CorpusId": 248266574
    },
    "publicationVenue": null,
    "title": "DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation",
    "abstract": "Task-oriented dialogue generation is challenging since the underlying knowledge is often dynamic and effectively incorporating knowledge into the learning process is hard. It is particularly challenging to generate both human-like and informative responses in this setting. Recent research primarily focused on various knowledge distillation methods where the underlying relationship between the facts in a knowledge base is not effectively captured. In this paper, we go one step further and demonstrate how the structural information of a knowledge graph can improve the system's inference capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue system that effectively incorporates knowledge into a language model. Our proposed system views relational knowledge as a knowledge graph and introduces (1) a structure-aware knowledge embedding technique, and (2) a knowledge graph-weighted attention masking strategy to facilitate the system selecting relevant information during the dialogue generation. An empirical evaluation demonstrates the effectiveness of DialoKG over state-of-the-art methods on several standard benchmark datasets.",
    "venue": "NAACL-HLT",
    "year": 2022,
    "referenceCount": 49,
    "citationCount": 29,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "120441491",
        "name": "Md. Rashad Al Hasan Rony"
      },
      {
        "authorId": "2370666",
        "name": "Ricardo Usbeck"
      },
      {
        "authorId": "71564931",
        "name": "Jens Lehmann"
      }
    ]
  },
  "227033623": {
    "paperId": "0ed502bc03b3fb6d7c4356d0daf34bd915daeb91",
    "externalIds": {
      "ACL": "2020.emnlp-main.743",
      "DBLP": "conf/emnlp/ZengYJYWZZZDZFZ20",
      "MAG": "3101223450",
      "DOI": "10.18653/v1/2020.emnlp-main.743",
      "CorpusId": 227033623
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "MedDialog: A Large-scale Medical Dialogue Dataset",
    "abstract": "Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build large-scale medical dialogue datasets \u2013 MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and doctor-like medical dialogues. We also study the transferability of models trained on MedDialog to low-resource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation. The datasets and code are available at https://github.com/UCSD-AI4H/Medical-Dialogue-System",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 24,
    "citationCount": 44,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2061248094",
        "name": "Guangtao Zeng"
      },
      {
        "authorId": "32412901",
        "name": "Wenmian Yang"
      },
      {
        "authorId": "1613055688",
        "name": "Zeqian Ju"
      },
      {
        "authorId": "2109410479",
        "name": "Yue Yang"
      },
      {
        "authorId": "2116422777",
        "name": "Sicheng Wang"
      },
      {
        "authorId": "3483566",
        "name": "Ruisi Zhang"
      },
      {
        "authorId": "2112494296",
        "name": "Meng Zhou"
      },
      {
        "authorId": "2072984384",
        "name": "Jiaqi Zeng"
      },
      {
        "authorId": "151257356",
        "name": "Xiangyu Dong"
      },
      {
        "authorId": "2110065346",
        "name": "Ruoyu Zhang"
      },
      {
        "authorId": "122851213",
        "name": "Hongchao Fang"
      },
      {
        "authorId": "11243844",
        "name": "Penghui Zhu"
      },
      {
        "authorId": "2107976513",
        "name": "Shu Chen"
      },
      {
        "authorId": "40526720",
        "name": "P. Xie"
      }
    ]
  },
  "202717047": {
    "paperId": "980456f50cd4b6e30649592afb693d5b6af8a703",
    "externalIds": {
      "DBLP": "journals/corr/abs-2308-11995",
      "ArXiv": "2308.11995",
      "MAG": "2972664115",
      "DOI": "10.21437/interspeech.2019-3079",
      "CorpusId": 202717047
    },
    "publicationVenue": {
      "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
      "name": "Interspeech",
      "type": "conference",
      "alternate_names": [
        "Conf Int Speech Commun Assoc",
        "INTERSPEECH",
        "Conference of the International Speech Communication Association"
      ],
      "issn": "2308-457X",
      "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
      "alternate_urls": [
        "http://www.isca-speech.org/"
      ]
    },
    "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations",
    "abstract": "Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don't have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking.",
    "venue": "Interspeech",
    "year": 2019,
    "referenceCount": 21,
    "citationCount": 312,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145916630",
        "name": "Karthik Gopalakrishnan"
      },
      {
        "authorId": "8869538",
        "name": "Behnam Hedayatnia"
      },
      {
        "authorId": "3465846",
        "name": "Qinlang Chen"
      },
      {
        "authorId": "1411423941",
        "name": "Anna Gottardi"
      },
      {
        "authorId": "1412838170",
        "name": "Sanjeev Kwatra"
      },
      {
        "authorId": "47851456",
        "name": "Anu Venkatesh"
      },
      {
        "authorId": "39303368",
        "name": "Raefer Gabriel"
      },
      {
        "authorId": "1395813836",
        "name": "Dilek Z. Hakkani-T\u00fcr"
      }
    ]
  },
  "7961699": {
    "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
    "externalIds": {
      "MAG": "2130942839",
      "DBLP": "conf/nips/SutskeverVL14",
      "ArXiv": "1409.3215",
      "CorpusId": 7961699
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Sequence to Sequence Learning with Neural Networks",
    "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
    "venue": "Neural Information Processing Systems",
    "year": 2014,
    "referenceCount": 40,
    "citationCount": 19593,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1701686",
        "name": "I. Sutskever"
      },
      {
        "authorId": "1689108",
        "name": "O. Vinyals"
      },
      {
        "authorId": "2827616",
        "name": "Quoc V. Le"
      }
    ]
  },
  "8314118": {
    "paperId": "668db48c6a79826456341680ee1175dfc4cced71",
    "externalIds": {
      "MAG": "2952913664",
      "DBLP": "journals/corr/SeeLM17",
      "ArXiv": "1704.04368",
      "ACL": "P17-1099",
      "DOI": "10.18653/v1/P17-1099",
      "CorpusId": 8314118
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Get To The Point: Summarization with Pointer-Generator Networks",
    "abstract": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2017,
    "referenceCount": 33,
    "citationCount": 3815,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "13070498",
        "name": "A. See"
      },
      {
        "authorId": "35025299",
        "name": "Peter J. Liu"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      }
    ]
  },
  "245005939": {
    "paperId": "6159a9048cf3efb9bcee231b175932d07be33e37",
    "externalIds": {
      "ArXiv": "2112.04554",
      "DBLP": "journals/corr/abs-2112-04554",
      "CorpusId": 245005939
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Whose Ground Truth? Accounting for Individual and Collective Identities Underlying Dataset Annotation",
    "abstract": "Human annotations play a crucial role in machine learning (ML) research and development. However, the ethical considerations around the processes and decisions that go into building ML datasets has not received nearly enough attention. In this paper, we survey an array of literature that provides insights into ethical considerations around crowdsourced dataset annotation. We synthesize these insights, and lay out the challenges in this space along two layers: (1) who the annotator is, and how the annotators' lived experiences can impact their annotations, and (2) the relationship between the annotators and the crowdsourcing platforms and what that relationship affords them. Finally, we put forth a concrete set of recommendations and considerations for dataset developers at various stages of the ML data pipeline: task formulation, selection of annotators, platform and infrastructure choices, dataset analysis and evaluation, and dataset documentation and release.",
    "venue": "arXiv.org",
    "year": 2021,
    "referenceCount": 30,
    "citationCount": 55,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "40081727",
        "name": "Emily L. Denton"
      },
      {
        "authorId": "2146515892",
        "name": "M. D'iaz"
      },
      {
        "authorId": "24643287",
        "name": "I. Kivlichan"
      },
      {
        "authorId": "3331141",
        "name": "Vinodkumar Prabhakaran"
      },
      {
        "authorId": "2144002544",
        "name": "Rachel Rosen"
      }
    ]
  },
  "258823008": {
    "paperId": "f4f154892800008894ebbf57add31fcaac4f27ca",
    "externalIds": {
      "ArXiv": "2305.11840",
      "ACL": "2023.acl-long.548",
      "DBLP": "conf/acl/JhaDRDPD23",
      "DOI": "10.48550/arXiv.2305.11840",
      "CorpusId": 258823008
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models",
    "abstract": "Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 57,
    "citationCount": 27,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "36701727",
        "name": "Akshita Jha"
      },
      {
        "authorId": "2132006618",
        "name": "A. Davani"
      },
      {
        "authorId": "144417522",
        "name": "Chandan K. Reddy"
      },
      {
        "authorId": "2160404",
        "name": "Shachi Dave"
      },
      {
        "authorId": "3331141",
        "name": "Vinodkumar Prabhakaran"
      },
      {
        "authorId": "50991767",
        "name": "Sunipa Dev"
      }
    ]
  },
  "247748753": {
    "paperId": "8a3d1ce6dd9dc9766a42e645a23de4b5f2b447ec",
    "externalIds": {
      "ArXiv": "2203.13722",
      "DBLP": "journals/corr/abs-2203-13722",
      "ACL": "2023.c3nlp-1.12",
      "DOI": "10.18653/v1/2023.c3nlp-1.12",
      "CorpusId": 247748753
    },
    "publicationVenue": null,
    "title": "Probing Pre-Trained Language Models for Cross-Cultural Differences in Values",
    "abstract": "Language embeds information about social, cultural, and political values people hold. Prior work has explored potentially harmful social biases encoded in Pre-trained Language Models (PLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures.In this paper, we introduce probes to study which cross-cultural values are embedded in these models, and whether they align with existing theories and cross-cultural values surveys. We find that PLMs capture differences in values across cultures, but those only weakly align with established values surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PLMs with values surveys.",
    "venue": "C3NLP",
    "year": 2022,
    "referenceCount": 75,
    "citationCount": 98,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1943255906",
        "name": "Arnav Arora"
      },
      {
        "authorId": "23319388",
        "name": "Lucie-Aim\u00e9e Kaffee"
      },
      {
        "authorId": "1736067",
        "name": "Isabelle Augenstein"
      }
    ]
  },
  "253802027": {
    "paperId": "6615e9d1641e5b8141efa8e947a90d12b3158075",
    "externalIds": {
      "ArXiv": "2211.13069",
      "DBLP": "journals/corr/abs-2211-13069",
      "DOI": "10.48550/arXiv.2211.13069",
      "CorpusId": 253802027
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Cultural Incongruencies in Artificial Intelligence",
    "abstract": "Artificial intelligence (AI) systems attempt to imitate human behavior. How well they do this imitation is often used to assess their utility and to attribute human-like (or artificial) intelligence to them. However, most work on AI refers to and relies on human intelligence without accounting for the fact that human behavior is inherently shaped by the cultural contexts they are embedded in, the values and beliefs they hold, and the social practices they follow. Additionally, since AI technologies are mostly conceived and developed in just a handful of countries, they embed the cultural values and practices of these countries. Similarly, the data that is used to train the models also fails to equitably represent global cultural diversity. Problems therefore arise when these technologies interact with globally diverse societies and cultures, with different values and interpretive practices. In this position paper, we describe a set of cultural dependencies and incongruencies in the context of AI-based language and vision technologies, and reflect on the possibilities of and potential strategies towards addressing these incongruencies.",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 27,
    "citationCount": 15,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3331141",
        "name": "Vinodkumar Prabhakaran"
      },
      {
        "authorId": "101513478",
        "name": "Rida Qadri"
      },
      {
        "authorId": "2044655623",
        "name": "Ben Hutchinson"
      }
    ]
  },
  "257833897": {
    "paperId": "ca94c924d8a3b77a2bd5b16ffc03b8723bce9c1f",
    "externalIds": {
      "DBLP": "journals/corr/abs-2303-17466",
      "ArXiv": "2303.17466",
      "ACL": "2023.c3nlp-1.7",
      "DOI": "10.48550/arXiv.2303.17466",
      "CorpusId": 257833897
    },
    "publicationVenue": null,
    "title": "Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study",
    "abstract": "The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like conversations. Given its usage by users from various nations and its training on a vast multilingual corpus that includes diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. In this paper, we investigate the underlying cultural background of ChatGPT by analyzing its responses to questions designed to quantify human cultural differences. Our findings suggest that, when prompted with American context, ChatGPT exhibits a strong alignment with American culture, but it adapts less effectively to other cultural contexts. Furthermore, by using different prompts to probe the model, we show that English prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards American culture. This study provides valuable insights into the cultural implications of ChatGPT and highlights the necessity of greater diversity and cultural awareness in language technologies.",
    "venue": "C3NLP",
    "year": 2023,
    "referenceCount": 45,
    "citationCount": 121,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2112402733",
        "name": "Yong Cao"
      },
      {
        "authorId": "2116635928",
        "name": "Li Zhou"
      },
      {
        "authorId": "2132579230",
        "name": "Seolhwa Lee"
      },
      {
        "authorId": "2092471782",
        "name": "Laura Cabello"
      },
      {
        "authorId": "2108557855",
        "name": "Min Chen"
      },
      {
        "authorId": "2064295987",
        "name": "Daniel Hershcovich"
      }
    ]
  },
  "250390642": {
    "paperId": "e7ac9ea2ae42a7a9125da69de1ff295efebfaff5",
    "externalIds": {
      "DBLP": "conf/naacl/ZhangJ022",
      "ACL": "2022.naacl-main.411",
      "DOI": "10.18653/v1/2022.naacl-main.411",
      "CorpusId": 250390642
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Label Definitions Improve Semantic Role Labeling",
    "abstract": "Argument classification is at the core of Semantic Role Labeling. Given a sentence and the predicate, a semantic role label is assigned to each argument of the predicate. While semantic roles come with meaningful definitions, existing work has treated them as symbolic. Learning symbolic labels usually requires ample training data, which is frequently unavailable due to the cost of annotation. We instead propose to retrieve and leverage the definitions of these labels from the annotation guidelines. For example, the verb predicate \u201cwork\u201d has arguments defined as \u201cworker\u201d, \u201cjob\u201d, \u201cemployer\u201d, etc. Our model achieves state-of-the-art performance on the CoNLL09 dataset injected with label definitions given the predicate senses. The performance improvement is even more pronounced in low-resource settings when training data is scarce.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 35,
    "citationCount": 4,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "72436283",
        "name": "Li Zhang"
      },
      {
        "authorId": "144377686",
        "name": "Ishan Jindal"
      },
      {
        "authorId": "1718694",
        "name": "Yunyao Li"
      }
    ]
  },
  "252370416": {
    "paperId": "83e3b3b19bbe92a85265bcee6634738206662f03",
    "externalIds": {
      "ACL": "2022.lrec-1.181",
      "DBLP": "conf/lrec/JindalRULNT0022",
      "CorpusId": 252370416
    },
    "publicationVenue": {
      "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
      "name": "International Conference on Language Resources and Evaluation",
      "type": "conference",
      "alternate_names": [
        "LREC",
        "Int Conf Lang Resour Evaluation"
      ],
      "url": "http://www.lrec-conf.org/"
    },
    "title": "Universal Proposition Bank 2.0",
    "abstract": "Semantic role labeling (SRL) represents the meaning of a sentence in the form of predicate-argument structures. Such shallow semantic analysis is helpful in a wide range of downstream NLP tasks and real-world applications. As treebanks enabled the development of powerful syntactic parsers, the accurate predicate-argument analysis demands training data in the form of propbanks. Unfortunately, most languages simply do not have corresponding propbanks due to the high cost required to construct such resources. To overcome such challenges, Universal Proposition Bank 1.0 (UP1.0) was released in 2017, with high-quality propbank data generated via a two-stage method exploiting monolingual SRL and multilingual parallel data. In this paper, we introduce Universal Proposition Bank 2.0 (UP2.0), with significant enhancements over UP1.0: (1) propbanks with higher quality by using a state-of-the-art monolingual SRL and improved auto-generation of annotations; (2) expanded language coverage (from 7 to 9 languages); (3) span annotation for the decoupling of syntactic analysis; and (4) Gold data for a subset of the languages. We also share our experimental results that confirm the significant quality improvements of the generated propbanks. In addition, we present a comprehensive experimental evaluation on how different implementation choices impact the quality of the resulting data. We release these resources to the research community and hope to encourage more research on cross-lingual SRL.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2022,
    "referenceCount": 57,
    "citationCount": 9,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144377686",
        "name": "Ishan Jindal"
      },
      {
        "authorId": "13836343",
        "name": "Alexandre Rademaker"
      },
      {
        "authorId": "2185433526",
        "name": "Micha\u0142 Ulewicz"
      },
      {
        "authorId": "2180951336",
        "name": "Linh H. Ha"
      },
      {
        "authorId": "145199659",
        "name": "Huyen Nguyen"
      },
      {
        "authorId": "1399212475",
        "name": "Khoi-Nguyen Tran"
      },
      {
        "authorId": "2115718238",
        "name": "Huaiyu Zhu"
      },
      {
        "authorId": "1718694",
        "name": "Yunyao Li"
      }
    ]
  },
  "5001921": {
    "paperId": "1c37654db8b6a86795b9c83d214d994fe46f6a37",
    "externalIds": {
      "DBLP": "conf/naacl/0004FTSS15",
      "MAG": "655477013",
      "ArXiv": "1805.10399",
      "ACL": "N15-1114",
      "DOI": "10.3115/v1/N15-1114",
      "CorpusId": 5001921
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Toward Abstractive Summarization Using Semantic Representations",
    "abstract": "We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-tograph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-totext generator. The framework is data-driven, trainable, and not specifically designed for a particular domain. Experiments on goldstandard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 43,
    "citationCount": 290,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144544919",
        "name": "Fei Liu"
      },
      {
        "authorId": "144683841",
        "name": "Jeffrey Flanigan"
      },
      {
        "authorId": "38094552",
        "name": "Sam Thomson"
      },
      {
        "authorId": "2464164",
        "name": "N. Sadeh"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      }
    ]
  },
  "9135033": {
    "paperId": "0729515f62042d1274c131360c33a121df71c856",
    "externalIds": {
      "MAG": "2468355276",
      "DBLP": "conf/naacl/FlaniganDSC16",
      "ACL": "N16-1087",
      "DOI": "10.18653/v1/N16-1087",
      "CorpusId": 9135033
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Generation from Abstract Meaning Representation using Tree Transducers",
    "abstract": "Language generation from purely semantic representations is a challenging task. This paper addresses generating English from the Ab-stract Meaning Representation (AMR), consisting of re-entrant graphs whose nodes are concepts and edges are relations. The new method is trained statistically from AMR-annotated English and consists of two major steps: (i) generating an appropriate spanning tree for the AMR, and (ii) applying tree-to-string transducers to generate English. The method relies on discriminative learning and an argument realization model to overcome data sparsity. Initial tests on held-out data show good promise despite the complexity of the task. The system is available open-source as part of JAMR at:",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 26,
    "citationCount": 102,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144683841",
        "name": "Jeffrey Flanigan"
      },
      {
        "authorId": "1745899",
        "name": "Chris Dyer"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      },
      {
        "authorId": "143712374",
        "name": "J. Carbonell"
      }
    ]
  },
  "15344879": {
    "paperId": "91830ca68422d6b42446631eeef696c84e602e87",
    "externalIds": {
      "ACL": "N15-1040",
      "DBLP": "conf/naacl/WangXP15",
      "MAG": "2296308987",
      "DOI": "10.3115/v1/N15-1040",
      "CorpusId": 15344879
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "A Transition-based Algorithm for AMR Parsing",
    "abstract": "We present a two-stage framework to parse a sentence into its Abstract Meaning Representation (AMR). We first use a dependency parser to generate a dependency tree for the sentence. In the second stage, we design a novel transition-based algorithm that transforms the dependency tree to an AMR graph. There are several advantages with this approach. First, the dependency parser can be trained on a training set much larger than the training set for the tree-to-graph algorithm, resulting in a more accurate AMR parser overall. Our parser yields an improvement of 5% absolute in F-measure over the best previous result. Second, the actions that we design are linguistically intuitive and capture the regularities in the mapping between the dependency structure and the AMR of a sentence. Third, our parser runs in nearly linear time in practice in spite of a worst-case complexity ofO(n 2 ).",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2015,
    "referenceCount": 17,
    "citationCount": 170,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47074942",
        "name": "Chuan Wang"
      },
      {
        "authorId": "1702849",
        "name": "Nianwen Xue"
      },
      {
        "authorId": "1735131",
        "name": "Sameer Pradhan"
      }
    ]
  },
  "260899983": {
    "paperId": "451a657dabf80ebc43f6a3be518250b2cd5dfe1a",
    "externalIds": {
      "ArXiv": "2308.07902",
      "ACL": "2023.ccl-2.8",
      "DBLP": "journals/corr/abs-2308-07902",
      "DOI": "10.48550/arXiv.2308.07902",
      "CorpusId": 260899983
    },
    "publicationVenue": {
      "id": "0242a0a8-eac8-4d42-a284-4789a579aa9b",
      "name": "China National Conference on Chinese Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "CCL",
        "Constraint Comput Log",
        "China National Conf Chin Comput Linguistics",
        "Constraints in Computational Logics"
      ],
      "issn": "0319-0080",
      "url": "http://ccl.uwinnipeg.ca/",
      "alternate_urls": [
        "http://ps-www.dfki.uni-sb.de/ccl/"
      ]
    },
    "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
    "abstract": "\u201cFrom pre-trained language model (PLM) to large language model (LLM), the field of naturallanguage processing (NLP) has witnessed steep performance gains and wide practical uses. Theevaluation of a research field guides its direction of improvement. However, LLMs are extremelyhard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inade-quate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficultto keep up with the wide range of applications in real-world scenarios. To tackle these problems,existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerousevaluation tasks in both academia and industry, we investigate multiple papers concerning LLMevaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, relia-bility, and safety. For every competency, we introduce its definition, corresponding benchmarks,and metrics. Under this competency architecture, similar tasks are combined to reflect corre-sponding ability, while new tasks can also be easily added into the system. Finally, we give oursuggestions on the future direction of LLM\u2019s evaluation.\u201d",
    "venue": "China National Conference on Chinese Computational Linguistics",
    "year": 2023,
    "referenceCount": 180,
    "citationCount": 7,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "26250168",
        "name": "Ziyu Zhuang"
      },
      {
        "authorId": "2133447633",
        "name": "Qiguang Chen"
      },
      {
        "authorId": "153132928",
        "name": "Longxuan Ma"
      },
      {
        "authorId": "2112109549",
        "name": "Mingda Li"
      },
      {
        "authorId": "2230014897",
        "name": "Yi Han"
      },
      {
        "authorId": "2229053371",
        "name": "Yushan Qian"
      },
      {
        "authorId": "2228758690",
        "name": "Haopeng Bai"
      },
      {
        "authorId": "2048146115",
        "name": "Zixian Feng"
      },
      {
        "authorId": "1806419",
        "name": "Weinan Zhang"
      },
      {
        "authorId": "2140034831",
        "name": "Ting Liu"
      }
    ]
  },
  "246822399": {
    "paperId": "e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb",
    "externalIds": {
      "DBLP": "journals/corr/abs-2202-06935",
      "ArXiv": "2202.06935",
      "DOI": "10.1613/jair.1.13715",
      "CorpusId": 246822399
    },
    "publicationVenue": {
      "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
      "name": "Journal of Artificial Intelligence Research",
      "type": "journal",
      "alternate_names": [
        "JAIR",
        "J Artif Intell Res",
        "The Journal of Artificial Intelligence Research"
      ],
      "issn": "1076-9757",
      "url": "http://www.jair.org/"
    },
    "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text",
    "abstract": "Evaluation practices in natural language generation (NLG) have many known flaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural generation models have improved to the point where their outputs can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their findings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for evaluation research and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 generation papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.",
    "venue": "Journal of Artificial Intelligence Research",
    "year": 2022,
    "referenceCount": 304,
    "citationCount": 156,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3159346",
        "name": "Sebastian Gehrmann"
      },
      {
        "authorId": "40684993",
        "name": "Elizabeth Clark"
      },
      {
        "authorId": "145450400",
        "name": "Thibault Sellam"
      }
    ]
  },
  "240420063": {
    "paperId": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
    "externalIds": {
      "ArXiv": "2111.01243",
      "DBLP": "journals/csur/MinRSVNSAHR24",
      "DOI": "10.1145/3605943",
      "CorpusId": 240420063
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey",
    "abstract": "Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.",
    "venue": "ACM Computing Surveys",
    "year": 2021,
    "referenceCount": 362,
    "citationCount": 717,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1875233",
        "name": "Bonan Min"
      },
      {
        "authorId": "2136457937",
        "name": "Hayley Ross"
      },
      {
        "authorId": "46185356",
        "name": "Elior Sulem"
      },
      {
        "authorId": "3460489",
        "name": "Amir Pouran Ben Veyseh"
      },
      {
        "authorId": "1811211",
        "name": "Thien Huu Nguyen"
      },
      {
        "authorId": "1724648481",
        "name": "Oscar Sainz"
      },
      {
        "authorId": "1733049",
        "name": "Eneko Agirre"
      },
      {
        "authorId": "2136480655",
        "name": "Ilana Heinz"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "263625818": {
    "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
    "externalIds": {
      "ArXiv": "2206.04615",
      "DBLP": "journals/corr/abs-2206-04615",
      "CorpusId": 263625818
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
    "abstract": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit\"breakthrough\"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 0,
    "citationCount": 1403,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2169175069",
        "name": "Aarohi Srivastava"
      },
      {
        "authorId": "2188497",
        "name": "Abhinav Rastogi"
      },
      {
        "authorId": "1484043592",
        "name": "Abhishek Rao"
      },
      {
        "authorId": "2248610",
        "name": "Abu Awal Md Shoeb"
      },
      {
        "authorId": "144948925",
        "name": "Abubakar Abid"
      },
      {
        "authorId": "2064150446",
        "name": "Adam Fisch"
      },
      {
        "authorId": "2254150367",
        "name": "Adam R. Brown"
      },
      {
        "authorId": "2253463637",
        "name": "Adam Santoro"
      },
      {
        "authorId": "2254858472",
        "name": "Aditya Gupta"
      },
      {
        "authorId": "1388513000",
        "name": "Adri\u00e0 Garriga-Alonso"
      },
      {
        "authorId": "2169552920",
        "name": "Agnieszka Kluska"
      },
      {
        "authorId": "102549875",
        "name": "Aitor Lewkowycz"
      },
      {
        "authorId": "2253472327",
        "name": "Akshat Agarwal"
      },
      {
        "authorId": "146162186",
        "name": "Alethea Power"
      },
      {
        "authorId": "2253525058",
        "name": "Alex Ray"
      },
      {
        "authorId": "46236380",
        "name": "Alex Warstadt"
      },
      {
        "authorId": "49502890",
        "name": "Alexander W. Kocurek"
      },
      {
        "authorId": "150920423",
        "name": "Ali Safaya"
      },
      {
        "authorId": "30615457",
        "name": "Ali Tazarv"
      },
      {
        "authorId": "2253627565",
        "name": "Alice Xiang"
      },
      {
        "authorId": "119389860",
        "name": "Alicia Parrish"
      },
      {
        "authorId": "2253756381",
        "name": "Allen Nie"
      },
      {
        "authorId": "2254089780",
        "name": "Aman Hussain"
      },
      {
        "authorId": "2220750220",
        "name": "Amanda Askell"
      },
      {
        "authorId": "2169546807",
        "name": "Amanda Dsouza"
      },
      {
        "authorId": "133666998",
        "name": "Ambrose Slone"
      },
      {
        "authorId": "1397271551",
        "name": "Ameet Rahane"
      },
      {
        "authorId": "2169441753",
        "name": "Anantharaman S. Iyer"
      },
      {
        "authorId": "39552848",
        "name": "Anders Andreassen"
      },
      {
        "authorId": "3064807",
        "name": "Andrea Madotto"
      },
      {
        "authorId": "2065039862",
        "name": "Andrea Santilli"
      },
      {
        "authorId": "2169579494",
        "name": "Andreas Stuhlmuller"
      },
      {
        "authorId": "2253757717",
        "name": "Andrew M. Dai"
      },
      {
        "authorId": "2253744892",
        "name": "Andrew La"
      },
      {
        "authorId": "32322945",
        "name": "Andrew Kyle Lampinen"
      },
      {
        "authorId": "1380103052",
        "name": "Andy Zou"
      },
      {
        "authorId": "2253471334",
        "name": "Angela Jiang"
      },
      {
        "authorId": "13336152",
        "name": "Angelica Chen"
      },
      {
        "authorId": "2064058890",
        "name": "Anh Vuong"
      },
      {
        "authorId": "2110763559",
        "name": "Animesh Gupta"
      },
      {
        "authorId": "1411423941",
        "name": "Anna Gottardi"
      },
      {
        "authorId": "1596822208",
        "name": "Antonio Norelli"
      },
      {
        "authorId": "47851456",
        "name": "Anu Venkatesh"
      },
      {
        "authorId": "1396646913",
        "name": "Arash Gholamidavoodi"
      },
      {
        "authorId": "11997563",
        "name": "Arfa Tabassum"
      },
      {
        "authorId": "2253477913",
        "name": "Arul Menezes"
      },
      {
        "authorId": "1417449913",
        "name": "Arun Kirubarajan"
      },
      {
        "authorId": "3567738",
        "name": "A. Mullokandov"
      },
      {
        "authorId": "48229640",
        "name": "Ashish Sabharwal"
      },
      {
        "authorId": "103829114",
        "name": "Austin Herrick"
      },
      {
        "authorId": "1388010852",
        "name": "Avia Efrat"
      },
      {
        "authorId": "2253752451",
        "name": "Aykut Erdem"
      },
      {
        "authorId": "2169560863",
        "name": "Ayla Karakacs"
      },
      {
        "authorId": "2253506707",
        "name": "B. R. Roberts"
      },
      {
        "authorId": "25229391",
        "name": "B. S. Loe"
      },
      {
        "authorId": "2368067",
        "name": "Barret Zoph"
      },
      {
        "authorId": "2169579430",
        "name": "Bartlomiej Bojanowski"
      },
      {
        "authorId": "2169579156",
        "name": "Batuhan Ozyurt"
      },
      {
        "authorId": "2127328167",
        "name": "Behnam Hedayatnia"
      },
      {
        "authorId": "3007442",
        "name": "Behnam Neyshabur"
      },
      {
        "authorId": "2911198",
        "name": "Benjamin Inden"
      },
      {
        "authorId": "1405867539",
        "name": "Benno Stein"
      },
      {
        "authorId": "3407537",
        "name": "Berk Ekmekci"
      },
      {
        "authorId": "51583409",
        "name": "Bill Yuchen Lin"
      },
      {
        "authorId": "1759660",
        "name": "B. Howald"
      },
      {
        "authorId": "2253742181",
        "name": "Bryan Orinion"
      },
      {
        "authorId": "2113828270",
        "name": "Cameron Diao"
      },
      {
        "authorId": "2169579131",
        "name": "Cameron Dour"
      },
      {
        "authorId": "2253742142",
        "name": "Catherine Stinson"
      },
      {
        "authorId": "2080698457",
        "name": "Cedrick Argueta"
      },
      {
        "authorId": "2169282994",
        "name": "C'esar Ferri Ram'irez"
      },
      {
        "authorId": "2253649838",
        "name": "Chandan Singh"
      },
      {
        "authorId": "30465886",
        "name": "Charles Rathkopf"
      },
      {
        "authorId": "83262128",
        "name": "Chenlin Meng"
      },
      {
        "authorId": "2064619864",
        "name": "Chitta Baral"
      },
      {
        "authorId": "2115397918",
        "name": "Chiyu Wu"
      },
      {
        "authorId": "1763608",
        "name": "Chris Callison-Burch"
      },
      {
        "authorId": "1451646307",
        "name": "Chris Waites"
      },
      {
        "authorId": "2253721294",
        "name": "Christian Voigt"
      },
      {
        "authorId": "2250402802",
        "name": "Christopher D. Manning"
      },
      {
        "authorId": "2253742954",
        "name": "Christopher Potts"
      },
      {
        "authorId": "1381594105",
        "name": "Cindy Ramirez"
      },
      {
        "authorId": "2253680456",
        "name": "Clara E. Rivera"
      },
      {
        "authorId": "2056776870",
        "name": "Clemencia Siro"
      },
      {
        "authorId": "2402716",
        "name": "Colin Raffel"
      },
      {
        "authorId": "2169255641",
        "name": "Courtney Ashcraft"
      },
      {
        "authorId": "3360992",
        "name": "Cristina Garbacea"
      },
      {
        "authorId": "2311890249",
        "name": "Damien Sileo"
      },
      {
        "authorId": "69045302",
        "name": "Daniel H Garrette"
      },
      {
        "authorId": "3422872",
        "name": "Dan Hendrycks"
      },
      {
        "authorId": "2051801993",
        "name": "D. Kilman"
      },
      {
        "authorId": "2249759427",
        "name": "Dan Roth"
      },
      {
        "authorId": "2253572738",
        "name": "Daniel Freeman"
      },
      {
        "authorId": "1783281",
        "name": "Daniel Khashabi"
      },
      {
        "authorId": "2052679852",
        "name": "Daniel Levy"
      },
      {
        "authorId": "1802312462",
        "name": "D. Gonz'alez"
      },
      {
        "authorId": "6472480",
        "name": "Danielle R. Perszyk"
      },
      {
        "authorId": "39182747",
        "name": "Danny Hernandez"
      },
      {
        "authorId": "2255489905",
        "name": "Danqi Chen"
      },
      {
        "authorId": "7975935",
        "name": "Daphne Ippolito"
      },
      {
        "authorId": "32994625",
        "name": "D. Gilboa"
      },
      {
        "authorId": "35363891",
        "name": "David Dohan"
      },
      {
        "authorId": "97501513",
        "name": "D. Drakard"
      },
      {
        "authorId": "3046220",
        "name": "David Jurgens"
      },
      {
        "authorId": "2852125",
        "name": "Debajyoti Datta"
      },
      {
        "authorId": "2081806483",
        "name": "Deep Ganguli"
      },
      {
        "authorId": "51889641",
        "name": "Denis Emelin"
      },
      {
        "authorId": "2193599771",
        "name": "Denis Kleyko"
      },
      {
        "authorId": "2253742671",
        "name": "Deniz Yuret"
      },
      {
        "authorId": "2253841704",
        "name": "Derek Chen"
      },
      {
        "authorId": "1390031652",
        "name": "Derek Tam"
      },
      {
        "authorId": "3449411",
        "name": "Dieuwke Hupkes"
      },
      {
        "authorId": "2253641873",
        "name": "Diganta Misra"
      },
      {
        "authorId": "2169553550",
        "name": "Dilyar Buzan"
      },
      {
        "authorId": "51127600",
        "name": "Dimitri Coelho Mollo"
      },
      {
        "authorId": "2254124345",
        "name": "Diyi Yang"
      },
      {
        "authorId": "2253883759",
        "name": "Dong-Ho Lee"
      },
      {
        "authorId": "2253751766",
        "name": "Dylan Schrader"
      },
      {
        "authorId": "2362276",
        "name": "Ekaterina Shutova"
      },
      {
        "authorId": "8132903",
        "name": "E. D. Cubuk"
      },
      {
        "authorId": "153401294",
        "name": "Elad Segal"
      },
      {
        "authorId": "83195245",
        "name": "Eleanor Hagerman"
      },
      {
        "authorId": "2057742918",
        "name": "Elizabeth Barnes"
      },
      {
        "authorId": "1602681246",
        "name": "E. Donoway"
      },
      {
        "authorId": "2949185",
        "name": "Ellie Pavlick"
      },
      {
        "authorId": "1796150",
        "name": "E. Rodol\u00e0"
      },
      {
        "authorId": "2253753565",
        "name": "Emma Lam"
      },
      {
        "authorId": "2253520497",
        "name": "Eric Chu"
      },
      {
        "authorId": "2090511698",
        "name": "Eric Tang"
      },
      {
        "authorId": "152330322",
        "name": "Erkut Erdem"
      },
      {
        "authorId": "48025720",
        "name": "Ernie Chang"
      },
      {
        "authorId": "2253469028",
        "name": "Ethan A. Chi"
      },
      {
        "authorId": "52136425",
        "name": "Ethan Dyer"
      },
      {
        "authorId": "87911177",
        "name": "E. Jerzak"
      },
      {
        "authorId": "2047591327",
        "name": "Ethan Kim"
      },
      {
        "authorId": "102184064",
        "name": "Eunice Engefu Manyasi"
      },
      {
        "authorId": "23913513",
        "name": "Evgenii Zheltonozhskii"
      },
      {
        "authorId": "2253581486",
        "name": "Fanyue Xia"
      },
      {
        "authorId": "16851583",
        "name": "F. Siar"
      },
      {
        "authorId": "2126497260",
        "name": "Fernando Mart'inez-Plumed"
      },
      {
        "authorId": "2169302468",
        "name": "Francesca Happ'e"
      },
      {
        "authorId": "1565641737",
        "name": "Fran\u00e7ois Chollet"
      },
      {
        "authorId": "2047004093",
        "name": "Frieda Rong"
      },
      {
        "authorId": "2159632445",
        "name": "Gaurav Mishra"
      },
      {
        "authorId": "9162688",
        "name": "Genta Indra Winata"
      },
      {
        "authorId": "2253752804",
        "name": "Gerard de Melo"
      },
      {
        "authorId": "2067996",
        "name": "Germ\u00e1n Kruszewski"
      },
      {
        "authorId": "50213542",
        "name": "Giambattista Parascandolo"
      },
      {
        "authorId": "2253744058",
        "name": "Giorgio Mariani"
      },
      {
        "authorId": "2143229090",
        "name": "Gloria Xinyue Wang"
      },
      {
        "authorId": "2169561208",
        "name": "Gonzalo Jaimovitch-L'opez"
      },
      {
        "authorId": "2253742347",
        "name": "Gregor Betz"
      },
      {
        "authorId": "2284681044",
        "name": "Guy Gur-Ari"
      },
      {
        "authorId": "2148236997",
        "name": "Hana Galijasevic"
      },
      {
        "authorId": "2254029693",
        "name": "Hannah Kim"
      },
      {
        "authorId": "2516777",
        "name": "Hannah Rashkin"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      },
      {
        "authorId": "18138802",
        "name": "Harsh Mehta"
      },
      {
        "authorId": "81356189",
        "name": "H. Bogar"
      },
      {
        "authorId": "66652934",
        "name": "Henry Shevlin"
      },
      {
        "authorId": "2261745622",
        "name": "Hinrich Schutze"
      },
      {
        "authorId": "2252055381",
        "name": "H. Yakura"
      },
      {
        "authorId": "2253956610",
        "name": "Hongming Zhang"
      },
      {
        "authorId": "2084554416",
        "name": "Hugh Mee Wong"
      },
      {
        "authorId": "2175479575",
        "name": "Ian Ng"
      },
      {
        "authorId": "146990369",
        "name": "Isaac Noble"
      },
      {
        "authorId": "2253757620",
        "name": "Jaap Jumelet"
      },
      {
        "authorId": "67288781",
        "name": "Jack Geissinger"
      },
      {
        "authorId": "1583434563",
        "name": "John Kernion"
      },
      {
        "authorId": "2052366271",
        "name": "Jacob Hilton"
      },
      {
        "authorId": "2253808003",
        "name": "Jaehoon Lee"
      },
      {
        "authorId": "1843342",
        "name": "J. Fisac"
      },
      {
        "authorId": "2253938438",
        "name": "James B. Simon"
      },
      {
        "authorId": "39465522",
        "name": "James Koppel"
      },
      {
        "authorId": "2254106317",
        "name": "James Zheng"
      },
      {
        "authorId": "2240530524",
        "name": "James Zou"
      },
      {
        "authorId": "2169553526",
        "name": "Jan Koco'n"
      },
      {
        "authorId": "2148444557",
        "name": "Jana Thompson"
      },
      {
        "authorId": "2253742801",
        "name": "Janelle Wingfield"
      },
      {
        "authorId": "2053807409",
        "name": "Jared Kaplan"
      },
      {
        "authorId": "2133262330",
        "name": "Jarema Radom"
      },
      {
        "authorId": "1407546424",
        "name": "Jascha Narain Sohl-Dickstein"
      },
      {
        "authorId": "80842917",
        "name": "Jason Phang"
      },
      {
        "authorId": "2253952872",
        "name": "Jason Wei"
      },
      {
        "authorId": "2965424",
        "name": "J. Yosinski"
      },
      {
        "authorId": "2848048",
        "name": "Jekaterina Novikova"
      },
      {
        "authorId": "2169562378",
        "name": "Jelle Bosscher"
      },
      {
        "authorId": "2253582254",
        "name": "Jennifer Marsh"
      },
      {
        "authorId": "2253951784",
        "name": "Jeremy Kim"
      },
      {
        "authorId": "2169553864",
        "name": "Jeroen Taal"
      },
      {
        "authorId": "2253555236",
        "name": "Jesse Engel"
      },
      {
        "authorId": "122367036",
        "name": "Jesujoba Oluwadara Alabi"
      },
      {
        "authorId": "2254142854",
        "name": "Jiacheng Xu"
      },
      {
        "authorId": "2250159648",
        "name": "Jiaming Song"
      },
      {
        "authorId": "82706161",
        "name": "Jillian Tang"
      },
      {
        "authorId": "147175687",
        "name": "Jane W Waweru"
      },
      {
        "authorId": "31502027",
        "name": "John Burden"
      },
      {
        "authorId": "2253928282",
        "name": "John Miller"
      },
      {
        "authorId": "2075220382",
        "name": "John U. Balis"
      },
      {
        "authorId": "2253742887",
        "name": "Jonathan Batchelder"
      },
      {
        "authorId": "1750652",
        "name": "Jonathan Berant"
      },
      {
        "authorId": "2146695800",
        "name": "Jorg Frohberg"
      },
      {
        "authorId": "120419790",
        "name": "Jos Rozen"
      },
      {
        "authorId": "1398777358",
        "name": "J. Hern\u00e1ndez-Orallo"
      },
      {
        "authorId": "2169583246",
        "name": "Joseph Boudeman"
      },
      {
        "authorId": "2253742339",
        "name": "Joseph Guerr"
      },
      {
        "authorId": "2169419125",
        "name": "Joseph Jones"
      },
      {
        "authorId": "2250220321",
        "name": "Joshua B. Tenenbaum"
      },
      {
        "authorId": "38219739",
        "name": "Joshua S. Rule"
      },
      {
        "authorId": "119803865",
        "name": "Joyce Chua"
      },
      {
        "authorId": "2007286374",
        "name": "Kamil Kanclerz"
      },
      {
        "authorId": "2924113",
        "name": "Karen Livescu"
      },
      {
        "authorId": "48778049",
        "name": "K. Krauth"
      },
      {
        "authorId": "145916630",
        "name": "Karthik Gopalakrishnan"
      },
      {
        "authorId": "2169583196",
        "name": "Katerina Ignatyeva"
      },
      {
        "authorId": "2253742520",
        "name": "K. Markert"
      },
      {
        "authorId": "4834571",
        "name": "Kaustubh D. Dhole"
      },
      {
        "authorId": "1700980",
        "name": "Kevin Gimpel"
      },
      {
        "authorId": "98151280",
        "name": "Kevin Omondi"
      },
      {
        "authorId": "2034344309",
        "name": "K. Mathewson"
      },
      {
        "authorId": "2169579399",
        "name": "Kristen Chiafullo"
      },
      {
        "authorId": "94055272",
        "name": "Ksenia Shkaruta"
      },
      {
        "authorId": "50812160",
        "name": "K. Shridhar"
      },
      {
        "authorId": "2049410219",
        "name": "Kyle McDonell"
      },
      {
        "authorId": "46666605",
        "name": "Kyle Richardson"
      },
      {
        "authorId": "2049583158",
        "name": "Laria Reynolds"
      },
      {
        "authorId": "2027599537",
        "name": "Leo Gao"
      },
      {
        "authorId": "72436283",
        "name": "Li Zhang"
      },
      {
        "authorId": "83863037",
        "name": "Liam Dugan"
      },
      {
        "authorId": "3444092",
        "name": "Lianhui Qin"
      },
      {
        "authorId": "1572944977",
        "name": "Lidia Contreras-Ochando"
      },
      {
        "authorId": "49933077",
        "name": "Louis-philippe Morency"
      },
      {
        "authorId": "2253396046",
        "name": "Luca Moschella"
      },
      {
        "authorId": "134309836",
        "name": "Luca Lam"
      },
      {
        "authorId": "2253759995",
        "name": "Lucy Noble"
      },
      {
        "authorId": "2253541812",
        "name": "Ludwig Schmidt"
      },
      {
        "authorId": "2253917827",
        "name": "Luheng He"
      },
      {
        "authorId": "2169579445",
        "name": "Luis Oliveros Col'on"
      },
      {
        "authorId": "2096458",
        "name": "Luke Metz"
      },
      {
        "authorId": "2126865294",
        "name": "Lutfi Kerem cSenel"
      },
      {
        "authorId": "40377863",
        "name": "Maarten Bosma"
      },
      {
        "authorId": "2729164",
        "name": "Maarten Sap"
      },
      {
        "authorId": "41096186",
        "name": "Maartje ter Hoeve"
      },
      {
        "authorId": "77751476",
        "name": "Maheen Farooqi"
      },
      {
        "authorId": "1779225",
        "name": "Manaal Faruqui"
      },
      {
        "authorId": "16787428",
        "name": "Mantas Mazeika"
      },
      {
        "authorId": "2169579697",
        "name": "Marco Baturan"
      },
      {
        "authorId": "2202507568",
        "name": "Marco Marelli"
      },
      {
        "authorId": "1388826344",
        "name": "Marco Maru"
      },
      {
        "authorId": "2253743804",
        "name": "Maria Jose Ram\u2019irez Quintana"
      },
      {
        "authorId": "46445780",
        "name": "M. Tolkiehn"
      },
      {
        "authorId": "24068173",
        "name": "Mario Giulianelli"
      },
      {
        "authorId": "2253999321",
        "name": "Martha Lewis"
      },
      {
        "authorId": "3046200",
        "name": "Martin Potthast"
      },
      {
        "authorId": "2240527814",
        "name": "Matthew L. Leavitt"
      },
      {
        "authorId": "145072133",
        "name": "Matthias Hagen"
      },
      {
        "authorId": "2072397293",
        "name": "M. Schubert"
      },
      {
        "authorId": "2399139",
        "name": "Medina Baitemirova"
      },
      {
        "authorId": "2253743368",
        "name": "Melody Arnaud"
      },
      {
        "authorId": "1410273721",
        "name": "M. McElrath"
      },
      {
        "authorId": "2253753325",
        "name": "Michael A. Yee"
      },
      {
        "authorId": "2253473243",
        "name": "Michael Cohen"
      },
      {
        "authorId": "2253764442",
        "name": "Michael Gu"
      },
      {
        "authorId": "51260702",
        "name": "Michael Ivanitskiy"
      },
      {
        "authorId": "2169579231",
        "name": "Michael Starritt"
      },
      {
        "authorId": "2253690372",
        "name": "M. Strube"
      },
      {
        "authorId": "2169561132",
        "name": "Michal Swkedrowski"
      },
      {
        "authorId": "2253475117",
        "name": "Michele Bevilacqua"
      },
      {
        "authorId": "19168196",
        "name": "Michihiro Yasunaga"
      },
      {
        "authorId": "26688118",
        "name": "Mihir Kale"
      },
      {
        "authorId": "2253762090",
        "name": "Mike Cain"
      },
      {
        "authorId": "2254456411",
        "name": "Mimee Xu"
      },
      {
        "authorId": "51903517",
        "name": "Mirac Suzgun"
      },
      {
        "authorId": "2253473026",
        "name": "Mitch Walker"
      },
      {
        "authorId": "2074227875",
        "name": "Monica Tiwari"
      },
      {
        "authorId": "2253762115",
        "name": "Mohit Bansal"
      },
      {
        "authorId": "2035522904",
        "name": "Moin Aminnaseri"
      },
      {
        "authorId": "22245981",
        "name": "Mor Geva"
      },
      {
        "authorId": "151114702",
        "name": "Mozhdeh Gheini"
      },
      {
        "authorId": "2007825839",
        "name": "T. MukundVarma"
      },
      {
        "authorId": "2253599901",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "2253474896",
        "name": "Nathan A. Chi"
      },
      {
        "authorId": "40221187",
        "name": "Nayeon Lee"
      },
      {
        "authorId": "2169562637",
        "name": "Neta Gur-Ari Krakover"
      },
      {
        "authorId": "2070244342",
        "name": "Nicholas Cameron"
      },
      {
        "authorId": "2253607509",
        "name": "Nicholas Roberts"
      },
      {
        "authorId": "2249758958",
        "name": "Nick Doiron"
      },
      {
        "authorId": "2253719808",
        "name": "Nicole Martinez"
      },
      {
        "authorId": "10666396",
        "name": "Nikita Nangia"
      },
      {
        "authorId": "40015417",
        "name": "Niklas Deckers"
      },
      {
        "authorId": "2037383772",
        "name": "Niklas Muennighoff"
      },
      {
        "authorId": "2844898",
        "name": "N. Keskar"
      },
      {
        "authorId": "2121286996",
        "name": "Niveditha Iyer"
      },
      {
        "authorId": "40832517",
        "name": "Noah Constant"
      },
      {
        "authorId": "22640071",
        "name": "Noah Fiedel"
      },
      {
        "authorId": "2253657042",
        "name": "Nuan Wen"
      },
      {
        "authorId": "2253742723",
        "name": "Oliver Zhang"
      },
      {
        "authorId": "2138307026",
        "name": "Omar Agha"
      },
      {
        "authorId": "2169583261",
        "name": "Omar Elbaghdadi"
      },
      {
        "authorId": "2253752918",
        "name": "Omer Levy"
      },
      {
        "authorId": "47107786",
        "name": "Owain Evans"
      },
      {
        "authorId": "145163583",
        "name": "Pablo Antonio Moreno Casares"
      },
      {
        "authorId": "1453712240",
        "name": "P. Doshi"
      },
      {
        "authorId": "40539650",
        "name": "Pascale Fung"
      },
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "2039154",
        "name": "Paul Vicol"
      },
      {
        "authorId": "1805993128",
        "name": "Pegah Alipoormolabashi"
      },
      {
        "authorId": "2253656797",
        "name": "Peiyuan Liao"
      },
      {
        "authorId": "2249641250",
        "name": "Percy Liang"
      },
      {
        "authorId": "2113642717",
        "name": "Peter Chang"
      },
      {
        "authorId": "2654106",
        "name": "P. Eckersley"
      },
      {
        "authorId": "41022736",
        "name": "Phu Mon Htut"
      },
      {
        "authorId": "46221673",
        "name": "P. Hwang"
      },
      {
        "authorId": "1413772109",
        "name": "P. Milkowski"
      },
      {
        "authorId": "2414098",
        "name": "P. Patil"
      },
      {
        "authorId": "1713436",
        "name": "Pouya Pezeshkpour"
      },
      {
        "authorId": "2051972539",
        "name": "Priti Oli"
      },
      {
        "authorId": "2286285875",
        "name": "Qiaozhu Mei"
      },
      {
        "authorId": "1904906987",
        "name": "Qing Lyu"
      },
      {
        "authorId": "2256591945",
        "name": "Qinlang Chen"
      },
      {
        "authorId": "2004401966",
        "name": "Rabin Banjade"
      },
      {
        "authorId": "2238019263",
        "name": "Rachel Etta Rudolph"
      },
      {
        "authorId": "39303368",
        "name": "Raefer Gabriel"
      },
      {
        "authorId": "2169583577",
        "name": "Rahel Habacker"
      },
      {
        "authorId": "2253742845",
        "name": "Ramon Risco"
      },
      {
        "authorId": "2249763478",
        "name": "Raphael Milliere"
      },
      {
        "authorId": "144914264",
        "name": "Rhythm Garg"
      },
      {
        "authorId": "2056119463",
        "name": "Richard Barnes"
      },
      {
        "authorId": "2278009",
        "name": "R. Saurous"
      },
      {
        "authorId": "83976651",
        "name": "Riku Arakawa"
      },
      {
        "authorId": "2143515065",
        "name": "Robbe Raymaekers"
      },
      {
        "authorId": "2253563490",
        "name": "Robert Frank"
      },
      {
        "authorId": "2169583539",
        "name": "Rohan Sikand"
      },
      {
        "authorId": "39068839",
        "name": "Roman Novak"
      },
      {
        "authorId": "2143205525",
        "name": "Roman Sitelew"
      },
      {
        "authorId": "39227408",
        "name": "Ronan Le Bras"
      },
      {
        "authorId": "48757909",
        "name": "Rosanne Liu"
      },
      {
        "authorId": "2169442080",
        "name": "Rowan Jacobs"
      },
      {
        "authorId": "2255464042",
        "name": "Rui Zhang"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      },
      {
        "authorId": "2121293578",
        "name": "Ryan Chi"
      },
      {
        "authorId": "2110680870",
        "name": "Ryan Lee"
      },
      {
        "authorId": "2096414946",
        "name": "Ryan Stovall"
      },
      {
        "authorId": "2131107966",
        "name": "Ryan Teehan"
      },
      {
        "authorId": "2170120947",
        "name": "Rylan Yang"
      },
      {
        "authorId": "2256771265",
        "name": "Sahib Singh"
      },
      {
        "authorId": "2253519805",
        "name": "Saif Mohammad"
      },
      {
        "authorId": "51177395",
        "name": "Sajant Anand"
      },
      {
        "authorId": "14149388",
        "name": "Sam Dillavou"
      },
      {
        "authorId": "88728159",
        "name": "Sam Shleifer"
      },
      {
        "authorId": "2844243",
        "name": "Sam Wiseman"
      },
      {
        "authorId": "46173414",
        "name": "Samuel Gruetter"
      },
      {
        "authorId": "2137213671",
        "name": "Samuel R. Bowman"
      },
      {
        "authorId": "2601641",
        "name": "S. Schoenholz"
      },
      {
        "authorId": "2254097408",
        "name": "Sanghyun Han"
      },
      {
        "authorId": "1412838170",
        "name": "Sanjeev Kwatra"
      },
      {
        "authorId": "121266477",
        "name": "Sarah A. Rous"
      },
      {
        "authorId": "3022427",
        "name": "Sarik Ghazarian"
      },
      {
        "authorId": "2143032877",
        "name": "Sayan Ghosh"
      },
      {
        "authorId": "2253475225",
        "name": "Sean Casey"
      },
      {
        "authorId": "32306415",
        "name": "Sebastian Bischoff"
      },
      {
        "authorId": "3159346",
        "name": "Sebastian Gehrmann"
      },
      {
        "authorId": "2067227206",
        "name": "Sebastian Schuster"
      },
      {
        "authorId": "2162733",
        "name": "Sepideh Sadeghi"
      },
      {
        "authorId": "1832987156",
        "name": "Shadi S. Hamdan"
      },
      {
        "authorId": "2111057669",
        "name": "Sharon Zhou"
      },
      {
        "authorId": "2253588064",
        "name": "Shashank Srivastava"
      },
      {
        "authorId": "2113258914",
        "name": "Sherry Shi"
      },
      {
        "authorId": "2108410562",
        "name": "Shikhar Singh"
      },
      {
        "authorId": "30462410",
        "name": "Shima Asaadi"
      },
      {
        "authorId": "2253699903",
        "name": "S. Gu"
      },
      {
        "authorId": "2169561291",
        "name": "Shubh Pachchigar"
      },
      {
        "authorId": "2634203",
        "name": "Shubham Toshniwal"
      },
      {
        "authorId": "33145619",
        "name": "Shyam Upadhyay"
      },
      {
        "authorId": "2169422265",
        "name": "Shyamolima Debnath"
      },
      {
        "authorId": "2944868",
        "name": "Siamak Shakeri"
      },
      {
        "authorId": "2169583327",
        "name": "Simon Thormeyer"
      },
      {
        "authorId": "1972186",
        "name": "S. Melzi"
      },
      {
        "authorId": "2256152032",
        "name": "Siva Reddy"
      },
      {
        "authorId": "118798524",
        "name": "S. Makini"
      },
      {
        "authorId": "2255599173",
        "name": "Soo-Hwan Lee"
      },
      {
        "authorId": "5910521",
        "name": "Spencer Bradley Torene"
      },
      {
        "authorId": "1659275411",
        "name": "Sriharsha Hatwar"
      },
      {
        "authorId": "2250013696",
        "name": "S. Dehaene"
      },
      {
        "authorId": "102987414",
        "name": "Stefan Divic"
      },
      {
        "authorId": "2490652",
        "name": "Stefano Ermon"
      },
      {
        "authorId": "103476203",
        "name": "Stella Biderman"
      },
      {
        "authorId": "2253840098",
        "name": "Stephanie Lin"
      },
      {
        "authorId": "2253781922",
        "name": "Stephen Prasad"
      },
      {
        "authorId": "2238331992",
        "name": "Steven T Piantadosi"
      },
      {
        "authorId": "1692491",
        "name": "Stuart M. Shieber"
      },
      {
        "authorId": "2169554101",
        "name": "Summer Misherghi"
      },
      {
        "authorId": "2886725",
        "name": "S. Kiritchenko"
      },
      {
        "authorId": "1817207",
        "name": "Swaroop Mishra"
      },
      {
        "authorId": "51223875",
        "name": "Tal Linzen"
      },
      {
        "authorId": "32303439",
        "name": "Tal Schuster"
      },
      {
        "authorId": "2149201962",
        "name": "Tao Li"
      },
      {
        "authorId": "2256865416",
        "name": "Tao Yu"
      },
      {
        "authorId": "2253522954",
        "name": "Tariq Ali"
      },
      {
        "authorId": "2253575091",
        "name": "Tatsunori Hashimoto"
      },
      {
        "authorId": "2134514770",
        "name": "Te-Lin Wu"
      },
      {
        "authorId": "88367918",
        "name": "T. Desbordes"
      },
      {
        "authorId": "2169561552",
        "name": "Theodore Rothschild"
      },
      {
        "authorId": "145127341",
        "name": "Thomas Phan"
      },
      {
        "authorId": "2255375735",
        "name": "Tianle Wang"
      },
      {
        "authorId": "101592141",
        "name": "Tiberius Nkinyili"
      },
      {
        "authorId": "32246932",
        "name": "Timo Schick"
      },
      {
        "authorId": "98873736",
        "name": "T. Kornev"
      },
      {
        "authorId": "2169579934",
        "name": "T. Tunduny"
      },
      {
        "authorId": "2697953",
        "name": "Tobias Gerstenberg"
      },
      {
        "authorId": "2107372733",
        "name": "T. Chang"
      },
      {
        "authorId": "10729963",
        "name": "Trishala Neeraj"
      },
      {
        "authorId": "2236429",
        "name": "Tushar Khot"
      },
      {
        "authorId": "2253757587",
        "name": "Tyler Shultz"
      },
      {
        "authorId": "50482645",
        "name": "Uri Shaham"
      },
      {
        "authorId": "40055795",
        "name": "Vedant Misra"
      },
      {
        "authorId": "2243444130",
        "name": "Vera Demberg"
      },
      {
        "authorId": "2169580169",
        "name": "Victoria Nyamai"
      },
      {
        "authorId": "24025563",
        "name": "Vikas Raunak"
      },
      {
        "authorId": "96641652",
        "name": "V. Ramasesh"
      },
      {
        "authorId": "2670978",
        "name": "Vinay Uday Prabhu"
      },
      {
        "authorId": "2044959912",
        "name": "Vishakh Padmakumar"
      },
      {
        "authorId": "3052879",
        "name": "Vivek Srikumar"
      },
      {
        "authorId": "26958176",
        "name": "W. Fedus"
      },
      {
        "authorId": "2058848938",
        "name": "W. Saunders"
      },
      {
        "authorId": "2115212355",
        "name": "William Zhang"
      },
      {
        "authorId": "2143514461",
        "name": "Wout Vossen"
      },
      {
        "authorId": "2256826104",
        "name": "Xiang Ren"
      },
      {
        "authorId": "2253762061",
        "name": "Xiaoyu Tong"
      },
      {
        "authorId": "1500662261",
        "name": "Xinran Zhao"
      },
      {
        "authorId": "2255396948",
        "name": "Xinyi Wu"
      },
      {
        "authorId": "2144058688",
        "name": "Xudong Shen"
      },
      {
        "authorId": "3261470",
        "name": "Yadollah Yaghoobzadeh"
      },
      {
        "authorId": "3051598",
        "name": "Yair Lakretz"
      },
      {
        "authorId": "2258804099",
        "name": "Yangqiu Song"
      },
      {
        "authorId": "12383244",
        "name": "Yasaman Bahri"
      },
      {
        "authorId": "2253903625",
        "name": "Yejin Choi"
      },
      {
        "authorId": "2108652034",
        "name": "Yichi Yang"
      },
      {
        "authorId": "2256343174",
        "name": "Yiding Hao"
      },
      {
        "authorId": "2253813584",
        "name": "Yifu Chen"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "2118739951",
        "name": "Yu Hou"
      },
      {
        "authorId": "2118739951",
        "name": "Yu Hou"
      },
      {
        "authorId": "1486307451",
        "name": "Yuntao Bai"
      },
      {
        "authorId": "2169561434",
        "name": "Zachary Seid"
      },
      {
        "authorId": "2254023165",
        "name": "Zhuoye Zhao"
      },
      {
        "authorId": "37571937",
        "name": "Zijian Wang"
      },
      {
        "authorId": "1390877819",
        "name": "Zijie J. Wang"
      },
      {
        "authorId": "2255392870",
        "name": "Zirui Wang"
      },
      {
        "authorId": "2253894469",
        "name": "Ziyi Wu"
      }
    ]
  },
  "233296808": {
    "paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
    "externalIds": {
      "DBLP": "journals/corr/abs-2104-08691",
      "ArXiv": "2104.08691",
      "ACL": "2021.emnlp-main.243",
      "DOI": "10.18653/v1/2021.emnlp-main.243",
      "CorpusId": 233296808
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
    "abstract": "In this work, we explore \u201cprompt tuning,\u201d a simple yet effective mechanism for learning \u201csoft prompts\u201d to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3\u2019s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \u201ccloses the gap\u201d and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \u201cprefix tuning\u201d of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \u201cprompt ensembling.\u201d We release code and model checkpoints to reproduce our experiments.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "referenceCount": 61,
    "citationCount": 3206,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144104130",
        "name": "Brian Lester"
      },
      {
        "authorId": "1388360943",
        "name": "Rami Al-Rfou"
      },
      {
        "authorId": "40832517",
        "name": "Noah Constant"
      }
    ]
  },
  "254408772": {
    "paperId": "d03a9b2a0e090cc9fd2ba0a457ecea35372f1018",
    "externalIds": {
      "ArXiv": "2212.04037",
      "DBLP": "journals/corr/abs-2212-04037",
      "DOI": "10.48550/arXiv.2212.04037",
      "CorpusId": 254408772
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Demystifying Prompts in Language Models via Perplexity Estimation",
    "abstract": "Language models can be prompted to perform a wide variety of zero- and few-shot learning problems. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens or how to pick the best prompts. In this work, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt is, the better the prompt is able to perform the task. As a result, we devise a method for creating prompts: (1) automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and (2) choose the lowest perplexity prompts to get significant gains in performance.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 48,
    "citationCount": 155,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1821892",
        "name": "Hila Gonen"
      },
      {
        "authorId": "1900163",
        "name": "Srini Iyer"
      },
      {
        "authorId": "3443287",
        "name": "Terra Blevins"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ]
  },
  "254853659": {
    "paperId": "6f4cc536f9ed83d0dbf7e919dc609be12aa0848a",
    "externalIds": {
      "ACL": "2023.acl-long.806",
      "ArXiv": "2212.09689",
      "DBLP": "conf/acl/HonovichSLS23",
      "DOI": "10.48550/arXiv.2212.09689",
      "CorpusId": 254853659
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor",
    "abstract": "Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 43,
    "citationCount": 300,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1754700648",
        "name": "Or Honovich"
      },
      {
        "authorId": "2073456043",
        "name": "Thomas Scialom"
      },
      {
        "authorId": "39455775",
        "name": "Omer Levy"
      },
      {
        "authorId": "32246932",
        "name": "Timo Schick"
      }
    ]
  },
  "254366640": {
    "paperId": "37255b091317aedc6854383104b3343e67ab5c80",
    "externalIds": {
      "DBLP": "journals/corr/abs-2212-03813",
      "ArXiv": "2212.03813",
      "DOI": "10.48550/arXiv.2212.03813",
      "CorpusId": 254366640
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Robustness of Learning from Task Instructions",
    "abstract": "Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-specific examples. This paradigm seriously hinders the development of task generalization since preparing a task-specific example set is costly. To build a system that can quickly and easily generalize to new tasks, task instructions have been adopted as an emerging trend of supervision recently. These instructions give the model the definition of the task and allow the model to output the appropriate answer based on the instructions and inputs. However, task instructions are often expressed in different forms, which can be interpreted from two threads: first, some instructions are short sentences and are pretrained language model (PLM) oriented, such as prompts, while other instructions are paragraphs and are human-oriented, such as those in Amazon MTurk; second, different end-users very likely explain the same task with instructions of different textual expressions. A robust system for task generalization should be able to handle any new tasks regardless of the variability of instructions. However, the system robustness in dealing with instruction-driven task generalization is still unexplored. This work investigates the system robustness when the instructions of new tasks are (i) manipulated, (ii) paraphrased, or (iii) from different levels of conciseness. To our knowledge, this is the first work that systematically studies how robust a PLM is when it is supervised by instructions with different factors of variability.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 33,
    "citationCount": 25,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "50771069",
        "name": "Jiasheng Gu"
      },
      {
        "authorId": "2143534669",
        "name": "Hanzi Xu"
      },
      {
        "authorId": "1382526237",
        "name": "Liang Nie"
      },
      {
        "authorId": "40483594",
        "name": "Wenpeng Yin"
      }
    ]
  },
  "259203613": {
    "paperId": "b0bac6aca93021105c8a4f165184a097a249fbce",
    "externalIds": {
      "ArXiv": "2306.11270",
      "DBLP": "journals/corr/abs-2306-11270",
      "DOI": "10.48550/arXiv.2306.11270",
      "CorpusId": 259203613
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
    "abstract": "Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re-phrasings. We propose a simple method to mitigate this issue by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions. We show that this method consistently improves the robustness of instruction-tuned models.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 0,
    "citationCount": 36,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2175443520",
        "name": "Jiu Sun"
      },
      {
        "authorId": "2008165628",
        "name": "Chantal Shaib"
      },
      {
        "authorId": "2111879324",
        "name": "Byron Wallace"
      }
    ]
  },
  "266693922": {
    "paperId": "8dce168f723158b771b526401113064c36fc875e",
    "externalIds": {
      "DBLP": "journals/tacl/MizrahiKMDSS24a",
      "ArXiv": "2401.00595",
      "DOI": "10.1162/tacl_a_00681",
      "CorpusId": 266693922
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
    "abstract": "Abstract Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a large-scale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 53,
    "citationCount": 69,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1396412520",
        "name": "Moran Mizrahi"
      },
      {
        "authorId": "2277302614",
        "name": "Guy Kaplan"
      },
      {
        "authorId": "2082022055",
        "name": "Daniel Malkin"
      },
      {
        "authorId": "3372941",
        "name": "Rotem Dror"
      },
      {
        "authorId": "1805894",
        "name": "Dafna Shahaf"
      },
      {
        "authorId": "2126417012",
        "name": "Gabriel Stanovsky"
      }
    ]
  },
  "221340941": {
    "paperId": "c6d38e105562ae0a5d9b21fb4333212f36a3e041",
    "externalIds": {
      "DBLP": "journals/corr/abs-2008-12009",
      "ArXiv": "2008.12009",
      "MAG": "3080855795",
      "DOI": "10.1145/3485766",
      "CorpusId": 221340941
    },
    "publicationVenue": {
      "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
      "name": "ACM Computing Surveys",
      "type": "journal",
      "alternate_names": [
        "ACM Comput Surv"
      ],
      "issn": "0360-0300",
      "url": "http://www.acm.org/pubs/surveys/",
      "alternate_urls": [
        "http://portal.acm.org/csur",
        "https://csur.acm.org/",
        "http://csur.acm.org/"
      ]
    },
    "title": "A Survey of Evaluation Metrics Used for NLG Systems",
    "abstract": "In the last few years, a large number of automatic evaluation metrics have been proposed for evaluating Natural Language Generation (NLG) systems. The rapid development and adoption of such automatic evaluation metrics in a relatively short time has created the need for a survey of these metrics. In this survey, we (i) highlight the challenges in automatically evaluating NLG systems, (ii) propose a coherent taxonomy for organising existing evaluation metrics, (iii) briefly describe different existing metrics, and finally (iv) discuss studies criticising the use of automatic evaluation metrics. We then conclude the article highlighting promising future directions of research.",
    "venue": "ACM Computing Surveys",
    "year": 2020,
    "referenceCount": 213,
    "citationCount": 194,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145338991",
        "name": "Ananya B. Sai"
      },
      {
        "authorId": "1389549528",
        "name": "Akash Kumar Mohankumar"
      },
      {
        "authorId": "2361078",
        "name": "Mitesh M. Khapra"
      }
    ]
  },
  "259129398": {
    "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
    "externalIds": {
      "ArXiv": "2306.05685",
      "DBLP": "journals/corr/abs-2306-05685",
      "DOI": "10.48550/arXiv.2306.05685",
      "CorpusId": 259129398
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
    "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 59,
    "citationCount": 2465,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2149970173",
        "name": "Lianmin Zheng"
      },
      {
        "authorId": "2537924",
        "name": "Wei-Lin Chiang"
      },
      {
        "authorId": "2209360681",
        "name": "Ying Sheng"
      },
      {
        "authorId": "92721493",
        "name": "Siyuan Zhuang"
      },
      {
        "authorId": "1390573666",
        "name": "Zhanghao Wu"
      },
      {
        "authorId": "2152482391",
        "name": "Yonghao Zhuang"
      },
      {
        "authorId": "143872641",
        "name": "Zi Lin"
      },
      {
        "authorId": "2141335450",
        "name": "Zhuohan Li"
      },
      {
        "authorId": "2117961435",
        "name": "Dacheng Li"
      },
      {
        "authorId": "143977260",
        "name": "E. Xing"
      },
      {
        "authorId": "145140331",
        "name": "Haotong Zhang"
      },
      {
        "authorId": "49988044",
        "name": "Joseph E. Gonzalez"
      },
      {
        "authorId": "2055174324",
        "name": "Ion Stoica"
      }
    ]
  },
  "261076362": {
    "paperId": "9a4765547cb43ab221fe262df7405f6795557d8c",
    "externalIds": {
      "ACL": "2024.naacl-long.139",
      "DBLP": "journals/corr/abs-2308-11696",
      "ArXiv": "2308.11696",
      "DOI": "10.48550/arXiv.2308.11696",
      "CorpusId": 261076362
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Efficient Benchmarking (of Language Models)",
    "abstract": "The increasing versatility of language models (LMs) has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs, extending to thousands of GPU hours per model. However, the efficiency aspect of these evaluation efforts had raised little discussion in the literature.In this work, we present the problem of Efficient Benchmarking, namely, intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case, we investigate how different benchmark design choices affect the computation-reliability trade-off. We propose to evaluate the reliability of such decisions, by using a new measure \u2013 Decision Impact on Reliability, DIoR for short.We find, for example, that a benchmark leader may change by merely removing a low-ranked model from the benchmark, and observe that a correct benchmark ranking can be obtained by considering only a fraction of the evaluation examples.Based on our findings, we outline a set of concrete recommendations for efficient benchmark design and utilization practices. To take a step further, we use our findings to propose an evaluation algorithm, that, when applied to the HELM benchmark, leads to dramatic cost savings with minimal loss of benchmark reliability, often reducing computation by x100 or more.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 90,
    "citationCount": 17,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "102376484",
        "name": "Yotam Perlitz"
      },
      {
        "authorId": "2072249334",
        "name": "Elron Bandel"
      },
      {
        "authorId": "48835746",
        "name": "Ariel Gera"
      },
      {
        "authorId": "1454512761",
        "name": "Ofir Arviv"
      },
      {
        "authorId": "1402680837",
        "name": "L. Ein-Dor"
      },
      {
        "authorId": "1734246",
        "name": "Eyal Shnarch"
      },
      {
        "authorId": "1766595",
        "name": "N. Slonim"
      },
      {
        "authorId": "1397653860",
        "name": "Michal Shmueli-Scheuer"
      },
      {
        "authorId": "41019330",
        "name": "Leshem Choshen"
      }
    ]
  },
  "262045288": {
    "paperId": "d4085ae0f004624a3141734d3a88a9ebbc803a55",
    "externalIds": {
      "ACL": "2024.eacl-long.95",
      "DBLP": "journals/corr/abs-2309-08638",
      "ArXiv": "2309.08638",
      "DOI": "10.48550/arXiv.2309.08638",
      "CorpusId": 262045288
    },
    "publicationVenue": {
      "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
      "name": "Conference of the European Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Conf Eur Chapter Assoc Comput Linguistics",
        "EACL"
      ],
      "url": "https://www.aclweb.org/anthology/venues/eacl/"
    },
    "title": "Anchor Points: Benchmarking Models with Much Fewer Examples",
    "abstract": "Modern language models often exhibit powerful but brittle behavior, leading to the development of larger and more diverse benchmarks to reliably assess their behavior. Here, we suggest that model performance can be benchmarked and elucidated with much smaller evaluation sets. We first show that in six popular language classification benchmarks, model confidence in the correct class on many pairs of points is strongly correlated across models. We build upon this phenomenon to propose Anchor Point Selection, a technique to select small subsets of datasets that capture model behavior across the entire dataset. Anchor points reliably rank models: across 87 diverse language model-prompt pairs, evaluating models using 1-30 anchor points outperforms uniform sampling and other baselines at accurately ranking models. Moreover, just a dozen anchor points can be used to estimate model per-class predictions on all other points in a dataset with low error, sufficient for gauging where the model is likely to fail. Lastly, we present Anchor Point Maps for visualizing these insights and facilitating comparisons of the performance of different models on various regions within the dataset distribution.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 83,
    "citationCount": 17,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2243189065",
        "name": "Rajan Vivek"
      },
      {
        "authorId": "10324691",
        "name": "Kawin Ethayarajh"
      },
      {
        "authorId": "2243367527",
        "name": "Diyi Yang"
      },
      {
        "authorId": "2111313627",
        "name": "Douwe Kiela"
      }
    ]
  },
  "263608566": {
    "paperId": "a56c93747666afd534ed8f5c019869bdda673236",
    "externalIds": {
      "ACL": "2023.humeval-1.2",
      "DBLP": "journals/corr/abs-2310-01917",
      "ArXiv": "2310.01917",
      "DOI": "10.48550/arXiv.2310.01917",
      "CorpusId": 263608566
    },
    "publicationVenue": null,
    "title": "Hierarchical Evaluation Framework: Best Practices for Human Evaluation",
    "abstract": "Human evaluation plays a crucial role in Natural Language Processing (NLP) as it assesses the quality and relevance of developed systems, thereby facilitating their enhancement. However, the absence of widely accepted human evaluation metrics in NLP hampers fair comparisons among different systems and the establishment of universal assessment standards. Through an extensive analysis of existing literature on human evaluation metrics, we identified several gaps in NLP evaluation methodologies. These gaps served as motivation for developing our own hierarchical evaluation framework. The proposed framework offers notable advantages, particularly in providing a more comprehensive representation of the NLP system\u2019s performance. We applied this framework to evaluate the developed Machine Reading Comprehension system, which was utilized within a human-AI symbiosis model. The results highlighted the associations between the quality of inputs and outputs, underscoring the necessity to evaluate both components rather than solely focusing on outputs. In future work, we will investigate the potential time-saving benefits of our proposed framework for evaluators assessing NLP systems.",
    "venue": "HUMEVAL",
    "year": 2023,
    "referenceCount": 26,
    "citationCount": 5,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "48765714",
        "name": "I. Bojic"
      },
      {
        "authorId": "2253951043",
        "name": "Jessica Chen"
      },
      {
        "authorId": "2254223460",
        "name": "Si Yuan Chang"
      },
      {
        "authorId": "2207683983",
        "name": "Qi Chwen Ong"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      },
      {
        "authorId": "2252857020",
        "name": "Josip Car"
      }
    ]
  },
  "259859021": {
    "paperId": "d0e67f3a8047547a5e3503a30a21f2896eb0fa85",
    "externalIds": {
      "DBLP": "conf/acl/BelzTRM23",
      "DOI": "10.18653/v1/2023.findings-acl.226",
      "CorpusId": 259859021
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Non-Repeatable Experiments and Non-Reproducible Results: The Reproducibility Crisis in Human Evaluation in NLP",
    "abstract": "Human evaluation is widely regarded as the lit-mus test of quality in NLP. A basic requirement of all evaluations, but in particular where used for meta-evaluation, is that they should support the same conclusions if repeated. However, the reproducibility of human evaluations is virtually never queried in NLP, let alone formally tested, and their repeatability and reproducibility of results is currently an open question. This paper reports our review of human evaluation experiments published in NLP papers over the past five years which we assessed in terms of (i) their ability to be rerun, and (ii) their re-sults being reproduced where they can be rerun. Overall, we estimate that just 5% of human evaluations are repeatable in the sense that (i) there are no prohibitive barriers to repetition, and (ii) sufficient information about experimental design is publicly available for rerunning them. Our estimate goes up to about 20% when author help is sought. We complement this investigation with a survey of results concerning the reproducibility of human evaluations where those are repeatable in the first place. Here we find worryingly low degrees of reproducibility, both in terms of similarity of scores and of the findings supported by them. We summarise what insights can be gleaned so far regarding how to make human evaluations in NLP more repeatable and more reproducible.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 39,
    "citationCount": 20,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "41052836",
        "name": "Anya Belz"
      },
      {
        "authorId": "144556458",
        "name": "Craig Thomson"
      },
      {
        "authorId": "2113922820",
        "name": "Ehud Reiter"
      },
      {
        "authorId": "2738095",
        "name": "Simon Mille"
      }
    ]
  },
  "235694638": {
    "paperId": "ba805d34f762df65f499183f2010647829351def",
    "externalIds": {
      "DBLP": "journals/tacl/LauscherWGG22",
      "ArXiv": "2107.00281",
      "DOI": "10.1162/tacl_a_00525",
      "CorpusId": 235694638
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Scientia Potentia Est\u2014On the Role of Knowledge in Computational Argumentation",
    "abstract": "Abstract Despite extensive research efforts in recent years, computational argumentation (CA) remains one of the most challenging areas of natural language processing. The reason for this is the inherent complexity of the cognitive processes behind human argumentation, which integrate a plethora of different types of knowledge, ranging from topic-specific facts and common sense to rhetorical knowledge. The integration of knowledge from such a wide range in CA requires modeling capabilities far beyond many other natural language understanding tasks. Existing research on mining, assessing, reasoning over, and generating arguments largely acknowledges that much more knowledge is needed to accurately model argumentation computationally. However, a systematic overview of the types of knowledge introduced in existing CA models is missing, hindering targeted progress in the field. Adopting the operational definition of knowledge as any task-relevant normative information not provided as input, the survey paper at hand fills this gap by (1) proposing a taxonomy of types of knowledge required in CA tasks, (2) systematizing the large body of CA work according to the reliance on and exploitation of these knowledge types for the four main research areas in CA, and (3) outlining and discussing directions for future research efforts in CA.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 237,
    "citationCount": 24,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "29891652",
        "name": "Anne Lauscher"
      },
      {
        "authorId": "2626599",
        "name": "Henning Wachsmuth"
      },
      {
        "authorId": "1730400",
        "name": "Iryna Gurevych"
      },
      {
        "authorId": "1666177566",
        "name": "Goran Glavavs"
      }
    ]
  },
  "259195540": {
    "paperId": "a1edb727d2412d0a447b1a553226b24741209f2f",
    "externalIds": {
      "DBLP": "conf/nldb/WangCV23",
      "DOI": "10.1007/978-3-031-35320-8_37",
      "CorpusId": 259195540
    },
    "publicationVenue": {
      "id": "7c0b75bf-65e3-4094-9d72-e8f59ebb154d",
      "name": "International Conference on Applications of Natural Language to Data Bases",
      "type": "conference",
      "alternate_names": [
        "Appl Nat Lang Data Base",
        "Int Conf Appl Nat Lang Data Base",
        "Applications of Natural Language to Data Bases",
        "NLDB"
      ],
      "url": "http://www.nldb.org/"
    },
    "title": "Argument and Counter-Argument Generation: A Critical Survey",
    "abstract": null,
    "venue": "International Conference on Applications of Natural Language to Data Bases",
    "year": 2023,
    "referenceCount": 0,
    "citationCount": 3,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2130137128",
        "name": "Xiaoou Wang"
      },
      {
        "authorId": "1772891",
        "name": "Elena Cabrio"
      },
      {
        "authorId": "1725656",
        "name": "S. Villata"
      }
    ]
  },
  "227230554": {
    "paperId": "296271d266b3a3010dd1bef4281db56e86af308f",
    "externalIds": {
      "ACL": "2020.argmining-1.6",
      "CorpusId": 227230554
    },
    "publicationVenue": {
      "id": "033381f9-4ca6-43a6-8634-c0e5d389dba9",
      "name": "Workshop on Argument Mining",
      "type": "conference",
      "alternate_names": [
        "ArgMining",
        "Workshop Argum Min"
      ]
    },
    "title": "Annotation and Detection of Arguments in Tweets",
    "abstract": "Notwithstanding the increasing role Twitter plays in modern political and social discourse, resources built for conducting argument mining on tweets remain limited. In this paper, we present a new corpus of German tweets annotated for argument components. To the best of our knowledge, this is the first corpus containing not only annotated full tweets but also argumentative spans within tweets. We further report first promising results using supervised classification (F1: 0.82) and sequence labeling (F1: 0.72) approaches.",
    "venue": "Workshop on Argument Mining",
    "year": 2020,
    "referenceCount": 23,
    "citationCount": 16,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Political Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "153363441",
        "name": "Robin Schaefer"
      },
      {
        "authorId": "2574304",
        "name": "Manfred Stede"
      }
    ]
  },
  "258865955": {
    "paperId": "6008c21064e1af11b883e5696189a624c898a24a",
    "externalIds": {
      "DBLP": "conf/acl/ZiegenbeinSLPW23",
      "ArXiv": "2305.14935",
      "ACL": "2023.acl-long.238",
      "DOI": "10.48550/arXiv.2305.14935",
      "CorpusId": 258865955
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Modeling Appropriate Language in Argumentation",
    "abstract": "Online discussion moderators must make ad-hoc decisions about whether the contributions of discussion participants are appropriate or should be removed to maintain civility. Existing research on offensive language and the resulting tools cover only one aspect among many involved in such decisions. The question of what is considered appropriate in a controversial discussion has not yet been systematically addressed. In this paper, we operationalize appropriate language in argumentation for the first time. In particular, we model appropriateness through the absence of flaws, grounded in research on argument quality assessment, especially in aspects from rhetoric. From these, we derive a new taxonomy of 14 dimensions that determine inappropriate language in online discussions. Building on three argument quality corpora, we then create a corpus of 2191 arguments annotated for the 14 dimensions. Empirical analyses support that the taxonomy covers the concept of appropriateness comprehensively, showing several plausible correlations with argument quality dimensions. Moreover, results of baseline approaches to assessing appropriateness suggest that all dimensions can be modeled computationally on the corpus.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 27,
    "citationCount": 1,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2047307140",
        "name": "Timon Ziegenbein"
      },
      {
        "authorId": "18417916",
        "name": "S. Syed"
      },
      {
        "authorId": "153689772",
        "name": "F. Lange"
      },
      {
        "authorId": "3046200",
        "name": "Martin Potthast"
      },
      {
        "authorId": "2626599",
        "name": "Henning Wachsmuth"
      }
    ]
  },
  "260813729": {
    "paperId": "40af24f2cb70a1d9f7d02bb08d98fdb61cdf94e8",
    "externalIds": {
      "DBLP": "conf/sigdial/SyedZHWP23",
      "ACL": "2023.sigdial-1.10",
      "DOI": "10.18653/v1/2023.sigdial-1.10",
      "CorpusId": 260813729
    },
    "publicationVenue": {
      "id": "6a470734-72c6-4809-a07d-d34dee0df4a1",
      "name": "SIGDIAL Conferences",
      "type": "conference",
      "alternate_names": [
        "SIGDIAL",
        "SIGDIAL Conf",
        "Annu Meet Sp\u00e9c Interest Group Discourse Dialogue",
        "Annual Meeting of the Special Interest Group on Discourse and Dialogue"
      ]
    },
    "title": "Frame-oriented Summarization of Argumentative Discussions",
    "abstract": "Online discussions on controversial topics with many participants frequently include hundreds of arguments that cover different framings of the topic. But these arguments and frames are often spread across the various branches of the discussion tree structure. This makes it difficult for interested participants to follow the discussion in its entirety as well as to introduce new arguments. In this paper, we present a new rank-based approach to extractive summarization of online discussions focusing on argumentation frames that capture the different aspects of a discussion. Our approach includes three retrieval tasks to find arguments in a discussion that are (1) relevant to a frame of interest, (2) relevant to the topic under discussion, and (3) informative to the reader. Based on a joint ranking by these three criteria for a set of user-selected frames, our approach allows readers to quickly access an ongoing discussion. We evaluate our approach using a test set of 100 controversial Reddit ChangeMyView discussions, for which the relevance of a total of 1871 arguments was manually annotated.",
    "venue": "SIGDIAL Conferences",
    "year": 2023,
    "referenceCount": 61,
    "citationCount": 5,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "18417916",
        "name": "S. Syed"
      },
      {
        "authorId": "2047307140",
        "name": "Timon Ziegenbein"
      },
      {
        "authorId": "2145327410",
        "name": "Philipp Heinisch"
      },
      {
        "authorId": "2626599",
        "name": "Henning Wachsmuth"
      },
      {
        "authorId": "3046200",
        "name": "Martin Potthast"
      }
    ]
  },
  "258947072": {
    "paperId": "8cfe47b4aee351982027aed8fb0d84bd59223ade",
    "externalIds": {
      "ArXiv": "2305.16799",
      "ACL": "2023.acl-long.880",
      "DBLP": "journals/corr/abs-2305-16799",
      "DOI": "10.48550/arXiv.2305.16799",
      "CorpusId": 258947072
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support",
    "abstract": "Optimizing the phrasing of argumentative text is crucial in higher education and professional development. However, assessing whether and how the different claims in a text should be revised is a hard task, especially for novice writers. In this work, we explore the main challenges to identifying argumentative claims in need of specific revisions. By learning from collaborative editing behaviors in online debates, we seek to capture implicit revision patterns in order to develop approaches aimed at guiding writers in how to further improve their arguments. We systematically compare the ability of common word embedding models to capture the differences between different versions of the same text, and we analyze their impact on various types of writing issues. To deal with the noisy nature of revision-based corpora, we propose a new sampling strategy based on revision distance. Opposed to approaches from prior work, such sampling can be done without employing additional annotations and judgments. Moreover, we provide evidence that using contextual information and domain knowledge can further improve prediction results. How useful a certain type of context is, depends on the issue the claim is suffering from, though.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 43,
    "citationCount": 6,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2478960",
        "name": "Gabriella Skitalinskaya"
      },
      {
        "authorId": "2626599",
        "name": "Henning Wachsmuth"
      }
    ]
  },
  "254854351": {
    "paperId": "c6c13eeddadfbdd8f1d1b3589c9d0507eedb7d7e",
    "externalIds": {
      "ACL": "2023.inlg-main.10",
      "DBLP": "conf/inlg/SkitalinskayaSW23",
      "ArXiv": "2212.08913",
      "DOI": "10.48550/arXiv.2212.08913",
      "CorpusId": 254854351
    },
    "publicationVenue": {
      "id": "8648a277-d0ec-4691-9eed-399b31ff9860",
      "name": "International Conference on Natural Language Generation",
      "type": "conference",
      "alternate_names": [
        "Int Conf Nat Lang Gener",
        "INLG"
      ],
      "url": "http://www.wikicfp.com/cfp/program?id=1613"
    },
    "title": "Claim Optimization in Computational Argumentation",
    "abstract": "An optimal delivery of arguments is key to persuasion in any debate, both for humans and for AI systems. This requires the use of clear and fluent claims relevant to the given debate. Prior work has studied the automatic assessment of argument quality extensively. Yet, no approach actually improves the quality so far. To fill this gap, this paper proposes the task of claim optimization: to rewrite argumentative claims in order to optimize their delivery. As multiple types of optimization are possible, we approach this task by first generating a diverse set of candidate claims using a large language model, such as BART, taking into account contextual information. Then, the best candidate is selected using various quality metrics. In automatic and human evaluation on an English-language corpus, our quality-based candidate selection outperforms several baselines, improving 60% of all claims (worsening 16% only). Follow-up analyses reveal that, beyond copy editing, our approach often specifies claims with details, whereas it adds less evidence than humans do. Moreover, its capabilities generalize well to other domains, such as instructional texts.",
    "venue": "International Conference on Natural Language Generation",
    "year": 2022,
    "referenceCount": 62,
    "citationCount": 5,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2478960",
        "name": "Gabriella Skitalinskaya"
      },
      {
        "authorId": "2190107770",
        "name": "Maximilian Spliethover"
      },
      {
        "authorId": "2626599",
        "name": "Henning Wachsmuth"
      }
    ]
  },
  "266725300": {
    "paperId": "0251bb95be75d472c8d5b873751615e7fe2feb1d",
    "externalIds": {
      "DBLP": "journals/corr/abs-2401-01286",
      "ArXiv": "2401.01286",
      "DOI": "10.48550/arXiv.2401.01286",
      "CorpusId": 266725300
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Comprehensive Study of Knowledge Editing for Large Language Models",
    "abstract": "Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the knowledge editing problem and then provide a comprehensive review of cutting-edge approaches. Drawing inspiration from educational and cognitive research theories, we propose a unified categorization criterion that classifies knowledge editing methods into three groups: resorting to external knowledge, merging knowledge into the model, and editing intrinsic knowledge. Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive empirical evaluation of representative knowledge editing approaches. Additionally, we provide an in-depth analysis of knowledge location, which can give a deeper understanding of the knowledge structures inherent within LLMs. Finally, we discuss several potential applications of knowledge editing, outlining its broad and impactful implications.",
    "venue": "arXiv.org",
    "year": 2024,
    "referenceCount": 244,
    "citationCount": 54,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2608639",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "4841460",
        "name": "Yunzhi Yao"
      },
      {
        "authorId": "2064522174",
        "name": "Bo Tian"
      },
      {
        "authorId": "2277701209",
        "name": "Peng Wang"
      },
      {
        "authorId": "2253472695",
        "name": "Shumin Deng"
      },
      {
        "authorId": "2218346459",
        "name": "Meng Wang"
      },
      {
        "authorId": "2229632309",
        "name": "Zekun Xi"
      },
      {
        "authorId": "2184189568",
        "name": "Shengyu Mao"
      },
      {
        "authorId": "2253784578",
        "name": "Jintian Zhang"
      },
      {
        "authorId": "2268493966",
        "name": "Yuansheng Ni"
      },
      {
        "authorId": "2258034882",
        "name": "Siyuan Cheng"
      },
      {
        "authorId": "2277568426",
        "name": "Ziwen Xu"
      },
      {
        "authorId": "2152775219",
        "name": "Xin Xu"
      },
      {
        "authorId": "3028818",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "2256747040",
        "name": "Yong Jiang"
      },
      {
        "authorId": "35930962",
        "name": "Pengjun Xie"
      },
      {
        "authorId": "2276428076",
        "name": "Fei Huang"
      },
      {
        "authorId": "2257515059",
        "name": "Lei Liang"
      },
      {
        "authorId": "2019870682",
        "name": "Zhiqiang Zhang"
      },
      {
        "authorId": "2323714374",
        "name": "Xiaowei Zhu"
      },
      {
        "authorId": "2272742832",
        "name": "Jun Zhou"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      }
    ]
  },
  "268553537": {
    "paperId": "33c8910107f3fcb17d140cc88554652508ae3674",
    "externalIds": {
      "ArXiv": "2403.14472",
      "DBLP": "conf/acl/Wang0XXDYZY0C24",
      "DOI": "10.48550/arXiv.2403.14472",
      "CorpusId": 268553537
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Detoxifying Large Language Models via Knowledge Editing",
    "abstract": "This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to detoxify LLMs with a limited impact on general performance efficiently. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at https://github.com/zjunlp/EasyEdit.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "referenceCount": 93,
    "citationCount": 35,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2218346459",
        "name": "Meng Wang"
      },
      {
        "authorId": "2153010067",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "2277568426",
        "name": "Ziwen Xu"
      },
      {
        "authorId": "2229632309",
        "name": "Zekun Xi"
      },
      {
        "authorId": "152931849",
        "name": "Shumin Deng"
      },
      {
        "authorId": "4841460",
        "name": "Yunzhi Yao"
      },
      {
        "authorId": "2292640695",
        "name": "Qishen Zhang"
      },
      {
        "authorId": "2292413132",
        "name": "Linyi Yang"
      },
      {
        "authorId": "2300128890",
        "name": "Jindong Wang"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      }
    ]
  },
  "268358212": {
    "paperId": "184753b614b35cde3f2e221cea3bc60fe016d29e",
    "externalIds": {
      "ArXiv": "2403.06259",
      "DBLP": "journals/corr/abs-2403-06259",
      "DOI": "10.48550/arXiv.2403.06259",
      "CorpusId": 268358212
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Editing Conceptual Knowledge for Large Language Models",
    "abstract": "Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.",
    "venue": "arXiv.org",
    "year": 2024,
    "referenceCount": 64,
    "citationCount": 9,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2141660877",
        "name": "Xiaohan Wang"
      },
      {
        "authorId": "2184189568",
        "name": "Shengyu Mao"
      },
      {
        "authorId": "2288030000",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "152931849",
        "name": "Shumin Deng"
      },
      {
        "authorId": "4841460",
        "name": "Yunzhi Yao"
      },
      {
        "authorId": "2290138094",
        "name": "Yue Shen"
      },
      {
        "authorId": "2285135033",
        "name": "Lei Liang"
      },
      {
        "authorId": "2269769748",
        "name": "Jinjie Gu"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      }
    ]
  },
  "260356612": {
    "paperId": "1fdf449c96fbac0789cf8dfae15b788905407fd3",
    "externalIds": {
      "DBLP": "journals/tacl/CohenBYGG24",
      "ACL": "2024.tacl-1.16",
      "ArXiv": "2307.12976",
      "DOI": "10.1162/tacl_a_00644",
      "CorpusId": 260356612
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Evaluating the Ripple Effects of Knowledge Editing in Language Models",
    "abstract": "Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g., \u201cJack Depp is the son of Johnny Depp\u201d) introduces a \u201cripple effect\u201d in the form of additional facts that the model needs to update (e.g., \u201cJack Depp is the sibling of Lily-Rose Depp\u201d). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RippleEdits, a diagnostic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model\u2019s knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing.1",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 52,
    "citationCount": 113,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2203359700",
        "name": "Roi Cohen"
      },
      {
        "authorId": "2226283125",
        "name": "Eden Biran"
      },
      {
        "authorId": "2074100429",
        "name": "Ori Yoran"
      },
      {
        "authorId": "1786843",
        "name": "A. Globerson"
      },
      {
        "authorId": "22245981",
        "name": "Mor Geva"
      }
    ]
  },
  "263908997": {
    "paperId": "e8d513bc7554a83161f2fb26c8299b471581cdb6",
    "externalIds": {
      "ArXiv": "2310.08475",
      "DBLP": "conf/emnlp/0008TL0WC023",
      "DOI": "10.48550/arXiv.2310.08475",
      "CorpusId": 263908997
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Can We Edit Multimodal Large Language Models?",
    "abstract": "In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 69,
    "citationCount": 18,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2258034882",
        "name": "Siyuan Cheng"
      },
      {
        "authorId": "2064522174",
        "name": "Bo Tian"
      },
      {
        "authorId": "2258682951",
        "name": "Qingbin Liu"
      },
      {
        "authorId": "48283576",
        "name": "Xi Chen"
      },
      {
        "authorId": "2257367497",
        "name": "Yongheng Wang"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      },
      {
        "authorId": "2153010067",
        "name": "Ningyu Zhang"
      }
    ]
  },
  "263608944": {
    "paperId": "427af7080ecfd7925f03439488ee0ae6aebe755b",
    "externalIds": {
      "ArXiv": "2310.02129",
      "DBLP": "journals/corr/abs-2310-02129",
      "DOI": "10.48550/arXiv.2310.02129",
      "CorpusId": 263608944
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models",
    "abstract": "As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of LLMs. Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works. Code and data are available at https://github.com/zjunlp/PitfallsKnowledgeEditing.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 0,
    "citationCount": 26,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2254137113",
        "name": "Zhoubo Li"
      },
      {
        "authorId": "2153010067",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "4841460",
        "name": "Yunzhi Yao"
      },
      {
        "authorId": "2218346459",
        "name": "Meng Wang"
      },
      {
        "authorId": "48283576",
        "name": "Xi Chen"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      }
    ]
  },
  "263608742": {
    "paperId": "f0c1ca2cc64c03efcf8970448f6a412740b00211",
    "externalIds": {
      "ArXiv": "2310.02168",
      "CorpusId": 263608742
    },
    "publicationVenue": null,
    "title": "Editing Personality for Large Language Models",
    "abstract": "This paper introduces an innovative task focused on editing the personality traits of Large Language Models (LLMs). This task seeks to adjust the models' responses to opinion-related questions on specified topics since an individual's personality often manifests in the form of their expressed opinions, thereby showcasing different personality traits. Specifically, we construct PersonalityEdit, a new benchmark dataset to address this task. Drawing on the theory in Social Psychology, we isolate three representative traits, namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our benchmark. We then gather data using GPT-4, generating responses that align with a specified topic and embody the targeted personality trait. We conduct comprehensive experiments involving various baselines and discuss the representation of personality behavior in LLMs. Our findings uncover potential challenges of the proposed task, illustrating several remaining issues. We anticipate that our work can stimulate further annotation in model editing and personality-related research. Code is available at https://github.com/zjunlp/EasyEdit.",
    "venue": "",
    "year": 2023,
    "referenceCount": 82,
    "citationCount": 8,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2184189568",
        "name": "Shengyu Mao"
      },
      {
        "authorId": "2153010067",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "2141660877",
        "name": "Xiaohan Wang"
      },
      {
        "authorId": "2218346459",
        "name": "Meng Wang"
      },
      {
        "authorId": "4841460",
        "name": "Yunzhi Yao"
      },
      {
        "authorId": "2256747040",
        "name": "Yong Jiang"
      },
      {
        "authorId": "35930962",
        "name": "Pengjun Xie"
      },
      {
        "authorId": "2256480060",
        "name": "Fei Huang"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      }
    ]
  },
  "256231427": {
    "paperId": "8bbdf825e6ccd197adfff5ebeecc9a5a5210e02f",
    "externalIds": {
      "ArXiv": "2301.10405",
      "DBLP": "journals/corr/abs-2301-10405",
      "DOI": "10.48550/arXiv.2301.10405",
      "CorpusId": 256231427
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "Editing Language Model-based Knowledge Graph Embeddings",
    "abstract": "Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, making them difficult to modify post-deployment without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. This task is designed to facilitate rapid, data-efficient updates to KG embeddings without compromising the performance of other aspects. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hypernetwork to edit/add facts. Our comprehensive experimental results reveal that KGEditor excels in updating specific facts without impacting the overall performance, even when faced with limited training resources. Code and datasets will be available at https://github.com/AnonymousForPapers/DeltaKG.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "referenceCount": 69,
    "citationCount": 17,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "46378881",
        "name": "Siyuan Cheng"
      },
      {
        "authorId": "2608639",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "2064522174",
        "name": "Bo Tian"
      },
      {
        "authorId": "2075345841",
        "name": "Zelin Dai"
      },
      {
        "authorId": "2068169902",
        "name": "Feiyu Xiong"
      },
      {
        "authorId": "2109155646",
        "name": "Wei Guo"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      }
    ]
  },
  "219602200": {
    "paperId": "20aa85c0b0b07c3bef15f435b9d5292781b7c751",
    "externalIds": {
      "MAG": "3097237974",
      "DBLP": "conf/i-semantics/HoferHDF20",
      "PubMedCentral": "7586439",
      "DOI": "10.1007/978-3-030-59833-4_1",
      "CorpusId": 219602200
    },
    "publicationVenue": {
      "id": "4360f990-6268-434d-ad80-56727c789ee0",
      "name": "International Conference on Semantic Systems",
      "type": "conference",
      "alternate_names": [
        "SEMANTICS",
        "I-SEMANTICS",
        "Int Conf Semantic Syst"
      ],
      "url": "http://i-semantics.tugraz.at/"
    },
    "title": "The New DBpedia Release Cycle: Increasing Agility and Efficiency in Knowledge Extraction Workflows",
    "abstract": null,
    "venue": "International Conference on Semantic Systems",
    "year": 2020,
    "referenceCount": 18,
    "citationCount": 17,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "24163430",
        "name": "M. Hofer"
      },
      {
        "authorId": "2024066",
        "name": "Sebastian Hellmann"
      },
      {
        "authorId": "1819564",
        "name": "Milan Dojchinovski"
      },
      {
        "authorId": "32114346",
        "name": "Johannes Frey"
      }
    ]
  },
  "1181640": {
    "paperId": "d2946a868682e4141beabc288d79253ae254c6e1",
    "externalIds": {
      "DBLP": "journals/semweb/LehmannIJJKMHMK15",
      "MAG": "1552847225",
      "DOI": "10.3233/SW-140134",
      "CorpusId": 1181640
    },
    "publicationVenue": null,
    "title": "DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia",
    "abstract": "The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely available on the Web using Semantic Web and Linked Data technologies. The project extracts knowledge from 111 different language editions of Wikipedia. The largest DBpedia knowledge base which is extracted from the English edition of Wikipedia consists of over 400 million facts that describe 3.7 million things. The DBpedia knowledge bases that are extracted from the other 110 Wikipedia editions together consist of 1.46 billion facts and describe 10 million additional things. The DBpedia project maps Wikipedia infoboxes from 27 different language editions to a single shared ontology consisting of 320 classes and 1,650 properties. The mappings are created via a world-wide crowd-sourcing effort and enable knowledge from the different Wikipedia editions to be combined. The project publishes releases of all DBpedia knowledge bases for download and provides SPARQL query access to 14 out of the 111 language editions via a global network of local DBpedia chapters. In addition to the regular releases, the project maintains a live knowledge base which is updated whenever a page in Wikipedia changes. DBpedia sets 27 million RDF links pointing into over 30 external data sources and thus enables data from these sources to be used together with DBpedia data. Several hundred data sets on the Web publish RDF links pointing to DBpedia themselves and make DBpedia one of the central interlinking hubs in the Linked Open Data (LOD) cloud. In this system report, we give an overview of the DBpedia community project, including its architecture, technical implementation, maintenance, internationalisation, usage statistics and applications.",
    "venue": "Semantic Web",
    "year": 2015,
    "referenceCount": 58,
    "citationCount": 3174,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144568027",
        "name": "Jens Lehmann"
      },
      {
        "authorId": "2968874",
        "name": "Robert Isele"
      },
      {
        "authorId": "2065890993",
        "name": "Max Jakob"
      },
      {
        "authorId": "2856259",
        "name": "Anja Jentzsch"
      },
      {
        "authorId": "2627116",
        "name": "D. Kontokostas"
      },
      {
        "authorId": "1692493",
        "name": "Pablo N. Mendes"
      },
      {
        "authorId": "2024066",
        "name": "Sebastian Hellmann"
      },
      {
        "authorId": "145022718",
        "name": "M. Morsey"
      },
      {
        "authorId": "2141758",
        "name": "Patrick van Kleef"
      },
      {
        "authorId": "145044578",
        "name": "S. Auer"
      },
      {
        "authorId": "1729154",
        "name": "Christian Bizer"
      }
    ]
  },
  "16081721": {
    "paperId": "58f72b53d576c6e4a42b4d8812e5542ffa2c03cc",
    "externalIds": {
      "DBLP": "journals/ws/BizerLKABCH09",
      "MAG": "2153225416",
      "DOI": "10.1016/J.WEBSEM.2009.07.002",
      "CorpusId": 16081721
    },
    "publicationVenue": {
      "id": "78d61825-c1d6-4e7f-ab9d-1525db71105c",
      "name": "Journal of Web Semantics",
      "type": "journal",
      "alternate_names": [
        "J Web Semant"
      ],
      "issn": "1570-8268",
      "url": "http://www.elsevier.com/locate/websem",
      "alternate_urls": [
        "http://www.sciencedirect.com/science/journal/15708268",
        "http://www.elsevier.com/wps/find/journaldescription.cws_home/671322/description"
      ]
    },
    "title": "DBpedia - A crystallization point for the Web of Data",
    "abstract": null,
    "venue": "Journal of Web Semantics",
    "year": 2009,
    "referenceCount": 29,
    "citationCount": 2430,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1729154",
        "name": "Christian Bizer"
      },
      {
        "authorId": "144568027",
        "name": "Jens Lehmann"
      },
      {
        "authorId": "2051816",
        "name": "Georgi Kobilarov"
      },
      {
        "authorId": "145044578",
        "name": "S. Auer"
      },
      {
        "authorId": "2068696031",
        "name": "Christian Becker"
      },
      {
        "authorId": "1702661",
        "name": "Richard Cyganiak"
      },
      {
        "authorId": "2024066",
        "name": "Sebastian Hellmann"
      }
    ]
  },
  "1427846": {
    "paperId": "5dd0ae971c88a817bb46160d1afc8af3c09fa69d",
    "externalIds": {
      "DBLP": "journals/tois/SunMGM11",
      "MAG": "2009556891",
      "DOI": "10.1145/1961209.1961215",
      "CorpusId": 1427846
    },
    "publicationVenue": null,
    "title": "Identifying, Indexing, and Ranking Chemical Formulae and Chemical Names in Digital Documents",
    "abstract": "End-users utilize chemical search engines to search for chemical formulae and chemical names. Chemical search engines identify and index chemical formulae and chemical names appearing in text documents to support efficient search and retrieval in the future. Identifying chemical formulae and chemical names in text automatically has been a hard problem that has met with varying degrees of success in the past. We propose algorithms for chemical formula and chemical name tagging using Conditional Random Fields (CRFs) and Support Vector Machines (SVMs) that achieve higher accuracy than existing (published) methods. After chemical entities have been identified in text documents, they must be indexed. In order to support user-provided search queries that require a partial match between the chemical name segment used as a keyword or a partial chemical formula, all possible (or a significant number of) subformulae of formulae that appear in any document and all possible subterms (e.g., \u201cmethyl\u201d) of chemical names (e.g., \u201cmethylethyl ketone\u201d) must be indexed. Indexing all possible subformulae and subterms results in an exponential increase in the storage and memory requirements as well as the time taken to process the indices. We propose techniques to prune the indices significantly without reducing the quality of the returned results significantly. Finally, we propose multiple query semantics to allow users to pose different types of partial search queries for chemical entities. We demonstrate empirically that our search engines improve the relevance of the returned results for search queries involving chemical entities.",
    "venue": "TOIS",
    "year": 2011,
    "referenceCount": 62,
    "citationCount": 18,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "47935371",
        "name": "Bingjun Sun"
      },
      {
        "authorId": "143930195",
        "name": "P. Mitra"
      },
      {
        "authorId": "145157784",
        "name": "C. Lee Giles"
      },
      {
        "authorId": "1759717",
        "name": "K. Mueller"
      }
    ]
  },
  "232342107": {
    "paperId": "e451e1717a8fd4238b7d36e06da478d2d3333f1a",
    "externalIds": {
      "DBLP": "journals/frma/HeNADTHAZFYACCB21",
      "PubMedCentral": "8028406",
      "DOI": "10.3389/frma.2021.654438",
      "CorpusId": 232342107,
      "PubMed": "33870071"
    },
    "publicationVenue": {
      "id": "009bf25d-4bf4-470e-af03-7580910f0e01",
      "name": "Frontiers in Research Metrics and Analytics",
      "type": "journal",
      "alternate_names": [
        "Front Res Metr Anal"
      ],
      "issn": "2504-0537",
      "url": "https://www.frontiersin.org/journals/research-metrics-and-analytics#articles",
      "alternate_urls": [
        "http://journal.frontiersin.org/journal/research-metrics-and-analytics#"
      ]
    },
    "title": "ChEMU 2020: Natural Language Processing Methods Are Effective for Information Extraction From Chemical Patents",
    "abstract": "Chemical patents represent a valuable source of information about new chemical compounds, which is critical to the drug discovery process. Automated information extraction over chemical patents is, however, a challenging task due to the large volume of existing patents and the complex linguistic properties of chemical patents. The Cheminformatics Elsevier Melbourne University (ChEMU) evaluation lab 2020, part of the Conference and Labs of the Evaluation Forum 2020 (CLEF2020), was introduced to support the development of advanced text mining techniques for chemical patents. The ChEMU 2020 lab proposed two fundamental information extraction tasks focusing on chemical reaction processes described in chemical patents: (1) chemical named entity recognition, requiring identification of essential chemical entities and their roles in chemical reactions, as well as reaction conditions; and (2) event extraction, which aims at identification of event steps relating the entities involved in chemical reactions. The ChEMU 2020 lab received 37 team registrations and 46 runs. Overall, the performance of submissions for these tasks exceeded our expectations, with the top systems outperforming strong baselines. We further show the methods to be robust to variations in sampling of the test data. We provide a detailed overview of the ChEMU 2020 corpus and its annotation, showing that inter-annotator agreement is very strong. We also present the methods adopted by participants, provide a detailed analysis of their performance, and carefully consider the potential impact of data leakage on interpretation of the results. The ChEMU 2020 Lab has shown the viability of automated methods to support information extraction of key information in chemical patents.",
    "venue": "Frontiers in Research Metrics and Analytics",
    "year": 2021,
    "referenceCount": 79,
    "citationCount": 31,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "150147667",
        "name": "Jiayuan He"
      },
      {
        "authorId": "34691913",
        "name": "Dat Quoc Nguyen"
      },
      {
        "authorId": "2830474",
        "name": "S. Akhondi"
      },
      {
        "authorId": "150055420",
        "name": "Christian Druckenbrodt"
      },
      {
        "authorId": "2093331",
        "name": "Camilo Thorne"
      },
      {
        "authorId": "1630460836",
        "name": "Ralph Hoessel"
      },
      {
        "authorId": "2685347",
        "name": "Z. Afzal"
      },
      {
        "authorId": "51230252",
        "name": "Zenan Zhai"
      },
      {
        "authorId": "153841255",
        "name": "Biaoyan Fang"
      },
      {
        "authorId": "2082741",
        "name": "Hiyori Yoshikawa"
      },
      {
        "authorId": "9581515",
        "name": "Ameer Albahem"
      },
      {
        "authorId": "1788025",
        "name": "L. Cavedon"
      },
      {
        "authorId": "1630460898",
        "name": "Trevor Cohn"
      },
      {
        "authorId": "145465286",
        "name": "Timothy Baldwin"
      },
      {
        "authorId": "144765178",
        "name": "Karin M. Verspoor"
      }
    ]
  },
  "247362020": {
    "paperId": "eee7997106834442f1704e4681a9a761df6696a1",
    "externalIds": {
      "DBLP": "journals/jcisd/LuZ22",
      "DOI": "10.1021/acs.jcim.1c01467",
      "CorpusId": 247362020,
      "PubMed": "35266390"
    },
    "publicationVenue": {
      "id": "3f16aef5-6b9f-4f87-baca-cbf8147e352f",
      "name": "Journal of Chemical Information and Modeling",
      "type": "journal",
      "alternate_names": [
        "J Chem Inf Model"
      ],
      "issn": "1549-9596",
      "url": "http://pubs.acs.org/jcim",
      "alternate_urls": [
        "http://pubs.acs.org/journals/jcisd8/index.html",
        "https://pubs.acs.org/journal/jcisd8"
      ]
    },
    "title": "Unified Deep Learning Model for Multitask Reaction Predictions with Explanation",
    "abstract": "There is significant interest and importance to develop robust machine learning models to assist organic chemistry synthesis. Typically, task-specific machine learning models for distinct reaction prediction tasks have been developed. In this work, we develop a unified deep learning model, T5Chem, for a variety of chemical reaction predictions tasks by adapting the \"Text-to-Text Transfer Transformer\" (T5) framework in natural language processing (NLP). On the basis of self-supervised pretraining with PubChem molecules, the T5Chem model can achieve state-of-the-art performances for four distinct types of task-specific reaction prediction tasks using four different open-source data sets, including reaction type classification on USPTO_TPL, forward reaction prediction on USPTO_MIT, single-step retrosynthesis on USPTO_50k, and reaction yield prediction on high-throughput C-N coupling reactions. Meanwhile, we introduced a new unified multitask reaction prediction data set USPTO_500_MT, which can be used to train and test five different types of reaction tasks, including the above four as well as a new reagent suggestion task. Our results showed that models trained with multiple tasks are more robust and can benefit from mutual learning on related tasks. Furthermore, we demonstrated the use of SHAP (SHapley Additive exPlanations) to explain T5Chem predictions at the functional group level, which provides a way to demystify sequence-based deep learning models in chemistry. T5Chem is accessible through https://yzhang.hpc.nyu.edu/T5Chem.",
    "venue": "Journal of Chemical Information and Modeling",
    "year": 2022,
    "referenceCount": 49,
    "citationCount": 56,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1998959323",
        "name": "Jieyu Lu"
      },
      {
        "authorId": "1591145699",
        "name": "Yingkai Zhang"
      }
    ]
  },
  "258059792": {
    "paperId": "354dcdebf3f8b5feeed5c62090e0bc1f0c28db06",
    "externalIds": {
      "DBLP": "journals/natmi/BranCSBWS24",
      "ArXiv": "2304.05376",
      "PubMedCentral": "11116106",
      "DOI": "10.1038/s42256-024-00832-8",
      "CorpusId": 258059792,
      "PubMed": "38799228"
    },
    "publicationVenue": null,
    "title": "Augmenting large language models with chemistry tools",
    "abstract": null,
    "venue": "Nat. Mac. Intell.",
    "year": 2023,
    "referenceCount": 114,
    "citationCount": 223,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Physics",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Chemistry",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2216007369",
        "name": "Andr\u00e9s M Bran"
      },
      {
        "authorId": "2161337138",
        "name": "Sam Cox"
      },
      {
        "authorId": "1820929773",
        "name": "Oliver Schilter"
      },
      {
        "authorId": "2251414370",
        "name": "Carlo Baldassari"
      },
      {
        "authorId": "2150199535",
        "name": "Andrew D. White"
      },
      {
        "authorId": "1379965853",
        "name": "P. Schwaller"
      }
    ]
  },
  "204824113": {
    "paperId": "530a059cb48477ad1e3d4f8f4b153274c8997332",
    "externalIds": {
      "DBLP": "journals/corr/abs-1910-10045",
      "MAG": "2997428643",
      "ArXiv": "1910.10045",
      "DOI": "10.1016/j.inffus.2019.12.012",
      "CorpusId": 204824113
    },
    "publicationVenue": {
      "id": "06afdd0b-0d85-413f-af8a-c3045c12c561",
      "name": "Information Fusion",
      "type": "journal",
      "alternate_names": [
        "Inf Fusion"
      ],
      "issn": "1566-2535",
      "url": "https://www.journals.elsevier.com/information-fusion",
      "alternate_urls": [
        "http://www.sciencedirect.com/science/journal/15662535"
      ]
    },
    "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",
    "abstract": null,
    "venue": "Information Fusion",
    "year": 2019,
    "referenceCount": 430,
    "citationCount": 5153,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1379511816",
        "name": "Alejandro Barredo Arrieta"
      },
      {
        "authorId": "2058921025",
        "name": "Natalia D\u00edaz Rodr\u00edguez"
      },
      {
        "authorId": "9221552",
        "name": "J. Ser"
      },
      {
        "authorId": "1379511786",
        "name": "Adrien Bennetot"
      },
      {
        "authorId": "3030006",
        "name": "S. Tabik"
      },
      {
        "authorId": "50449165",
        "name": "A. Barbado"
      },
      {
        "authorId": "39558258",
        "name": "S. Garc\u00eda"
      },
      {
        "authorId": "1402195255",
        "name": "S. Gil-Lopez"
      },
      {
        "authorId": "145337392",
        "name": "D. Molina"
      },
      {
        "authorId": "2445552",
        "name": "Richard Benjamins"
      },
      {
        "authorId": "2091924780",
        "name": "Raja Chatila"
      },
      {
        "authorId": "2098723448",
        "name": "Francisco Herrera"
      }
    ]
  },
  "220305304": {
    "paperId": "02a715a0bee9065a259f78876d8f4f92090cad01",
    "externalIds": {
      "MAG": "3038445625",
      "DBLP": "journals/corr/abs-2102-03732",
      "ArXiv": "2102.03732",
      "DOI": "10.1007/978-981-15-5573-2",
      "CorpusId": 220305304
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Representation Learning for Natural Language Processing",
    "abstract": null,
    "venue": "arXiv.org",
    "year": 2021,
    "referenceCount": 0,
    "citationCount": 12,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49293587",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2427350",
        "name": "Yankai Lin"
      },
      {
        "authorId": "1753344",
        "name": "Maosong Sun"
      }
    ]
  },
  "24461982": {
    "paperId": "c41516420ddbd0f29e010ca259a74c1fc2da0466",
    "externalIds": {
      "MAG": "2964204621",
      "DBLP": "conf/acl/BaroniBLKC18",
      "ArXiv": "1805.01070",
      "ACL": "P18-1198",
      "DOI": "10.18653/v1/P18-1198",
      "CorpusId": 24461982
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
    "abstract": "Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. \u201cDownstream\u201d tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 45,
    "citationCount": 843,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2480903",
        "name": "Alexis Conneau"
      },
      {
        "authorId": "2067996",
        "name": "Germ\u00e1n Kruszewski"
      },
      {
        "authorId": "1830914",
        "name": "Guillaume Lample"
      },
      {
        "authorId": "2934336",
        "name": "Lo\u00efc Barrault"
      },
      {
        "authorId": "145283199",
        "name": "Marco Baroni"
      }
    ]
  },
  "220683893": {
    "paperId": "f4c671ba8608d187e3cdba21232489e18fac67e6",
    "externalIds": {
      "DBLP": "conf/cogsci/Kelly0CR20",
      "DOI": "10.31234/osf.io/9jsnz",
      "CorpusId": 220683893
    },
    "publicationVenue": {
      "id": "9c06885c-ecb6-4a76-ba1e-fbad53521efd",
      "name": "Annual Meeting of the Cognitive Science Society",
      "type": "conference",
      "alternate_names": [
        "CogSci",
        "Annu Meet Cogn Sci Soc"
      ],
      "url": "http://cognitivesciencesociety.org/"
    },
    "title": "Which sentence embeddings and which layers encode syntactic structure?",
    "abstract": "Recent models of language have eliminated syntactic-semantic dividing lines. We explore the psycholinguistic implications of this development by comparing different types of sentence embeddings in their ability to encode syntactic constructions. Our study uses contrasting sentence structures known to cause syntactic priming effects, that is, the tendency in humans to repeat sentence structures after recent exposure. We compare how syntactic alternatives are captured by sentence embeddings produced by a neural language model (BERT) or by the composition of word embeddings (BEAGLE, HHM, GloVe). Dative double object vs. prepositional object and active vs. passive sentences are separable in the high-dimensional space of the sentence embeddings and can be classified with a high degree of accuracy. The results lend empirical support to the modern, computational, integrated accounts of semantics and syntax, and they shed light on the information stored at different layers in deep language models such as BERT.",
    "venue": "Annual Meeting of the Cognitive Science Society",
    "year": 2020,
    "referenceCount": 33,
    "citationCount": 8,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2814317",
        "name": "M. Kelly"
      },
      {
        "authorId": "2115682413",
        "name": "Yang Xu"
      },
      {
        "authorId": "2053897",
        "name": "Jes\u00fas Calvillo"
      },
      {
        "authorId": "1781409",
        "name": "D. Reitter"
      }
    ]
  },
  "227231833": {
    "paperId": "c331a3e3e55d95beb8be5cec9ccc772e72b32282",
    "externalIds": {
      "MAG": "3117575631",
      "ACL": "2020.coling-main.300",
      "DBLP": "conf/coling/ZhuM20",
      "DOI": "10.18653/V1/2020.COLING-MAIN.300",
      "CorpusId": 227231833
    },
    "publicationVenue": {
      "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
      "name": "International Conference on Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Int Conf Comput Linguistics",
        "COLING"
      ],
      "url": "https://www.aclweb.org/anthology/venues/coling/"
    },
    "title": "Sentence Analogies: Linguistic Regularities in Sentence Embeddings",
    "abstract": "While important properties of word vector representations have been studied extensively, far less is known about the properties of sentence vector representations. Word vectors are often evaluated by assessing to what degree they exhibit regularities with regard to relationships of the sort considered in word analogies. In this paper, we investigate to what extent commonly used sentence vector representation spaces as well reflect certain kinds of regularities. We propose a number of schemes to induce evaluation data, based on lexical analogy data as well as semantic relationships between sentences. Our experiments consider a wide range of sentence embedding methods, including ones based on BERT-style contextual embeddings. We find that different models differ substantially in their ability to reflect such regularities.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2020,
    "referenceCount": 39,
    "citationCount": 30,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51121868",
        "name": "Xunjie Zhu"
      },
      {
        "authorId": "144608002",
        "name": "Gerard de Melo"
      }
    ]
  },
  "247410985": {
    "paperId": "2886734184eb7efc1dca1b33c35d33bc41cdfe5c",
    "externalIds": {
      "ArXiv": "2203.05877",
      "DBLP": "journals/corr/abs-2203-05877",
      "ACL": "2022.findings-acl.22",
      "DOI": "10.48550/arXiv.2203.05877",
      "CorpusId": 247410985
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings",
    "abstract": "Contrastive learning has shown great potential in unsupervised sentence embedding tasks, e.g., SimCSE (CITATION).However, these existing solutions are heavily affected by superficial features like the length of sentences or syntactic structures. In this paper, we propose a semantic-aware contrastive learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT), which is able to explore the pseudo-token space (i.e., latent semantic space) representation of a sentence while eliminating the impact of superficial features such as sentence length and syntax. Specifically, we introduce an additional pseudo token embedding layer independent of the BERT encoder to map each sentence into a sequence of pseudo tokens in a fixed length. Leveraging these pseudo sequences, we are able to construct same-length positive and negative pairs based on the attention mechanism to perform contrastive learning. In addition, we utilize both the gradient-updating and momentum-updating encoders to encode instances while dynamically maintaining an additional queue to store the representation of sentence embeddings, enhancing the encoder\u2019s learning performance for negative examples. Experiments show that our model outperforms the state-of-the-art baselines on six standard semantic textual similarity (STS) tasks. Furthermore, experiments on alignments and uniformity losses, as well as hard examples with different sentence lengths and syntax, consistently verify the effectiveness of our method.",
    "venue": "Findings",
    "year": 2022,
    "referenceCount": 55,
    "citationCount": 15,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2114894741",
        "name": "Haochen Tan"
      },
      {
        "authorId": "152348954",
        "name": "Wei Shao"
      },
      {
        "authorId": "2109295256",
        "name": "Han Wu"
      },
      {
        "authorId": "2119301597",
        "name": "Ke Yang"
      },
      {
        "authorId": "38117987",
        "name": "Linqi Song"
      }
    ]
  },
  "253224438": {
    "paperId": "b7395263bbb1612ab6fdf9ca331792a6542d713a",
    "externalIds": {
      "DBLP": "conf/ijcnlp/OpitzF22",
      "ACL": "2022.aacl-main.48",
      "ArXiv": "2206.07023",
      "CorpusId": 253224438
    },
    "publicationVenue": null,
    "title": "SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features",
    "abstract": "Models based on large-pretrained language models, such as S(entence)BERT, provide effective and efficient sentence embeddings that show high correlation to human similarity ratings, but lack interpretability. On the other hand, graph metrics for graph-based meaning representations (e.g., Abstract Meaning Representation, AMR) can make explicit the semantic aspects in which two sentences are similar. However, such metrics tend to be slow, rely on parsers, and do not reach state-of-the-art performance when rating sentence similarity. In this work, we aim at the best of both worlds, by learning to induce Semantically Structured Sentence BERT embeddings (S^3BERT). Our S^3BERT embeddings are composed of explainable sub-embeddings that emphasize various sentence meaning features (e.g., semantic roles, negation, or quantification). We show how to i) learn a decomposition of the sentence embeddings into meaning features, through approximation of a suite of interpretable semantic AMR graph metrics, and how to ii) preserve the overall power of the neural embeddings by controlling the decomposition learning process with a second objective that enforces consistency with the similarity ratings of an SBERT teacher model. In our experimental studies, we show that our approach offers interpretability \u2013 while preserving the effectiveness and efficiency of the neural sentence embeddings.",
    "venue": "AACL",
    "year": 2022,
    "referenceCount": 63,
    "citationCount": 26,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32781138",
        "name": "Juri Opitz"
      },
      {
        "authorId": "143876555",
        "name": "A. Frank"
      }
    ]
  },
  "252992519": {
    "paperId": "8aa2c70563bb2c03e17297d8d726890da3d5f001",
    "externalIds": {
      "DBLP": "journals/sigkdd/UchenduL023",
      "ArXiv": "2210.10488",
      "DOI": "10.1145/3606274.3606276",
      "CorpusId": 252992519
    },
    "publicationVenue": {
      "id": "e3c9b2eb-d870-43b6-811e-7b451583a8df",
      "name": "SIGKDD Explorations",
      "type": "journal",
      "alternate_names": [
        "Sigkdd Explor",
        "SIGKDD Explor",
        "Sigkdd Explorations"
      ],
      "issn": "1931-0145",
      "url": "http://dl.acm.org/toc.cfm?id=J721",
      "alternate_urls": [
        "http://www.sigkdd.org/explorations/archive.php",
        "http://portal.acm.org/sigkdd/newsletter/",
        "http://www.sigkdd.org/explorations/about.php"
      ]
    },
    "title": "Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective",
    "abstract": "Two interlocking research questions of growing interest and importance in privacy research are Authorship Attribution (AA) and Authorship Obfuscation (AO). Given an artifact, especially a text t in question, an AA solution aims to accurately attribute t to its true author out of many candidate authors while an AO solution aims to modify t to hide its true authorship. Traditionally, the notion of authorship and its accompanying privacy concern is only toward human authors. However, in recent years, due to the explosive advancements in Neural Text Generation (NTG) techniques in NLP, capable of synthesizing human-quality openended texts (so-called \"neural texts\"), one has to now consider authorships by humans, machines, or their combination. Due to the implications and potential threats of neural texts when used maliciously, it has become critical to understand the limitations of traditional AA/AO solutions and develop novel AA/AO solutions in dealing with neural texts. In this survey, therefore, we make a comprehensive review of recent literature on the attribution and obfuscation of neural text authorship from a Data Mining perspective, and share our view on their limitations and promising research directions.",
    "venue": "SIGKDD Explorations",
    "year": 2022,
    "referenceCount": 131,
    "citationCount": 33,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "150035131",
        "name": "Adaku Uchendu"
      },
      {
        "authorId": "145535348",
        "name": "Thai Le"
      },
      {
        "authorId": "2158951945",
        "name": "Dongwon Lee"
      }
    ]
  },
  "227118606": {
    "paperId": "3a1f8829e641b46f661775f64a7f27b933a46103",
    "externalIds": {
      "DBLP": "conf/emnlp/QiCLYLS21",
      "ACL": "2021.emnlp-main.752",
      "ArXiv": "2011.10369",
      "MAG": "3109409894",
      "DOI": "10.18653/v1/2021.emnlp-main.752",
      "CorpusId": 227118606
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "ONION: A Simple and Effective Defense Against Textual Backdoor Attacks",
    "abstract": "Backdoor attacks are a kind of emergent training-time threat to deep neural networks (DNNs). They can manipulate the output of DNNs and possess high insidiousness. In the field of natural language processing, some attack methods have been proposed and achieve very high attack success rates on multiple popular models. Nevertheless, there are few studies on defending against textual backdoor attacks. In this paper, we propose a simple and effective textual backdoor defense named ONION, which is based on outlier word detection and, to the best of our knowledge, is the first method that can handle all the textual backdoor attack situations. Experiments demonstrate the effectiveness of our model in defending BiLSTM and BERT against five different backdoor attacks. All the code and data of this paper can be obtained at https://github.com/thunlp/ONION.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 30,
    "citationCount": 203,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "51466208",
        "name": "Fanchao Qi"
      },
      {
        "authorId": "123331686",
        "name": "Yangyi Chen"
      },
      {
        "authorId": "2027599235",
        "name": "Mukai Li"
      },
      {
        "authorId": "49293587",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "1753344",
        "name": "Maosong Sun"
      }
    ]
  },
  "259309096": {
    "paperId": "f5fa0b3c2ecbf17ba922932432bed46a1447ed23",
    "externalIds": {
      "ArXiv": "2306.17194",
      "DBLP": "journals/corr/abs-2306-17194",
      "DOI": "10.48550/arXiv.2306.17194",
      "CorpusId": 259309096
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "On the Exploitability of Instruction Tuning",
    "abstract": "Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \\textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs. Code is available at \\url{https://github.com/azshue/AutoPoison}.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 56,
    "citationCount": 69,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1643697854",
        "name": "Manli Shu"
      },
      {
        "authorId": "2110170885",
        "name": "Jiong Wang"
      },
      {
        "authorId": "1431754650",
        "name": "Chen Zhu"
      },
      {
        "authorId": "8284185",
        "name": "Jonas Geiping"
      },
      {
        "authorId": "2723309",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "1962083",
        "name": "T. Goldstein"
      }
    ]
  },
  "258866212": {
    "paperId": "82fe948f18ca0138d035f553286c5e4b712dbdbe",
    "externalIds": {
      "DBLP": "conf/naacl/XuMWXC24",
      "ACL": "2024.naacl-long.171",
      "ArXiv": "2305.14710",
      "DOI": "10.48550/arXiv.2305.14710",
      "CorpusId": 258866212
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
    "abstract": "We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions (~1000 tokens) and control model behavior through data poisoning, without even the need to modify data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets. As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks, such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner; instruction transfer where attackers can directly apply poisoned instruction on many other datasets; and poison resistance to continual finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree. These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 65,
    "citationCount": 56,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2110519123",
        "name": "Jiashu Xu"
      },
      {
        "authorId": "144592155",
        "name": "Mingyu Derek Ma"
      },
      {
        "authorId": "47939052",
        "name": "Fei Wang"
      },
      {
        "authorId": "2723309",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      }
    ]
  },
  "258865399": {
    "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-14950",
      "ArXiv": "2305.14950",
      "DOI": "10.48550/arXiv.2305.14950",
      "CorpusId": 258865399
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Adversarial Demonstration Attacks on Large Language Models",
    "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 43,
    "citationCount": 42,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2110170885",
        "name": "Jiong Wang"
      },
      {
        "authorId": "2155718616",
        "name": "Zi-yang Liu"
      },
      {
        "authorId": "2218344932",
        "name": "Keun Hee Park"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      },
      {
        "authorId": "2723309",
        "name": "Chaowei Xiao"
      }
    ]
  },
  "235293967": {
    "paperId": "ff9d04fc15a2c52d982b5b7daa787a373ed7f899",
    "externalIds": {
      "ArXiv": "2106.01221",
      "DBLP": "conf/acl/YueDWLSC21",
      "ACL": "2021.findings-acl.337",
      "DOI": "10.18653/v1/2021.findings-acl.337",
      "CorpusId": 235293967
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Differential Privacy for Text Analytics via Natural Text Sanitization",
    "abstract": "Texts convey sophisticated knowledge. However, texts also convey sensitive information. Despite the success of general-purpose language models and domain-specific mechanisms with differential privacy (DP), existing text sanitization mechanisms still provide low utility, as cursed by the high-dimensional text representation. The companion issue of utilizing sanitized texts for downstream analytics is also under-explored. This paper takes a direct approach to text sanitization. Our insight is to consider both sensitivity and similarity via our new local DP notion. The sanitized texts also contribute to our sanitization-aware pretraining and fine-tuning, enabling privacy-preserving natural language processing over the BERT language model with promising utility. Surprisingly, the high utility does not boost up the success rate of inference attacks.",
    "venue": "Findings",
    "year": 2021,
    "referenceCount": 48,
    "citationCount": 58,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145548079",
        "name": "Xiang Yue"
      },
      {
        "authorId": "2938213",
        "name": "Minxin Du"
      },
      {
        "authorId": "49980880",
        "name": "Tianhao Wang"
      },
      {
        "authorId": "2110479359",
        "name": "Yaliang Li"
      },
      {
        "authorId": "1515546612",
        "name": "Huan Sun"
      },
      {
        "authorId": "145876490",
        "name": "Sherman S. M. Chow"
      }
    ]
  },
  "253116660": {
    "paperId": "58996964dbcd15045b66201c2b850b5570ba74cb",
    "externalIds": {
      "DBLP": "journals/corr/abs-2210-14348",
      "ACL": "2023.acl-long.74",
      "ArXiv": "2210.14348",
      "DOI": "10.48550/arXiv.2210.14348",
      "CorpusId": 253116660
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe",
    "abstract": "Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe in the text domain is effective: simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection. Through extensive empirical analyses on both benchmark and private customer data, we demonstrate that our method produces synthetic text that is competitive in terms of utility with its non-private counterpart, meanwhile providing strong protection against potential privacy leakages.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 73,
    "citationCount": 56,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145548079",
        "name": "Xiang Yue"
      },
      {
        "authorId": "3058104",
        "name": "Huseyin A. Inan"
      },
      {
        "authorId": "2145429039",
        "name": "Xuechen Li"
      },
      {
        "authorId": "2090135421",
        "name": "Girish Kumar"
      },
      {
        "authorId": "69041346",
        "name": "Julia McAnallen"
      },
      {
        "authorId": "1515546612",
        "name": "Huan Sun"
      },
      {
        "authorId": "2188832906",
        "name": "David Levitan"
      },
      {
        "authorId": "1562202621",
        "name": "Robert Sim"
      }
    ]
  },
  "246823897": {
    "paperId": "62d17b6f6ad77fd71ef9954c7784700d5e316f1f",
    "externalIds": {
      "ArXiv": "2202.05520",
      "DBLP": "conf/fat/BrownLMST22",
      "DOI": "10.1145/3531146.3534642",
      "CorpusId": 246823897
    },
    "publicationVenue": {
      "id": "cdc83875-a82d-445c-b097-cbe91afe99a8",
      "name": "Conference on Fairness, Accountability and Transparency",
      "type": "conference",
      "alternate_names": [
        "FAccT",
        "Conf Fairness Account Transpar"
      ],
      "url": "https://facctconference.org/"
    },
    "title": "What Does it Mean for a Language Model to Preserve Privacy?",
    "abstract": "Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.",
    "venue": "Conference on Fairness, Accountability and Transparency",
    "year": 2022,
    "referenceCount": 108,
    "citationCount": 177,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2105643848",
        "name": "Hannah Brown"
      },
      {
        "authorId": "3844009",
        "name": "Katherine Lee"
      },
      {
        "authorId": "52195885",
        "name": "FatemehSadat Mireshghallah"
      },
      {
        "authorId": "2520493",
        "name": "R. Shokri"
      },
      {
        "authorId": "2444919",
        "name": "Florian Tram\u00e8r"
      }
    ]
  },
  "256194179": {
    "paperId": "cb5b71a622aff47014d4f28a958679629a8b6363",
    "externalIds": {
      "DBLP": "journals/corr/abs-2301-10226",
      "ArXiv": "2301.10226",
      "DOI": "10.48550/arXiv.2301.10226",
      "CorpusId": 256194179
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "A Watermark for Large Language Models",
    "abstract": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "referenceCount": 62,
    "citationCount": 345,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2166053502",
        "name": "John Kirchenbauer"
      },
      {
        "authorId": "8284185",
        "name": "Jonas Geiping"
      },
      {
        "authorId": "123191916",
        "name": "Yuxin Wen"
      },
      {
        "authorId": "143975296",
        "name": "Jonathan Katz"
      },
      {
        "authorId": "2679804",
        "name": "Ian Miers"
      },
      {
        "authorId": "1962083",
        "name": "T. Goldstein"
      }
    ]
  },
  "256627372": {
    "paperId": "c25d2a27f1abe169d7b68078071b6698f0980469",
    "externalIds": {
      "DBLP": "conf/icml/ZhaoWL23",
      "ArXiv": "2302.03162",
      "DOI": "10.48550/arXiv.2302.03162",
      "CorpusId": 256627372
    },
    "publicationVenue": {
      "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning",
      "type": "conference",
      "alternate_names": [
        "ICML",
        "Int Conf Mach Learn"
      ],
      "url": "https://icml.cc/"
    },
    "title": "Protecting Language Generation Models via Invisible Watermarking",
    "abstract": "Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as\"synonym randomization\". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (mAP) in detecting suspects compared to previous methods against watermark removal attacks.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "referenceCount": 38,
    "citationCount": 64,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "150345512",
        "name": "Xuandong Zhao"
      },
      {
        "authorId": "2143529300",
        "name": "Yu-Xiang Wang"
      },
      {
        "authorId": "143900005",
        "name": "Lei Li"
      }
    ]
  },
  "257900638": {
    "paperId": "49f1fa0d609ff06564b46270cbc022b7d9d195f4",
    "externalIds": {
      "ArXiv": "2303.18190",
      "DBLP": "journals/corr/abs-2303-18190",
      "DOI": "10.48550/arXiv.2303.18190",
      "CorpusId": 257900638
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Assessing Language Model Deployment with Risk Cards",
    "abstract": "This paper introduces RiskCards, a framework for structured assessment and documentation of risks associated with an application of language models. As with all language, text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text. Prior work establishes a wide variety of language model harms to many different actors: existing taxonomies identify categories of harms posed by language models; benchmarks establish automated tests of these harms; and documentation standards for models, tasks and datasets encourage transparent reporting. However, there is no risk-centric framework for documenting the complexity of a landscape in which some risks are shared across models and contexts, while others are specific, and where certain conditions may be required for risks to manifest as harms. RiskCards address this methodological gap by providing a generic framework for assessing the use of a given language model in a given scenario. Each RiskCard makes clear the routes for the risk to manifest harm, their placement in harm taxonomies, and example prompt-output pairs. While RiskCards are designed to be open-source, dynamic and participatory, we present a\"starter set\"of RiskCards taken from a broad literature survey, each of which details a concrete risk presentation. Language model RiskCards initiate a community knowledge base which permits the mapping of risks and harms to a specific model or its application scenario, ultimately contributing to a better, safer and shared understanding of the risk landscape.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 67,
    "citationCount": 35,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "113320522",
        "name": "Leon Derczynski"
      },
      {
        "authorId": "90729626",
        "name": "Hannah Rose Kirk"
      },
      {
        "authorId": "143820870",
        "name": "Vidhisha Balachandran"
      },
      {
        "authorId": "51467955",
        "name": "Sachin Kumar"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "119004240",
        "name": "M. Leiser"
      },
      {
        "authorId": "2057036852",
        "name": "Saif Mohammad"
      }
    ]
  },
  "256627571": {
    "paperId": "cd0988714ea326642d2b1bb18753e187fec71e42",
    "externalIds": {
      "DBLP": "journals/corr/abs-2302-03494",
      "ArXiv": "2302.03494",
      "DOI": "10.48550/arXiv.2302.03494",
      "CorpusId": 256627571
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "A Categorical Archive of ChatGPT Failures",
    "abstract": "Large language models have been demonstrated to be valuable in different fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 72,
    "citationCount": 325,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3177797",
        "name": "A. Borji"
      }
    ]
  },
  "18301384": {
    "paperId": "f5b1b05e8313aee94ccd98e80eab3ec56dbd2c97",
    "externalIds": {
      "MAG": "2484934946",
      "DOI": "10.1093/ACPROF:OSOBL/9780199548033.003.0016",
      "CorpusId": 18301384
    },
    "publicationVenue": null,
    "title": "\u2018How does it Work?\u2019 vs. \u2018What are the Laws?\u2019",
    "abstract": "1. In the beginning In the beginning, there was the DN (Deductive Nomological) model of explanation, articulated by Hempel and Oppenheim (1948). According to DN, scientific explanation is subsumption under natural law. Individual events are explained by deducing them from laws together with initial conditions (or boundary conditions), and laws are explained by deriving them from other more fundamental laws, as, for example, the simple pendulum law is derived from Newton's laws of motion. It is well-known that DN is vulnerable to a wide variety of counter-As a result, DN is not widely defended. But it is, I think, still widely believed that scientific explanation is subsumption under law. This is something of a scandal: Given DN's miserable track record in spite of spirited defense by many ingenious believers, one is led to ask why so many cleave so faithfully to a doctrine that has proved so indefensible? There are two factors that work to keep DN in place. First, there is the fact that every experimental paper one picks up involves the explanation of some data by appeal to some hypothesis or other. It is tempting to conclude that philosophers' continued failure to articulate this practice in some defensible way is a point against philosophers, not against DN. And second, there is the fact that there is no widely understood and compelling alternative to DN on the",
    "venue": "",
    "year": 2010,
    "referenceCount": 28,
    "citationCount": 54,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3869963",
        "name": "R. Cummins"
      }
    ]
  },
  "271092556": {
    "paperId": "c75acce5a6267b49f0d99e1d6d1cd2670067ce44",
    "externalIds": {
      "MAG": "2119858020",
      "DOI": "10.1146/ANNUREV.PSYCH.57.102904.190100",
      "CorpusId": 271092556,
      "PubMed": "16318595"
    },
    "publicationVenue": {
      "id": "35ed31a9-a45e-4601-ab4f-49374a1d870f",
      "name": "Annual Review of Psychology",
      "type": "journal",
      "alternate_names": [
        "Annu Rev Psychol"
      ],
      "issn": "0066-4308",
      "url": "https://www.annualreviews.org/journal/psych",
      "alternate_urls": [
        "http://arjournals.annualreviews.org/loi/psych",
        "http://psych.annualreviews.org/"
      ]
    },
    "title": "Explanation and understanding.",
    "abstract": "The study of explanation, while related to intuitive theories, concepts, and mental models, offers important new perspectives on high-level thought. Explanations sort themselves into several distinct types corresponding to patterns of causation, content domains, and explanatory stances, all of which have cognitive consequences. Although explanations are necessarily incomplete--often dramatically so in laypeople--those gaps are difficult to discern. Despite such gaps and the failure to recognize them fully, people do have skeletal explanatory senses, often implicit, of the causal structure of the world. They further leverage those skeletal understandings by knowing how to access additional explanatory knowledge in other minds and by being particularly adept at using situational support to build explanations on the fly in real time. Across development and cultures, there are differences in preferred explanatory schemes, but rarely are any kinds of schemes completely unavailable to a group.",
    "venue": "Annual Review of Psychology",
    "year": 2006,
    "referenceCount": 134,
    "citationCount": 112,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2310633823",
        "name": "Frank C. Keil"
      }
    ]
  },
  "36024272": {
    "paperId": "e89dfa306723e8ef031765e9c44e5f6f94fd8fda",
    "externalIds": {
      "ArXiv": "1706.07269",
      "MAG": "2953283116",
      "DBLP": "journals/ai/Miller19",
      "DOI": "10.1016/J.ARTINT.2018.07.007",
      "CorpusId": 36024272
    },
    "publicationVenue": {
      "id": "96018464-22dc-4b5c-a172-c2f4a30ce131",
      "name": "Artificial Intelligence",
      "type": "journal",
      "alternate_names": [
        "Artif Intell"
      ],
      "issn": "0004-3702",
      "alternate_issns": [
        "2633-1403",
        "2710-1673",
        "2710-1681"
      ],
      "url": "http://www.elsevier.com/locate/artint",
      "alternate_urls": [
        "http://www.sciencedirect.com/science/journal/00043702",
        "https://www.journals.elsevier.com/artificial-intelligence"
      ]
    },
    "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences",
    "abstract": null,
    "venue": "Artificial Intelligence",
    "year": 2017,
    "referenceCount": 200,
    "citationCount": 3787,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144658641",
        "name": "Tim Miller"
      }
    ]
  },
  "249989109": {
    "paperId": "faf565a1c2287279166eb55df68361b0a5d185d8",
    "externalIds": {
      "DBLP": "journals/cogsci/VrantsidisL22",
      "DOI": "10.1111/cogs.13169",
      "CorpusId": 249989109,
      "PubMed": "35738485"
    },
    "publicationVenue": {
      "id": "c33b01b0-31b4-470e-a9f9-8432e02c3cb9",
      "name": "Cognitive Sciences",
      "type": "journal",
      "alternate_names": [
        "Cognitive Science",
        "Cogn Sci"
      ],
      "issn": "1935-8059",
      "alternate_issns": [
        "0364-0213"
      ],
      "url": "http://www.informaworld.com/openurl?genre=journal&issn=1551-6709",
      "alternate_urls": [
        "http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1551-6709",
        "http://www3.interscience.wiley.com/cgi-bin/jtoc?ID=121670282",
        "https://onlinelibrary.wiley.com/journal/15516709",
        "http://www.sciencedirect.com/science/journal/03640213",
        "http://www.leaonline.com/loi/cog"
      ]
    },
    "title": "Simplicity as a Cue to Probability: Multiple Roles for Simplicity in Evaluating Explanations",
    "abstract": "People often face the challenge of evaluating competing explanations. One approach is to assess the explanations' relative probabilities-for example, applying Bayesian inference to compute their posterior probabilities. Another approach is to consider an explanation's qualities or \"virtues,\" such as its relative simplicity (i.e., the number of unexplained causes it invokes). The current work investigates how these two approaches are related. Study 1 found that simplicity is used to infer the inputs to Bayesian inference (explanations' priors and likelihoods). Studies 1 and 2 found that simplicity is also used as a direct cue to the outputs of Bayesian inference (the posterior probability of an explanation), such that simplicity affects estimates of posterior probability even after controlling for elicited (Study 1) or provided (Study 2) priors and likelihoods, with simplicity having a larger effect in Study 1, where posteriors are more uncertain and difficult to compute. Comparing Studies 1 and 2 also suggested that simplicity plays additional roles unrelated to approximating probabilities, as reflected in simplicity's effect on how \"satisfying\" (vs. probable) an explanation is, which remained largely unaffected by the difficulty of computing posteriors. Together, these results suggest that the virtue of simplicity is used in multiple ways to approximate probabilities (i.e., serving as a cue to priors, likelihoods, and posteriors) when these probabilities are otherwise uncertain or difficult to compute, but that the influence of simplicity also goes beyond these roles.",
    "venue": "Cognitive Sciences",
    "year": 2022,
    "referenceCount": 54,
    "citationCount": 10,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2298882681",
        "name": "Thalia Vrantsidis"
      },
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      }
    ]
  },
  "13138761": {
    "paperId": "3d35362c4afcf9319e59fd3e2365e0b509791e1d",
    "externalIds": {
      "MAG": "2787006412",
      "DBLP": "journals/cogsci/VasilyevaBL18",
      "DOI": "10.1111/cogs.12605",
      "CorpusId": 13138761,
      "PubMed": "29687462"
    },
    "publicationVenue": {
      "id": "9c06885c-ecb6-4a76-ba1e-fbad53521efd",
      "name": "Annual Meeting of the Cognitive Science Society",
      "type": "conference",
      "alternate_names": [
        "CogSci",
        "Annu Meet Cogn Sci Soc"
      ],
      "url": "http://cognitivesciencesociety.org/"
    },
    "title": "Stable Causal Relationships are Better Causal Relationships",
    "abstract": "We report three experiments investigating whether people's judgments about causal relationships are sensitive to the robustness or stability of such relationships across a range of background circumstances. In Experiment 1, we demonstrate that people are more willing to endorse causal and explanatory claims based on stable (as opposed to unstable) relationships, even when the overall causal strength of the relationship is held constant. In Experiment 2, we show that this effect is not driven by a causal generalization's actual scope of application. In Experiment 3, we offer evidence that stable causal relationships may be seen as better guides to action. Collectively, these experiments document a previously underappreciated factor that shapes people's causal reasoning: the stability of the causal relationship.",
    "venue": "Annual Meeting of the Cognitive Science Society",
    "year": 2018,
    "referenceCount": 46,
    "citationCount": 32,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145541797",
        "name": "N. Vasilyeva"
      },
      {
        "authorId": "90002703",
        "name": "Thomas Blanchard"
      },
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      }
    ]
  },
  "254221257": {
    "paperId": "ab72b242834192b39c245d76b40adae903c7dd28",
    "externalIds": {
      "ArXiv": "2211.05667",
      "CorpusId": 254221257
    },
    "publicationVenue": null,
    "title": "What Makes a Good Explanation?: A Harmonized View of Properties of Explanations",
    "abstract": "Interpretability provides a means for humans to verify aspects of machine learning (ML) models and empower human+ML teaming in situations where the task cannot be fully automated. Different contexts require explanations with different properties. For example, the kind of explanation required to determine if an early cardiac arrest warning system is ready to be integrated into a care setting is very different from the type of explanation required for a loan applicant to help determine the actions they might need to take to make their application successful. Unfortunately, there is a lack of standardization when it comes to properties of explanations: different papers may use the same term to mean different quantities, and different terms to mean the same quantity. This lack of a standardized terminology and categorization of the properties of ML explanations prevents us from both rigorously comparing interpretable machine learning methods and identifying what properties are needed in what contexts. In this work, we survey properties defined in interpretable machine learning papers, synthesize them based on what they actually measure, and describe the trade-offs between different formulations of these properties. In doing so, we enable more informed selection of task-appropriate formulations of explanation properties as well as standardization for future work in interpretable machine learning.",
    "venue": "",
    "year": 2022,
    "referenceCount": 114,
    "citationCount": 15,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "11832370",
        "name": "Zixi Chen"
      },
      {
        "authorId": "1932037094",
        "name": "Varshini Subhash"
      },
      {
        "authorId": "51040874",
        "name": "Marton Havasi"
      },
      {
        "authorId": "3291783",
        "name": "Weiwei Pan"
      },
      {
        "authorId": "1388372395",
        "name": "F. Doshi-Velez"
      }
    ]
  },
  "249926487": {
    "paperId": "4d6e6bd7b9b3aface4a2a8908d74c99634cb2479",
    "externalIds": {
      "ArXiv": "2206.10847",
      "DBLP": "conf/hcomp/LiaoZLDD22",
      "DOI": "10.48550/arXiv.2206.10847",
      "CorpusId": 249926487
    },
    "publicationVenue": {
      "id": "b01f161c-f4c9-46eb-9461-a6c5fa8fd002",
      "name": "AAAI Conference on Human Computation & Crowdsourcing",
      "type": "conference",
      "alternate_names": [
        "HCOMP",
        "AAAI Conf Hum Comput  Crowdsourcing"
      ]
    },
    "title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI",
    "abstract": "Recent years have seen a surge of interest in the field of explainable AI (XAI), with a plethora of algorithms proposed in the literature. However, a lack of consensus on how to evaluate XAI hinders the advancement of the field. We highlight that XAI is not a monolithic set of technologies---researchers and practitioners have begun to leverage XAI algorithms to build XAI systems that serve different usage contexts, such as model debugging and decision-support. Algorithmic research of XAI, however, often does not account for these diverse downstream usage contexts, resulting in limited effectiveness or even unintended consequences for actual users, as well as difficulties for practitioners to make technical choices. We argue that one way to close the gap is to develop evaluation methods that account for different user requirements in these usage contexts. Towards this goal, we introduce a perspective of contextualized XAI evaluation by considering the relative importance of XAI evaluation criteria for prototypical usage contexts of XAI. To explore the context dependency of XAI evaluation criteria, we conduct two survey studies, one with XAI topical experts and another with crowd workers. Our results urge for responsible AI research with usage-informed evaluation practices, and provide a nuanced understanding of user requirements for XAI in different usage contexts.",
    "venue": "AAAI Conference on Human Computation & Crowdsourcing",
    "year": 2022,
    "referenceCount": 60,
    "citationCount": 62,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144921048",
        "name": "Q. Liao"
      },
      {
        "authorId": "2108127520",
        "name": "Yunfeng Zhang"
      },
      {
        "authorId": "1695275",
        "name": "Ronny Luss"
      },
      {
        "authorId": "1388372395",
        "name": "F. Doshi-Velez"
      },
      {
        "authorId": "2145784",
        "name": "Amit Dhurandhar"
      },
      {
        "authorId": "68973648",
        "name": "Microsoft Research"
      },
      {
        "authorId": "2227018833",
        "name": "Twitter Inc"
      },
      {
        "authorId": "2085884554",
        "name": "Ibm Research"
      }
    ]
  },
  "266374512": {
    "paperId": "17f9fea1d9fd31d111a1eb8f6fc7328f046a3cee",
    "externalIds": {
      "DBLP": "journals/corr/abs-2312-12747",
      "ArXiv": "2312.12747",
      "DOI": "10.48550/arXiv.2312.12747",
      "CorpusId": 266374512
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "ALMANACS: A Simulatability Benchmark for Language Model Explainability",
    "abstract": "How do we measure the efficacy of language model explainability methods? While many explainability methods have been developed, they are typically evaluated on bespoke tasks, preventing an apples-to-apples comparison. To help fill this gap, we present ALMANACS, a language model explainability benchmark. ALMANACS scores explainability methods on simulatability, i.e., how well the explanations improve behavior prediction on new inputs. The ALMANACS scenarios span twelve safety-relevant topics such as ethical reasoning and advanced AI behaviors; they have idiosyncratic premises to invoke model-specific behavior; and they have a train-test distributional shift to encourage faithful explanations. By using another language model to predict behavior based on the explanations, ALMANACS is a fully automated benchmark. We use ALMANACS to evaluate counterfactuals, rationalizations, attention, and Integrated Gradients explanations. Our results are sobering: when averaged across all topics, no explanation method outperforms the explanation-free control. We conclude that despite modest successes in prior work, developing an explanation method that aids simulatability in ALMANACS remains an open challenge.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 71,
    "citationCount": 4,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Mathematics",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2275158766",
        "name": "Edmund Mills"
      },
      {
        "authorId": "2275412153",
        "name": "Shiye Su"
      },
      {
        "authorId": "2237426863",
        "name": "Stuart Russell"
      },
      {
        "authorId": "2237427074",
        "name": "Scott Emmons"
      }
    ]
  },
  "218502350": {
    "paperId": "cffd8f947ba03644f62baea31c64c8920b06288e",
    "externalIds": {
      "MAG": "3035371891",
      "ACL": "2020.acl-main.491",
      "ArXiv": "2005.01831",
      "DBLP": "conf/acl/HaseB20",
      "DOI": "10.18653/v1/2020.acl-main.491",
      "CorpusId": 218502350
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?",
    "abstract": "Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 34,
    "citationCount": 264,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144625004",
        "name": "Peter Hase"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      }
    ]
  },
  "240990378": {
    "paperId": "3d6ff845b6383838e18a429acea6f09e225076ac",
    "externalIds": {
      "DOI": "10.31219/osf.io/ws79n",
      "CorpusId": 240990378,
      "PubMed": "34928686"
    },
    "publicationVenue": {
      "id": "ba388b36-981e-4c1b-8048-464cdaa9c9fc",
      "name": "Journal of experimental psychology. General",
      "type": "journal",
      "alternate_names": [
        "J Exp Psychol Gen",
        "J exp psychol Gen",
        "Journal of Experimental Psychology: General"
      ],
      "issn": "0096-3445",
      "url": "http://www.apa.org/journals/xge.html",
      "alternate_urls": [
        "http://content.apa.org/journals/xge",
        "http://www.apa.org/pubs/journals/xge/index.aspx"
      ]
    },
    "title": "Explaining the existential: Scientific and religious explanations play different functional roles.",
    "abstract": "How did the universe come to exist? What happens after we die? Answers to existential questions tend to elicit both scientific and religious explanations, offering a unique opportunity to evaluate how these domains differ in their psychological roles. Across 3 studies (N = 1,647), we investigate whether (and by whom) scientific and religious explanations are perceived to have epistemic merits-such as evidential and logical support-versus nonepistemic merits-such as social, emotional, or moral benefits. We find that scientific explanations are attributed more epistemic merits than are religious explanations (Study 1), that an explanation's perceived epistemic merits are more strongly predicted by endorsement of that explanation for science than for religion (Study 2), and that scientific explanations are more likely to be generated when participants are prompted for an explanation high in epistemic merits (Study 3). By contrast, we find that religious explanations are attributed more nonepistemic merits than are scientific explanations (Study 1), that an explanation's perceived nonepistemic merits are more strongly predicted by endorsement of that explanation for religion than for science (Study 2), and that religious explanations are more likely to be generated when participants are prompted for an explanation high in nonepistemic merits (Study 3). These findings inform theories of the relationship between religion and science, and they provide insight into accounts of the coexistence of scientific and religious cognition. (PsycInfo Database Record (c) 2021 APA, all rights reserved).",
    "venue": "Journal of experimental psychology. General",
    "year": 2020,
    "referenceCount": 0,
    "citationCount": 14,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "7213431",
        "name": "Telli Davoodi"
      },
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      }
    ]
  },
  "8510430": {
    "paperId": "19ca12d11e574036ca182250d5b220c7e3bb3c60",
    "externalIds": {
      "DBLP": "conf/cogsci/VasilyevaL15",
      "PubMedCentral": "9377274",
      "MAG": "2399564276",
      "DOI": "10.3389/fpsyg.2022.911177",
      "CorpusId": 8510430,
      "PubMed": "35978769"
    },
    "publicationVenue": {
      "id": "89097a03-8be6-4e2d-ae2c-a6df64c77a06",
      "name": "Frontiers in Psychology",
      "type": "journal",
      "alternate_names": [
        "Front Psychol"
      ],
      "issn": "1664-1078",
      "url": "http://www.frontiersin.org/Cultural_Psychology",
      "alternate_urls": [
        "http://frontiersin.org/psychology/",
        "https://www.frontiersin.org/journals/psychology",
        "http://journal.frontiersin.org/journal/psychology",
        "http://www.frontiersin.org/psychology"
      ]
    },
    "title": "Explanations and Causal Judgments Are Differentially Sensitive to Covariation and Mechanism Information",
    "abstract": "Are causal explanations (e.g., \u201cshe switched careers because of the COVID pandemic\u201d) treated differently from the corresponding claims that one factor caused another (e.g., \u201cthe COVID pandemic caused her to switch careers\u201d)? We examined whether explanatory and causal claims diverge in their responsiveness to two different types of information: covariation strength and mechanism information. We report five experiments with 1,730 participants total, showing that compared to judgments of causal strength, explanatory judgments tend to be more sensitive to mechanism and less sensitive to covariation \u2013 even though explanatory judgments respond to both types of information. We also report exploratory comparisons to judgments of understanding, and discuss implications of our findings for theories of explanation, understanding, and causal attribution. These findings shed light on the potentially unique role of explanation in cognition.",
    "venue": "Frontiers in Psychology",
    "year": 2022,
    "referenceCount": 67,
    "citationCount": 8,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145541797",
        "name": "N. Vasilyeva"
      },
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      }
    ]
  },
  "244868314": {
    "paperId": "470876c64558e6849d7f3209ca33e8346e6ea747",
    "externalIds": {
      "DOI": "10.1016/j.cogpsych.2021.101453",
      "CorpusId": 244868314,
      "PubMed": "34875484"
    },
    "publicationVenue": {
      "id": "96ec4932-399f-4839-b1c9-12ddad2382cc",
      "name": "Cognitive Psychology",
      "type": "journal",
      "alternate_names": [
        "Cogn Psychol"
      ],
      "issn": "0010-0285",
      "url": "https://www.journals.elsevier.com/cognitive-psychology",
      "alternate_urls": [
        "http://www.idealibrary.com/links/toc/cogp",
        "http://www.sciencedirect.com/science/journal/00100285"
      ]
    },
    "title": "Motivated to learn: An account of explanatory satisfaction",
    "abstract": null,
    "venue": "Cognitive Psychology",
    "year": 2021,
    "referenceCount": 105,
    "citationCount": 16,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "32268922",
        "name": "Emily G. Liquin"
      },
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      }
    ]
  },
  "246680394": {
    "paperId": "101309da0732648e72929a5327341d325fca57fa",
    "externalIds": {
      "DBLP": "journals/corr/abs-2202-04092",
      "ArXiv": "2202.04092",
      "DOI": "10.1145/3593013.3593970",
      "CorpusId": 246680394
    },
    "publicationVenue": null,
    "title": "Machine Explanations and Human Understanding",
    "abstract": "Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. To address this question, we first identify three core concepts that cover most existing quantitative measures of understanding: task decision boundary, model decision boundary, and model error. Using adapted causal diagrams, we provide a formal characterization of the relationship between these concepts and human approximations (i.e., understanding) of them. The relationship varies by the level of human intuition in different task types, such as emulation and discovery, which are often ignored when building or evaluating explanation methods. Our key result is that human intuitions are necessary for generating and evaluating machine explanations in human-AI decision making: without assumptions about human intuitions, explanations may improve human understanding of model decision boundary, but cannot improve human understanding of task decision boundary or model error. To validate our theoretical claims, we conduct human subject studies to show the importance of human intuitions. Together with our theoretical contributions, we provide a new paradigm for designing behavioral studies towards a rigorous view of the role of machine explanations across different tasks of human-AI decision making.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2022,
    "referenceCount": 77,
    "citationCount": 22,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2131059444",
        "name": "Chacha Chen"
      },
      {
        "authorId": "2113511266",
        "name": "Shi Feng"
      },
      {
        "authorId": "2143678801",
        "name": "Amit Sharma"
      },
      {
        "authorId": "2111727675",
        "name": "Chenhao Tan"
      }
    ]
  },
  "258865670": {
    "paperId": "4bf3fd4859cb0d37e333ee9ed4024387e265c99e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-14770",
      "ArXiv": "2305.14770",
      "DOI": "10.48550/arXiv.2305.14770",
      "CorpusId": 258865670
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Using Natural Language Explanations to Rescale Human Judgments",
    "abstract": "The rise of large language models (LLMs) has brought a critical need for high-quality human-labeled data, particularly for processes like human feedback and evaluation. A common practice is to label data via consensus annotation over human judgments. However, annotators' judgments for subjective tasks can differ in many ways: they may reflect different qualitative judgments about an example, and they may be mapped to a labeling scheme in different ways. We show that these nuances can be captured by natural language explanations, and propose a method to rescale ordinal annotations and explanations using LLMs. Specifically, we feed annotators' Likert ratings and corresponding explanations into an LLM and prompt it to produce a numeric score anchored in a scoring rubric. These scores should reflect the annotators' underlying assessments of the example. The rubric can be designed or modified after annotation, and include distinctions that may not have been known when the original error taxonomy was devised. We explore our technique in the context of rating system outputs for a document-grounded question answering task, where LLMs achieve near-human performance. Our method rescales the raw judgments without impacting agreement and brings the scores closer to human judgments grounded in the same scoring rubric.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 83,
    "citationCount": 6,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "7341605",
        "name": "Manya Wadhwa"
      },
      {
        "authorId": "3100431",
        "name": "Jifan Chen"
      },
      {
        "authorId": "22319255",
        "name": "Junyi Jessy Li"
      },
      {
        "authorId": "1814094",
        "name": "Greg Durrett"
      }
    ]
  },
  "268064502": {
    "paperId": "44088885e451744849acddde9ad3ca9c1973c6fe",
    "externalIds": {
      "DBLP": "conf/nips/SahaHB23",
      "CorpusId": 268064502
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Can Language Models Teach? Teacher Explanations Improve Student Performance via Personalization",
    "abstract": null,
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 0,
    "citationCount": 10,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "35106509",
        "name": "Swarnadeep Saha"
      },
      {
        "authorId": "2266467463",
        "name": "Peter Hase"
      },
      {
        "authorId": "2265552472",
        "name": "Mohit Bansal"
      }
    ]
  },
  "262217015": {
    "paperId": "024087f125814fa70c6411fba9a4caff75878983",
    "externalIds": {
      "DBLP": "journals/corr/abs-2309-12482",
      "ArXiv": "2309.12482",
      "DOI": "10.48550/arXiv.2309.12482",
      "CorpusId": 262217015
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding",
    "abstract": "As more non-AI experts use complex AI systems for daily tasks, there has been an increasing effort to develop methods that produce explanations of AI decision making that are understandable by non-AI experts. Towards this effort, leveraging higher-level concepts and producing concept-based explanations have become a popular method. Most concept-based explanations have been developed for classification techniques, and we posit that the few existing methods for sequential decision making are limited in scope. In this work, we first contribute a desiderata for defining concepts in sequential decision making settings. Additionally, inspired by the Protege Effect which states explaining knowledge often reinforces one's self-learning, we explore how concept-based explanations of an RL agent's decision making can in turn improve the agent's learning rate, as well as improve end-user understanding of the agent's decision making. To this end, we contribute a unified framework, State2Explanation (S2E), that involves learning a joint embedding model between state-action pairs and concept-based explanations, and leveraging such learned model to both (1) inform reward shaping during an agent's training, and (2) provide explanations to end-users at deployment for improved task performance. Our experimental validations, in Connect 4 and Lunar Lander, demonstrate the success of S2E in providing a dual-benefit, successfully informing reward shaping and improving agent learning rate, as well as significantly improving end user task performance at deployment time.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 57,
    "citationCount": 13,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "66436329",
        "name": "Devleena Das"
      },
      {
        "authorId": "144753437",
        "name": "S. Chernova"
      },
      {
        "authorId": "2244818102",
        "name": "Been Kim"
      }
    ]
  },
  "264555229": {
    "paperId": "ff4113bcddab4470fff70f06fb0e3860dddab6ac",
    "externalIds": {
      "DBLP": "conf/emnlp/HsuDXK23",
      "ArXiv": "2310.17711",
      "DOI": "10.48550/arXiv.2310.17711",
      "CorpusId": 264555229
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term",
    "abstract": "With advancements in natural language processing (NLP) models, automatic explanation generation has been proposed to mitigate misinformation on social media platforms in addition to adding warning labels to identified fake news. While many researchers have focused on generating good explanations, how these explanations can really help humans combat fake news is under-explored. In this study, we compare the effectiveness of a warning label and the state-of-the-art counterfactual explanations generated by GPT-4 in debunking misinformation. In a two-wave, online human-subject study, participants (N = 215) were randomly assigned to a control group in which false contents are shown without any intervention, a warning tag group in which the false claims were labeled, or an explanation group in which the false contents were accompanied by GPT-4 generated explanations. Our results show that both interventions significantly decrease participants' self-reported belief in fake claims in an equivalent manner for the short-term and long-term. We discuss the implications of our findings and directions for future NLP-based misinformation debunking strategies.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 52,
    "citationCount": 2,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2069602444",
        "name": "Yi-Li Hsu"
      },
      {
        "authorId": "3400291",
        "name": "Shih-Chieh Dai"
      },
      {
        "authorId": "2261362789",
        "name": "Aiping Xiong"
      },
      {
        "authorId": "1746959",
        "name": "Lun-Wei Ku"
      }
    ]
  },
  "266163379": {
    "paperId": "57378899b7a68365eb5d9ec86c27ef75208fdbb6",
    "externalIds": {
      "DBLP": "journals/corr/abs-2312-06032",
      "ArXiv": "2312.06032",
      "DOI": "10.48550/arXiv.2312.06032",
      "CorpusId": 266163379
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Evaluating the Utility of Model Explanations for Model Development",
    "abstract": "One of the motivations for explainable AI is to allow humans to make better and more informed decisions regarding the use and deployment of AI models. But careful evaluations are needed to assess whether this expectation has been fulfilled. Current evaluations mainly focus on algorithmic properties of explanations, and those that involve human subjects often employ subjective questions to test human's perception of explanation usefulness, without being grounded in objective metrics and measurements. In this work, we evaluate whether explanations can improve human decision-making in practical scenarios of machine learning model development. We conduct a mixed-methods user study involving image data to evaluate saliency maps generated by SmoothGrad, GradCAM, and an oracle explanation on two tasks: model selection and counterfactual simulation. To our surprise, we did not find evidence of significant improvement on these tasks when users were provided with any of the saliency maps, even the synthetic oracle explanation designed to be simple to understand and highly indicative of the answer. Nonetheless, explanations did help users more accurately describe the models. These findings suggest caution regarding the usefulness and potential for misunderstanding in saliency-based explanations.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 34,
    "citationCount": 1,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2273566293",
        "name": "Shawn Im"
      },
      {
        "authorId": "2273558290",
        "name": "Jacob Andreas"
      },
      {
        "authorId": "2273755916",
        "name": "Yilun Zhou"
      }
    ]
  },
  "201058651": {
    "paperId": "a550f576ff20b8cce98f3ddad0043d3783fbc9b4",
    "externalIds": {
      "DBLP": "conf/iclr/BhagavatulaBMSH20",
      "MAG": "2968629361",
      "ArXiv": "1908.05739",
      "CorpusId": 201058651
    },
    "publicationVenue": {
      "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations",
      "type": "conference",
      "alternate_names": [
        "Int Conf Learn Represent",
        "ICLR"
      ],
      "url": "https://iclr.cc/"
    },
    "title": "Abductive Commonsense Reasoning",
    "abstract": "Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks -- (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform--despite their strong performance on the related but more narrowly defined task of entailment NLI--pointing to interesting avenues for future research.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "referenceCount": 59,
    "citationCount": 430,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1857797",
        "name": "Chandra Bhagavatula"
      },
      {
        "authorId": "39227408",
        "name": "Ronan Le Bras"
      },
      {
        "authorId": "8805254",
        "name": "Chaitanya Malaviya"
      },
      {
        "authorId": "2325708",
        "name": "Keisuke Sakaguchi"
      },
      {
        "authorId": "14487640",
        "name": "Ari Holtzman"
      },
      {
        "authorId": "2516777",
        "name": "Hannah Rashkin"
      },
      {
        "authorId": "145612610",
        "name": "Doug Downey"
      },
      {
        "authorId": "3156075",
        "name": "S. Yih"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      }
    ]
  },
  "226283602": {
    "paperId": "47e799f83b0850f3d036a2e3a66bb337661b7e68",
    "externalIds": {
      "DBLP": "conf/emnlp/RudingerSHBFBSC20",
      "MAG": "3102749280",
      "ACL": "2020.findings-emnlp.418",
      "DOI": "10.18653/v1/2020.findings-emnlp.418",
      "CorpusId": 226283602
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Thinking Like a Skeptic: Defeasible Inference in Natural Language",
    "abstract": "Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on natural language inference and commonsense reasoning. We introduce Defeasible NLI (abbreviated \\delta-NLI), a dataset for defeasible inference in natural language. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68% of the time.",
    "venue": "Findings",
    "year": 2020,
    "referenceCount": 57,
    "citationCount": 88,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2034613",
        "name": "Rachel Rudinger"
      },
      {
        "authorId": "3103343",
        "name": "Vered Shwartz"
      },
      {
        "authorId": "2012510",
        "name": "Jena D. Hwang"
      },
      {
        "authorId": "1857797",
        "name": "Chandra Bhagavatula"
      },
      {
        "authorId": "39191185",
        "name": "Maxwell Forbes"
      },
      {
        "authorId": "39227408",
        "name": "Ronan Le Bras"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      }
    ]
  },
  "264590174": {
    "paperId": "93055cf2cf5f59adbca7bf928810094a39055f93",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-18937",
      "ArXiv": "2310.18937",
      "DOI": "10.48550/arXiv.2310.18937",
      "CorpusId": 264590174
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "The Utility of \"Even if...\" Semifactual Explanation to Optimise Positive Outcomes",
    "abstract": "When users receive either a positive or negative outcome from an automated system, Explainable AI (XAI) has almost exclusively focused on how to mutate negative outcomes into positive ones by crossing a decision boundary using counterfactuals (e.g., \\textit{\"If you earn 2k more, we will accept your loan application\"}). Here, we instead focus on \\textit{positive} outcomes, and take the novel step of using XAI to optimise them (e.g., \\textit{\"Even if you wish to half your down-payment, we will still accept your loan application\"}). Explanations such as these that employ\"even if...\"reasoning, and do not cross a decision boundary, are known as semifactuals. To instantiate semifactuals in this context, we introduce the concept of \\textit{Gain} (i.e., how much a user stands to benefit from the explanation), and consider the first causal formalisation of semifactuals. Tests on benchmark datasets show our algorithms are better at maximising gain compared to prior work, and that causality is important in the process. Most importantly however, a user study supports our main hypothesis by showing people find semifactual explanations more useful than counterfactuals when they receive the positive outcome of a loan acceptance.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 60,
    "citationCount": 7,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "122695020",
        "name": "Eoin M. Kenny"
      },
      {
        "authorId": "2262479323",
        "name": "Weipeng Huang"
      }
    ]
  },
  "264590303": {
    "paperId": "968a5d418c8a4b68eb8b95ec9f72dce2b776d4c9",
    "externalIds": {
      "ArXiv": "2310.18526",
      "DBLP": "conf/nips/TsaiYR23",
      "DOI": "10.48550/arXiv.2310.18526",
      "CorpusId": 264590303
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Sample based Explanations via Generalized Representers",
    "abstract": "We propose a general class of sample based explanations of machine learning models, which we term generalized representers. To measure the effect of a training sample on a model's test prediction, generalized representers use two components: a global sample importance that quantifies the importance of the training point to the model and is invariant to test samples, and a local sample importance that measures similarity between the training sample and the test point with a kernel. A key contribution of the paper is to show that generalized representers are the only class of sample based explanations satisfying a natural set of axiomatic properties. We discuss approaches to extract global importances given a kernel, and also natural choices of kernels given modern non-linear models. As we show, many popular existing sample based explanations could be cast as generalized representers with particular choices of kernels and approaches to extract global importances. Additionally, we conduct empirical comparisons of different generalized representers on two image and two text classification datasets.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 62,
    "citationCount": 8,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Mathematics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "41018842",
        "name": "Che-Ping Tsai"
      },
      {
        "authorId": "3438923",
        "name": "Chih-Kuan Yeh"
      },
      {
        "authorId": "2268759763",
        "name": "Pradeep Ravikumar"
      }
    ]
  },
  "264451539": {
    "paperId": "db27a18d04e22ddc91ff74204e074c1aaa6d239b",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-16520",
      "ArXiv": "2310.16520",
      "DOI": "10.48550/arXiv.2310.16520",
      "CorpusId": 264451539
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Towards Self-Interpretable Graph-Level Anomaly Detection",
    "abstract": "Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit notable dissimilarity compared to the majority in a collection. However, current works primarily focus on evaluating graph-level abnormality while failing to provide meaningful explanations for the predictions, which largely limits their reliability and application scope. In this paper, we investigate a new challenging problem, explainable GLAD, where the learning objective is to predict the abnormality of each graph sample with corresponding explanations, i.e., the vital subgraph that leads to the predictions. To address this challenging problem, we propose a Self-Interpretable Graph aNomaly dETection model (SIGNET for short) that detects anomalous graphs as well as generates informative explanations simultaneously. Specifically, we first introduce the multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD approach. This way SIGNET is able to not only measure the abnormality of each graph based on cross-view mutual information but also provide informative graph rationales by extracting bottleneck subgraphs from the input graph and its dual hypergraph in a self-supervised way. Extensive experiments on 16 datasets demonstrate the anomaly detection capability and self-interpretability of SIGNET.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 89,
    "citationCount": 31,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2242962967",
        "name": "Yixin Liu"
      },
      {
        "authorId": "2261590627",
        "name": "Kaize Ding"
      },
      {
        "authorId": "2262196971",
        "name": "Qinghua Lu"
      },
      {
        "authorId": "2203372264",
        "name": "Fuyi Li"
      },
      {
        "authorId": "2326167038",
        "name": "Leo Yu Zhang"
      },
      {
        "authorId": "2240550258",
        "name": "Shirui Pan"
      }
    ]
  },
  "265212879": {
    "paperId": "ebf58b3b6e0fc6612faedf94282161235dc1927c",
    "externalIds": {
      "DBLP": "journals/corr/abs-2311-08644",
      "DOI": "10.48550/arXiv.2311.08644",
      "CorpusId": 265212879
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Interpretable by Design: Wrapper Boxes Combine Neural Performance with Faithful Explanations",
    "abstract": "Can we preserve the accuracy of neural models while also providing faithful explanations? We present wrapper boxes , a general approach to generate faithful, example-based explanations for model predictions while maintaining predictive performance. After training a neural model as usual, its learned feature representation is input to a classic, interpretable model to perform the actual prediction. This simple strategy is surprisingly effective, with results largely comparable to those of the original neural model, as shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics. More-over, because these classic models are inter-pretable by design , the subset of training examples that determine classic model predictions can be shown directly to users.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 88,
    "citationCount": 1,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2266811705",
        "name": "Yiheng Su"
      },
      {
        "authorId": "2266803758",
        "name": "Juni Jessy Li"
      },
      {
        "authorId": "2266750604",
        "name": "Matthew Lease"
      }
    ]
  },
  "256105735": {
    "paperId": "1cdfa7c3465943a295f8df2d2097c4bb3e222426",
    "externalIds": {
      "PubMedCentral": "10560994",
      "DBLP": "journals/corr/abs-2301-08912",
      "ArXiv": "2301.08912",
      "DOI": "10.3389/frai.2023.1225093",
      "CorpusId": 256105735,
      "PubMed": "37818431"
    },
    "publicationVenue": null,
    "title": "Rationalization for explainable NLP: a survey",
    "abstract": "Recent advances in deep learning have improved the performance of many Natural Language Processing (NLP) tasks such as translation, question-answering, and text classification. However, this improvement comes at the expense of model explainability. Black-box models make it difficult to understand the internals of a system and the process it takes to arrive at an output. Numerical (LIME, Shapley) and visualization (saliency heatmap) explainability techniques are helpful; however, they are insufficient because they require specialized knowledge. These factors led rationalization to emerge as a more accessible explainable technique in NLP. Rationalization justifies a model's output by providing a natural language explanation (rationale). Recent improvements in natural language generation have made rationalization an attractive technique because it is intuitive, human-comprehensible, and accessible to non-technical users. Since rationalization is a relatively new field, it is disorganized. As the first survey, rationalization literature in NLP from 2007 to 2022 is analyzed. This survey presents available methods, explainable evaluations, code, and datasets used across various NLP tasks that use rationalization. Further, a new subfield in Explainable AI (XAI), namely, Rational AI (RAI), is introduced to advance the current state of rationalization. A discussion on observed insights, challenges, and future directions is provided to point to promising research opportunities.",
    "venue": "Frontiers Artif. Intell.",
    "year": 2023,
    "referenceCount": 111,
    "citationCount": 21,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1573578354",
        "name": "Sai Gurrapu"
      },
      {
        "authorId": "2054229124",
        "name": "Ajay Kulkarni"
      },
      {
        "authorId": "34170717",
        "name": "Lifu Huang"
      },
      {
        "authorId": "2099420",
        "name": "Ismini Lourentzou"
      },
      {
        "authorId": "2106224635",
        "name": "Laura J. Freeman"
      },
      {
        "authorId": "144105786",
        "name": "Feras A. Batarseh"
      }
    ]
  },
  "263830336": {
    "paperId": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
    "externalIds": {
      "DBLP": "conf/emnlp/FeldhusWACO023",
      "ArXiv": "2310.05592",
      "DOI": "10.48550/arXiv.2310.05592",
      "CorpusId": 263830336
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations",
    "abstract": "While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 87,
    "citationCount": 9,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1641658310",
        "name": "Nils Feldhus"
      },
      {
        "authorId": "2257126685",
        "name": "Qianli Wang"
      },
      {
        "authorId": "2256998268",
        "name": "Tatiana Anikina"
      },
      {
        "authorId": "2279120166",
        "name": "Sahil Chopra"
      },
      {
        "authorId": "2064503692",
        "name": "Cennet Oguz"
      },
      {
        "authorId": "2261403640",
        "name": "Sebastian M\u00f6ller"
      }
    ]
  },
  "237940863": {
    "paperId": "4abb90edf2ec4045ae62cf6e25725043209bf57b",
    "externalIds": {
      "DBLP": "journals/patterns/EhsanR24a",
      "PubMedCentral": "11240172",
      "ArXiv": "2109.12480",
      "DOI": "10.1016/j.patter.2024.100971",
      "CorpusId": 237940863,
      "PubMed": "39005480"
    },
    "publicationVenue": {
      "id": "17bac89e-3dba-467a-b9d4-71e3baefb08b",
      "name": "Patterns",
      "type": "journal",
      "issn": "2666-3899",
      "url": "https://www.cell.com/patterns",
      "alternate_urls": [
        "https://www.sciencedirect.com/journal/patterns"
      ]
    },
    "title": "Explainability pitfalls: Beyond dark patterns in explainable AI",
    "abstract": null,
    "venue": "Patterns",
    "year": 2021,
    "referenceCount": 76,
    "citationCount": 44,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "9553105",
        "name": "Upol Ehsan"
      },
      {
        "authorId": "2757194",
        "name": "Mark O. Riedl"
      }
    ]
  },
  "247158013": {
    "paperId": "faaa21f4c2062d4099e4d24997a189f1c1400304",
    "externalIds": {
      "DBLP": "conf/emnlp/JinLVSDLSMS22",
      "ArXiv": "2202.13758",
      "DOI": "10.18653/v1/2022.findings-emnlp.532",
      "CorpusId": 247158013
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Logical Fallacy Detection",
    "abstract": "Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally found in text, together with an additional challenge set for detecting logical fallacies in climate change claims (LogicClimate). Detecting logical fallacies is a hard problem as the model must understand the underlying logical structure of the argument. We find that existing pretrained large language models perform poorly on this task. In contrast, we show that a simple structure-aware classifier outperforms the best language model by 5.46% on Logic and 4.51% on LogicClimate. We encourage future work to explore this task as (a) it can serve as a new reasoning challenge for language models, and (b) it can have potential applications in tackling the spread of misinformation. Our dataset and code are available at https://github.com/causalNLP/logical-fallacy",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "referenceCount": 63,
    "citationCount": 29,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Environmental Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2111472502",
        "name": "Zhijing Jin"
      },
      {
        "authorId": "2156581931",
        "name": "Abhinav Lalwani"
      },
      {
        "authorId": "2008210195",
        "name": "Tejas Vaidhya"
      },
      {
        "authorId": "2562211",
        "name": "Xiaoyu Shen"
      },
      {
        "authorId": "2152155002",
        "name": "Yiwen Ding"
      },
      {
        "authorId": "2114227440",
        "name": "Zhiheng Lyu"
      },
      {
        "authorId": "2790926",
        "name": "Mrinmaya Sachan"
      },
      {
        "authorId": "145557251",
        "name": "Rada Mihalcea"
      },
      {
        "authorId": "1707625",
        "name": "B. Scholkopf"
      }
    ]
  },
  "235606327": {
    "paperId": "2c45657dc669b5b730cc92eccd05d2a4f7ec0e75",
    "externalIds": {
      "DBLP": "journals/corr/abs-2106-11988",
      "ArXiv": "2106.11988",
      "ACL": "2022.naacl-main.158",
      "DOI": "10.18653/v1/2022.naacl-main.158",
      "CorpusId": 235606327
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "On the Diversity and Limits of Human Explanations",
    "abstract": "A growing effort in NLP aims to build datasets of human explanations. However, it remains unclear whether these datasets serve their intended goals. This problem is exacerbated by the fact that the term explanation is overloaded and refers to a broad range of notions with different properties and ramifications. Our goal is to provide an overview of the diversity of explanations, discuss human limitations in providing explanations, and ultimately provide implications for collecting and using human explanations in NLP.Inspired by prior work in psychology and cognitive sciences, we group existing human explanations in NLP into three categories: proximal mechanism, evidence, and procedure. These three types differ in nature and have implications for the resultant explanations. For instance, procedure is not considered explanation in psychology and connects with a rich body of work on learning from instructions. The diversity of explanations is further evidenced by proxy questions that are needed for annotators to interpret and answer \u201cwhy is [input] assigned [label]\u201d. Finally, giving explanations may require different, often deeper, understandings than predictions, which casts doubt on whether humans can provide valid explanations in some tasks.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 73,
    "citationCount": 30,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      },
      {
        "category": "Philosophy",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2111727675",
        "name": "Chenhao Tan"
      }
    ]
  },
  "258556812": {
    "paperId": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
    "externalIds": {
      "DBLP": "conf/nips/TurpinMPB23",
      "ArXiv": "2305.04388",
      "DOI": "10.48550/arXiv.2305.04388",
      "CorpusId": 258556812
    },
    "publicationVenue": {
      "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems",
      "type": "conference",
      "alternate_names": [
        "Neural Inf Process Syst",
        "NeurIPS",
        "NIPS"
      ],
      "url": "http://neurips.cc/"
    },
    "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
    "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 75,
    "citationCount": 275,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144196816",
        "name": "Miles Turpin"
      },
      {
        "authorId": "38614754",
        "name": "Julian Michael"
      },
      {
        "authorId": "3439053",
        "name": "Ethan Perez"
      },
      {
        "authorId": "1799822",
        "name": "Sam Bowman"
      }
    ]
  },
  "263866950": {
    "paperId": "f0d6db2186d8bf2d8530f01de6c6518bb9711392",
    "externalIds": {
      "DBLP": "conf/acl/BelinkovGP20",
      "MAG": "3037116584",
      "ACL": "2020.acl-tutorials.1",
      "DOI": "10.18653/v1/2020.acl-tutorials.1",
      "CorpusId": 263866950
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Interpretability and Analysis in Neural NLP",
    "abstract": "While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in NLP. This body of work is so far lacking a common framework and methodology. Moreover, approaching the analysis of modern neural networks can be difficult for newcomers to the field. This tutorial aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The tutorial will cover the main lines of analysis work, such as structural analyses using probing classifiers, behavioral studies and test suites, and interactive visualizations. We will highlight not only the most commonly applied analysis methods, but also the specific limitations and shortcomings of current approaches, in order to inform participants where to focus future efforts.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "referenceCount": 41,
    "citationCount": 68,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "2289158484",
        "name": "Sebastian Gehrmann"
      },
      {
        "authorId": "2260118854",
        "name": "Ellie Pavlick"
      }
    ]
  },
  "226283687": {
    "paperId": "80e34f2b7f816113130c536dddd8aa980c95dfd2",
    "externalIds": {
      "MAG": "3098415518",
      "DBLP": "conf/emnlp/WallaceGS20",
      "ACL": "2020.emnlp-tutorials.3",
      "DOI": "10.18653/v1/2020.emnlp-tutorials.3",
      "CorpusId": 226283687
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Interpreting Predictions of NLP Models",
    "abstract": "Although neural NLP models are highly expressive and empirically successful, they also systematically fail in counterintuitive ways and are opaque in their decision-making process. This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the predictions of NLP models. We will first situate example-specific interpretations in the context of other ways to understand models (e.g., probing, dataset analyses). Next, we will present a thorough study of example-specific interpretations, including saliency maps, input perturbations (e.g., LIME, input reduction), adversarial attacks, and influence functions. Alongside these descriptions, we will walk through source code that creates and visualizes interpretations for a diverse set of NLP tasks. Finally, we will discuss open problems in the field, e.g., evaluating, extending, and improving interpretation methods.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 36,
    "citationCount": 28,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145217343",
        "name": "Eric Wallace"
      },
      {
        "authorId": "2061751696",
        "name": "M. Gardner"
      },
      {
        "authorId": "34650964",
        "name": "Sameer Singh"
      }
    ]
  },
  "250390689": {
    "paperId": "1da8ab00e17f96e88fce0ad480794328e37daa99",
    "externalIds": {
      "ACL": "2022.naacl-tutorials.4",
      "DOI": "10.18653/v1/2022.naacl-tutorials.4",
      "CorpusId": 250390689
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Human-Centered Evaluation of Explanations",
    "abstract": "The NLP community are increasingly interested in providing explanations for NLP models to help people make sense of model behavior and potentially improve human interaction with models. In addition to computational challenges in generating these explanations, evaluations of the generated explanations require human-centered perspectives and approaches. This tutorial will provide an overview of human-centered evaluations of explanations. First, we will give a brief introduction to the psychological foundation of explanations as well as types of NLP model explanations and their corresponding presentation, to provide the necessary background. We will then present a taxonomy of human-centered evaluation of explanations and dive into depth in the two categories: 1) evaluation based on human-annotated explanations; 2) evaluation with human-subjects studies. We will conclude by discussing future directions. We will also adopt a flipped format to maximize the in- teractive components for the live audience.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 55,
    "citationCount": 12,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1389036863",
        "name": "Jordan L. Boyd-Graber"
      },
      {
        "authorId": "40502796",
        "name": "Samuel Carton"
      },
      {
        "authorId": "2113511266",
        "name": "Shi Feng"
      },
      {
        "authorId": "144921048",
        "name": "Q. Liao"
      },
      {
        "authorId": "2464187",
        "name": "T. Lombrozo"
      },
      {
        "authorId": "1405364873",
        "name": "Alison Smith-Renner"
      },
      {
        "authorId": "2111727675",
        "name": "Chenhao Tan"
      }
    ]
  },
  "433382": {
    "paperId": "bdb73be49c4fdcbd0c79ca62e5703155915fa4c4",
    "externalIds": {
      "ACL": "P16-2003",
      "MAG": "2512549881",
      "DBLP": "conf/acl/BentonAD16",
      "DOI": "10.18653/v1/P16-2003",
      "CorpusId": 433382
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Learning Multiview Embeddings of Twitter Users",
    "abstract": "Low-dimensional vector representations are widely used as stand-ins for the text of words, sentences, and entire documents. These embeddings are used to identify similar words or make predictions about documents. In this work, we consider embeddings for social media users and demonstrate that these can be used to identify users who behave similarly or to predict attributes of users. In order to capture information from all aspects of a user\u2019s online life, we take a multiview approach, applying a weighted variant of Generalized Canonical Correlation Analysis (GCCA) to a collection of over 100,000 Twitter users. We demonstrate the utility of these multiview embeddings on three downstream tasks: user engagement, friend selection, and demographic attribute prediction.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 25,
    "citationCount": 86,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145583569",
        "name": "Adrian Benton"
      },
      {
        "authorId": "144365054",
        "name": "R. Arora"
      },
      {
        "authorId": "1782853",
        "name": "Mark Dredze"
      }
    ]
  },
  "248693617": {
    "paperId": "791a76536cbe4d7f17275fbc9ff4a4d4967b04b8",
    "externalIds": {
      "DBLP": "conf/acl/0002MBS22",
      "ACL": "2022.findings-acl.52",
      "ArXiv": "2205.05128",
      "DOI": "10.48550/arXiv.2205.05128",
      "CorpusId": 248693617
    },
    "publicationVenue": {
      "id": "479d5605-51be-4346-b1d6-4334084504df",
      "name": "Findings",
      "type": "journal",
      "issn": "2652-8800",
      "url": "https://findingspress.org/"
    },
    "title": "Human Language Modeling",
    "abstract": "Natural language is generated by people, yet traditional language modeling views words or documents as if generated independently. Here, we propose human language modeling (HuLM), a hierarchical extension to the language modeling problem where by a human- level exists to connect sequences of documents (e.g. social media messages) and capture the notion that human language is moderated by changing human states. We introduce, HaRT, a large-scale transformer model for solving HuLM, pre-trained on approximately 100,000 social media users, and demonstrate it\u2019s effectiveness in terms of both language modeling (perplexity) for social media and fine-tuning for 4 downstream tasks spanning document- and user-levels. Results on all tasks meet or surpass the current state-of-the-art.",
    "venue": "Findings",
    "year": 2022,
    "referenceCount": 50,
    "citationCount": 7,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145297996",
        "name": "Nikita Soni"
      },
      {
        "authorId": "1386902685",
        "name": "Matthew Matero"
      },
      {
        "authorId": "35217367",
        "name": "Niranjan Balasubramanian"
      },
      {
        "authorId": "145035129",
        "name": "H. A. Schwartz"
      }
    ]
  },
  "2955580": {
    "paperId": "1ea75cdb7ce8c4f5f2599165e3698034b4142e08",
    "externalIds": {
      "MAG": "2311783643",
      "ArXiv": "1603.06155",
      "DBLP": "journals/corr/LiGBGD16",
      "ACL": "P16-1094",
      "DOI": "10.18653/v1/P16-1094",
      "CorpusId": 2955580
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Persona-Based Neural Conversation Model",
    "abstract": "We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2016,
    "referenceCount": 35,
    "citationCount": 1012,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49298465",
        "name": "Jiwei Li"
      },
      {
        "authorId": "1947267",
        "name": "Michel Galley"
      },
      {
        "authorId": "3125776",
        "name": "Chris Brockett"
      },
      {
        "authorId": "3130583",
        "name": "Georgios P. Spithourakis"
      },
      {
        "authorId": "1800422",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "83415753",
        "name": "W. Dolan"
      }
    ]
  },
  "14021168": {
    "paperId": "737bb106a35d1ebe6b0acd1cb77582738cf0e09c",
    "externalIds": {
      "ACL": "P15-1073",
      "MAG": "2252241921",
      "DBLP": "conf/acl/Hovy15",
      "DOI": "10.3115/v1/P15-1073",
      "CorpusId": 14021168
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "Demographic Factors Improve Classification Performance",
    "abstract": "Extra-linguistic factors influence language use, and are accounted for by speakers and listeners. Most natural language processing (NLP) tasks to date, however, treat language as uniform. This assumption can harm performance. We investigate the effect of including demographic information on performance in a variety of text-classification tasks. We find that by including age or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2015,
    "referenceCount": 48,
    "citationCount": 174,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2022288",
        "name": "Dirk Hovy"
      }
    ]
  },
  "26192090": {
    "paperId": "06920ce34abe17848bd02561a721afd930fe9581",
    "externalIds": {
      "MAG": "2759869292",
      "DBLP": "conf/emnlp/LynnSKBS17",
      "ACL": "D17-1119",
      "DOI": "10.18653/v1/D17-1119",
      "CorpusId": 26192090
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "Human Centered NLP with User-Factor Adaptation",
    "abstract": "We pose the general task of user-factor adaptation \u2013 adapting supervised learning models to real-valued user factors inferred from a background of their language, reflecting the idea that a piece of text should be understood within the context of the user that wrote it. We introduce a continuous adaptation technique, suited for real-valued user factors that are common in social science and bringing us closer to personalized NLP, adapting to each user uniquely. We apply this technique with known user factors including age, gender, and personality traits, as well as latent factors, evaluating over five tasks: POS tagging, PP-attachment, sentiment analysis, sarcasm detection, and stance detection. Adaptation provides statistically significant benefits for 3 of the 5 tasks: up to +1.2 points for PP-attachment, +3.4 points for sarcasm, and +3.0 points for stance.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2017,
    "referenceCount": 44,
    "citationCount": 50,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "5536113",
        "name": "Veronica E. Lynn"
      },
      {
        "authorId": "22254278",
        "name": "Youngseo Son"
      },
      {
        "authorId": "144592382",
        "name": "Vivek Kulkarni"
      },
      {
        "authorId": "35217367",
        "name": "Niranjan Balasubramanian"
      },
      {
        "authorId": "145035129",
        "name": "H. A. Schwartz"
      }
    ]
  },
  "266191532": {
    "paperId": "dc4d9b0c3c9c9cd9eb3a4c8d3ffa415d9953f77d",
    "externalIds": {
      "ACL": "2024.naacl-long.477",
      "ArXiv": "2312.07751",
      "DBLP": "journals/corr/abs-2312-07751",
      "DOI": "10.48550/arXiv.2312.07751",
      "CorpusId": 266191532
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "Large Human Language Models: A Need and the Challenges",
    "abstract": "As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human context. We refer to relevant advances and present open challenges that need to be addressed and their possible solutions in realizing these goals.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 120,
    "citationCount": 5,
    "isOpenAccess": false,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "145297996",
        "name": "Nikita Soni"
      },
      {
        "authorId": "2273933343",
        "name": "H. A. Schwartz"
      },
      {
        "authorId": "2662374",
        "name": "Jo\u00e3o Sedoc"
      },
      {
        "authorId": "2273927019",
        "name": "Niranjan Balasubramanian"
      }
    ]
  },
  "250041118": {
    "paperId": "71fc53f702883b62774dd6d3ebc971d82ea9a0d9",
    "externalIds": {
      "PubMedCentral": "9229362",
      "DOI": "10.1093/pnasnexus/pgac022",
      "CorpusId": 250041118,
      "PubMed": "35774418"
    },
    "publicationVenue": {
      "id": "0473502f-71fa-4945-8e3a-5c0618a4ec32",
      "name": "PNAS Nexus",
      "type": "journal",
      "issn": "2752-6542",
      "url": "https://academic.oup.com/pnasnexus"
    },
    "title": "Tracking group identity through natural language within groups",
    "abstract": "Abstract To what degree can we determine people's connections with groups through the language they use? In recent years, large archives of behavioral data from social media communities have become available to social scientists, opening the possibility of tracking naturally occurring group identity processes. A feature of most digital groups is that they rely exclusively on the written word. Across 3 studies, we developed and validated a language-based metric of group identity strength and demonstrated its potential in tracking identity processes in online communities. In Studies 1a\u20131c, 873 people wrote about their connections to various groups (country, college, or religion). A total of 2 language markers of group identity strength were found: high affiliation (more words like we, togetherness) and low cognitive processing or questioning (fewer words like think, unsure). Using these markers, a language-based unquestioning affiliation index was developed and applied to in-class stream-of-consciousness essays of 2,161 college students (Study 2). Greater levels of unquestioning affiliation expressed in language predicted not only self-reported university identity but also students\u2019 likelihood of remaining enrolled in college a year later. In Study 3, the index was applied to naturalistic Reddit conversations of 270,784 people in 2 online communities of supporters of the 2016 presidential candidates\u2014Hillary Clinton and Donald Trump. The index predicted how long people would remain in the group (3a) and revealed temporal shifts mirroring members\u2019 joining and leaving of groups (3b). Together, the studies highlight the promise of a language-based approach for tracking and studying group identity processes in online groups.",
    "venue": "PNAS Nexus",
    "year": 2022,
    "referenceCount": 42,
    "citationCount": 16,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Medicine",
        "source": "external"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "1390108796",
        "name": "A. Ashokkumar"
      },
      {
        "authorId": "1854783",
        "name": "J. Pennebaker"
      }
    ]
  },
  "2851638": {
    "paperId": "fe1850826165fd89d51eca05c46a087bd3b32fa2",
    "externalIds": {
      "MAG": "2547654003",
      "DOI": "10.1177/0003122416671873",
      "CorpusId": 2851638
    },
    "publicationVenue": null,
    "title": "Fitting In or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness",
    "abstract": "A recurring theme in sociological research is the tradeoff between fitting in and standing out. Prior work examining this tension tends to take either a structural or a cultural perspective. We fuse these two traditions to develop a theory of how structural and cultural embeddedness jointly relate to individual attainment within organizations. Given that organizational culture is hard to observe, we develop a novel approach to assessing individuals\u2019 cultural fit with their colleagues based on the language expressed in internal e-mail communications. Drawing on a unique dataset that includes a corpus of 10.24 million e-mail messages exchanged over five years among 601 employees in a high-technology firm, we find that network constraint impedes, whereas cultural fit promotes, individual attainment. More importantly, we find evidence of a tradeoff between the two forms of embeddedness: cultural fit benefits individuals with low network constraint (i.e., brokers), whereas network constraint promotes attainment for people with low cultural fit.",
    "venue": "",
    "year": 2015,
    "referenceCount": 130,
    "citationCount": 170,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Sociology",
        "source": "external"
      },
      {
        "category": "Psychology",
        "source": "external"
      },
      {
        "category": "Sociology",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3727566",
        "name": "Amir Goldberg"
      },
      {
        "authorId": "3106444",
        "name": "S. Srivastava"
      },
      {
        "authorId": "144020466",
        "name": "V. Manian"
      },
      {
        "authorId": "145768639",
        "name": "Will Monroe"
      },
      {
        "authorId": "144922861",
        "name": "Christopher Potts"
      }
    ]
  },
  "258714781": {
    "paperId": "e16a782b529a7adbea1669236c97efc653196b8c",
    "externalIds": {
      "ArXiv": "2305.08982",
      "DBLP": "journals/corr/abs-2305-08982",
      "DOI": "10.48550/arXiv.2305.08982",
      "CorpusId": 258714781
    },
    "publicationVenue": {
      "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org",
      "alternate_names": [
        "ArXiv"
      ],
      "issn": "2331-8422",
      "url": "https://arxiv.org"
    },
    "title": "Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback",
    "abstract": "Millions of users come to online peer counseling platforms to seek support on diverse topics ranging from relationship stress to anxiety. However, studies show that online peer support groups are not always as effective as expected largely due to users' negative experiences with unhelpful counselors. Peer counselors are key to the success of online peer counseling platforms, but most of them often do not have systematic ways to receive guidelines or supervision. In this work, we introduce CARE: an interactive AI-based tool to empower peer counselors through automatic suggestion generation. During the practical training stage, CARE helps diagnose which specific counseling strategies are most suitable in the given context and provides tailored example responses as suggestions. Counselors can choose to select, modify, or ignore any suggestion before replying to the support seeker. Building upon the Motivational Interviewing framework, CARE utilizes large-scale counseling conversation data together with advanced natural language generation techniques to achieve these functionalities. We demonstrate the efficacy of CARE by performing both quantitative evaluations and qualitative user studies through simulated chats and semi-structured interviews. We also find that CARE especially helps novice counselors respond better in challenging situations.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 89,
    "citationCount": 10,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Psychology",
        "source": "s2-fos-model"
      },
      {
        "category": "Education",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2147213057",
        "name": "Shang-ling Hsu"
      },
      {
        "authorId": "2051264008",
        "name": "Raj Sanjay Shah"
      },
      {
        "authorId": "2217341381",
        "name": "Prathik Senthil"
      },
      {
        "authorId": "2220428",
        "name": "Zahra Ashktorab"
      },
      {
        "authorId": "2391727",
        "name": "Casey Dugan"
      },
      {
        "authorId": "143774641",
        "name": "Werner Geyer"
      },
      {
        "authorId": "2022168",
        "name": "Diyi Yang"
      }
    ]
  },
  "254854296": {
    "paperId": "a640cdafc10181517b7694ab589db515595b3490",
    "externalIds": {
      "DBLP": "journals/corr/abs-2212-09746",
      "ArXiv": "2212.09746",
      "DOI": "10.48550/arXiv.2212.09746",
      "CorpusId": 254854296
    },
    "publicationVenue": null,
    "title": "Evaluating Human-Language Model Interaction",
    "abstract": "Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurassic-1), we find that better non-interactive performance does not always translate to better human-LM interaction. In particular, we highlight three cases where the results from non-interactive and interactive metrics diverge and underscore the importance of human-LM interaction for LM evaluation.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2022,
    "referenceCount": 207,
    "citationCount": 80,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49316195",
        "name": "Mina Lee"
      },
      {
        "authorId": "2143366464",
        "name": "Megha Srivastava"
      },
      {
        "authorId": "1914650552",
        "name": "Amelia Hardy"
      },
      {
        "authorId": "50343904",
        "name": "John Thickstun"
      },
      {
        "authorId": "41152329",
        "name": "Esin Durmus"
      },
      {
        "authorId": "40404493",
        "name": "Ashwin Paranjape"
      },
      {
        "authorId": "2193248562",
        "name": "Ines Gerard-Ursin"
      },
      {
        "authorId": "32551341",
        "name": "Xiang Lisa Li"
      },
      {
        "authorId": "8759332",
        "name": "Faisal Ladhak"
      },
      {
        "authorId": "2047004093",
        "name": "Frieda Rong"
      },
      {
        "authorId": "2155890009",
        "name": "Rose E. Wang"
      },
      {
        "authorId": "37909625",
        "name": "Minae Kwon"
      },
      {
        "authorId": "2197475360",
        "name": "Joon Sung Park"
      },
      {
        "authorId": "2196927606",
        "name": "Hancheng Cao"
      },
      {
        "authorId": "2110585783",
        "name": "Tony Lee"
      },
      {
        "authorId": "150272855",
        "name": "Rishi Bommasani"
      },
      {
        "authorId": "145879842",
        "name": "Michael S. Bernstein"
      },
      {
        "authorId": "145419642",
        "name": "Percy Liang"
      }
    ]
  },
  "248228026": {
    "paperId": "2a1acf6dce142d9a87d353c7edbda0127dbe69d7",
    "externalIds": {
      "ArXiv": "2204.08292",
      "DBLP": "journals/corr/abs-2204-08292",
      "DOI": "10.48550/arXiv.2204.08292",
      "CorpusId": 248228026
    },
    "publicationVenue": {
      "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence",
      "type": "conference",
      "alternate_names": [
        "National Conference on Artificial Intelligence",
        "National Conf Artif Intell",
        "AAAI Conf Artif Intell",
        "AAAI"
      ],
      "url": "http://www.aaai.org/"
    },
    "title": "StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts",
    "abstract": "Inferring spatial relations in natural language is a crucial ability an intelligent system should possess. The bAbI dataset tries to capture tasks relevant to this domain (task 17 and 19). However, these tasks have several limitations. Most importantly, they are limited to fixed expressions, they are limited in the number of reasoning steps required to solve them, and they fail to test the robustness of models to input that contains irrelevant or redundant information. In this paper, we present a new Question-Answering dataset called StepGame for robust multi-step spatial reasoning in texts. Our experiments demonstrate that state-of-the-art models on the bAbI dataset struggle on the StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental results on both datasets show that our model outperforms all the baselines with superior generalization and robustness performance.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2022,
    "referenceCount": 44,
    "citationCount": 55,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2110398077",
        "name": "Zhengxiang Shi"
      },
      {
        "authorId": "40344116",
        "name": "Qiang Zhang"
      },
      {
        "authorId": "2121294485",
        "name": "Aldo Lipani"
      }
    ]
  },
  "233219660": {
    "paperId": "de74100ac5a16f9169b8a7fe98dd05647b0e8777",
    "externalIds": {
      "DBLP": "conf/naacl/MirzaeeFNK21",
      "MAG": "3153839026",
      "ArXiv": "2104.05832",
      "ACL": "2021.naacl-main.364",
      "DOI": "10.18653/V1/2021.NAACL-MAIN.364",
      "CorpusId": 233219660
    },
    "publicationVenue": {
      "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "North Am Chapter Assoc Comput Linguistics",
        "NAACL"
      ],
      "url": "https://www.aclweb.org/portal/naacl"
    },
    "title": "SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning",
    "abstract": "This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs\u2019 capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "referenceCount": 56,
    "citationCount": 60,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2066164982",
        "name": "Roshanak Mirzaee"
      },
      {
        "authorId": "117133855",
        "name": "Hossein Rajaby Faghihi"
      },
      {
        "authorId": "3333257",
        "name": "Qiang Ning"
      },
      {
        "authorId": "2074098349",
        "name": "Parisa Kordjmashidi"
      }
    ]
  },
  "5066019": {
    "paperId": "bff8ae9e28323d217b9ad5a7321e58f79607f557",
    "externalIds": {
      "DBLP": "conf/acl/RothWN18",
      "MAG": "2953025938",
      "ACL": "P18-1122",
      "ArXiv": "1804.07828",
      "DOI": "10.18653/v1/P18-1122",
      "CorpusId": 5066019
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Multi-Axis Annotation Scheme for Event Temporal Relations",
    "abstract": "Existing temporal relation (TempRel) annotation schemes often have low inter-annotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition. This paper proposes a new multi-axis modeling to better capture the temporal structure of events. In addition, we identify that event end-points are a major source of confusion in annotation, so we also propose to annotate TempRels based on start-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60\u2019s to 80\u2019s (Cohen\u2019s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 33,
    "citationCount": 162,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3333257",
        "name": "Qiang Ning"
      },
      {
        "authorId": "2119796958",
        "name": "Hao Wu"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "218470560": {
    "paperId": "36d6c8895bbc755964b8b2136c6fd6087a7af089",
    "externalIds": {
      "MAG": "3023326105",
      "ArXiv": "2005.00242",
      "DBLP": "journals/corr/abs-2005-00242",
      "ACL": "2020.emnlp-main.88",
      "DOI": "10.18653/v1/2020.emnlp-main.88",
      "CorpusId": 218470560
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions",
    "abstract": "A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as \"what happened before/after [some event]?\" We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "referenceCount": 51,
    "citationCount": 98,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3333257",
        "name": "Qiang Ning"
      },
      {
        "authorId": "1664776313",
        "name": "Hao Wu"
      },
      {
        "authorId": "2217978934",
        "name": "Rujun Han"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ]
  },
  "248780347": {
    "paperId": "b672d2ec81c9395c312d57a27931864d07664592",
    "externalIds": {
      "DBLP": "conf/acl/NingZ0PF022",
      "ACL": "2022.acl-long.195",
      "DOI": "10.18653/v1/2022.acl-long.195",
      "CorpusId": 248780347
    },
    "publicationVenue": {
      "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "alternate_names": [
        "Annu Meet Assoc Comput Linguistics",
        "Meeting of the Association for Computational Linguistics",
        "ACL",
        "Meet Assoc Comput Linguistics"
      ],
      "url": "https://www.aclweb.org/anthology/venues/acl/"
    },
    "title": "A Meta-framework for Spatiotemporal Quantity Extraction from Text",
    "abstract": "News events are often associated with quantities (e.g., the number of COVID-19 patients or the number of arrests in a protest), and it is often important to extract their type, time, and location from unstructured text in order to analyze these quantity events. This paper thus formulates the NLP problem of spatiotemporal quantity extraction, and proposes the first meta-framework for solving it. This meta-framework contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline models. We demonstrate the meta-framework in three domains\u2014the COVID-19 pandemic, Black Lives Matter protests, and 2020 California wildfires\u2014to show that the formalism is general and extensible, the crowdsourcing pipeline facilitates fast and high-quality data annotation, and the baseline system can handle spatiotemporal quantity extraction well enough to be practically useful. We release all resources for future research on this topic at https://github.com/steqe.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 53,
    "citationCount": 7,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "3333257",
        "name": "Qiang Ning"
      },
      {
        "authorId": "145360756",
        "name": "Ben Zhou"
      },
      {
        "authorId": "2119796958",
        "name": "Hao Wu"
      },
      {
        "authorId": "1981962",
        "name": "Haoruo Peng"
      },
      {
        "authorId": "2112734485",
        "name": "Chuchu Fan"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      }
    ]
  },
  "267069344": {
    "paperId": "a3ca77456142b78367dd5d53138b50dfac8086ca",
    "externalIds": {
      "DBLP": "conf/cvpr/0003XKISGX24",
      "ArXiv": "2401.12168",
      "DOI": "10.1109/CVPR52733.2024.01370",
      "CorpusId": 267069344
    },
    "publicationVenue": {
      "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition",
      "type": "conference",
      "alternate_names": [
        "CVPR",
        "Comput Vis Pattern Recognit"
      ],
      "issn": "1063-6919",
      "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": [
        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
      ]
    },
    "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities",
    "abstract": "Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size difference. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in training recipe including data quality, training pipeline and VLM architecture. Our workfeatures the first Internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qual-itative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of thought spatial reasoning and robotics due to its quantitative estimation capability. Website: https://spatial-vlm.github.iol",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2024,
    "referenceCount": 72,
    "citationCount": 70,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Engineering",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "8786274",
        "name": "Boyuan Chen"
      },
      {
        "authorId": "2265456732",
        "name": "Zhuo Xu"
      },
      {
        "authorId": "51881277",
        "name": "Sean Kirmani"
      },
      {
        "authorId": "2704814",
        "name": "Brian Ichter"
      },
      {
        "authorId": "2283848260",
        "name": "Danny Driess"
      },
      {
        "authorId": "2264974363",
        "name": "Pete Florence"
      },
      {
        "authorId": "1779671",
        "name": "Dorsa Sadigh"
      },
      {
        "authorId": "2231868572",
        "name": "Leonidas J. Guibas"
      },
      {
        "authorId": "2267320085",
        "name": "Fei Xia"
      }
    ]
  },
  "248496506": {
    "paperId": "354b48677e314ef2f47512c5a81723cfd17dd05d",
    "externalIds": {
      "ACL": "2023.tacl-1.37",
      "DBLP": "journals/corr/abs-2205-00363",
      "ArXiv": "2205.00363",
      "DOI": "10.1162/tacl_a_00566",
      "CorpusId": 248496506
    },
    "publicationVenue": {
      "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
      "name": "Transactions of the Association for Computational Linguistics",
      "type": "journal",
      "alternate_names": [
        "Trans Assoc Comput Linguistics",
        "TACL"
      ],
      "issn": "2307-387X",
      "url": "https://www.mitpressjournals.org/loi/tacl",
      "alternate_urls": [
        "http://www.transacl.org/"
      ]
    },
    "title": "Visual Spatial Reasoning",
    "abstract": "Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (e.g., under, in front of, facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs\u2019 by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.1",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2022,
    "referenceCount": 56,
    "citationCount": 115,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Linguistics",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "144097210",
        "name": "Fangyu Liu"
      },
      {
        "authorId": "145406160",
        "name": "Guy Edward Toh Emerson"
      },
      {
        "authorId": "50638196",
        "name": "Nigel Collier"
      }
    ]
  },
  "245986629": {
    "paperId": "2bd60237eeb80c7163dba4c2a19d48d9b1ff7b63",
    "externalIds": {
      "ArXiv": "2201.05247",
      "DBLP": "journals/ral/SunCMF22",
      "DOI": "10.1109/lra.2022.3146951",
      "CorpusId": 245986629
    },
    "publicationVenue": {
      "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
      "name": "IEEE Robotics and Automation Letters",
      "alternate_names": [
        "IEEE Robot Autom Lett"
      ],
      "issn": "2377-3766",
      "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
      "alternate_urls": [
        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
      ]
    },
    "title": "Multi-agent Motion Planning from Signal Temporal Logic Specifications",
    "abstract": "We tackle the challenging problem of multi-agent cooperative motion planning for complex tasks described using signal temporal logic (STL), where robots can have nonlinear and nonholonomic dynamics. Existing methods in multi-agent motion planning, especially those based on discrete abstractions and model predictive control (MPC), suffer from limited scalability with respect to the complexity of the task, the size of the workspace, and the planning horizon. We present a method based on {\\em timed waypoints\\/} to address this issue. We show that timed waypoints can help abstract nonlinear behaviors of the system as safety envelopes around the reference path defined by those waypoints. Then the search for waypoints satisfying the STL specifications can be inductively encoded as a mixed-integer linear program. The agents following the synthesized timed waypoints have their tasks automatically allocated, and are guaranteed to satisfy the STL specifications while avoiding collisions. We evaluate the algorithm both in simulation and on the Robotarium platform. Results show that it supports multi-agent planning from complex specification over long planning horizons, and significantly outperforms state-of-the-art abstraction-based and MPC-based motion planning tools.",
    "venue": "IEEE Robotics and Automation Letters",
    "year": 2022,
    "referenceCount": 34,
    "citationCount": 66,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Engineering",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      },
      {
        "category": "Engineering",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "49200570",
        "name": "Dawei Sun"
      },
      {
        "authorId": "2108183300",
        "name": "Jingkai Chen"
      },
      {
        "authorId": "33782901",
        "name": "S. Mitra"
      },
      {
        "authorId": "2344739",
        "name": "Chuchu Fan"
      }
    ]
  },
  "258686669": {
    "paperId": "72a2cff51bb9a87bbe4fc41325f5a4afc82a0366",
    "externalIds": {
      "DBLP": "conf/emnlp/ChenGZF23",
      "ArXiv": "2305.07766",
      "DOI": "10.48550/arXiv.2305.07766",
      "CorpusId": 258686669
    },
    "publicationVenue": {
      "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing",
      "type": "conference",
      "alternate_names": [
        "Empir Method Nat Lang Process",
        "Empirical Methods in Natural Language Processing",
        "Conf Empir Method Nat Lang Process",
        "EMNLP"
      ],
      "url": "https://www.aclweb.org/portal/emnlp"
    },
    "title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models",
    "abstract": "Temporal Logic (TL) can be used to rigorously specify complex high-level specification for systems in many engineering applications. The translation between natural language (NL) and TL has been under-explored due to the lack of dataset and generalizable model across different application domains. In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages. Our contributions are twofold. First, we develop a framework to create a dataset of NL-TL pairs combining LLMs and human annotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5 models on the lifted versions (i.e., the specific Atomic Propositions (AP) are hidden) of the NL and TL. The enhanced generalizability originates from two aspects: 1) Usage of lifted NL-TL characterizes common logical structures, without constraints of specific domains. 2) Application of LLMs in dataset creation largely enhances corpus richness. We test the generalization of trained models on five varied domains. To achieve full NL-TL transformation, we either combine the lifted model with AP recognition task or do the further finetuning on each specific domain. During the further finetuning, our model achieves higher accuracy (>95%) using only<10% training data, compared with the baseline sequence to sequence (Seq2Seq) model.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 37,
    "citationCount": 28,
    "isOpenAccess": true,
    "s2FieldsOfStudy": [
      {
        "category": "Computer Science",
        "source": "external"
      },
      {
        "category": "Computer Science",
        "source": "s2-fos-model"
      }
    ],
    "authors": [
      {
        "authorId": "2155261737",
        "name": "Yongchao Chen"
      },
      {
        "authorId": "2217252073",
        "name": "Rujul Gandhi"
      },
      {
        "authorId": "1698138",
        "name": "Yang Zhang"
      },
      {
        "authorId": "2112734485",
        "name": "Chuchu Fan"
      }
    ]
  }
}