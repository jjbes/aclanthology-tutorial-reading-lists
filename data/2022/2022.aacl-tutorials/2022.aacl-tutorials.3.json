{
    "2022.aacl-tutorials.3": {
        "title": "When Cantonese NLP Meets Pre-training: Progress and Challenges",
        "author": "Rong Xiang, Hanzhuo Tan, Jing Li, Mingyu Wan, Kam-Fai Wong",
        "year": 2022,
        "url": "https://aclanthology.org/2022.aacl-tutorials.3",
        "doi": null,
        "abstract": "Cantonese is an influential Chinese variant with a large population of speakers worldwide. However, it is under-resourced in terms of the data scale and diversity, excluding Cantonese Natural Language Processing (NLP) from the stateof-the-art (SOTA) “pre-training and fine-tuning” paradigm. This tutorial will start with a substantially review of the linguistics and NLP progress for shaping language specificity, resources, and methodologies. It will be followed by an introduction to the trendy transformerbased pre-training methods, which have been largely advancing the SOTA performance of a wide range of downstream NLP tasks in numerous majority languages (e.g., English and Chinese). Based on the above, we will present the main challenges for Cantonese NLP in relation to Cantonese language idiosyncrasies of colloquialism and multilingualism, followed by the future directions to line NLP for Cantonese and other low-resource languages up to the cutting-edge pre-training practice.",
        "readingList": [
            {
                "sectionName": null,
                "subsectionName": null,
                "referencesIds": [
                    null,
                    "ef8d3369f0d5d78f1c0989e2dd59d5ca8f045441",
                    "484cdbf507139714232d40a6139b17dc72c7919f",
                    "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
                    "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
                    "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
                    "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
                    "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
                    "72cdd6ebe0221fb568ef20534f44ba5b35190a56"
                ]
            }
        ]
    }
}

