{
    "2022.acl-tutorials.1": {
        "title": "A Gentle Introduction to Deep Nets and Opportunities for the Future",
        "author": "Kenneth Church, Valia Kordoni, Gary Marcus, Ernest Davis, Yanjun Ma, Zeyu Chen",
        "year": 2022,
        "url": "https://aclanthology.org/2022.acl-tutorials.1",
        "doi": "10.18653/v1/2022.acl-tutorials.1",
        "abstract": "The first half of this tutorial will make deep nets more accessible to a broader audience, following “Deep Nets for Poets” and “A Gentle Introduction to Fine-Tuning.” We will also introduce GFT (general fine tuning), a little language for fine tuning deep nets with short (one line) programs that are as easy to code as regression in statistics packages such as R using glm (general linear models). Based on the success of these methods on a number of benchmarks, one might come away with the impression that deep nets are all we need. However, we believe the glass is half-full: while there is much that can be done with deep nets, there is always more to do. The second half of this tutorial will discuss some of these opportunities.",
        "readingList": [
            {
                "sectionName": null,
                "subsectionName": null,
                "referencesIds": []
            }
        ]
    }
}

