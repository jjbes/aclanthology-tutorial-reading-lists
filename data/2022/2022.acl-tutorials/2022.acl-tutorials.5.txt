Reading List

1. An Empirical Survey of Data Augmentation for Limited Data Learning in NLP (Chen et al., 2021);
2. MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification (Chen et al., 2020a);
3. Understanding Back-Translation at Scale (Edunov et al., 2018);
4. Cross-lingual Language Model Pretraining (Conneau and Lample, 2019);
5. Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank (Chau et al., 2020);
6. TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP (Morris et al., 2020);
7. Self-training Improves Pre-training for Natural Language Understanding (Du et al.)