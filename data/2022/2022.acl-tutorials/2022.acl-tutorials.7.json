{
    "2022.acl-tutorials.7": {
        "title": "Vision-Language Pretraining: Current Trends and the Future",
        "author": "Aishwarya Agrawal, Damien Teney, Aida Nematzadeh",
        "year": 2022,
        "url": "https://aclanthology.org/2022.acl-tutorials.7",
        "doi": "10.18653/v1/2022.acl-tutorials.7",
        "abstract": "In the last few years, there has been an increased interest in building multimodal (vision-language) models that are pretrained on larger but noisier datasets where the two modalities (e.g., image and text) loosely correspond to each other (e.g., Lu et al., 2019; Radford et al., 2021). Given a task (such as visual question answering), these models are then often fine-tuned on task-specific supervised datasets. (e.g., Lu et al., 2019; Chen et al.,2020; Tan and Bansal, 2019; Li et al., 2020a,b). In addition to the larger pretraining datasets, the transformer architecture (Vaswani et al., 2017) and in particular self-attention applied to two modalities are responsible for the impressive performance of the recent pretrained models on downstream tasks (Hendricks et al., 2021). In this tutorial, we focus on recent vision-language pretraining paradigms. Our goal is to first provide the background on imageâ€“language datasets, benchmarks, and modeling innovations before the multimodal pretraining area. Next we discuss the different family of models used for vision-language pretraining, highlighting their strengths and shortcomings. Finally, we discuss the limits of vision-language pretraining through statistical learning, and the need for alternative approaches such as causal representation learning.",
        "readingList": [
            {
                "sectionName": "Popular vision-language tasks, datasets and benchmarks",
                "subsectionName": null,
                "referencesIds": [
                    "11c9c31dff70de92ada9160c78ff8bb46b2912d6",
                    "92c141447f51b6732242376164ff961e464731c8",
                    "e65142010431ffc089b272a1174214e00693e503",
                    "696ca58d93f6404fea0fc75c62d1d7b378f47628",
                    "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
                    "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
                    "a7ac99d7cf3f568ab1a741392144b646b856ae0c"
                ]
            },
            {
                "sectionName": "Task specific modelling approaches before the pretraining era",
                "subsectionName": null,
                "referencesIds": [
                    "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
                    "2c1890864c1c2b750f48316dc8b650ba4772adc5",
                    "fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b",
                    "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8",
                    "12f7de07f9b00315418e381b2bd797d21f12b419",
                    "21c99706bb26e9012bfb4d8d48009a3d45af59b2"
                ]
            },
            {
                "sectionName": "Pretraining models in NLP",
                "subsectionName": null,
                "referencesIds": [
                    "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
                    "90abbc2cf38462b954ae1b772fac9532e2ccd8b0"
                ]
            },
            {
                "sectionName": "VLP models with task-specific heads",
                "subsectionName": null,
                "referencesIds": [
                    "65a9c7b0800c86a196bc14e7621ff895cc6ab287",
                    "79c93274429d6355959f1e4374c2147bb81ea649",
                    "dfc7b58b67c31932b48586b3e23a43cc94695290",
                    "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0",
                    "63c74d15940af1af9b386b5762e4445e54c73719"
                ]
            },
            {
                "sectionName": "VLP models without task-specific heads",
                "subsectionName": null,
                "referencesIds": [
                    "cb596bffc5c5042c254058b62317a57fa156fea4",
                    "5e00596fa946670d894b1bdaeff5a98e3867ef13"
                ]
            },
            {
                "sectionName": "VLP models for improving performance on vision tasks",
                "subsectionName": null,
                "referencesIds": [
                    "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
                    "141a5033d9994242b18bb3b217e79582f1ee9306"
                ]
            },
            {
                "sectionName": "VLP models for improving performance on language tasks",
                "subsectionName": null,
                "referencesIds": [
                    "dedcdc1fb3a6def9772dce674d89150923dd75b9",
                    "e3575ca62373639152adf84ca0b33c52a4f892ef",
                    "cb596bffc5c5042c254058b62317a57fa156fea4",
                    "5e00596fa946670d894b1bdaeff5a98e3867ef13"
                ]
            },
            {
                "sectionName": "Analyzing VLP models",
                "subsectionName": null,
                "referencesIds": [
                    "81002fbb777f860f9aac2bbc24467a62345af279",
                    "de2f6a8921f9bf984fdc2b46964a03d3b83e2fcd",
                    "5c09c7b9d749e7a1f90573b0cfd53606f1038d73",
                    "dec2f6d3215de9aa2d87d358b7933fb21eeb3bc0"
                ]
            },
            {
                "sectionName": "Shortcomings of vision-language models",
                "subsectionName": null,
                "referencesIds": [
                    "8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5",
                    "4921243268c81d0d6db99053a9d004852225a622",
                    "2f5f81bc516a6d085d39479378af1fc27104f91e",
                    "0af07469cd031061ac6c5783d5bd0399ac48812e",
                    "f4b654433c53b26b71d5f39dd8ecdc4d8a2acb1f",
                    "9bbc952adb3e3c6091d45d800e806d3373a52bac",
                    "22a0e7c06f1e64729211177f80d1b9dcaab3c706",
                    "135bafc83e9a73c88e759f98a28edfdb5c02f81d"
                ]
            },
            {
                "sectionName": "Methods and evaluation benchmarks that go beyond the traditional i.i.d. setting",
                "subsectionName": null,
                "referencesIds": [
                    "90873a97aa9a43775e5aeea01b03aea54b28bfbd",
                    "ab9cc2c6a8d35d7a145cf608ff9dd7af87213253",
                    "45ec1446f42c0a7c7fe74319118335c76e0f7b19",
                    "ad322ec0617a9bdf1dabd2a51e626a9c474ed9e3",
                    "753b7a701adc1b6072378bd048cfa8567885d9c7",
                    "55daceb1d28be049b457ec53bc3ffa582c021317",
                    "72eb95e6424d0e667fe8df8d6ebd439b831406f6",
                    "29121a31e4d684839cfd0bb358f33ea1266cece5",
                    "74246b7d26078538c01b02586c56b01486a029a6",
                    "10e597be22f83757b2cbc9426d0ad7f397336764"
                ]
            }
        ]
    }
}

