Reading List

Reading the following papers is nice to have but not required for attendance.

• Language Models are Few-Shot Learners (Brown et al., 2020)
• It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners (Schick and Schütze, 2021b)
• Finetuned Language Models are Zero-Shot Learners (Wei et al., 2021)
• FLEX: Unifying Evaluation for Few-Shot NLP (Bragg et al., 2021)