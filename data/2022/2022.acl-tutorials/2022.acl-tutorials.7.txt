Reading List  

• Popular vision-language tasks, datasets and benchmarks
(Plummer et al., 2015; 
Kazemzadeh et al., 2014; 
Mao et al., 2015; 
Chen et al., 2015; 
Antol et al., 2015; 
Krishna et al., 2016; 
Hudson and Manning, 2019). 

• Task specific modelling approaches before the pretraining era
(Antol et al., 2015; 
Yang et al., 2015; 
Lu et al., 2016; 
Anderson et al., 2017; 
Fukui et al., 2016; 
Andreas et al., 2015). 

• ∗Pretraining models in NLP
(Devlin et al., 2018; 
Brown et al., 2020). 

• VLP models with task-specific heads
(Lu et al., 2019; 
Tan and Bansal, 2019; 
Chen et al., 2020; 
Li et al., 2020b; 
Zhang et al., 2021). 

• VLP models without task-specific heads
(Cho et al., 2021; 
Wang et al., 2021). 

• VLP models for improving performance on vision tasks
(Radford et al., 2021; 
Jia et al., 2021). 

• VLP models for improving performance on language tasks
(Tan and Bansal, 2020; 
Huang et al., 2020; 
Cho et al., 2021; 
Wang et al., 2021). 

• Analyzing VLP models
(Hendricks et al., 2021; 
Frank et al., 2021; 
Hendricks and Ne- matzadeh, 2021; 
Bugliarello et al., 2020). 

• Shortcomings of vision-language models
(Agrawal et al., 2016; 
Rohrbach et al., 2018; 
Gan et al., 2020; 
Ross et al., 2020; 
van Mil- tenburg, 2016; 
Misra et al., 2015; 
Raji et al., 2020; 
Zhao et al., 2017a). 

• Methods and evaluation benchmarks that go beyond the traditional i.i.d. setting
(Agrawal et al., 2017; 
Cadène et al., 2019; 
Ramakrish- nan et al., 2018; 
Teney et al., 2020c; 
Arjovsky et al., 2019; 
Teney et al., 2020b; 
Gokhale et al., 2020; 
Teney et al., 2020a; 
Ilse et al., 2020; 
Agarwal et al., 2019).  

∗ It would be great if the audience could read these papers before the tutorial, but it is okay even if they do not get a chance, as we will briefly cover these topics in the tutorial.